{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7126,"status":"ok","timestamp":1708912690850,"user":{"displayName":"Karan 29","userId":"02986125489029553801"},"user_tz":300},"id":"ASrzW5lZPT-J","outputId":"5f1af7c8-efbc-45a8-dde7-653bd8f917aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.0)\n","Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.10)\n","Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2024.2.2)\n"]}],"source":["pip install arxiv"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708912690850,"user":{"displayName":"Karan 29","userId":"02986125489029553801"},"user_tz":300},"id":"Xilhy3ubOxqK","outputId":"3e406e98-8baf-4b9d-fe5a-9ebcad4039db"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Bidirectional, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import pandas as pd\n","import arxiv  # To fetch papers from arXiv\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","import re\n","import string\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708912642203,"user":{"displayName":"Karan 29","userId":"02986125489029553801"},"user_tz":300},"id":"MjNfZpabPVgm"},"outputs":[],"source":["import pandas as pd\n","import time\n","import arxiv\n","\n","def arxiv_papers(max_results=6000, wait_time=5):\n","    keywords = [\"Data Science\", \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Natural Language Processing\", \"Artificial Intelligence\", \"Computational Linguistics\", \"Generative Models\", \"Predictive Analytics\", \"Big Data\", \"Statistical Learning\", \"Supervised Learning\", \"Unsupervised Learning\", \"Neural Networks\"]\n","\n","    categorized_papers = []\n","\n","    for keyword in keywords:\n","        print(f\"Fetching papers related to: {keyword}\")\n","        query = f'(\"{keyword}\") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.NE OR cat:stat.ML)'\n","        search = arxiv.Search(\n","            query=query,\n","            max_results=int(max_results/len(keywords)),  # Distribute max_results evenly among keywords\n","            sort_by=arxiv.SortCriterion.SubmittedDate\n","        )\n","\n","        fetched_results = 0  # Track the number of fetched results for each keyword\n","        try:\n","            # Fetch papers batch by batch\n","            for result in search.results():\n","                if fetched_results >= max_results:\n","                    break  # Break if max_results reached\n","                categorized_papers.append({\n","                    'title': result.title,\n","                    'summary': result.summary,\n","                    'categories': result.categories,\n","                    'keyword': keyword  # Categorize each paper by the current keyword\n","                })\n","                fetched_results += 1\n","                if fetched_results % int(max_results/len(keywords)) == 0:\n","                    time.sleep(wait_time)  # Wait before making the next API call\n","        except Exception as e:\n","            print(f\"Error fetching papers for keyword '{keyword}': {e}\")\n","\n","    # Convert the list of categorized papers into a DataFrame\n","    df_papers = pd.DataFrame(categorized_papers)\n","    return df_papers\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"ee-EZRAhPj9G","executionInfo":{"status":"error","timestamp":1708912647084,"user_tz":300,"elapsed":4884,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"2f3126e2-2e56-4660-faa4-c6ad36b96636"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fetching papers related to: Data Science\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-794ea5b31309>:22: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n","  for result in search.results():\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-c13b9b68a899>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marxiv_papers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-794ea5b31309>\u001b[0m in \u001b[0;36marxiv_papers\u001b[0;34m(max_results, wait_time)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Fetch papers batch by batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfetched_results\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Break if max_results reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arxiv/__init__.py\u001b[0m in \u001b[0;36m_results\u001b[0;34m(self, search, offset)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mpage_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arxiv/__init__.py\u001b[0m in \u001b[0;36m_parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \"\"\"\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             return self.__try_parse_feed(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_try_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arxiv/__init__.py\u001b[0m in \u001b[0;36m__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user-agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"arxiv.py/2.1.0\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_request_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["papers = arxiv_papers(max_results=6000)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"H2RsXw3MPnD5","executionInfo":{"status":"error","timestamp":1708912649503,"user_tz":300,"elapsed":153,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"9ad529fc-0628-406e-fb06-404ede3019b0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'papers' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-794af17d8403>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arxiv.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'papers' is not defined"]}],"source":["papers.to_csv('arxiv.csv')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"p7XQpBCWZbS6","executionInfo":{"status":"ok","timestamp":1708912691785,"user_tz":300,"elapsed":436,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"cb6ba591-34a9-4bce-dc10-1daf11f07a14"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0                                              title  \\\n","0           0  Control-Theoretic Techniques for Online Adapta...   \n","1           1  On the Robustness of Cross-Concentrated Sampli...   \n","2           2  GuardML: Efficient Privacy-Preserving Machine ...   \n","3           3  Navigating Dataset Documentations in AI: A Lar...   \n","4           4  Cheap Learning: Maximising Performance of Lang...   \n","\n","                                             summary  \\\n","0  Deep neural networks (DNNs), trained with grad...   \n","1  Matrix completion is one of the crucial tools ...   \n","2  Machine Learning (ML) has emerged as one of da...   \n","3  Advances in machine learning are closely tied ...   \n","4  The field of machine learning has recently mad...   \n","\n","                                          categories       keyword  \n","0    ['cs.LG', 'cs.NE', 'cs.RO', 'cs.SY', 'eess.SY']  Data Science  \n","1  ['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math...  Data Science  \n","2                                 ['cs.LG', 'cs.CR']  Data Science  \n","3                                 ['cs.LG', 'cs.AI']  Data Science  \n","4                            ['cs.CL', 'I.2.7; J.4']  Data Science  "],"text/html":["\n","  <div id=\"df-bc779fc3-c734-478c-a59f-719c66ec99b5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>title</th>\n","      <th>summary</th>\n","      <th>categories</th>\n","      <th>keyword</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Control-Theoretic Techniques for Online Adapta...</td>\n","      <td>Deep neural networks (DNNs), trained with grad...</td>\n","      <td>['cs.LG', 'cs.NE', 'cs.RO', 'cs.SY', 'eess.SY']</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>On the Robustness of Cross-Concentrated Sampli...</td>\n","      <td>Matrix completion is one of the crucial tools ...</td>\n","      <td>['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'math...</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>GuardML: Efficient Privacy-Preserving Machine ...</td>\n","      <td>Machine Learning (ML) has emerged as one of da...</td>\n","      <td>['cs.LG', 'cs.CR']</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Navigating Dataset Documentations in AI: A Lar...</td>\n","      <td>Advances in machine learning are closely tied ...</td>\n","      <td>['cs.LG', 'cs.AI']</td>\n","      <td>Data Science</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Cheap Learning: Maximising Performance of Lang...</td>\n","      <td>The field of machine learning has recently mad...</td>\n","      <td>['cs.CL', 'I.2.7; J.4']</td>\n","      <td>Data Science</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc779fc3-c734-478c-a59f-719c66ec99b5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bc779fc3-c734-478c-a59f-719c66ec99b5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bc779fc3-c734-478c-a59f-719c66ec99b5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2205b8c4-3eca-422c-afd4-fd8ad4664034\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2205b8c4-3eca-422c-afd4-fd8ad4664034')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2205b8c4-3eca-422c-afd4-fd8ad4664034 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5667,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1636,\n        \"min\": 0,\n        \"max\": 5666,\n        \"num_unique_values\": 5667,\n        \"samples\": [\n          4716,\n          4328,\n          4954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4953,\n        \"samples\": [\n          \"Enhancing Pre-trained Models with Text Structure Knowledge for Question Generation\",\n          \"Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks\",\n          \"An Unconstrained Symmetric Nonnegative Latent Factor Analysis for Large-scale Undirected Weighted Networks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4952,\n        \"samples\": [\n          \"Principal component analysis (PCA) is a workhorse of modern data science.\\nPractitioners typically perform PCA assuming the data conforms to Euclidean\\ngeometry. However, for specific data types, such as hierarchical data, other\\ngeometrical spaces may be more appropriate. We study PCA in space forms; that\\nis, those with constant positive (spherical) and negative (hyperbolic)\\ncurvatures, in addition to zero-curvature (Euclidean) spaces. At any point on a\\nRiemannian manifold, one can define a Riemannian affine subspace based on a set\\nof tangent vectors and use invertible maps to project tangent vectors to the\\nmanifold and vice versa. Finding a low-dimensional Riemannian affine subspace\\nfor a set of points in a space form amounts to dimensionality reduction\\nbecause, as we show, any such affine subspace is isometric to a space form of\\nthe same dimension and curvature. To find principal components, we seek a\\n(Riemannian) affine subspace that best represents a set of manifold-valued data\\npoints with the minimum average cost of projecting data points onto the affine\\nsubspace. We propose specific cost functions that bring about two major\\nbenefits: (1) the affine subspace can be estimated by solving an eigenequation\\n-- similar to that of Euclidean PCA, and (2) optimal affine subspaces of\\ndifferent dimensions form a nested set. These properties provide advances over\\nexisting methods which are mostly iterative algorithms with slow convergence\\nand weaker theoretical guarantees. Specifically for hyperbolic PCA, the\\nassociated eigenequation operates in the Lorentzian space, endowed with an\\nindefinite inner product; we thus establish a connection between Lorentzian and\\nEuclidean eigenequations. We evaluate the proposed space form PCA on data sets\\nsimulated in spherical and hyperbolic spaces and show that it outperforms\\nalternative methods in convergence speed or accuracy, often both.\",\n          \"Reinforcement learning (RL) offers a capable and intuitive structure for the\\nfundamental sequential decision-making problem. Despite impressive\\nbreakthroughs, it can still be difficult to employ RL in practice in many\\nsimple applications. In this paper, we try to address this issue by introducing\\na method for designing the components of the RL environment for a given,\\nuser-intended application. We provide an initial formalization for the problem\\nof RL component design, that concentrates on designing a good representation\\nfor observation and action space. We propose a method named DeLF: Designing\\nLearning Environments with Foundation Models, that employs large language\\nmodels to design and codify the user's intended learning scenario. By testing\\nour method on four different learning environments, we demonstrate that DeLF\\ncan obtain executable environment codes for the corresponding RL problems.\",\n          \"Artificial intelligence and machine learning have significantly bolstered the\\ntechnological world. This paper explores the potential of transfer learning in\\nnatural language processing focusing mainly on sentiment analysis. The models\\ntrained on the big data can also be used where data are scarce. The claim is\\nthat, compared to training models from scratch, transfer learning, using\\npre-trained BERT models, can increase sentiment classification accuracy. The\\nstudy adopts a sophisticated experimental design that uses the IMDb dataset of\\nsentimentally labelled movie reviews. Pre-processing includes tokenization and\\nencoding of text data, making it suitable for NLP models. The dataset is used\\non a BERT based model, measuring its performance using accuracy. The result\\ncomes out to be 100 per cent accurate. Although the complete accuracy could\\nappear impressive, it might be the result of overfitting or a lack of\\ngeneralization. Further analysis is required to ensure the model's ability to\\nhandle diverse and unseen data. The findings underscore the effectiveness of\\ntransfer learning in NLP, showcasing its potential to excel in sentiment\\nanalysis tasks. However, the research calls for a cautious interpretation of\\nperfect accuracy and emphasizes the need for additional measures to validate\\nthe model's generalization.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1486,\n        \"samples\": [\n          \"['cs.CL', 'cs.LG', 'q-fin.CP']\",\n          \"['cs.CL', 'cs.AI', 'cs.PF']\",\n          \"['cs.DC', 'cs.AI']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Big Data\",\n          \"Supervised Learning\",\n          \"Data Science\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":12}],"source":["df = pd.read_csv('arxiv.csv')\n","df.head()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"aVN_Gr0ugIE7","executionInfo":{"status":"ok","timestamp":1708912698552,"user_tz":300,"elapsed":3972,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"e4c2f6f4-193b-4698-973e-ce82ca91ce0c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     keyword  \\\n","0    Artificial Intelligence   \n","1                   Big Data   \n","2  Computational Linguistics   \n","3               Data Science   \n","4              Deep Learning   \n","\n","                                             summary  \n","0  Planning has been part of the core pursuit for...  \n","1  Emerging Distributed AI systems are revolution...  \n","2  This research explores strategies for steering...  \n","3  Deep neural networks (DNNs), trained with grad...  \n","4  Current deep learning models are not designed ...  "],"text/html":["\n","  <div id=\"df-877dcb92-ada2-4f9f-9ec9-6f105c4c35d6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>keyword</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Artificial Intelligence</td>\n","      <td>Planning has been part of the core pursuit for...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Big Data</td>\n","      <td>Emerging Distributed AI systems are revolution...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Computational Linguistics</td>\n","      <td>This research explores strategies for steering...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Data Science</td>\n","      <td>Deep neural networks (DNNs), trained with grad...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Deep Learning</td>\n","      <td>Current deep learning models are not designed ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-877dcb92-ada2-4f9f-9ec9-6f105c4c35d6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-877dcb92-ada2-4f9f-9ec9-6f105c4c35d6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-877dcb92-ada2-4f9f-9ec9-6f105c4c35d6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-af57e0f3-2a69-4a00-908e-a2a4388afec1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af57e0f3-2a69-4a00-908e-a2a4388afec1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-af57e0f3-2a69-4a00-908e-a2a4388afec1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dfg","summary":"{\n  \"name\": \"dfg\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Predictive Analytics\",\n          \"Statistical Learning\",\n          \"Artificial Intelligence\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"In this study, we investigated the application of bio-inspired optimization\\nalgorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale\\nOptimization Algorithm, for feature selection in chronic disease prediction.\\nThe primary goal was to enhance the predictive accuracy of models streamline\\ndata dimensionality, and make predictions more interpretable and actionable.\\n  The research encompassed a comparative analysis of the three bio-inspired\\nfeature selection approaches across diverse chronic diseases, including\\ndiabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such\\nas accuracy, precision, recall, and f1 score are used to assess the\\neffectiveness of the algorithms in reducing the number of features needed for\\naccurate classification.\\n  The results in general demonstrate that the bio-inspired optimization\\nalgorithms are effective in reducing the number of features required for\\naccurate classification. However, there have been variations in the performance\\nof the algorithms on different datasets.\\n  The study highlights the importance of data pre-processing and cleaning in\\nensuring the reliability and effectiveness of the analysis.\\n  This study contributes to the advancement of predictive analytics in the\\nrealm of chronic diseases. The potential impact of this work extends to early\\nintervention, precision medicine, and improved patient outcomes, providing new\\navenues for the delivery of healthcare services tailored to individual needs.\\nThe findings underscore the potential benefits of using bio-inspired\\noptimization algorithms for feature selection in chronic disease prediction,\\noffering valuable insights for improving healthcare outcomes. This paper presents a comprehensive evaluation of three distinct\\ncomputational algorithms applied to the decision-making process of real estate\\npurchases. Specifically, we analyze the efficacy of Linear Regression from\\nScikit-learn library, Gaussian Elimination with partial pivoting, and LU\\nDecomposition in predicting the advisability of buying a house in the State of\\nConnecticut based on a set of financial and market-related parameters. The\\nalgorithms' performances were compared using a dataset encompassing\\ntown-specific details, yearly data, interest rates, and median sale ratios. Our\\nresults demonstrate significant differences in predictive accuracy, with Linear\\nRegression and LU Decomposition providing the most reliable recommendations and\\nGaussian Elimination showing limitations in stability and performance. The\\nstudy's findings emphasize the importance of algorithm selection in predictive\\nanalytic and offer insights into the practical applications of computational\\nmethods in real estate investment strategies. By evaluating model efficacy\\nthrough metrics such as R-squared scores and Mean Squared Error, we provide a\\nnuanced understanding of each method's strengths and weaknesses, contributing\\nvaluable knowledge to the fields of real estate analysis and predictive\\nmodeling. The advent of Blockchain technology (BT) revolutionised the way remittance\\ntransactions are recorded. Banks and remittance organisations have shown a\\ngrowing interest in exploring blockchain's potential advantages over\\ntraditional practices. This paper presents a data-driven predictive decision\\nsupport approach as an innovative artefact designed for the blockchain-oriented\\nremittance industry. Employing a theory-generating Design Science Research\\n(DSR) approach, we have uncovered the emergence of predictive capabilities\\ndriven by transactional big data. The artefact integrates predictive analytics\\nand Machine Learning (ML) to enable real-time remittance monitoring, empowering\\nmanagement decision-makers to address challenges in the uncertain digitised\\nlandscape of blockchain-oriented remittance companies. Bridging the gap between\\ntheory and practice, this research not only enhances the security of the\\nremittance ecosystem but also lays the foundation for future predictive\\ndecision support solutions, extending the potential of predictive analytics to\\nother domains. Additionally, the generated theory from the artifact's\\nimplementation enriches the DSR approach and fosters grounded and stakeholder\\ntheory development in the information systems domain. Sepsis requires urgent diagnosis, but research is predominantly focused on\\nWestern datasets. In this study, we perform a comparative analysis of two\\nensemble learning methods, LightGBM and XGBoost, using the public eICU-CRD\\ndataset and a private South Korean St. Mary's Hospital's dataset. Our analysis\\nreveals the effectiveness of these methods in addressing healthcare data\\nimbalance and enhancing sepsis detection. Specifically, LightGBM shows a slight\\nedge in computational efficiency and scalability. The study paves the way for\\nthe broader application of machine learning in critical care, thereby expanding\\nthe reach of predictive analytics in healthcare globally. Pediatric brain and spinal cancers remain the leading cause of cancer-related\\ndeath in children. Advancements in clinical decision-support in pediatric\\nneuro-oncology utilizing the wealth of radiology imaging data collected through\\nstandard care, however, has significantly lagged other domains. Such data is\\nripe for use with predictive analytics such as artificial intelligence (AI)\\nmethods, which require large datasets. To address this unmet need, we provide a\\nmulti-institutional, large-scale pediatric dataset of 23,101 multi-parametric\\nMRI exams acquired through routine care for 1,526 brain tumor patients, as part\\nof the Children's Brain Tumor Network. This includes longitudinal MRIs across\\nvarious cancer diagnoses, with associated patient-level clinical information,\\ndigital pathology slides, as well as tissue genotype and omics data. To\\nfacilitate downstream analysis, treatment-na\\\\\\\"ive images for 370 subjects were\\nprocessed and released through the NCI Childhood Cancer Data Initiative via the\\nCancer Data Service. Through ongoing efforts to continuously build these\\nimaging repositories, our aim is to accelerate discovery and translational AI\\nmodels with real-world data, to ultimately empower precision medicine for\\nchildren. The usage of Lithium-ion (Li-ion) batteries has gained widespread popularity\\nacross various industries, from powering portable electronic devices to\\npropelling electric vehicles and supporting energy storage systems. A central\\nchallenge in Li-ion battery reliability lies in accurately predicting their\\nRemaining Useful Life (RUL), which is a critical measure for proactive\\nmaintenance and predictive analytics. This study presents a novel approach that\\nharnesses the power of multiple denoising modules, each trained to address\\nspecific types of noise commonly encountered in battery data. Specifically, a\\ndenoising auto-encoder and a wavelet denoiser are used to generate\\nencoded/decomposed representations, which are subsequently processed through\\ndedicated self-attention transformer encoders. After extensive experimentation\\non NASA and CALCE data, a broad spectrum of health indicator values are\\nestimated under a set of diverse noise patterns. The reported error metrics on\\nthese data are on par with or better than the state-of-the-art reported in\\nrecent literature. Predicting startup success presents a formidable challenge due to the\\ninherently volatile landscape of the entrepreneurial ecosystem. The advent of\\nextensive databases like Crunchbase jointly with available open data enables\\nthe application of machine learning and artificial intelligence for more\\naccurate predictive analytics. This paper focuses on startups at their Series B\\nand Series C investment stages, aiming to predict key success milestones such\\nas achieving an Initial Public Offering (IPO), attaining unicorn status, or\\nexecuting a successful Merger and Acquisition (M\\\\&A). We introduce novel deep\\nlearning model for predicting startup success, integrating a variety of factors\\nsuch as funding metrics, founder features, industry category. A distinctive\\nfeature of our research is the use of a comprehensive backtesting algorithm\\ndesigned to simulate the venture capital investment process. This simulation\\nallows for a robust evaluation of our model's performance against historical\\ndata, providing actionable insights into its practical utility in real-world\\ninvestment contexts. Evaluating our model on Crunchbase's, we achieved a 14\\ntimes capital growth and successfully identified on B round high-potential\\nstartups including Revolut, DigitalOcean, Klarna, Github and others. Our\\nempirical findings illuminate the importance of incorporating diverse feature\\nsets in enhancing the model's predictive accuracy. In summary, our work\\ndemonstrates the considerable promise of deep learning models and alternative\\nunstructured data in predicting startup success and sets the stage for future\\nadvancements in this research area. This article introduces an advanced analytical approach for predicting\\nbackorders in inventory management. Backorder refers to an order that cannot be\\nimmediately fulfilled due to stock depletion. Multiple classification\\ntechniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational\\nAutoencoder - Generative Adversarial Networks, and Multi-layer Perceptron\\nclassifiers, are assessed in this work using performance evaluation metrics\\nsuch as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function\\nand misclassification costs, considering the financial implications and costs\\nassociated with inventory management and backorder handling. The study suggests\\nthat a combination of modeling approaches, including ensemble techniques and\\nVAE, can effectively address imbalanced datasets in inventory management,\\nemphasizing interpretability and reducing false positives and false negatives.\\nThis research contributes to the advancement of predictive analytics and offers\\nvaluable insights for future investigations in backorder forecasting and\\ninventory control optimization for decision-making. Prediction-oriented machine learning is becoming increasingly valuable to\\norganizations, as it may drive applications in crucial business areas. However,\\ndecision-makers from companies across various industries are still largely\\nreluctant to employ applications based on modern machine learning algorithms.\\nWe ascribe this issue to the widely held view on advanced machine learning\\nalgorithms as \\\"black boxes\\\" whose complexity does not allow for uncovering the\\nfactors that drive the output of a corresponding system. To contribute to\\novercome this adoption barrier, we argue that research in information systems\\nshould devote more attention to the design of prototypical prediction-oriented\\nmachine learning applications (i.e., artifacts) whose predictions can be\\nexplained to human decision-makers. However, despite the recent emergence of a\\nvariety of tools that facilitate the development of such artifacts, there has\\nso far been little research on their development. We attribute this research\\ngap to the lack of methodological guidance to support the creation of these\\nartifacts. For this reason, we develop a methodology which unifies\\nmethodological knowledge from design science research and predictive analytics\\nwith state-of-the-art approaches to explainable artificial intelligence.\\nMoreover, we showcase the methodology using the example of price prediction in\\nthe sharing economy (i.e., on Airbnb). Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\\nand artificial intelligence (AI), outperforming traditional ML methods,\\nespecially in handling unstructured and large datasets. Its impact spans across\\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\\ncybersecurity, predictive analytics, and more. However, the complexity and\\ndynamic nature of real-world problems present challenges in designing effective\\ndeep learning models. Consequently, several deep learning models have been\\ndeveloped to address different problems and applications. In this article, we\\nconduct a comprehensive survey of various deep learning models, including\\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\\nLearning. We examine the structure, applications, benefits, and limitations of\\neach model. Furthermore, we perform an analysis using three publicly available\\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU. Federated weather forecasting is a promising collaborative learning framework\\nfor analyzing meteorological data across participants from different countries\\nand regions, thus embodying a global-scale real-time weather data predictive\\nanalytics platform to tackle climate change. This paper is to model the\\nmeteorological data in a federated setting where many distributed low-resourced\\nsensors are deployed in different locations. Specifically, we model the\\nspatial-temporal weather data into a federated prompt learning framework that\\nleverages lightweight prompts to share meaningful representation and structural\\nknowledge among participants. Prompts-based communication allows the server to\\nestablish the structural topology relationships among participants and further\\nexplore the complex spatial-temporal correlations without transmitting private\\ndata while mitigating communication overhead. Moreover, in addition to a\\nglobally shared large model at the server, our proposed method enables each\\nparticipant to acquire a personalized model that is highly customized to tackle\\nclimate changes in a specific geographic area. We have demonstrated the\\neffectiveness of our method on classical weather forecasting tasks by utilizing\\nthree spatial-temporal multivariate time-series weather data. The cold start problem is a common challenge in various domains, including\\nmedia use cases such as predicting viewership for newly launched shows on\\nOver-The-Top (OTT) platforms. In this study, we propose a generic approach to\\ntackle cold start problems by leveraging metadata and employing multi-model\\nensemble techniques. Our methodology includes feature engineering, model\\nselection, and an ensemble approach based on a weighted average of predictions.\\nThe performance of our proposed method is evaluated using various performance\\nmetrics. Our results indicate that the multi-model ensemble approach\\nsignificantly improves prediction accuracy compared to individual models. In response to the global need for sustainable energy, green technology may\\nhelp fight climate change. Before green infrastructure to be easily integrated\\ninto the world's energy system, it needs upgrading. By improving energy\\ninfrastructure and decision-making, artificial intelligence (AI) may help solve\\nthis challenge. EHVs have grown in popularity because to concerns about global\\nwarming and the need for more ecologically friendly transportation. EHVs may\\nwork better with cutting-edge technologies like AI. Electric vehicles (EVs)\\nreduce greenhouse gas emissions and promote sustainable mobility. Electric\\nautomobiles (EVs) are growing in popularity due to their benefits for climate\\nchange mitigation and sustainable mobility. Unfortunately, EV production\\nconsumes a lot of energy and materials, which may harm nature. EV production is\\nbeing improved using green technologies like artificial intelligence and\\npredictive analysis. Electric and hybrid vehicles (EHVs) may help meet the need\\nfor ecologically friendly transportation. However, the Battery Management\\nSystem (BMS) controls EHV performance and longevity. AI may improve EHV energy\\nefficiency, emissions reduction, and sustainability. Remote hijacking, security\\nbreaches, and unauthorized access are EHV cybersecurity vulnerabilities\\naddressed in the article. AI research and development may help make\\ntransportation more sustainable, as may optimizing EHVs and charging\\ninfrastructure. In this paper, we study the predict-then-optimize problem where the output of\\na machine learning prediction task is used as the input of some downstream\\noptimization problem, say, the objective coefficient vector of a linear\\nprogram. The problem is also known as predictive analytics or contextual linear\\nprogramming. The existing approaches largely suffer from either (i)\\noptimization intractability (a non-convex objective function)/statistical\\ninefficiency (a suboptimal generalization bound) or (ii) requiring strong\\ncondition(s) such as no constraint or loss calibration. We develop a new\\napproach to the problem called \\\\textit{maximum optimality margin} which designs\\nthe machine learning loss function by the optimality condition of the\\ndownstream optimization. The max-margin formulation enjoys both computational\\nefficiency and good theoretical properties for the learning procedure. More\\nimportantly, our new approach only needs the observations of the optimal\\nsolution in the training data rather than the objective function, which makes\\nit a new and natural approach to the inverse linear programming problem under\\nboth contextual and context-free settings; we also analyze the proposed method\\nunder both offline and online settings, and demonstrate its performance using\\nnumerical experiments. Complex IoT ecosystems often require the usage of Digital Twins (DTs) of\\ntheir physical assets in order to perform predictive analytics and simulate\\nwhat-if scenarios. DTs are able to replicate IoT devices and adapt over time to\\ntheir behavioral changes. However, DTs in IoT are typically tailored to a\\nspecific use case, without the possibility to seamlessly adapt to different\\nscenarios. Further, the fragmentation of IoT poses additional challenges on how\\nto deploy DTs in heterogeneous scenarios characterized by the usage of multiple\\ndata formats and IoT network protocols. In this paper, we propose the\\nRelativistic Digital Twin (RDT) framework, through which we automatically\\ngenerate general-purpose DTs of IoT entities and tune their behavioral models\\nover time by constantly observing their real counterparts. The framework relies\\non the object representation via the Web of Things (WoT), to offer a\\nstandardized interface to each of the IoT devices as well as to their DTs. To\\nthis purpose, we extended the W3C WoT standard in order to encompass the\\nconcept of behavioral model and define it in the Thing Description (TD) through\\na new vocabulary. Finally, we evaluated the RDT framework over two disjoint use\\ncases to assess its correctness and learning performance, i.e., the DT of a\\nsimulated smart home scenario with the capability of forecasting the indoor\\ntemperature, and the DT of a real-world drone with the capability of\\nforecasting its trajectory in an outdoor scenario. Experiments show that the\\ngenerated DT can estimate the behavior of its real counterpart after an\\nobservation stage, regardless of the considered scenario. Traditional learning-based approaches to student modeling (e.g., predicting\\ngrades based on measured activities) generalize poorly to\\nunderrepresented/minority student groups due to biases in data availability. In\\nthis paper, we propose a Multi-Layer Personalized Federated Learning (MLPFL)\\nmethodology which optimizes inference accuracy over different layers of student\\ngrouping criteria, such as by course and by demographic subgroups within each\\ncourse. In our approach, personalized models for individual student subgroups\\nare derived from a global model, which is trained in a distributed fashion via\\nmeta-gradient updates that account for subgroup heterogeneity while preserving\\nmodeling commonalities that exist across the full dataset. To evaluate our\\nmethodology, we consider case studies of two popular downstream student\\nmodeling tasks, knowledge tracing and outcome prediction, which leverage\\nmultiple modalities of student behavior (e.g., visits to lecture videos and\\nparticipation on forums) in model training. Experiments on three real-world\\ndatasets from online courses demonstrate that our approach obtains substantial\\nimprovements over existing student modeling baselines in terms of increasing\\nthe average and decreasing the variance of prediction quality across different\\nstudent subgroups. Visual analysis of the resulting students' knowledge state\\nembeddings confirm that our personalization methodology extracts activity\\npatterns which cluster into different student subgroups, consistent with the\\nperformance enhancements we obtain over the baselines. A significant body of recent research in the field of Learning Analytics has\\nfocused on leveraging machine learning approaches for predicting at-risk\\nstudents in order to initiate timely interventions and thereby elevate\\nretention and completion rates. The overarching feature of the majority of\\nthese research studies has been on the science of prediction only. The\\ncomponent of predictive analytics concerned with interpreting the internals of\\nthe models and explaining their predictions for individual cases to\\nstakeholders has largely been neglected. Additionally, works that attempt to\\nemploy data-driven prescriptive analytics to automatically generate\\nevidence-based remedial advice for at-risk learners are in their infancy.\\n  eXplainable AI is a field that has recently emerged providing cutting-edge\\ntools which support transparent predictive analytics and techniques for\\ngenerating tailored advice for at-risk students. This study proposes a novel\\nframework that unifies both transparent machine learning as well as techniques\\nfor enabling prescriptive analytics, while integrating the latest advances in\\nlarge language models. This work practically demonstrates the proposed\\nframework using predictive models for identifying at-risk learners of programme\\nnon-completion. The study then further demonstrates how predictive modelling\\ncan be augmented with prescriptive analytics on two case studies in order to\\ngenerate human-readable prescriptive feedback for those who are at risk using\\nChatGPT. Predictive Process Analytics is becoming an essential aid for organizations,\\nproviding online operational support of their processes. However, process\\nstakeholders need to be provided with an explanation of the reasons why a given\\nprocess execution is predicted to behave in a certain way. Otherwise, they will\\nbe unlikely to trust the predictive monitoring technology and, hence, adopt it.\\nThis paper proposes a predictive analytics framework that is also equipped with\\nexplanation capabilities based on the game theory of Shapley Values. The\\nframework has been implemented in the IBM Process Mining suite and\\ncommercialized for business users. The framework has been tested on real-life\\nevent data to assess the quality of the predictions and the corresponding\\nevaluations. In particular, a user evaluation has been performed in order to\\nunderstand if the explanations provided by the system were intelligible to\\nprocess stakeholders. In the survey we consider the case studies on sales time series forecasting,\\nthe deep learning approach for forecasting non-stationary time series using\\ntime trend correction, dynamic price and supply optimization using Q-learning,\\nBitcoin price modeling, COVID-19 spread impact on stock market, using social\\nnetworks signals in analytics. The use of machine learning and Bayesian\\ninference in predictive analytics has been analyzed. Child welfare agencies across the United States are turning to data-driven\\npredictive technologies (commonly called predictive analytics) which use\\ngovernment administrative data to assist workers' decision-making. While some\\nprior work has explored impacted stakeholders' concerns with current uses of\\ndata-driven predictive risk models (PRMs), less work has asked stakeholders\\nwhether such tools ought to be used in the first place. In this work, we\\nconducted a set of seven design workshops with 35 stakeholders who have been\\nimpacted by the child welfare system or who work in it to understand their\\nbeliefs and concerns around PRMs, and to engage them in imagining new uses of\\ndata and technologies in the child welfare system. We found that participants\\nworried current PRMs perpetuate or exacerbate existing problems in child\\nwelfare. Participants suggested new ways to use data and data-driven tools to\\nbetter support impacted communities and suggested paths to mitigate possible\\nharms of these tools. Participants also suggested low-tech or no-tech\\nalternatives to PRMs to address problems in child welfare. Our study sheds\\nlight on how researchers and designers can work in solidarity with impacted\\ncommunities, possibly to circumvent or oppose child welfare agencies. One of the service providers in the financial service sector, who provide\\npremium service to the customers, wanted to harness the power of data analytics\\nas data mining can uncover valuable insights for better decision making.\\nTherefore, the author aimed to use predictive analytics to discover crucial\\nfactors that will affect the customers' showing up for their appointment and\\nbooking the service. The first model predicts whether a customer will show up\\nfor the meeting, while the second model indicates whether a customer will book\\na premium service. Both models produce accurate results with more than a 75%\\naccuracy rate, thus providing a more robust model for implementation than gut\\nfeeling and intuition. Finally, this paper offers a framework for resource\\nplanning using the predicted demand. Object-centric processes (a.k.a. Artifact-centric processes) are\\nimplementations of a paradigm where an instance of one process is not executed\\nin isolation but interacts with other instances of the same or other processes.\\nInteractions take place through bridging events where instances exchange data.\\nObject-centric processes are recently gaining popularity in academia and\\nindustry, because their nature is observed in many application scenarios. This\\nposes significant challenges in predictive analytics due to the complex\\nintricacy of the process instances that relate to each other via many-to-many\\nassociations. Existing research is unable to directly exploit the benefits of\\nthese interactions, thus limiting the prediction quality. This paper proposes\\nan approach to incorporate the information about the object interactions into\\nthe predictive models. The approach is assessed on real-life object-centric\\nprocess event data, using different KPIs. The results are compared with a naive\\napproach that overlooks the object interactions, thus illustrating the benefits\\nof their use on the prediction quality. The negative impact of stroke in society has led to concerted efforts to\\nimprove the management and diagnosis of stroke. With an increased synergy\\nbetween technology and medical diagnosis, caregivers create opportunities for\\nbetter patient management by systematically mining and archiving the patients'\\nmedical records. Therefore, it is vital to study the interdependency of these\\nrisk factors in patients' health records and understand their relative\\ncontribution to stroke prediction. This paper systematically analyzes the\\nvarious factors in electronic health records for effective stroke prediction.\\nUsing various statistical techniques and principal component analysis, we\\nidentify the most important factors for stroke prediction. We conclude that\\nage, heart disease, average glucose level, and hypertension are the most\\nimportant factors for detecting stroke in patients. Furthermore, a perceptron\\nneural network using these four attributes provides the highest accuracy rate\\nand lowest miss rate compared to using all available input features and other\\nbenchmarking algorithms. As the dataset is highly imbalanced concerning the\\noccurrence of stroke, we report our results on a balanced dataset created via\\nsub-sampling techniques. Cloud computing has become a major approach to help reproduce computational\\nexperiments. Yet there are still two main difficulties in reproducing batch\\nbased big data analytics (including descriptive and predictive analytics) in\\nthe cloud. The first is how to automate end-to-end scalable execution of\\nanalytics including distributed environment provisioning, analytics pipeline\\ndescription, parallel execution, and resource termination. The second is that\\nan application developed for one cloud is difficult to be reproduced in another\\ncloud, a.k.a. vendor lock-in problem. To tackle these problems, we leverage\\nserverless computing and containerization techniques for automated scalable\\nexecution and reproducibility, and utilize the adapter design pattern to enable\\napplication portability and reproducibility across different clouds. We propose\\nand develop an open-source toolkit that supports 1) fully automated end-to-end\\nexecution and reproduction via a single command, 2) automated data and\\nconfiguration storage for each execution, 3) flexible client modes based on\\nuser preferences, 4) execution history query, and 5) simple reproduction of\\nexisting executions in the same environment or a different environment. We did\\nextensive experiments on both AWS and Azure using four big data analytics\\napplications that run on virtual CPU/GPU clusters. The experiments show our\\ntoolkit can achieve good execution performance, scalability, and efficient\\nreproducibility for cloud-based big data analytics. Vertical Federated learning (VFL) is a promising paradigm for predictive\\nanalytics, empowering an organization (i.e., task party) to enhance its\\npredictive models through collaborations with multiple data suppliers (i.e.,\\ndata parties) in a decentralized and privacy-preserving way. Despite the\\nfast-growing interest in VFL, the lack of effective and secure tools for\\nassessing the value of data owned by data parties hinders the application of\\nVFL in business contexts. In response, we propose FedValue, a\\nprivacy-preserving, task-specific but model-free data valuation method for VFL,\\nwhich consists of a data valuation metric and a federated computation method.\\nSpecifically, we first introduce a novel data valuation metric, namely\\nMShapley-CMI. The metric evaluates a data party's contribution to a predictive\\nanalytics task without the need of executing a machine learning model, making\\nit well-suited for real-world applications of VFL. Next, we develop an\\ninnovative federated computation method that calculates the MShapley-CMI value\\nfor each data party in a privacy-preserving manner. Extensive experiments\\nconducted on six public datasets validate the efficacy of FedValue for data\\nvaluation in the context of VFL. In addition, we illustrate the practical\\nutility of FedValue with a case study involving federated movie\\nrecommendations. Predictive analytics aims to build machine learning models to predict\\nbehavior patterns and use predictions to guide decision-making. Predictive\\nanalytics is human involved, thus the machine learning model is preferred to be\\ninterpretable. In literature, Generalized Additive Model (GAM) is a standard\\nfor interpretability. However, due to the one-to-many and many-to-one phenomena\\nwhich appear commonly in real-world scenarios, existing GAMs have limitations\\nto serve predictive analytics in terms of both accuracy and training\\nefficiency. In this paper, we propose FXAM (Fast and eXplainable Additive\\nModel), a unified and fast interpretable model for predictive analytics. FXAM\\nextends GAM's modeling capability with a unified additive model for numerical,\\ncategorical, and temporal features. FXAM conducts a novel training procedure\\ncalled Three-Stage Iteration (TSI). TSI corresponds to learning over numerical,\\ncategorical, and temporal features respectively. Each stage learns a local\\noptimum by fixing the parameters of other stages. We design joint learning over\\ncategorical features and partial learning over temporal features to achieve\\nhigh accuracy and training efficiency. We prove that TSI is guaranteed to\\nconverge to the global optimum. We further propose a set of optimization\\ntechniques to speed up FXAM's training algorithm to meet the needs of\\ninteractive analysis. Thorough evaluations conducted on diverse data sets\\nverify that FXAM significantly outperforms existing GAMs in terms of training\\nspeed, and modeling categorical and temporal features. In terms of\\ninterpretability, we compare FXAM with the typical post-hoc approach\\nXGBoost+SHAP on two real-world scenarios, which shows the superiority of FXAM's\\ninherent interpretability for predictive analytics. Numerous COVID-19 clinical decision support systems have been developed.\\nHowever many of these systems do not have the merit for validity due to\\nmethodological shortcomings including algorithmic bias. Methods Logistic\\nregression models were created to predict COVID-19 mortality, ventilator status\\nand inpatient status using a real-world dataset consisting of four hospitals in\\nNew York City and analyzed for biases against race, gender and age. Simple\\nthresholding adjustments were applied in the training process to establish more\\nequitable models. Results Compared to the naively trained models, the\\ncalibrated models showed a 57% decrease in the number of biased trials, while\\npredictive performance, measured by area under the receiver/operating curve\\n(AUC), remained unchanged. After calibration, the average sensitivity of the\\npredictive models increased from 0.527 to 0.955. Conclusion We demonstrate that\\nnaively training and deploying machine learning models on real world data for\\npredictive analytics of COVID-19 has a high risk of bias. Simple implemented\\nadjustments or calibrations during model training can lead to substantial and\\nsustained gains in fairness on subsequent deployment. Colleges and universities use predictive analytics in a variety of ways to\\nincrease student success rates. Despite the potential for predictive analytics,\\ntwo major barriers exist to their adoption in higher education: (a) the lack of\\ndemocratization in deployment, and (b) the potential to exacerbate\\ninequalities. Education researchers and policymakers encounter numerous\\nchallenges in deploying predictive modeling in practice. These challenges\\npresent in different steps of modeling including data preparation, model\\ndevelopment, and evaluation. Nevertheless, each of these steps can introduce\\nadditional bias to the system if not appropriately performed. Most large-scale\\nand nationally representative education data sets suffer from a significant\\nnumber of incomplete responses from the research participants. While many\\neducation-related studies addressed the challenges of missing data, little is\\nknown about the impact of handling missing values on the fairness of predictive\\noutcomes in practice. In this paper, we set out to first assess the disparities\\nin predictive modeling outcomes for college-student success, then investigate\\nthe impact of imputation techniques on the model performance and fairness using\\na commonly used set of metrics. We conduct a prospective evaluation to provide\\na less biased estimation of future performance and fairness than an evaluation\\nof historical data. Our comprehensive analysis of a real large-scale education\\ndataset reveals key insights on modeling disparities and how imputation\\ntechniques impact the fairness of the student-success predictive outcome under\\ndifferent testing scenarios. Our results indicate that imputation introduces\\nbias if the testing set follows the historical distribution. However, if the\\ninjustice in society is addressed and consequently the upcoming batch of\\nobservations is equalized, the model would be less biased. Sequential diagnosis prediction on the Electronic Health Record (EHR) has\\nbeen proven crucial for predictive analytics in the medical domain. EHR data,\\nsequential records of a patient's interactions with healthcare systems, has\\nnumerous inherent characteristics of temporality, irregularity and data\\ninsufficiency. Some recent works train healthcare predictive models by making\\nuse of sequential information in EHR data, but they are vulnerable to\\nirregular, temporal EHR data with the states of admission/discharge from\\nhospital, and insufficient data. To mitigate this, we propose an end-to-end\\nrobust transformer-based model called SETOR, which exploits neural ordinary\\ndifferential equation to handle both irregular intervals between a patient's\\nvisits with admitted timestamps and length of stay in each visit, to alleviate\\nthe limitation of insufficient data by integrating medical ontology, and to\\ncapture the dependencies between the patient's visits by employing multi-layer\\ntransformer blocks. Experiments conducted on two real-world healthcare datasets\\nshow that, our sequential diagnoses prediction model SETOR not only achieves\\nbetter predictive results than previous state-of-the-art approaches,\\nirrespective of sufficient or insufficient training data, but also derives more\\ninterpretable embeddings of medical codes. The experimental codes are available\\nat the GitHub repository (https://github.com/Xueping/SETOR). Traditional machine learning methods face two main challenges in dealing with\\nhealthcare predictive analytics tasks. First, the high-dimensional nature of\\nhealthcare data needs labor-intensive and time-consuming processes to select an\\nappropriate set of features for each new task. Second, these methods depend on\\nfeature engineering to capture the sequential nature of patient data, which may\\nnot adequately leverage the temporal patterns of the medical events and their\\ndependencies. Recent deep learning methods have shown promising performance for\\nvarious healthcare prediction tasks by addressing the high-dimensional and\\ntemporal challenges of medical data. These methods can learn useful\\nrepresentations of key factors (e.g., medical concepts or patients) and their\\ninteractions from high-dimensional raw or minimally-processed healthcare data.\\nIn this paper we systematically reviewed studies focused on advancing and using\\ndeep neural networks to leverage patients structured time series data for\\nhealthcare prediction tasks. To identify relevant studies, MEDLINE, IEEE,\\nScopus and ACM digital library were searched for studies published up to\\nFebruary 7th 2021. We found that researchers have contributed to deep time\\nseries prediction literature in ten research streams: deep learning models,\\nmissing value handling, irregularity handling, patient representation, static\\ndata inclusion, attention mechanisms, interpretation, incorporating medical\\nontologies, learning strategies, and scalability. This study summarizes\\nresearch insights from these literature streams, identifies several critical\\nresearch gaps, and suggests future research opportunities for deep learning in\\npatient time series data. Purpose: When a learner fails to reach a milestone, educators often wonder if\\nthere had been any warning signs that could have allowed them to intervene\\nsooner. Machine learning can predict which students are at risk of failing a\\nhigh-stakes certification exam. If predictions can be made well in advance of\\nthe exam, then educators can meaningfully intervene before students take the\\nexam to reduce the chances of a failing score.\\n  Methods: Using already-collected, first-year student assessment data from\\nfive cohorts in a Master of Physician Assistant Studies program, the authors\\nimplement an \\\"adaptive minimum match\\\" version of the k-nearest neighbors\\nalgorithm (AMMKNN), using changing numbers of neighbors to predict each\\nstudent's future exam scores on the Physician Assistant National Certifying\\nExamination (PANCE). Validation occurred in two ways: Leave-one-out\\ncross-validation (LOOCV) and evaluating the predictions in a new cohort.\\n  Results: AMMKNN achieved an accuracy of 93% in LOOCV. AMMKNN generates a\\npredicted PANCE score for each student, one year before they are scheduled to\\ntake the exam. Students can then be classified into extra support, optional\\nextra support, or no extra support groups. The educator then has one year to\\nprovide the appropriate customized support to each category of student.\\n  Conclusions: Predictive analytics can identify at-risk students, so they can\\nreceive additional support or remediation when preparing for high-stakes\\ncertification exams. Educators can use the included methods and code to\\ngenerate predicted test outcomes for students. The authors recommend that\\neducators use this or similar predictive methods responsibly and transparently,\\nas one of many tools used to support students. For the last few decades, classical machine learning has allowed us to\\nimprove the lives of many through automation, natural language processing,\\npredictive analytics and much more. However, a major concern is the fact that\\nwe're fast approach the threshold of the maximum possible computational\\ncapacity available to us by the means of classical computing devices including\\nCPUs, GPUs and Application Specific Integrated Circuits (ASICs). This is due to\\nthe exponential increase in model sizes which now have parameters in the\\nmagnitude of billions and trillions, requiring a significant amount of\\ncomputing resources across a significant amount of time, just to converge one\\nsingle model. To observe the efficacy of using quantum computing for certain\\nmachine learning tasks and explore the improved potential of convergence, error\\nreduction and robustness to noisy data, this paper will look forth to test and\\nverify the aspects in which quantum machine learning can help improve over\\nclassical machine learning approaches while also shedding light on the likely\\nlimitations that have prevented quantum approaches to become the mainstream. A\\nmajor focus will be to recreate the work by Farhi et al and conduct experiments\\nusing their theory of performing machine learning in a quantum context, with\\nassistance from the Tensorflow Quantum documentation. Accurate and trustworthy epidemic forecasting is an important problem that\\nhas impact on public health planning and disease mitigation. Most existing\\nepidemic forecasting models disregard uncertainty quantification, resulting in\\nmis-calibrated predictions. Recent works in deep neural models for\\nuncertainty-aware time-series forecasting also have several limitations; e.g.\\nit is difficult to specify meaningful priors in Bayesian NNs, while methods\\nlike deep ensembling are computationally expensive in practice. In this paper,\\nwe fill this important gap. We model the forecasting task as a probabilistic\\ngenerative process and propose a functional neural process model called EPIFNP,\\nwhich directly models the probability density of the forecast value. EPIFNP\\nleverages a dynamic stochastic correlation graph to model the correlations\\nbetween sequences in a non-parametric way, and designs different stochastic\\nlatent variables to capture functional uncertainty from different perspectives.\\nOur extensive experiments in a real-time flu forecasting setting show that\\nEPIFNP significantly outperforms previous state-of-the-art models in both\\naccuracy and calibration metrics, up to 2.5x in accuracy and 2.4x in\\ncalibration. Additionally, due to properties of its generative process,EPIFNP\\nlearns the relations between the current season and similar patterns of\\nhistorical seasons,enabling interpretable forecasts. Beyond epidemic\\nforecasting, the EPIFNP can be of independent interest for advancing principled\\nuncertainty quantification in deep sequential models for predictive analytics Using big data to analyze consumer behavior can provide effective\\ndecision-making tools for preventing customer attrition (churn) in customer\\nrelationship management (CRM). Focusing on a CRM dataset with several different\\ncategories of factors that impact customer heterogeneity (i.e., usage of\\nself-care service channels, duration of service, and responsiveness to\\nmarketing actions), we provide new predictive analytics of customer churn rate\\nbased on a machine learning method that enhances the classification of logistic\\nregression by adding a mixed penalty term. The proposed penalized logistic\\nregression can prevent overfitting when dealing with big data and minimize the\\nloss function when balancing the cost from the median (absolute value) and mean\\n(squared value) regularization. We show the analytical properties of the\\nproposed method and its computational advantage in this research. In addition,\\nwe investigate the performance of the proposed method with a CRM data set (that\\nhas a large number of features) under different settings by efficiently\\neliminating the disturbance of (1) least important features and (2) sensitivity\\nfrom the minority (churn) class. Our empirical results confirm the expected\\nperformance of the proposed method in full compliance with the common\\nclassification criteria (i.e., accuracy, precision, and recall) for evaluating\\nmachine learning methods. From global pandemics to geopolitical turmoil, leaders in logistics, product\\nallocation, procurement and operations are facing increasing difficulty with\\nsafeguarding their organizations against supply chain vulnerabilities. It is\\nrecommended to opt for forecasting against trending based benchmark because\\nauditing a future forecast puts more focus on seasonality. The forecasting\\nmodels provide with end-to-end, real time oversight of the entire supply chain,\\nwhile utilizing predictive analytics and artificial intelligence to identify\\npotential disruptions before they occur. By combining internal and external\\ndata points, coming up with an AI-enabled modelling engine can greatly reduce\\nrisk by helping retail companies proactively respond to supply and demand\\nvariability. This research paper puts focus on creating an ingenious way to\\ntackle the impact of COVID19 on Supply chain, product allocation, trending and\\nseasonality.\\n  Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing,\\nseasonality, trending, retail. Healthcare predictive analytics aids medical decision-making, diagnosis\\nprediction and drug review analysis. Therefore, prediction accuracy is an\\nimportant criteria which also necessitates robust predictive language models.\\nHowever, the models using deep learning have been proven vulnerable towards\\ninsignificantly perturbed input instances which are less likely to be\\nmisclassified by humans. Recent efforts of generating adversaries using\\nrule-based synonyms and BERT-MLMs have been witnessed in general domain, but\\nthe ever increasing biomedical literature poses unique challenges. We propose\\nBBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box\\nattack algorithm for biomedical text classification, leveraging the strengths\\nof both domain-specific synonym replacement for biomedical named entities and\\nBERTMLM predictions, spelling variation and number replacement. Through\\nautomatic and human evaluation on two datasets, we demonstrate that BBAEG\\nperforms stronger attack with better language fluency, semantic coherence as\\ncompared to prior work. Flight delays impose challenges that impact any flight transportation system.\\nPredicting when they are going to occur is an important way to mitigate this\\nissue. However, the behavior of the flight delay system varies through time.\\nThis phenomenon is known in predictive analytics as concept drift. This paper\\ninvestigates the prediction performance of different drift handling strategies\\nin aviation under different scales (models trained from flights related to a\\nsingle airport or the entire flight system). Specifically, two research\\nquestions were proposed and answered: (i) How do drift handling strategies\\ninfluence the prediction performance of delays? (ii) Do different scales change\\nthe results of drift handling strategies? In our analysis, drift handling\\nstrategies are relevant, and their impacts vary according to scale and machine\\nlearning models used. As a well-established approach, factorization machine (FM) is capable of\\nautomatically learning high-order interactions among features to make\\npredictions without the need for manual feature engineering. With the prominent\\ndevelopment of deep neural networks (DNNs), there is a recent and ongoing trend\\nof enhancing the expressiveness of FM-based models with DNNs. However, though\\nbetter results are obtained with DNN-based FM variants, such performance gain\\nis paid off by an enormous amount (usually millions) of excessive model\\nparameters on top of the plain FM. Consequently, the heavy parameterization\\nimpedes the real-life practicality of those deep models, especially efficient\\ndeployment on resource-constrained IoT and edge devices. In this paper, we move\\nbeyond the traditional real space where most deep FM-based models are defined,\\nand seek solutions from quaternion representations within the hypercomplex\\nspace. Specifically, we propose the quaternion factorization machine (QFM) and\\nquaternion neural factorization machine (QNFM), which are two novel lightweight\\nand memory-efficient quaternion-valued models for sparse predictive analytics.\\nBy introducing a brand new take on FM-based models with the notion of\\nquaternion algebra, our models not only enable expressive inter-component\\nfeature interactions, but also significantly reduce the parameter size due to\\nlower degrees of freedom in the hypercomplex Hamilton product compared with\\nreal-valued matrix multiplication. Extensive experimental results on three\\nlarge-scale datasets demonstrate that QFM achieves 4.36% performance\\nimprovement over the plain FM without introducing any extra parameters, while\\nQNFM outperforms all baselines with up to two magnitudes' parameter size\\nreduction in comparison to state-of-the-art peer methods. Evaluating predictive models is a crucial task in predictive analytics. This\\nprocess is especially challenging with time series data where the observations\\nshow temporal dependencies. Several studies have analysed how different\\nperformance estimation methods compare with each other for approximating the\\ntrue loss incurred by a given forecasting model. However, these studies do not\\naddress how the estimators behave for model selection: the ability to select\\nthe best solution among a set of alternatives. We address this issue and\\ncompare a set of estimation methods for model selection in time series\\nforecasting tasks. We attempt to answer two main questions: (i) how often is\\nthe best possible model selected by the estimators; and (ii) what is the\\nperformance loss when it does not. We empirically found that the accuracy of\\nthe estimators for selecting the best solution is low, and the overall\\nforecasting performance loss associated with the model selection process ranges\\nfrom 1.2% to 2.3%. We also discovered that some factors, such as the sample\\nsize, are important in the relative performance of the estimators. The wide adoption of Electronic Health Records (EHR) has resulted in large\\namounts of clinical data becoming available, which promises to support service\\ndelivery and advance clinical and informatics research. Deep learning\\ntechniques have demonstrated performance in predictive analytic tasks using\\nEHRs yet they typically lack model result transparency or explainability\\nfunctionalities and require cumbersome pre-processing tasks. Moreover, EHRs\\ncontain heterogeneous and multi-modal data points such as text, numbers and\\ntime series which further hinder visualisation and interpretability. This paper\\nproposes a deep learning framework to: 1) encode patient pathways from EHRs\\ninto images, 2) highlight important events within pathway images, and 3) enable\\nmore complex predictions with additional intelligibility. The proposed method\\nrelies on a deep attention mechanism for visualisation of the predictions and\\nallows predicting multiple sequential outcomes. When multiple parties that deal with private data aim for a collaborative\\nprediction task such as medical image classification, they are often\\nconstrained by data protection regulations and lack of trust among\\ncollaborating parties. If done in a privacy-preserving manner, predictive\\nanalytics can benefit from the collective prediction capability of multiple\\nparties holding complementary datasets on the same machine learning task. This\\npaper presents PRICURE, a system that combines complementary strengths of\\nsecure multi-party computation (SMPC) and differential privacy (DP) to enable\\nprivacy-preserving collaborative prediction among multiple model owners. SMPC\\nenables secret-sharing of private models and client inputs with non-colluding\\nsecure servers to compute predictions without leaking model parameters and\\ninputs. DP masks true prediction results via noisy aggregation so as to deter a\\nsemi-honest client who may mount membership inference attacks. We evaluate\\nPRICURE on neural networks across four datasets including benchmark medical\\nimage classification datasets. Our results suggest PRICURE guarantees privacy\\nfor tens of model owners and clients with acceptable accuracy loss. We also\\nshow that DP reduces membership inference attack exposure without hurting\\naccuracy. Predictive analytics over mobility data are of great importance since they\\ncan assist an analyst to predict events, such as collisions, encounters,\\ntraffic jams, etc. A typical example of such analytics is future location\\nprediction, where the goal is to predict the future location of a moving\\nobject,given a look-ahead time. What is even more challenging is being able to\\naccurately predict collective behavioural patterns of movement, such as\\nco-movement patterns. In this paper, we provide an accurate solution to the\\nproblem of Online Prediction of Co-movement Patterns. In more detail, we split\\nthe original problem into two sub-problems, namely Future Location Prediction\\nand Evolving Cluster Detection. Furthermore, in order to be able to calculate\\nthe accuracy of our solution, we propose a co-movement pattern similarity\\nmeasure, which facilitates us to match the predicted clusters with the actual\\nones. Finally, the accuracy of our solution is demonstrated experimentally over\\na real dataset from the maritime domain. A novel coronavirus disease has emerged (later named COVID-19) and caused the\\nworld to enter a new reality, with many direct and indirect factors influencing\\nit. Some are human-controllable (e.g. interventional policies, mobility and the\\nvaccine); some are not (e.g. the weather). We have sought to test how a change\\nin these human-controllable factors might influence two measures: the number of\\ndaily cases against economic impact. If applied at the right level and with\\nup-to-date data to measure, policymakers would be able to make targeted\\ninterventions and measure their cost. This study aims to provide a predictive\\nanalytics framework to model, predict and simulate COVID-19 propagation and the\\nsocio-economic impact of interventions intended to reduce the spread of the\\ndisease such as policy and/or vaccine. It allows policymakers, government\\nrepresentatives and business leaders to make better-informed decisions about\\nthe potential effect of various interventions with forward-looking views via\\nscenario planning. We have leveraged a recently launched open-source COVID-19\\nbig data platform and used published research to find potentially relevant\\nvariables (features) and leveraged in-depth data quality checks and analytics\\nfor feature selection and predictions. An advanced machine learning pipeline\\nhas been developed armed with a self-evolving model, deployed on a modern\\nmachine learning architecture. It has high accuracy for trend prediction\\n(back-tested with r-squared) and is augmented with interpretability for deeper\\ninsights. The algorithmic fairness of predictive analytic tools in the public sector\\nhas increasingly become a topic of rigorous exploration. While instruments\\npertaining to criminal recidivism and academic admissions, for example, have\\ngarnered much attention, the predictive instruments of Child Welfare\\njurisdictions have received considerably less attention. This is in part\\nbecause comparatively few such instruments exist and because even fewer have\\nbeen scrutinized through the lens of algorithmic fairness. In this work, we\\nseek to address both of these gaps. To this end, a novel classification\\nalgorithm for predicting reunification success within Oregon Child Welfare is\\npresented, including all of the relevant details associated with building such\\nan instrument. The purpose of this tool is to maximize the number of stable\\nreunifications and identify potentially unstable reunifications which may\\nrequire additional resources and scrutiny. Additionally, because the\\nalgorithmic fairness of the resulting tool, if left unaltered, is\\nunquestionably lacking, the utilized procedure for mitigating such unfairness\\nis presented, along with the rationale behind each difficult and unavoidable\\nchoice. This procedure, though similar to other post-processing group-specific\\nthresholding methods, is novel in its use of a penalized optimizer and\\ncontextually requisite subsampling. These novel methodological components yield\\na rich and informative empirical understanding of the trade-off continuum\\nbetween fairness and accuracy. As the developed procedure is generalizable\\nacross a variety of group-level definitions of algorithmic fairness, as well as\\nacross an arbitrary number of protected attribute levels and risk thresholds,\\nthe approach is broadly applicable both within and beyond Child Welfare. The longitudinal analysis of patient response time course following doses of\\ntherapeutics is currently performed using Pharmacokinetic/Pharmacodynamic\\n(PK/PD) methodologies, which requires significant human experience and\\nexpertise in the modeling of dynamical systems. By utilizing recent\\nadvancements in deep learning, we show that the governing differential\\nequations can be learnt directly from longitudinal patient data. In particular,\\nwe propose a novel neural-PK/PD framework that combines key pharmacological\\nprinciples with neural ordinary differential equations. We applied it to an\\nanalysis of drug concentration and platelet response from a clinical dataset\\nconsisting of over 600 patients. We show that the neural-PK/PD model improves\\nupon a state-of-the-art model with respect to metrics for temporal prediction.\\nFurthermore, by incorporating key PK/PD concepts into its architecture, the\\nmodel can generalize and enable the simulations of patient responses to\\nuntested dosing regimens. These results demonstrate the potential of\\nneural-PK/PD for automated predictive analytics of patient response time\\ncourse. Recent studies on Graph Convolutional Networks (GCNs) reveal that the initial\\nnode representations (i.e., the node representations before the first-time\\ngraph convolution) largely affect the final model performance. However, when\\nlearning the initial representation for a node, most existing work linearly\\ncombines the embeddings of node features, without considering the interactions\\namong the features (or feature embeddings). We argue that when the node\\nfeatures are categorical, e.g., in many real-world applications like user\\nprofiling and recommender system, feature interactions usually carry important\\nsignals for predictive analytics. Ignoring them will result in suboptimal\\ninitial node representation and thus weaken the effectiveness of the follow-up\\ngraph convolution. In this paper, we propose a new GCN model named CatGCN,\\nwhich is tailored for graph learning when the node features are categorical.\\nSpecifically, we integrate two ways of explicit interaction modeling into the\\nlearning of initial node representation, i.e., local interaction modeling on\\neach pair of node features and global interaction modeling on an artificial\\nfeature graph. We then refine the enhanced initial node representations with\\nthe neighborhood aggregation-based graph convolution. We train CatGCN in an\\nend-to-end fashion and demonstrate it on semi-supervised node classification.\\nExtensive experiments on three tasks of user profiling (the prediction of user\\nage, city, and purchase level) from Tencent and Alibaba datasets validate the\\neffectiveness of CatGCN, especially the positive effect of performing feature\\ninteraction modeling before graph convolution. Applications of machine learning in the non-profit and public sectors often\\nfeature an iterative workflow of data acquisition, prediction, and optimization\\nof interventions. There are four major pain points that a machine learning\\npipeline must overcome in order to be actually useful in these settings: small\\ndata, data collected only under the default intervention, unmodeled objectives\\ndue to communication gap, and unforeseen consequences of the intervention. In\\nthis paper, we introduce bandit data-driven optimization, the first iterative\\nprediction-prescription framework to address these pain points. Bandit\\ndata-driven optimization combines the advantages of online bandit learning and\\noffline predictive analytics in an integrated framework. We propose PROOF, a\\nnovel algorithm for this framework and formally prove that it has no-regret.\\nUsing numerical simulations, we show that PROOF achieves superior performance\\nthan existing baseline. We also apply PROOF in a detailed case study of food\\nrescue volunteer recommendation, and show that PROOF as a framework works well\\nwith the intricacies of ML models in real-world AI for non-profit and public\\nsector applications. Many institutions within the healthcare ecosystem are making significant\\ninvestments in AI technologies to optimize their business operations at lower\\ncost with improved patient outcomes. Despite the hype with AI, the full\\nrealization of this potential is seriously hindered by several systemic\\nproblems, including data privacy, security, bias, fairness, and explainability.\\nIn this paper, we propose a novel canonical architecture for the development of\\nAI models in healthcare that addresses these challenges. This system enables\\nthe creation and management of AI predictive models throughout all the phases\\nof their life cycle, including data ingestion, model building, and model\\npromotion in production environments. This paper describes this architecture in\\ndetail, along with a qualitative evaluation of our experience of using it on\\nreal world problems. A novel multi-task Gaussian process (GP) framework is proposed, by using a\\ncommon mean process for sharing information across tasks. In particular, we\\ninvestigate the problem of time series forecasting, with the objective to\\nimprove multiple-step-ahead predictions. The common mean process is defined as\\na GP for which the hyper-posterior distribution is tractable. Therefore an EM\\nalgorithm is derived for handling both hyper-parameters optimisation and\\nhyper-posterior computation. Unlike previous approaches in the literature, the\\nmodel fully accounts for uncertainty and can handle irregular grids of\\nobservations while maintaining explicit formulations, by modelling the mean\\nprocess in a unified GP framework. Predictive analytical equations are\\nprovided, integrating information shared across tasks through a relevant prior\\nmean. This approach greatly improves the predictive performances, even far from\\nobservations, and may reduce significantly the computational complexity\\ncompared to traditional multi-task GP models. Our overall algorithm is called\\n\\\\textsc{Magma} (standing for Multi tAsk Gaussian processes with common MeAn).\\nThe quality of the mean process estimation, predictive performances, and\\ncomparisons to alternatives are assessed in various simulated scenarios and on\\nreal datasets. Each year, almost 10% of claims are denied by payers (i.e., health insurance\\nplans). With the cost to recover these denials and underpayments, predicting\\npayer response (likelihood of payment) from claims data with a high degree of\\naccuracy and precision is anticipated to improve healthcare staffs' performance\\nproductivity and drive better patient financial experience and satisfaction in\\nthe revenue cycle (Barkholz, 2017). However, constructing advanced predictive\\nanalytics models has been considered challenging in the last twenty years. That\\nsaid, we propose a (low-level) context-dependent compact representation of\\npatients' historical claim records by effectively learning complicated\\ndependencies in the (high-level) claim inputs. Built on this new latent\\nrepresentation, we demonstrate that a deep learning-based framework, Deep\\nClaim, can accurately predict various responses from multiple payers using\\n2,905,026 de-identified claims data from two US health systems. Deep Claim's\\nimprovements over carefully chosen baselines in predicting claim denials are\\nmost pronounced as 22.21% relative recall gain (at 95% precision) on Health\\nSystem A, which implies Deep Claim can find 22.21% more denials than the best\\nbaseline system. Understanding performance and prioritizing resources for the maintenance of\\nthe drinking-water pipe network throughout its life-cycle is a key part of\\nwater asset management. Renovation of this vital network is generally hindered\\nby the difficulty or impossibility to gain physical access to the pipes. We\\nstudy a statistical and machine learning framework for the prediction of water\\npipe failures. We employ classical and modern classifiers for a short-term\\nprediction and survival analysis to provide a broader perspective and long-term\\nforecast, usually needed for the economic analysis of the renovation. To enrich\\nthese models, we introduce new predictors based on water distribution domain\\nknowledge and employ a modern oversampling technique to remedy the high\\nimbalance coming from the few failures observed each year. For our case study,\\nwe use a dataset containing the failure records of all pipes within the water\\ndistribution network in Barcelona, Spain. The results shed light on the effect\\nof important risk factors, such as pipe geometry, age, material, and soil\\ncover, among others, and can help utility managers conduct more informed\\npredictive maintenance tasks. Machine learning (ML) has shown increasing abilities for predictive analytics\\nover the last decades. It is becoming ubiquitous in different fields, such as\\nhealthcare, criminal justice, finance and smart city. For instance, the\\nUniversity of Northampton is building a smart system with multiple layers of\\nIoT and software-defined networks (SDN) on its new Waterside Campus. The system\\ncan be used to optimize smart buildings energy efficiency, improve the health\\nand safety of its tenants and visitors, assist crowd management and\\nway-finding, and improve the Internet connectivity. Regression analysis is an important machine learning task used for predictive\\nanalytic in business, sports analysis, etc. In regression analysis,\\noptimization algorithms play a significant role in search the coefficients in\\nthe regression model. In this paper, nonlinear regression analysis using a\\nrecently developed meta-heuristic Multi-Verse Optimizer (MVO) is proposed. The\\nproposed method is applied to 10 well-known benchmark nonlinear regression\\nproblems. A comparative study has been conducted with Particle Swarm Optimizer\\n(PSO). The experimental results demonstrate that the proposed method\\nstatistically outperforms PSO algorithm. Feature selection for predictive analytics is the problem of identifying a\\nminimal-size subset of features that is maximally predictive of an outcome of\\ninterest. To apply to molecular data, feature selection algorithms need to be\\nscalable to tens of thousands of available features. In this paper, we propose\\ngOMP, a highly-scalable generalisation of the Orthogonal Matching Pursuit\\nfeature selection algorithm to several directions: (a) different types of\\noutcomes, such as continuous, binary, nominal, and time-to-event, (b) different\\ntypes of predictive models (e.g., linear least squares, logistic regression),\\n(c) different types of predictive features (continuous, categorical), and (d)\\ndifferent, statistical-based stopping criteria. We compare the proposed\\nalgorithm against LASSO, a prototypical, widely used algorithm for\\nhigh-dimensional data. On dozens of simulated datasets, as well as, real gene\\nexpression datasets, gOMP is on par, or outperforms LASSO for case-control\\nbinary classification, quantified outcomes (regression), and (censored)\\nsurvival times (time-to-event) analysis. gOMP has also several theoretical\\nadvantages that are discussed. While gOMP is based on quite simple and basic\\nstatistical ideas, easy to implement and to generalize, we also show in an\\nextensive evaluation that it is also quite effective in bioinformatics analysis\\nsettings. Social media, especially Twitter, is being increasingly used for research\\nwith predictive analytics. In social media studies, natural language processing\\n(NLP) techniques are used in conjunction with expert-based, manual and\\nqualitative analyses. However, social media data are unstructured and must\\nundergo complex manipulation for research use. The manual annotation is the\\nmost resource and time-consuming process that multiple expert raters have to\\nreach consensus on every item, but is essential to create gold-standard\\ndatasets for training NLP-based machine learning classifiers. To reduce the\\nburden of the manual annotation, yet maintaining its reliability, we devised a\\ncrowdsourcing pipeline combined with active learning strategies. We\\ndemonstrated its effectiveness through a case study that identifies job loss\\nevents from individual tweets. We used Amazon Mechanical Turk platform to\\nrecruit annotators from the Internet and designed a number of quality control\\nmeasures to assure annotation accuracy. We evaluated 4 different active\\nlearning strategies (i.e., least confident, entropy, vote entropy, and\\nKullback-Leibler divergence). The active learning strategies aim at reducing\\nthe number of tweets needed to reach a desired performance of automated\\nclassification. Results show that crowdsourcing is useful to create\\nhigh-quality annotations and active learning helps in reducing the number of\\nrequired tweets, although there was no substantial difference among the\\nstrategies tested. The increasing popularity of e-learning has created demand for improving\\nonline education through techniques such as predictive analytics and content\\nrecommendations. In this paper, we study learner outcome predictions, i.e.,\\npredictions of how they will perform at the end of a course. We propose a novel\\nTwo Branch Decision Network for performance prediction that incorporates two\\nimportant factors: how learners progress through the course and how the content\\nprogresses through the course. We combine clickstream features which log every\\naction the learner takes while learning, and textual features which are\\ngenerated through pre-trained GloVe word embeddings. To assess the performance\\nof our proposed network, we collect data from a short online course designed\\nfor corporate training and evaluate both neural network and non-neural network\\nbased algorithms on it. Our proposed algorithm achieves 95.7% accuracy and\\n0.958 AUC score, which outperforms all other models. The results also indicate\\nthe combination of behavior features and text features are more predictive than\\nbehavior features only and neural network models are powerful in capturing the\\njoint relationship between user behavior and course content. Modern predictive analytics underpinned by machine learning techniques has\\nbecome a key enabler to the automation of data-driven decision making. In the\\ncontext of business process management, predictive analytics has been applied\\nto making predictions about the future state of an ongoing business process\\ninstance, for example, when will the process instance complete and what will be\\nthe outcome upon completion. Machine learning models can be trained on event\\nlog data recording historical process execution to build the underlying\\npredictive models. Multiple techniques have been proposed so far which encode\\nthe information available in an event log and construct input features required\\nto train a predictive model. While accuracy has been a dominant criterion in\\nthe choice of various techniques, they are often applied as a black-box in\\nbuilding predictive models. In this paper, we derive explanations using\\ninterpretable machine learning techniques to compare and contrast the\\nsuitability of multiple predictive models of high accuracy. The explanations\\nallow us to gain an understanding of the underlying reasons for a prediction\\nand highlight scenarios where accuracy alone may not be sufficient in assessing\\nthe suitability of techniques used to encode event log data to features used by\\na predictive model. Findings from this study motivate the need and importance\\nto incorporate interpretability in predictive process analytics. In various web applications like targeted advertising and recommender\\nsystems, the available categorical features (e.g., product type) are often of\\ngreat importance but sparse. As a widely adopted solution, models based on\\nFactorization Machines (FMs) are capable of modelling high-order interactions\\namong features for effective sparse predictive analytics. As the volume of\\nweb-scale data grows exponentially over time, sparse predictive analytics\\ninevitably involves dynamic and sequential features. However, existing FM-based\\nmodels assume no temporal orders in the data, and are unable to capture the\\nsequential dependencies or patterns within the dynamic features, impeding the\\nperformance and adaptivity of these methods. Hence, in this paper, we propose a\\nnovel Sequence-Aware Factorization Machine (SeqFM) for temporal predictive\\nanalytics, which models feature interactions by fully investigating the effect\\nof sequential dependencies. As static features (e.g., user gender) and dynamic\\nfeatures (e.g., user interacted items) express different semantics, we\\ninnovatively devise a multi-view self-attention scheme that separately models\\nthe effect of static features, dynamic features and the mutual interactions\\nbetween static and dynamic features in three different views. In SeqFM, we\\nfurther map the learned representations of feature interactions to the desired\\noutput with a shared residual network. To showcase the versatility and\\ngeneralizability of SeqFM, we test SeqFM in three popular application scenarios\\nfor FM-based models, namely ranking, classification and regression tasks.\\nExtensive experimental results on six large-scale datasets demonstrate the\\nsuperior effectiveness and efficiency of SeqFM. Industry 4.0 is the latest industrial revolution primarily merging automation\\nwith advanced manufacturing to reduce direct human effort and resources.\\nPredictive maintenance (PdM) is an industry 4.0 solution, which facilitates\\npredicting faults in a component or a system powered by state-of-the-art\\nmachine learning (ML) algorithms and the Internet-of-Things (IoT) sensors.\\nHowever, IoT sensors and deep learning (DL) algorithms, both are known for\\ntheir vulnerabilities to cyber-attacks. In the context of PdM systems, such\\nattacks can have catastrophic consequences as they are hard to detect due to\\nthe nature of the attack. To date, the majority of the published literature\\nfocuses on the accuracy of DL enabled PdM systems and often ignores the effect\\nof such attacks. In this paper, we demonstrate the effect of IoT sensor attacks\\non a PdM system. At first, we use three state-of-the-art DL algorithms,\\nspecifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and\\nConvolutional Neural Network (CNN) for predicting the Remaining Useful Life\\n(RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results\\nshow that the GRU-based PdM model outperforms some of the recent literature on\\nRUL prediction using the C-MAPSS dataset. Afterward, we model two different\\ntypes of false data injection attacks (FDIA) on turbofan engine sensor data and\\nevaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained\\nresults demonstrate that FDI attacks on even a few IoT sensors can strongly\\ndefect the RUL prediction. However, the GRU-based PdM model performs better in\\nterms of accuracy and resiliency. Lastly, we perform a study on the GRU-based\\nPdM model using four different GRU networks with different sequence lengths.\\nOur experiments reveal an interesting relationship between the accuracy,\\nresiliency and sequence length for the GRU-based PdM models. In this chapter, we provide a brief overview of applying machine learning\\ntechniques for clinical prediction tasks. We begin with a quick introduction to\\nthe concepts of machine learning and outline some of the most common machine\\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\\nappropriate toolkits to conduct machine learning experiments for clinical\\nprediction tasks. The objectives of this chapter are to (1) understand the\\nbasics of machine learning techniques and the reasons behind why they are\\nuseful for solving clinical prediction problems, (2) understand the intuition\\nbehind some machine learning models, including regression, decision trees, and\\nsupport vector machines, and (3) understand how to apply these models to\\nclinical prediction problems using publicly available datasets via case\\nstudies. Sepsis, a dysregulated immune system response to infection, is among the\\nleading causes of morbidity, mortality, and cost overruns in the Intensive Care\\nUnit (ICU). Early prediction of sepsis can improve situational awareness\\namongst clinicians and facilitate timely, protective interventions. While the\\napplication of predictive analytics in ICU patients has shown early promising\\nresults, much of the work has been encumbered by high false-alarm rates.\\nEfforts to improve specificity have been limited by several factors, most\\nnotably the difficulty of labeling sepsis onset time and the low prevalence of\\nseptic-events in the ICU. Here, we present DeepAISE (Deep Artificial\\nIntelligence Sepsis Expert), a recurrent neural survival model for the early\\nprediction of sepsis. We show that by coupling a clinical criterion for\\ndefining sepsis onset time with a treatment policy (e.g., initiation of\\nantibiotics within one hour of meeting the criterion), one may rank the\\nrelative utility of various criteria through offline policy evaluation. Given\\nthe optimal criterion, DeepAISE automatically learns predictive features\\nrelated to higher-order interactions and temporal patterns among clinical risk\\nfactors that maximize the data likelihood of observed time to septic events.\\nDeepAISE has been incorporated into a clinical workflow, which provides\\nreal-time hourly sepsis risk scores. A comparative study of four baseline\\nmodels indicates that DeepAISE produces the most accurate predictions (AUC=0.90\\nand 0.87) and the lowest false alarm rates (FAR=0.20 and 0.26) in two separate\\ncohorts (internal and external, respectively), while simultaneously producing\\ninterpretable representations of the clinical time series and risk factors. We analyze damage propagation modeling of turbo-engines in a data-driven\\napproach. We investigate subspace tracking assuming a low dimensional manifold\\nstructure and a static behavior during the healthy state of the machines. Our\\ndamage propagation model is based on the deviation of the data from the static\\nbehavior and uses the notion of health index as a measure of the condition.\\nHence, we incorporate condition-based maintenance and estimate the remaining\\nuseful life based on the current and previous health indexes. This paper\\nproposes an algorithm that adapts well to the dynamics of the data and\\nunderlying system, and reduces the computational complexity by utilizing the\\nlow dimensional manifold structure of the data. A significant performance\\nimprovement is demonstrated over existing methods by using the proposed\\nalgorithm on CMAPSS Turbo-engine datasets. Predictive policing is a data-based, predictive analytical technique used in\\nlaw enforcement. In this paper, we give an overview of the current situation in\\nAustria and discuss technical, sociopolitical and legal questions raised by the\\nuse of PP, such as the lack of awareness of discriminatory structures in\\nsociety, the biases in data underlying PP and the lack of reflection on the\\nbasic premises and feedback mechanisms of PP. Violations of fundamental rights\\nwithout cause are not allowed by the Austrian Code of Criminal Procedure\\n(Strafproze{\\\\ss}ordnung, StPO), the Security Police Act\\n(Sicherheitspolizeigesetz, SPG) or the Act concerning Police Protection of the\\nState (Polizeiliches Staatsschutzgesetz, PStSG); the principle of allowing\\npolice intervention only on the basis of concrete threats or suspicion must\\nremain absolute. Considering the numerous problems (not least from the point of\\nview of legal policy), we conclude that the use of PP should be eschewed and\\nthat resources and planning should instead be focussed on solving the social\\nproblems which actually cause crime.\\n  -----\\n  Predictive Policing ist ein datenbasiertes und prognosegetriebenes Modell\\nf\\\\\\\"ur Polizeiarbeit. Wir geben in diesem Artikel einen \\\\\\\"Uberblick \\\\\\\"uber den\\naktuellen Stand in \\\\\\\"Osterreich und diskutieren technische,\\npolitisch-gesellschaftliche und rechtliche Probleme, die sich daraus ergeben --\\netwa das mangelhafte Bewusstsein f\\\\\\\"ur Prozesse gesellschaftlicher\\nDiskriminierung, die verzerrte Datenbasis, die PP zugrundeliegt, und fehlende\\nReflexion \\\\\\\"uber zugrundeliegende Annahmen und R\\\\\\\"uckkopplungseffekte.\\nAnlasslose Grundrechtseingriffe sind weder durch die StPO noch das SPG oder das\\nPStSG gedeckt; dem Grundgedanken, dass Polizei erst bei konkreter Gefahrenlage\\noder Tatverdacht t\\\\\\\"atig werden darf, muss weiterhin Rechnung getragen werden.\\nAus unserer Sicht sollte angesichts der zahlreichen Probleme (und auch aus\\nrechtspolitischen Erw\\\\\\\"agungen) auf PP verzichtet werden und stattdessen\\nRessourcen und \\\\\\\"Uberlegung in die L\\\\\\\"osung jener gesellschaftlicher Probleme\\ninvestiert werden, die zu Kriminalit\\\\\\\"at f\\\\\\\"uhren. Product recommender systems and customer profiling techniques have always\\nbeen a priority in online retail. Recent machine learning research advances and\\nalso wide availability of massive parallel numerical computing has enabled\\nvarious approaches and directions of recommender systems advancement. Worth to\\nmention is the fact that in past years multiple traditional \\\"offline\\\" retail\\nbusiness are gearing more and more towards employing inferential and even\\npredictive analytics both to stock-related problems such as predictive\\nreplenishment but also to enrich customer interaction experience. One of the\\nmost important areas of recommender systems research and development is that of\\nDeep Learning based models which employ representational learning to model\\nconsumer behavioral patterns. Current state of the art in Deep Learning based\\nrecommender systems uses multiple approaches ranging from already classical\\nmethods such as the ones based on learning product representation vector, to\\nrecurrent analysis of customer transactional time-series and up to generative\\nmodels based on adversarial training. Each of these methods has multiple\\nadvantages and inherent weaknesses such as inability of understanding the\\nactual user-journey, ability to propose only single product recommendation or\\ntop-k product recommendations without prediction of actual next-best-offer. In\\nour work we will present a new and innovative architectural approach of\\napplying state-of-the-art hierarchical multi-module encoder-decoder\\narchitecture in order to solve several of current state-of-the-art recommender\\nsystems issues. Our approach will also produce by-products such as product\\nneed-based segmentation and customer behavioral segmentation - all in an\\nend-to-end trainable approach. Finally, we will present a couple methods that\\nsolve known retail & distribution pain-points based on the proposed\\narchitecture. Pre-trained deep learning models are increasingly being used to offer a\\nvariety of compute-intensive predictive analytics services such as fitness\\ntracking, speech and image recognition. The stateless and highly parallelizable\\nnature of deep learning models makes them well-suited for serverless computing\\nparadigm. However, making effective resource management decisions for these\\nservices is a hard problem due to the dynamic workloads and diverse set of\\navailable resource configurations that have their deployment and management\\ncosts. To address these challenges, we present a distributed and scalable\\ndeep-learning prediction serving system called Barista and make the following\\ncontributions. First, we present a fast and effective methodology for\\nforecasting workloads by identifying various trends. Second, we formulate an\\noptimization problem to minimize the total cost incurred while ensuring bounded\\nprediction latency with reasonable accuracy. Third, we propose an efficient\\nheuristic to identify suitable compute resource configurations. Fourth, we\\npropose an intelligent agent to allocate and manage the compute resources by\\nhorizontal and vertical scaling to maintain the required prediction latency.\\nFinally, using representative real-world workloads for urban transportation\\nservice, we demonstrate and validate the capabilities of Barista. Predictive analytics systems are currently one of the most important areas of\\nresearch and development within the Artificial Intelligence domain and\\nparticularly in Machine Learning. One of the \\\"holy grails\\\" of predictive\\nanalytics is the research and development of the \\\"perfect\\\" recommendation\\nsystem. In our paper, we propose an advanced pipeline model for the multi-task\\nobjective of determining product complementarity, similarity and sales\\nprediction using deep neural models applied to big-data sequential transaction\\nsystems. Our highly parallelized hybrid model pipeline consists of both\\nunsupervised and supervised models, used for the objectives of generating\\nsemantic product embeddings and predicting sales, respectively. Our\\nexperimentation and benchmarking processes have been done using pharma industry\\nretail real-life transactional Big-Data streams. The International Classification of Functioning, Disability, and Health for\\nChildren and Youth (ICF-CY) is a scaffold for designating and systematizing\\ndata on functioning and disability. It offers a standard semantic and a\\ntheoretical foundation for the demarcation and extent of wellbeing and\\ninfirmity. The multidimensional layout of ICF-CY comprehends a plethora of\\ninformation with about 1400 categories making it difficult to analyze. Our\\nresearch proposes a predictive model that classify self-care problems on\\nSelf-Care Activities Dataset based on the ICF- CY. The data used in this study\\nresides 206 attributes of 70 children with motor and physical disability. Our\\nstudy implements, compare and analyze Random Forest, Support vector machine,\\nNaive Bayes, Hoeffding tree, and Lazy locally weighted learning using\\ntwo-tailed T-test at 95% confidence interval. Boruta algorithm involved in the\\nstudy minimizes the data dimensionality to advocate the minimal-optimal set of\\npredictors. Random forest gave the best classification accuracy of 84.75%; root\\nmean squared error of 0.18 and receiver operating characteristic of 0.99.\\nPredictive analytics can simplify the usage of ICF-CY by automating the\\nclassification process of disability, functioning, and health. Objective: A patient medical insurance coverage plays an essential role in\\ndetermining the post-acute care (PAC) discharge disposition. The prior health\\ninsurance authorization process postpones the PAC discharge disposition,\\nincreases the inpatient length of stay, and effects patient health. Our study\\nimplements predictive analytics for the early prediction of the PAC discharge\\ndisposition to reduce the deferments caused by prior health insurance\\nauthorization, the inpatient length of stay and inpatient stay expenses.\\nMethodology: We conducted a group discussion involving 25 patient care\\nfacilitators (PCFs) and two registered nurses (RNs) and retrieved 1600 patient\\ndata records from the initial nursing assessment and discharge notes to conduct\\na retrospective analysis of PAC discharge dispositions using predictive\\nanalytics. Results: The chi-squared automatic interaction detector (CHAID)\\nalgorithm enabled the early prediction of the PAC discharge disposition,\\naccelerated the prior health insurance process, decreased the inpatient length\\nof stay by an average of 22.22%, and reduced inpatient stay expenses by \\\\$1,974\\nfor state government hospitals, \\\\$2,346 for non-profit hospitals and \\\\$1,798\\nfor for-profit hospitals per day. The CHAID algorithm produced an overall\\naccuracy of 84.16% and an area under the receiver operating characteristic\\n(ROC) curve value of 0.81. Conclusion: The early prediction of PAC discharge\\ndispositions can condense the PAC deferment caused by the prior health\\ninsurance authorization process and simultaneously minimize the inpatient\\nlength of stay and related expenses incurred by the hospital. Predictive analytics is increasingly used to guide decision-making in many\\napplications. However, in practice, we often have limited data on the true\\npredictive task of interest, and must instead rely on more abundant data on a\\nclosely-related proxy predictive task. For example, e-commerce platforms use\\nabundant customer click data (proxy) to make product recommendations rather\\nthan the relatively sparse customer purchase data (true outcome of interest);\\nalternatively, hospitals often rely on medical risk scores trained on a\\ndifferent patient population (proxy) rather than their own patient population\\n(true cohort of interest) to assign interventions. Yet, not accounting for the\\nbias in the proxy can lead to sub-optimal decisions. Using real datasets, we\\nfind that this bias can often be captured by a sparse function of the features.\\nThus, we propose a novel two-step estimator that uses techniques from\\nhigh-dimensional statistics to efficiently combine a large amount of proxy data\\nand a small amount of true data. We prove upper bounds on the error of our\\nproposed estimator and lower bounds on several heuristics used by data\\nscientists; in particular, our proposed estimator can achieve the same accuracy\\nwith exponentially less true data (in the number of features). Our proof relies\\non a new LASSO tail inequality for approximately sparse vectors. Finally, we\\ndemonstrate the effectiveness of our approach on e-commerce and healthcare\\ndatasets; in both cases, we achieve significantly better predictive accuracy as\\nwell as managerial insights into the nature of the bias in the proxy data. With the emergence of the Hospital Readmission Reduction Program of the\\nCenter for Medicare and Medicaid Services on October 1, 2012, forecasting\\nunplanned patient readmission risk became crucial to the healthcare domain.\\nThere are tangible works in the literature emphasizing on developing\\nreadmission risk prediction models; However, the models are not accurate enough\\nto be deployed in an actual clinical setting. Our study considers patient\\nreadmission risk as the objective for optimization and develops a useful risk\\nprediction model to address unplanned readmissions. Furthermore, Genetic\\nAlgorithm and Greedy Ensemble is used to optimize the developed model\\nconstraints. Detecting patterns in real time streaming data has been an interesting and\\nchallenging data analytics problem. With the proliferation of a variety of\\nsensor devices, real-time analytics of data from the Internet of Things (IoT)\\nto learn regular and irregular patterns has become an important machine\\nlearning problem to enable predictive analytics for automated notification and\\ndecision support. In this work, we address the problem of learning an irregular\\nhuman activity pattern, fall, from streaming IoT data from wearable sensors. We\\npresent a deep neural network model for detecting fall based on accelerometer\\ndata giving 98.75 percent accuracy using an online physical activity monitoring\\ndataset called \\\"MobiAct\\\", which was published by Vavoulas et al. The initial\\nmodel was developed using IBM Watson studio and then later transferred and\\ndeployed on IBM Cloud with the streaming analytics service supported by IBM\\nStreams for monitoring real-time IoT data. We also present the systems\\narchitecture of the real-time fall detection framework that we intend to use\\nwith mbientlabs wearable health monitoring sensors for real time patient\\nmonitoring at retirement homes or rehabilitation clinics. The tremendous growth of positioning technologies and GPS enabled devices has\\nproduced huge volumes of tracking data during the recent years. This source of\\ninformation constitutes a rich input for data analytics processes, either\\noffline (e.g. cluster analysis, hot motion discovery) or online (e.g.\\nshort-term forecasting of forthcoming positions). This paper focuses on\\npredictive analytics for moving objects (could be pedestrians, cars, vessels,\\nplanes, animals, etc.) and surveys the state-of-the-art in the context of\\nfuture location and trajectory prediction. We provide an extensive review of\\nover 50 works, also proposing a novel taxonomy of predictive algorithms over\\nmoving objects. We also list the properties of several real datasets used in\\nthe past for validation purposes of those works and, motivated by this, we\\ndiscuss challenges that arise in the transition from conventional to Big Data\\napplications.\\n  CCS Concepts: Information systems > Spatial-temporal systems; Information\\nsystems > Data analytics; Information systems > Data mining; Computing\\nmethodologies > Machine learning Additional Key Words and Phrases: mobility\\ndata, moving object trajectories, trajectory prediction, future location\\nprediction. A measure of relative importance of variables is often desired by researchers\\nwhen the explanatory aspects of econometric methods are of interest. To this\\nend, the author briefly reviews the limitations of conventional econometrics in\\nconstructing a reliable measure of variable importance. The author highlights\\nthe relative stature of explanatory and predictive analysis in economics and\\nthe emergence of fruitful collaborations between econometrics and computer\\nscience. Learning lessons from both, the author proposes a hybrid approach\\nbased on conventional econometrics and advanced machine learning (ML)\\nalgorithms, which are otherwise, used in predictive analytics. The purpose of\\nthis article is two-fold, to propose a hybrid approach to assess relative\\nimportance and demonstrate its applicability in addressing policy priority\\nissues with an example of food inflation in India, followed by a broader aim to\\nintroduce the possibility of conflation of ML and conventional econometrics to\\nan audience of researchers in economics and social sciences, in general. We address the problem of predicting whether sufficient memory and CPU\\nresources have been requested for jobs at submission time. For this purpose, we\\nexamine the task of training a supervised machine learning system to predict\\nthe outcome - whether the job will fail specifically due to insufficient\\nresources - as a classification task. Sufficiently high accuracy, precision,\\nand recall at this task facilitates more anticipatory decision support\\napplications in the domain of HPC resource allocation. Our preliminary results\\nusing a new test bed show that the probability of failed jobs is associated\\nwith information freely available at job submission time and may thus be usable\\nby a learning system for user modeling that gives personalized feedback to\\nusers. For highly sensitive real-world predictive analytic applications such as\\nhealthcare and medicine, having good prediction accuracy alone is often not\\nenough. These kinds of applications require a decision making process which\\nuses uncertainty estimation as input whenever possible. Quality of uncertainty\\nestimation is a subject of over or under confident prediction, which is often\\nnot addressed in many models. In this paper we show several extensions to the\\nGaussian Conditional Random Fields model, which aim to provide higher quality\\nuncertainty estimation. These extensions are applied to the temporal disease\\ngraph built from the State Inpatient Database (SID) of California, acquired\\nfrom the HCUP. Our experiments demonstrate benefits of using graph information\\nin modeling temporal disease properties as well as improvements in uncertainty\\nestimation provided by given extensions of the Gaussian Conditional Random\\nFields method. In recent years, the utilization of rotating parts, e.g. bearings and gears,\\nhas been continuously supporting the manufacturing line to produce consistent\\noutput quality. Due to their critical role, the breakdown of these components\\nmight significantly impact the production rate. A proper condition based\\nmonitoring (CBM) is among a few ways to maintain and monitor the rotating\\nsystems. Prognosis, as one of the major tasks in CBM that predicts and\\nestimates the remaining useful life of the machine, has attracted significant\\ninterest in decades. This paper presents a literature review on prognosis\\napproaches from published papers in the last decade. The prognostic approaches\\nare described comprehensively to provide a better idea on how to select an\\nappropriate prognosis method for specific needs. An advanced predictive\\nanalytics, namely Parsimonious Network Based on Fuzzy Inference System\\n(PANFIS), was proposed and tested into the low speed slew bearing data. PANFIS\\ndiffers itself from conventional prognostic approaches in which it supports for\\nonline lifelong prognostics without the requirement of retraining or\\nreconfiguration phase. The method is applied to normal-to-failure bearing\\nvibration data collected for 139 days and to predict the time-domain features\\nof vibration slew bearing signals. The performance of the proposed method is\\ncompared to some established methods such as ANFIS, eTS, and Simp_eTS. From the\\nresults, it is suggested that PANFIS offers outstanding performance compared to\\nthose of other methods. The importance of interpretability of machine learning models has been\\nincreasing due to emerging enterprise predictive analytics, threat of data\\nprivacy, accountability of artificial intelligence in society, and so on.\\nPiecewise linear models have been actively studied to achieve both accuracy and\\ninterpretability. They often produce competitive accuracy against\\nstate-of-the-art non-linear methods. In addition, their representations (i.e.,\\nrule-based segmentation plus sparse linear formula) are often preferred by\\ndomain experts. A disadvantage of such models, however, is high computational\\ncost for simultaneous determinations of the number of \\\"pieces\\\" and cardinality\\nof each linear predictor, which has restricted their applicability to\\nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic\\nBayesian (FAB) inference of learning piece-wise sparse linear models on\\ndistributed memory architectures. The distributed FAB inference solves the\\nsimultaneous model selection issue without communicating $O(N)$ data where N is\\nthe number of training samples and achieves linear scale-out against the number\\nof CPU cores. Experimental results demonstrate that the distributed FAB\\ninference achieves high prediction accuracy and performance scalability with\\nboth synthetic and benchmark data. Data-driven predictive analytics are in use today across a number of\\nindustrial applications, but further integration is hindered by the requirement\\nof similarity among model training and test data distributions. This paper\\naddresses the need of learning from possibly nonstationary data streams, or\\nunder concept drift, a commonly seen phenomenon in practical applications. A\\nsimple dual-learner ensemble strategy, alternating learners framework, is\\nproposed. A long-memory model learns stable concepts from a long relevant time\\nwindow, while a short-memory model learns transient concepts from a small\\nrecent window. The difference in prediction performance of these two models is\\nmonitored and induces an alternating policy to select, update and reset the two\\nmodels. The method features an online updating mechanism to maintain the\\nensemble accuracy, and a concept-dependent trigger to focus on relevant data.\\nThrough empirical studies the method demonstrates effective tracking and\\nprediction when the steaming data carry abrupt and/or gradual changes. Feature selection with high-dimensional data and a very small proportion of\\nrelevant features poses a severe challenge to standard statistical methods. We\\nhave developed a new approach (HARVEST) that is straightforward to apply,\\nalbeit somewhat computer-intensive. This algorithm can be used to pre-screen a\\nlarge number of features to identify those that are potentially useful. The\\nbasic idea is to evaluate each feature in the context of many random subsets of\\nother features. HARVEST is predicated on the assumption that an irrelevant\\nfeature can add no real predictive value, regardless of which other features\\nare included in the subset. Motivated by this idea, we have derived a simple\\nstatistical test for feature relevance. Empirical analyses and simulations\\nproduced so far indicate that the HARVEST algorithm is highly effective in\\npredictive analytics, both in science and business. Many predictive tasks of web applications need to model categorical\\nvariables, such as user IDs and demographics like genders and occupations. To\\napply standard machine learning techniques, these categorical predictors are\\nalways converted to a set of binary features via one-hot encoding, making the\\nresultant feature vector highly sparse. To learn from such sparse data\\neffectively, it is crucial to account for the interactions between features.\\n  Factorization Machines (FMs) are a popular solution for efficiently using the\\nsecond-order feature interactions. However, FM models feature interactions in a\\nlinear way, which can be insufficient for capturing the non-linear and complex\\ninherent structure of real-world data. While deep neural networks have recently\\nbeen applied to learn non-linear feature interactions in industry, such as the\\nWide&Deep by Google and DeepCross by Microsoft, the deep structure meanwhile\\nmakes them difficult to train.\\n  In this paper, we propose a novel model Neural Factorization Machine (NFM)\\nfor prediction under sparse settings. NFM seamlessly combines the linearity of\\nFM in modelling second-order feature interactions and the non-linearity of\\nneural network in modelling higher-order feature interactions. Conceptually,\\nNFM is more expressive than FM since FM can be seen as a special case of NFM\\nwithout hidden layers. Empirical results on two regression tasks show that with\\none hidden layer only, NFM significantly outperforms FM with a 7.3% relative\\nimprovement. Compared to the recent deep learning methods Wide&Deep and\\nDeepCross, our NFM uses a shallower structure but offers better performance,\\nbeing much easier to train and tune in practice. In this study, the wind data series from five locations in Aegean Sea\\nislands, the most active `hotspots' in terms of refugee influx during the\\nOct/2015 - Jan/2016 period, are investigated. The analysis of the\\nthree-per-site data series includes standard statistical analysis and\\nparametric distributions, auto-correlation analysis, cross-correlation analysis\\nbetween the sites, as well as various ARMA models for estimating the\\nfeasibility and accuracy of such spatio-temporal linear regressors for\\npredictive analytics. Strong correlations are detected across specific sites\\nand appropriately trained ARMA(7,5) models achieve 1-day look-ahead error\\n(RMSE) of less than 1.9 km/h on average wind speed. The results show that such\\ndata-driven statistical approaches are extremely useful in identifying\\nunexpected and sometimes counter-intuitive associations between the available\\nspatial data nodes, which is very important when designing corresponding models\\nfor short-term forecasting of sea condition, especially average wave height and\\ndirection, which is in fact what defines the associated weather risk of\\ncrossing these passages in refugee influx patterns. Feature engineering is one of the most important and time consuming tasks in\\npredictive analytics projects. It involves understanding domain knowledge and\\ndata exploration to discover relevant hand-crafted features from raw data. In\\nthis paper, we introduce a system called One Button Machine, or OneBM for\\nshort, which automates feature discovery in relational databases. OneBM\\nautomatically performs a key activity of data scientists, namely, joining of\\ndatabase tables and applying advanced data transformations to extract useful\\nfeatures from data. We validated OneBM in Kaggle competitions in which OneBM\\nachieved performance as good as top 16% to 24% data scientists in three Kaggle\\ncompetitions. More importantly, OneBM outperformed the state-of-the-art system\\nin a Kaggle competition in terms of prediction accuracy and ranking on Kaggle\\nleaderboard. The results show that OneBM can be useful for both data scientists\\nand non-experts. It helps data scientists reduce data exploration time allowing\\nthem to try and error many ideas in short time. On the other hand, it enables\\nnon-experts, who are not familiar with data science, to quickly extract value\\nfrom their data with a little effort, time and cost. The explosive growth of the location-enabled devices coupled with the\\nincreasing use of Internet services has led to an increasing awareness of the\\nimportance and usage of geospatial information in many applications. The\\nnavigation apps (often called Maps), use a variety of available data sources to\\ncalculate and predict the travel time as well as several options for routing in\\npublic transportation, car or pedestrian modes. This paper evaluates the\\npedestrian mode of Maps apps in three major smartphone operating systems\\n(Android, iOS and Windows Phone). In the paper, we will show that the Maps apps\\non iOS, Android and Windows Phone in pedestrian mode, predict travel time\\nwithout learning from the individual's movement profile. In addition, we will\\nexemplify that those apps suffer from a specific data quality issue which\\nrelates to the absence of information about location and type of pedestrian\\ncrossings. Finally, we will illustrate learning from movement profile of\\nindividuals using various predictive analytics models to improve the accuracy\\nof travel time estimation. The process of exploring and exploiting Oil and Gas (O&G) generates a lot of\\ndata that can bring more efficiency to the industry. The opportunities for\\nusing data mining techniques in the \\\"digital oil-field\\\" remain largely\\nunexplored or uncharted. With the high rate of data expansion, companies are\\nscrambling to develop ways to develop near-real-time predictive analytics, data\\nmining and machine learning capabilities, and are expanding their data storage\\ninfrastructure and resources. With these new goals, come the challenges of\\nmanaging data growth, integrating intelligence tools, and analyzing the data to\\nglean useful insights. Oil and Gas companies need data solutions to\\neconomically extract value from very large volumes of a wide variety of data\\ngenerated from exploration, well drilling and production devices and sensors.\\n  Data mining for oil and gas industry throughout the lifecycle of the\\nreservoir includes the following roles: locating hydrocarbons, managing\\ngeological data, drilling and formation evaluation, well construction, well\\ncompletion, and optimizing production through the life of the oil field. For\\neach of these phases during the lifecycle of oil field, data mining play a\\nsignificant role. Based on which phase were talking about, knowledge creation\\nthrough scientific models, data analytics and machine learning, a effective,\\nproductive, and on demand data insight is critical for decision making within\\nthe organization.\\n  The significant challenges posed by this complex and economically vital field\\njustify a meeting of data scientists that are willing to share their experience\\nand knowledge. Thus, the Worskhop on Data Mining for Oil and Gas (DM4OG) aims\\nto provide a quality forum for researchers that work on the significant\\nchallenges arising from the synergy between data science, machine learning, and\\nthe modeling and optimization problems in the O&G industry. Linear Discriminant Analysis (LDA) is a well-known technique for feature\\nextraction and dimension reduction. The performance of classical LDA, however,\\nsignificantly degrades on the High Dimension Low Sample Size (HDLSS) data for\\nthe ill-posed inverse problem. Existing approaches for HDLSS data\\nclassification typically assume the data in question are with Gaussian\\ndistribution and deal the HDLSS classification problem with regularization.\\nHowever, these assumptions are too strict to hold in many emerging real-life\\napplications, such as enabling personalized predictive analysis using\\nElectronic Health Records (EHRs) data collected from an extremely limited\\nnumber of patients who have been diagnosed with or without the target disease\\nfor prediction. In this paper, we revised the problem of predictive analysis of\\ndisease using personal EHR data and LDA classifier. To fill the gap, in this\\npaper, we first studied an analytical model that understands the accuracy of\\nLDA for classifying data with arbitrary distribution. The model gives a\\ntheoretical upper bound of LDA error rate that is controlled by two factors:\\n(1) the statistical convergence rate of (inverse) covariance matrix estimators\\nand (2) the divergence of the training/testing datasets to fitted\\ndistributions. To this end, we could lower the error rate by balancing the two\\nfactors for better classification performance. Hereby, we further proposed a\\nnovel LDA classifier De-Sparse that leverages De-sparsified Graphical Lasso to\\nimprove the estimation of LDA, which outperforms state-of-the-art LDA\\napproaches developed for HDLSS data. Such advances and effectiveness are\\nfurther demonstrated by both theoretical analysis and extensive experiments on\\nEHR datasets. With the advancement of huge data generation and data handling capability,\\nMachine Learning and Probabilistic modelling enables an immense opportunity to\\nemploy predictive analytics platform in high security critical industries\\nnamely data centers, electricity grids, utilities, airport etc. where downtime\\nminimization is one of the primary objectives. This paper proposes a novel,\\ncomplete architecture of an intelligent predictive analytics platform, Fault\\nEngine, for huge device network connected with electrical/information flow.\\nThree unique modules, here proposed, seamlessly integrate with available\\ntechnology stack of data handling and connect with middleware to produce online\\nintelligent prediction in critical failure scenarios. The Markov Failure module\\npredicts the severity of a failure along with survival probability of a device\\nat any given instances. The Root Cause Analysis model indicates probable\\ndevices as potential root cause employing Bayesian probability assignment and\\ntopological sort. Finally, a community detection algorithm produces correlated\\nclusters of device in terms of failure probability which will further narrow\\ndown the search space of finding route cause. The whole Engine has been tested\\nwith different size of network with simulated failure environments and shows\\nits potential to be scalable in real-time implementation. Table (database) / Relational database Classification for big/smart/fast data\\nmachine learning is one of the most important tasks of predictive analytics and\\nextracting valuable information from data. It is core applied technique for\\nwhat now understood under data science and/or artificial intelligence. Widely\\nused Decision Tree (Random Forest) and rare used rule based PRISM , VFST, etc\\nclassifiers are empirical substitutions of theoretically correct to use Boolean\\nfunctions minimization. Developing Minimization of Boolean functions algorithms\\nis started long time ago by Edward Veitch's 1952. Since it, big efforts by wide\\nscientific/industrial community was done to find feasible solution of Boolean\\nfunctions minimization. In this paper we propose consider table data\\nclassification from mathematical point of view, as minimization of Boolean\\nfunctions. It is shown that data representation may be transformed to Boolean\\nfunctions form and how to use known algorithms. For simplicity, binary output\\nfunction is used for development, what opens doors for multivalued outputs\\ndevelopments. Machine learning (ML) models may be deemed confidential due to their\\nsensitive training data, commercial value, or use in security applications.\\nIncreasingly often, confidential ML models are being deployed with publicly\\naccessible query interfaces. ML-as-a-service (\\\"predictive analytics\\\") systems\\nare an example: Some allow users to train models on potentially sensitive data\\nand charge others for access on a pay-per-query basis.\\n  The tension between model confidentiality and public access motivates our\\ninvestigation of model extraction attacks. In such attacks, an adversary with\\nblack-box access, but no prior knowledge of an ML model's parameters or\\ntraining data, aims to duplicate the functionality of (i.e., \\\"steal\\\") the\\nmodel. Unlike in classical learning theory settings, ML-as-a-service offerings\\nmay accept partial feature vectors as inputs and include confidence values with\\npredictions. Given these practices, we show simple, efficient attacks that\\nextract target ML models with near-perfect fidelity for popular model classes\\nincluding logistic regression, neural networks, and decision trees. We\\ndemonstrate these attacks against the online services of BigML and Amazon\\nMachine Learning. We further show that the natural countermeasure of omitting\\nconfidence values from model outputs still admits potentially harmful model\\nextraction attacks. Our results highlight the need for careful ML model\\ndeployment and new model extraction countermeasures. Availability of both massive datasets and computing resources have made\\nmachine learning and predictive analytics extremely pervasive. In this work we\\npresent a synchronous algorithm and architecture for distributed optimization\\nmotivated by privacy requirements posed by applications in machine learning. We\\npresent an algorithm for the recently proposed multi-parameter-server\\narchitecture. We consider a group of parameter servers that learn a model based\\non randomized gradients received from clients. Clients are computational\\nentities with private datasets (inducing a private objective function), that\\nevaluate and upload randomized gradients to the parameter servers. The\\nparameter servers perform model updates based on received gradients and share\\nthe model parameters with other servers. We prove that the proposed algorithm\\ncan optimize the overall objective function for a very general architecture\\ninvolving $C$ clients connected to $S$ parameter servers in an arbitrary time\\nvarying topology and the parameter servers forming a connected network. Deep learning became the method of choice in recent year for solving a wide\\nvariety of predictive analytics tasks. For sequence prediction, recurrent\\nneural networks (RNN) are often the go-to architecture for exploiting\\nsequential information where the output is dependent on previous computation.\\nHowever, the dependencies of the computation lie in the latent domain which may\\nnot be suitable for certain applications involving the prediction of a\\nstep-wise transformation sequence that is dependent on the previous computation\\nonly in the visible domain. We propose that a hybrid architecture of\\nconvolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient\\nto learn a sequence of actions that nonlinearly transforms an input shape or\\ndistribution into a target shape or distribution with the same support. While\\nsuch a framework can be useful in a variety of problems such as robotic path\\nplanning, sequential decision-making in games, and identifying material\\nprocessing pathways to achieve desired microstructures, the application of the\\nframework is exemplified by the control of fluid deformations in a microfluidic\\nchannel by deliberately placing a sequence of pillars. Learning of a multistep\\ntopological transform has significant implications for rapid advances in\\nmaterial science and biomedical applications. This document describes the R package UBL that allows the use of several\\nmethods for handling utility-based learning problems. Classification and\\nregression problems that assume non-uniform costs and/or benefits pose serious\\nchallenges to predictive analytic tasks. In the context of meteorology,\\nfinance, medicine, ecology, among many other, specific domain information\\nconcerning the preference bias of the users must be taken into account to\\nenhance the models predictive performance. To deal with this problem, a large\\nnumber of techniques was proposed by the research community for both\\nclassification and regression tasks. The main goal of UBL package is to\\nfacilitate the utility-based predictive analytic task by providing a set of\\nmethods to deal with this type of problems in the R environment. It is a\\nversatile tool that provides mechanisms to handle both regression and\\nclassification (binary and multiclass) tasks. Moreover, UBL package allows the\\nuser to specify his domain preferences, but it also provides some automatic\\nmethods that try to infer those preference bias from the domain, considering\\nsome common known settings. This work is motivated by the needs of predictive analytics on healthcare\\ndata as represented by Electronic Medical Records. Such data is invariably\\nproblematic: noisy, with missing entries, with imbalance in classes of\\ninterests, leading to serious bias in predictive modeling. Since standard data\\nmining methods often produce poor performance measures, we argue for\\ndevelopment of specialized techniques of data-preprocessing and classification.\\nIn this paper, we propose a new method to simultaneously classify large\\ndatasets and reduce the effects of missing values. It is based on a multilevel\\nframework of the cost-sensitive SVM and the expected maximization imputation\\nmethod for missing values, which relies on iterated regression analyses. We\\ncompare classification results of multilevel SVM-based algorithms on public\\nbenchmark datasets with imbalanced classes and missing values as well as real\\ndata in health applications, and show that our multilevel SVM-based method\\nproduces fast, and more accurate and robust classification results. The behaviors of patients with depression are usually difficult to predict\\nbecause the patients demonstrate the symptoms of a depressive episode without a\\nwarning at unexpected times. The goal of this research is to build algorithms\\nthat detect signals of such unusual moments so that doctors can be proactive in\\napproaching already diagnosed patients before they fall in depression. Each\\npatient is equipped with a smartphone with the capability to track its sensors.\\nWe first find the home location of a patient, which is then augmented with\\nother sensor data to identify sleep patterns and select communication patterns.\\nThe algorithms require two to three weeks of training data to build standard\\npatterns, which are considered normal behaviors; and then, the methods identify\\nany anomalies in day-to-day data readings of sensors. Four smartphone sensors,\\nincluding the accelerometer, the gyroscope, the location probe and the\\ncommunication log probe are used for anomaly detection in sleeping and\\ncommunication patterns. The rise of Big Data has led to new demands for Machine Learning (ML) systems\\nto learn complex models with millions to billions of parameters, that promise\\nadequate capacity to digest massive datasets and offer powerful predictive\\nanalytics thereupon. In order to run ML algorithms at such scales, on a\\ndistributed cluster with 10s to 1000s of machines, it is often the case that\\nsignificant engineering efforts are required --- and one might fairly ask if\\nsuch engineering truly falls within the domain of ML research or not. Taking\\nthe view that Big ML systems can benefit greatly from ML-rooted statistical and\\nalgorithmic insights --- and that ML researchers should therefore not shy away\\nfrom such systems design --- we discuss a series of principles and strategies\\ndistilled from our recent efforts on industrial-scale ML solutions. These\\nprinciples and strategies span a continuum from application, to engineering,\\nand to theoretical research and development of Big ML systems and\\narchitectures, with the goal of understanding how to make them efficient,\\ngenerally-applicable, and supported with convergence and scaling guarantees.\\nThey concern four key questions which traditionally receive little attention in\\nML research: How to distribute an ML program over a cluster? How to bridge ML\\ncomputation with inter-machine communication? How to perform such\\ncommunication? What should be communicated between machines? By exposing\\nunderlying statistical and algorithmic characteristics unique to ML programs\\nbut not typically seen in traditional computer programs, and by dissecting\\nsuccessful cases to reveal how we have harnessed these principles to design and\\ndevelop both high-performance distributed ML software as well as\\ngeneral-purpose ML frameworks, we present opportunities for ML researchers and\\npractitioners to further shape and grow the area that lies between ML and\\nsystems. This article presents the ALOJA project and its analytics tools, which\\nleverages machine learning to interpret Big Data benchmark performance data and\\ntuning. ALOJA is part of a long-term collaboration between BSC and Microsoft to\\nautomate the characterization of cost-effectiveness on Big Data deployments,\\ncurrently focusing on Hadoop. Hadoop presents a complex run-time environment,\\nwhere costs and performance depend on a large number of configuration choices.\\nThe ALOJA project has created an open, vendor-neutral repository, featuring\\nover 40,000 Hadoop job executions and their performance details. The repository\\nis accompanied by a test-bed and tools to deploy and evaluate the\\ncost-effectiveness of different hardware configurations, parameters and Cloud\\nservices. Despite early success within ALOJA, a comprehensive study requires\\nautomation of modeling procedures to allow an analysis of large and\\nresource-constrained search spaces. The predictive analytics extension,\\nALOJA-ML, provides an automated system allowing knowledge discovery by modeling\\nenvironments from observed executions. The resulting models can forecast\\nexecution behaviors, predicting execution times for new configurations and\\nhardware choices. That also enables model-based anomaly detection or efficient\\nbenchmark guidance by prioritizing executions. In addition, the community can\\nbenefit from ALOJA data-sets and framework to improve the design and deployment\\nof Big Data applications. Telecommunications operators (telcos) traditional sources of income, voice\\nand SMS, are shrinking due to customers using over-the-top (OTT) applications\\nsuch as WhatsApp or Viber. In this challenging environment it is critical for\\ntelcos to maintain or grow their market share, by providing users with as good\\nan experience as possible on their network.\\n  But the task of extracting customer insights from the vast amounts of data\\ncollected by telcos is growing in complexity and scale everey day. How can we\\nmeasure and predict the quality of a user's experience on a telco network in\\nreal-time? That is the problem that we address in this paper.\\n  We present an approach to capture, in (near) real-time, the mobile customer\\nexperience in order to assess which conditions lead the user to place a call to\\na telco's customer care center. To this end, we follow a supervised learning\\napproach for prediction and train our 'Restricted Random Forest' model using,\\nas a proxy for bad experience, the observed customer transactions in the telco\\ndata feed before the user places a call to a customer care center.\\n  We evaluate our approach using a rich dataset provided by a major African\\ntelecommunication's company and a novel big data architecture for both the\\ntraining and scoring of predictive models. Our empirical study shows our\\nsolution to be effective at predicting user experience by inferring if a\\ncustomer will place a call based on his current context.\\n  These promising results open new possibilities for improved customer service,\\nwhich will help telcos to reduce churn rates and improve customer experience,\\nboth factors that directly impact their revenue growth. Nowadays, engineers have to develop software often without even knowing which\\nhardware it will eventually run on in numerous mobile phones, tablets,\\ndesktops, laptops, data centers, supercomputers and cloud services.\\nUnfortunately, optimizing compilers are not keeping pace with ever increasing\\ncomplexity of computer systems anymore and may produce severely underperforming\\nexecutable codes while wasting expensive resources and energy.\\n  We present our practical and collaborative solution to this problem via\\nlight-weight wrappers around any software piece when more than one\\nimplementation or optimization choice available. These wrappers are connected\\nwith a public Collective Mind autotuning infrastructure and repository of\\nknowledge (c-mind.org/repo) to continuously monitor various important\\ncharacteristics of these pieces (computational species) across numerous\\nexisting hardware configurations together with randomly selected optimizations.\\nSimilar to natural sciences, we can now continuously track winning solutions\\n(optimizations for a given hardware) that minimize all costs of a computation\\n(execution time, energy spent, code size, failures, memory and storage\\nfootprint, optimization time, faults, contentions, inaccuracy and so on) of a\\ngiven species on a Pareto frontier along with any unexpected behavior. The\\ncommunity can then collaboratively classify solutions, prune redundant ones,\\nand correlate them with various features of software, its inputs (data sets)\\nand used hardware either manually or using powerful predictive analytics\\ntechniques. Our approach can then help create a large, realistic, diverse,\\nrepresentative, and continuously evolving benchmark with related optimization\\nknowledge while gradually covering all possible software and hardware to be\\nable to predict best optimizations and improve compilers and hardware depending\\non usage scenarios and requirements. Many real world data mining applications involve obtaining predictive models\\nusing data sets with strongly imbalanced distributions of the target variable.\\nFrequently, the least common values of this target variable are associated with\\nevents that are highly relevant for end users (e.g. fraud detection, unusual\\nreturns on stock markets, anticipation of catastrophes, etc.). Moreover, the\\nevents may have different costs and benefits, which when associated with the\\nrarity of some of them on the available training data creates serious problems\\nto predictive modelling techniques. This paper presents a survey of existing\\ntechniques for handling these important applications of predictive analytics.\\nAlthough most of the existing work addresses classification tasks (nominal\\ntarget variables), we also describe methods designed to handle similar problems\\nwithin regression tasks (numeric target variables). In this survey we discuss\\nthe main challenges raised by imbalanced distributions, describe the main\\napproaches to these problems, propose a taxonomy of these methods and refer to\\nsome related problems within predictive modelling. This paper provides a theoretical and computational justification of the long\\nheld claim that of the similarity of the probit and logit link functions often\\nused in binary classification. Despite this widespread recognition of the\\nstrong similarities between these two link functions, very few (if any)\\nresearchers have dedicated time to carry out a formal study aimed at\\nestablishing and characterizing firmly all the aspects of the similarities and\\ndifferences. This paper proposes a definition of both structural and predictive\\nequivalence of link functions-based binary regression models, and explores the\\nvarious ways in which they are either similar or dissimilar. From a predictive\\nanalytics perspective, it turns out that not only are probit and logit\\nperfectly predictively concordant, but the other link functions like cauchit\\nand complementary log log enjoy very high percentage of predictive equivalence.\\nThroughout this paper, simulated and real life examples demonstrate all the\\nequivalence results that we prove theoretically. The proliferation of massive datasets combined with the development of\\nsophisticated analytical techniques have enabled a wide variety of novel\\napplications such as improved product recommendations, automatic image tagging,\\nand improved speech-driven interfaces. These and many other applications can be\\nsupported by Predictive Analytic Queries (PAQs). A major obstacle to supporting\\nPAQs is the challenging and expensive process of identifying and training an\\nappropriate predictive model. Recent efforts aiming to automate this process\\nhave focused on single node implementations and have assumed that model\\ntraining itself is a black box, thus limiting the effectiveness of such\\napproaches on large-scale problems. In this work, we build upon these recent\\nefforts and propose an integrated PAQ planning architecture that combines\\nadvanced model search techniques, bandit resource allocation via runtime\\nalgorithm introspection, and physical optimization via batching. The result is\\nTuPAQ, a component of the MLbase system, which solves the PAQ planning problem\\nwith comparable quality to exhaustive strategies but an order of magnitude more\\nefficiently than the standard baseline approach, and can scale to models\\ntrained on terabytes of data across hundreds of machines. In view of the paradigm shift that makes science ever more data-driven, in\\nthis thesis we propose a synthesis method for encoding and managing large-scale\\ndeterministic scientific hypotheses as uncertain and probabilistic data.\\n  In the form of mathematical equations, hypotheses symmetrically relate\\naspects of the studied phenomena. For computing predictions, however,\\ndeterministic hypotheses can be abstracted as functions. We build upon Simon's\\nnotion of structural equations in order to efficiently extract the (so-called)\\ncausal ordering between variables, implicit in a hypothesis structure (set of\\nmathematical equations).\\n  We show how to process the hypothesis predictive structure effectively\\nthrough original algorithms for encoding it into a set of functional\\ndependencies (fd's) and then performing causal reasoning in terms of acyclic\\npseudo-transitive reasoning over fd's. Such reasoning reveals important causal\\ndependencies implicit in the hypothesis predictive data and guide our synthesis\\nof a probabilistic database. Like in the field of graphical models in AI, such\\na probabilistic database should be normalized so that the uncertainty arisen\\nfrom competing hypotheses is decomposed into factors and propagated properly\\nonto predictive data by recovering its joint probability distribution through a\\nlossless join. That is motivated as a design-theoretic principle for\\ndata-driven hypothesis management and predictive analytics.\\n  The method is applicable to both quantitative and qualitative deterministic\\nhypotheses and demonstrated in realistic use cases from computational science. Online services routinely mine user data to predict user preferences, make\\nrecommendations, and place targeted ads. Recent research has demonstrated that\\nseveral private user attributes (such as political affiliation, sexual\\norientation, and gender) can be inferred from such data. Can a\\nprivacy-conscious user benefit from personalization while simultaneously\\nprotecting her private attributes? We study this question in the context of a\\nrating prediction service based on matrix factorization. We construct a\\nprotocol of interactions between the service and users that has remarkable\\noptimality properties: it is privacy-preserving, in that no inference algorithm\\ncan succeed in inferring a user's private attribute with a probability better\\nthan random guessing; it has maximal accuracy, in that no other\\nprivacy-preserving protocol improves rating prediction; and, finally, it\\ninvolves a minimal disclosure, as the prediction accuracy strictly decreases\\nwhen the service reveals less information. We extensively evaluate our protocol\\nusing several rating datasets, demonstrating that it successfully blocks the\\ninference of gender, age and political affiliation, while incurring less than\\n5% decrease in the accuracy of rating prediction. Hospital readmission has become a critical metric of quality and cost of\\nhealthcare. Medicare anticipates that nearly $17 billion is paid out on the 20%\\nof patients who are readmitted within 30 days of discharge. Although several\\ninterventions such as transition care management and discharge reengineering\\nhave been practiced in recent years, the effectiveness and sustainability\\ndepends on how well they can identify and target patients at high risk of\\nrehospitalization. Based on the literature, most current risk prediction models\\nfail to reach an acceptable accuracy level; none of them considers patient's\\nhistory of readmission and impacts of patient attribute changes over time; and\\nthey often do not discriminate between planned and unnecessary readmissions.\\nTackling such drawbacks, we develop a new readmission metric based on\\nadministrative data that can identify potentially avoidable readmissions from\\nall other types of readmission. We further propose a tree based classification\\nmethod to estimate the predicted probability of readmission that can directly\\nincorporate patient's history of readmission and risk factors changes over\\ntime. The proposed methods are validated with 2011-12 Veterans Health\\nAdministration data from inpatients hospitalized for heart failure, acute\\nmyocardial infarction, pneumonia, or chronic obstructive pulmonary disease in\\nthe State of Michigan. Results shows improved discrimination power compared to\\nthe literature (c-statistics>80%) and good calibration.\",\n          \"Statistical learning theory is the foundation of machine learning, providing\\ntheoretical bounds for the risk of models learnt from a (single) training set,\\nassumed to issue from an unknown probability distribution. In actual\\ndeployment, however, the data distribution may (and often does) vary, causing\\ndomain adaptation/generalization issues. In this paper we lay the foundations\\nfor a `credal' theory of learning, using convex sets of probabilities (credal\\nsets) to model the variability in the data-generating distribution. Such credal\\nsets, we argue, may be inferred from a finite sample of training sets. Bounds\\nare derived for the case of finite hypotheses spaces (both assuming\\nrealizability or not) as well as infinite model spaces, which directly\\ngeneralize classical results. Nearest-neighbor methods have become popular in statistics and play a key\\nrole in statistical learning. Important decisions in nearest-neighbor methods\\nconcern the variables to use (when many potential candidates exist) and how to\\nmeasure the dissimilarity between units. The first decision depends on the\\nscope of the application while second depends mainly on the type of variables.\\nUnfortunately, relatively few options permit to handle mixed-type variables, a\\nsituation frequently encountered in practical applications. The most popular\\ndissimilarity for mixed-type variables is derived as the complement to one of\\nthe Gower's similarity coefficient. It is appealing because ranges between 0\\nand 1, being an average of the scaled dissimilarities calculated variable by\\nvariable, handles missing values and allows for a user-defined weighting scheme\\nwhen averaging dissimilarities. The discussion on the weighting schemes is\\nsometimes misleading since it often ignores that the unweighted \\\"standard\\\"\\nsetting hides an unbalanced contribution of the single variables to the overall\\ndissimilarity. We address this drawback following the recent idea of\\nintroducing a weighting scheme that minimizes the differences in the\\ncorrelation between each contributing dissimilarity and the resulting weighted\\nGower's dissimilarity. In particular, this note proposes different approaches\\nfor measuring the correlation depending on the type of variables. The\\nperformances of the proposed approaches are evaluated in simulation studies\\nrelated to classification and imputation of missing values. In today's data-driven world, the proliferation of publicly available\\ninformation intensifies the challenge of information leakage (IL), raising\\nsecurity concerns. IL involves unintentionally exposing secret (sensitive)\\ninformation to unauthorized parties via systems' observable information.\\nConventional statistical approaches, which estimate mutual information (MI)\\nbetween observable and secret information for detecting IL, face challenges\\nsuch as the curse of dimensionality, convergence, computational complexity, and\\nMI misestimation. Furthermore, emerging supervised machine learning (ML)\\nmethods, though effective, are limited to binary system-sensitive information\\nand lack a comprehensive theoretical framework. To address these limitations,\\nwe establish a theoretical framework using statistical learning theory and\\ninformation theory to accurately quantify and detect IL. We demonstrate that MI\\ncan be accurately estimated by approximating the log-loss and accuracy of the\\nBayes predictor. As the Bayes predictor is typically unknown in practice, we\\npropose to approximate it with the help of automated machine learning (AutoML).\\nFirst, we compare our MI estimation approaches against current baselines, using\\nsynthetic data sets generated using the multivariate normal (MVN) distribution\\nwith known MI. Second, we introduce a cut-off technique using one-sided\\nstatistical tests to detect IL, employing the Holm-Bonferroni correction to\\nincrease confidence in detection decisions. Our study evaluates IL detection\\nperformance on real-world data sets, highlighting the effectiveness of the\\nBayes predictor's log-loss estimation, and finds our proposed method to\\neffectively estimate MI on synthetic data sets and thus detect ILs accurately. The decision making involved behind the mode choice is critical for\\ntransportation planning. While statistical learning techniques like discrete\\nchoice models have been used traditionally, machine learning (ML) models have\\ngained traction recently among the transportation planners due to their higher\\npredictive performance. However, the black box nature of ML models pose\\nsignificant interpretability challenges, limiting their practical application\\nin decision and policy making. This study utilised a dataset of $1350$\\nhouseholds belonging to low and low-middle income bracket in the city of\\nBengaluru to investigate mode choice decision making behaviour using\\nMultinomial logit model and ML classifiers like decision trees, random forests,\\nextreme gradient boosting and support vector machines. In terms of accuracy,\\nrandom forest model performed the best ($0.788$ on training data and $0.605$ on\\ntesting data) compared to all the other models. This research has adopted\\nmodern interpretability techniques like feature importance and individual\\nconditional expectation plots to explain the decision making behaviour using ML\\nmodels. A higher travel costs significantly reduce the predicted probability of\\nbus usage compared to other modes (a $0.66\\\\%$ and $0.34\\\\%$ reduction using\\nRandom Forests and XGBoost model for $10\\\\%$ increase in travel cost). However,\\nreducing travel time by $10\\\\%$ increases the preference for the metro ($0.16\\\\%$\\nin Random Forests and 0.42% in XGBoost). This research augments the ongoing\\nresearch on mode choice analysis using machine learning techniques, which would\\nhelp in improving the understanding of the performance of these models with\\nreal-world data in terms of both accuracy and interpretability. Recent works demonstrated the existence of a double-descent phenomenon for\\nthe generalization error of neural networks, where highly overparameterized\\nmodels escape overfitting and achieve good test performance, at odds with the\\nstandard bias-variance trade-off described by statistical learning theory. In\\nthe present work, we explore a link between this phenomenon and the increase of\\ncomplexity and sensitivity of the function represented by neural networks. In\\nparticular, we study the Boolean mean dimension (BMD), a metric developed in\\nthe context of Boolean function analysis. Focusing on a simple teacher-student\\nsetting for the random feature model, we derive a theoretical analysis based on\\nthe replica method that yields an interpretable expression for the BMD, in the\\nhigh dimensional regime where the number of data points, the number of\\nfeatures, and the input size grow to infinity. We find that, as the degree of\\noverparameterization of the network is increased, the BMD reaches an evident\\npeak at the interpolation threshold, in correspondence with the generalization\\nerror peak, and then slowly approaches a low asymptotic value. The same\\nphenomenology is then traced in numerical experiments with different model\\nclasses and training setups. Moreover, we find empirically that adversarially\\ninitialized models tend to show higher BMD values, and that models that are\\nmore robust to adversarial attacks exhibit a lower BMD. Quantum machine learning, which involves running machine learning algorithms\\non quantum devices, has garnered significant attention in both academic and\\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\\nof the various concepts that have emerged in the field of quantum machine\\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\\n(NISQ) technologies and approaches for algorithms compatible with\\nfault-tolerant quantum computing hardware. Our review covers fundamental\\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\\nmachine learning. Recent Newton-type federated learning algorithms have demonstrated linear\\nconvergence with respect to the communication rounds. However, communicating\\nHessian matrices is often unfeasible due to their quadratic communication\\ncomplexity. In this paper, we introduce a novel approach to tackle this issue\\nwhile still achieving fast convergence rates. Our proposed method, named as\\nFederated Newton Sketch methods (FedNS), approximates the centralized Newton's\\nmethod by communicating the sketched square-root Hessian instead of the exact\\nHessian. To enhance communication efficiency, we reduce the sketch size to\\nmatch the effective dimension of the Hessian matrix. We provide convergence\\nanalysis based on statistical learning for the federated Newton sketch\\napproaches. Specifically, our approaches reach super-linear convergence rates\\nw.r.t. the communication rounds for the first time. We validate the\\neffectiveness of our algorithms through various experiments, which coincide\\nwith our theoretical findings. Simplicity bias is an intriguing phenomenon prevalent in various input-output\\nmaps, characterized by a preference for simpler, more regular, or symmetric\\noutputs. Notably, these maps typically feature high-probability outputs with\\nsimple patterns, whereas complex patterns are exponentially less probable. This\\nbias has been extensively examined and attributed to principles derived from\\nalgorithmic information theory and algorithmic probability. In a significant\\nadvancement, it has been demonstrated that the renowned logistic map\\n$x_{k+1}=\\\\mu x_k(1-x_k)$, and other one-dimensional maps exhibit simplicity\\nbias when conceptualized as input-output systems. Building upon this\\nfoundational work, our research delves into the manifestations of simplicity\\nbias within the random logistic map, specifically focusing on scenarios\\ninvolving additive noise. This investigation is driven by the overarching goal\\nof formulating a comprehensive theory for the prediction and analysis of time\\nseries.Our primary contributions are multifaceted. We discover that simplicity\\nbias is observable in the random logistic map for specific ranges of $\\\\mu$ and\\nnoise magnitudes. Additionally, we find that this bias persists even with the\\nintroduction of small measurement noise, though it diminishes as noise levels\\nincrease. Our studies also revisit the phenomenon of noise-induced chaos,\\nparticularly when $\\\\mu=3.83$, revealing its characteristics through\\ncomplexity-probability plots. Intriguingly, we employ the logistic map to\\nunderscore a paradoxical aspect of data analysis: more data adhering to a\\nconsistent trend can occasionally lead to reduced confidence in extrapolation\\npredictions, challenging conventional wisdom.We propose that adopting a\\nprobability-complexity perspective in analyzing dynamical systems could\\nsignificantly enrich statistical learning theories related to series\\nprediction. The author's goal in this paper is to explore how artificial intelligence\\n(AI) has been utilised to inform our understanding of and ability to estimate\\nat scale a critical aspect of musical creativity - musical tempo. The central\\nimportance of tempo to musical creativity can be seen in how it is used to\\nexpress specific emotions (Eerola and Vuoskoski 2013), suggest particular\\nmusical styles (Li and Chan 2011), influence perception of expression (Webster\\nand Weir 2005) and mediate the urge to move one's body in time to the music\\n(Burger et al. 2014). Traditional tempo estimation methods typically detect\\nsignal periodicities that reflect the underlying rhythmic structure of the\\nmusic, often using some form of autocorrelation of the amplitude envelope\\n(Lartillot and Toiviainen 2007). Recently, AI-based methods utilising\\nconvolutional or recurrent neural networks (CNNs, RNNs) on spectral\\nrepresentations of the audio signal have enjoyed significant improvements in\\naccuracy (Aarabi and Peeters 2022). Common AI-based techniques include those\\nbased on probability (e.g., Bayesian approaches, hidden Markov models (HMM)),\\nclassification and statistical learning (e.g., support vector machines (SVM)),\\nand artificial neural networks (ANNs) (e.g., self-organising maps (SOMs), CNNs,\\nRNNs, deep learning (DL)). The aim here is to provide an overview of some of\\nthe more common AI-based tempo estimation algorithms and to shine a light on\\nnotable benefits and potential drawbacks of each. Limitations of AI in this\\nfield in general are also considered, as is the capacity for such methods to\\naccount for idiosyncrasies inherent in tempo perception, i.e., how well\\nAI-based approaches are able to think and act like humans. Inverse reinforcement learning (IRL) usually assumes the model of the reward\\nfunction is pre-specified and estimates the parameter only. However, how to\\ndetermine a proper reward model is nontrivial. A simplistic model is less\\nlikely to contain the real reward function, while a model with high complexity\\nleads to substantial computation cost and risks overfitting. This paper\\naddresses this trade-off in IRL model selection by introducing the structural\\nrisk minimization (SRM) method from statistical learning. SRM selects an\\noptimal reward function class from a hypothesis set minimizing both estimation\\nerror and model complexity. To formulate an SRM scheme for IRL, we estimate\\npolicy gradient by demonstration serving as empirical risk and establish the\\nupper bound of Rademacher complexity of hypothesis classes as model penalty.\\nThe learning guarantee is further presented. In particular, we provide explicit\\nSRM for the common linear weighted sum setting in IRL. Simulations demonstrate\\nthe performance and efficiency of our scheme. We provide an overview of recent progress in statistical inverse problems\\nwith random experimental design, covering both linear and nonlinear inverse\\nproblems. Different regularization schemes have been studied to produce robust\\nand stable solutions. We discuss recent results in spectral regularization\\nmethods and regularization by projection, exploring both approaches within the\\ncontext of Hilbert scales and presenting new insights particularly in\\nregularization by projection. Additionally, we overview recent advancements in\\nregularization using convex penalties. Convergence rates are analyzed in terms\\nof the sample size in a probabilistic sense, yielding minimax rates in both\\nexpectation and probability. To achieve these results, the structure of\\nreproducing kernel Hilbert spaces is leveraged to establish minimax rates in\\nthe statistical learning setting. We detail the assumptions underpinning these\\nkey elements of our proofs. Finally, we demonstrate the application of these\\nconcepts to nonlinear inverse problems in pharmacokinetic/pharmacodynamic\\n(PK/PD) models, where the task is to predict changes in drug concentrations in\\npatients. We present a novel quantum high-dimensional linear regression algorithm with\\nan $\\\\ell_1$-penalty based on the classical LARS (Least Angle Regression)\\npathwise algorithm. Similarly to available classical numerical algorithms for\\nLasso, our quantum algorithm provides the full regularisation path as the\\npenalty term varies, but quadratically faster per iteration under specific\\nconditions. A quadratic speedup on the number of features/predictors $d$ is\\npossible by using the simple quantum minimum-finding subroutine from D\\\\\\\"urr and\\nHoyer (arXiv'96) in order to obtain the joining time at each iteration. We then\\nimprove upon this simple quantum algorithm and obtain a quadratic speedup both\\nin the number of features $d$ and the number of observations $n$ by using the\\nrecent approximate quantum minimum-finding subroutine from Chen and de Wolf\\n(ICALP'23). As one of our main contributions, we construct a quantum unitary\\nbased on quantum amplitude estimation to approximately compute the joining\\ntimes to be searched over by the approximate quantum minimum finding. Since the\\njoining times are no longer exactly computed, it is no longer clear that the\\nresulting approximate quantum algorithm obtains a good solution. As our second\\nmain contribution, we prove, via an approximate version of the KKT conditions\\nand a duality gap, that the LARS algorithm (and therefore our quantum\\nalgorithm) is robust to errors. This means that it still outputs a path that\\nminimises the Lasso cost function up to a small error if the joining times are\\nonly approximately computed. Finally, in the model where the observations are\\ngenerated by an underlying linear model with an unknown coefficient vector, we\\nprove bounds on the difference between the unknown coefficient vector and the\\napproximate Lasso solution, which generalises known results about convergence\\nrates in classical statistical learning theory analysis. Learning-based and data-driven techniques have recently become a subject of\\nprimary interest in the field of reconstruction and regularization of inverse\\nproblems. Besides the development of novel methods, yielding excellent results\\nin several applications, their theoretical investigation has attracted growing\\ninterest, e.g., on the topics of reliability, stability, and interpretability.\\nIn this work, a general framework is described, allowing us to interpret many\\nof these techniques in the context of statistical learning. This is not\\nintended to provide a complete survey of existing methods, but rather to put\\nthem in a working perspective, which naturally allows their theoretical\\ntreatment. The main goal of this dissertation is thereby to address the\\ngeneralization properties of learned reconstruction methods, and specifically\\nto perform their sample error analysis. This task, well-developed in\\nstatistical learning, consists in estimating the dependence of the learned\\noperators with respect to the data employed for their training. A rather\\ngeneral strategy is proposed, whose assumptions are met for a large class of\\ninverse problems and learned methods, as depicted via a selection of examples. This paper considers the epistemic justification for a simplicity preference\\nin inductive inference that may be obtained from the machine learning framework\\nof statistical learning theory. Uniting elements from both earlier arguments\\nsuggesting and rejecting such a justification, the paper spells out a qualified\\nmeans-ends and model-relative justificatory argument, built on statistical\\nlearning theory's central mathematical learning guarantee for the method of\\nempirical risk minimization. In this paper, we discuss a potential agenda for future work in the theory of\\nrandom sets and belief functions, touching upon a number of focal issues: the\\ndevelopment of a fully-fledged theory of statistical reasoning with random\\nsets, including the generalisation of logistic regression and of the classical\\nlaws of probability; the further development of the geometric approach to\\nuncertainty, to include general random sets, a wider range of uncertainty\\nmeasures and alternative geometric representations; the application of this new\\ntheory to high-impact areas such as climate change, machine learning and\\nstatistical learning theory. We study the problem of $(\\\\epsilon,\\\\delta)$-certified machine unlearning for\\nminimax models. Most of the existing works focus on unlearning from standard\\nstatistical learning models that have a single variable and their unlearning\\nsteps hinge on the direct Hessian-based conventional Newton update. We develop\\na new $(\\\\epsilon,\\\\delta)$-certified machine unlearning algorithm for minimax\\nmodels. It proposes a minimax unlearning step consisting of a\\ntotal-Hessian-based complete Newton update and the Gaussian mechanism borrowed\\nfrom differential privacy. To obtain the unlearning certification, our method\\ninjects calibrated Gaussian noises by carefully analyzing the \\\"sensitivity\\\" of\\nthe minimax unlearning step (i.e., the closeness between the minimax unlearning\\nvariables and the retraining-from-scratch variables). We derive the\\ngeneralization rates in terms of population strong and weak primal-dual risk\\nfor three different cases of loss functions, i.e.,\\n(strongly-)convex-(strongly-)concave losses. We also provide the deletion\\ncapacity to guarantee that a desired population risk can be maintained as long\\nas the number of deleted samples does not exceed the derived amount. With\\ntraining samples $n$ and model dimension $d$, it yields the order $\\\\mathcal\\nO(n/d^{1/4})$, which shows a strict gap over the baseline method of\\ndifferentially private minimax learning that has $\\\\mathcal O(n/d^{1/2})$. In\\naddition, our rates of generalization and deletion capacity match the\\nstate-of-the-art rates derived previously for standard statistical learning\\nmodels. In the realm of machine learning, the data may contain additional attributes,\\nknown as privileged information (PI). The main purpose of PI is to assist in\\nthe training of the model and then utilize the acquired knowledge to make\\npredictions for unseen samples. Support vector regression (SVR) is an effective\\nregression model, however, it has a low learning speed due to solving a convex\\nquadratic problem (QP) subject to a pair of constraints. In contrast, twin\\nsupport vector regression (TSVR) is more efficient than SVR as it solves two\\nQPs each subject to one set of constraints. However, TSVR and its variants are\\ntrained only on regular features and do not use privileged features for\\ntraining. To fill this gap, we introduce a fusion of TSVR with learning using\\nprivileged information (LUPI) and propose a novel approach called twin support\\nvector regression with privileged information (TSVR+). The regularization terms\\nin the proposed TSVR+ capture the essence of statistical learning theory and\\nimplement the structural risk minimization principle. We use the successive\\noverrelaxation (SOR) technique to solve the optimization problem of the\\nproposed TSVR+, which enhances the training efficiency. As far as our knowledge\\nextends, the integration of the LUPI concept into twin variants of regression\\nmodels is a novel advancement. The numerical experiments conducted on UCI,\\nstock and time series data collectively demonstrate the superiority of the\\nproposed model. In this work we present an overview of statistical learning, followed by a\\nsurvey of robust streaming techniques and challenges, culminating in several\\nrigorous results proving the relationship that we motivate and hint at\\nthroughout the journey. Furthermore, we unify often disjoint theorems in a\\nshared framework and notation to clarify the deep connections that are\\ndiscovered. We hope that by approaching these results from a shared\\nperspective, already aware of the technical connections that exist, we can\\nenlighten the study of both fields and perhaps motivate new and previously\\nunconsidered directions of research. Recently, there has been a surge of interest in employing neural networks for\\ngraph generation, a fundamental statistical learning problem with critical\\napplications like molecule design and community analysis. However, most\\napproaches encounter significant limitations when generating large-scale\\ngraphs. This is due to their requirement to output the full adjacency matrices\\nwhose size grows quadratically with the number of nodes. In response to this\\nchallenge, we introduce a new, simple, and scalable graph representation named\\ngap encoded edge list (GEEL) that has a small representation size that aligns\\nwith the number of edges. In addition, GEEL significantly reduces the\\nvocabulary size by incorporating the gap encoding and bandwidth restriction\\nschemes. GEEL can be autoregressively generated with the incorporation of node\\npositional encoding, and we further extend GEEL to deal with attributed graphs\\nby designing a new grammar. Our findings reveal that the adoption of this\\ncompact representation not only enhances scalability but also bolsters\\nperformance by simplifying the graph generation process. We conduct a\\ncomprehensive evaluation across ten non-attributed and two molecular graph\\ngeneration tasks, demonstrating the effectiveness of GEEL. We present an efficient parameter-free approach for statistical learning from\\ncorrupted training sets. We identify corrupted and non-corrupted samples using\\nlatent Bernoulli variables, and therefore formulate the robust learning problem\\nas maximization of the likelihood where latent variables are marginalized out.\\nThe resulting optimization problem is solved via variational inference using an\\nefficient Expectation-Maximization based method. The proposed approach improves\\nover the state-of-the-art by automatically inferring the corruption level and\\nidentifying outliers, while adding minimal computational overhead. We\\ndemonstrate our robust learning method on a wide variety of machine learning\\ntasks including online learning and deep learning where it exhibits ability to\\nadapt to different levels of noise and attain high prediction accuracy. Contrastive learning is a highly successful technique for learning\\nrepresentations of data from labeled tuples, specifying the distance relations\\nwithin the tuple. We study the sample complexity of contrastive learning, i.e.\\nthe minimum number of labeled tuples sufficient for getting high generalization\\naccuracy. We give tight bounds on the sample complexity in a variety of\\nsettings, focusing on arbitrary distance functions, both general\\n$\\\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal\\nbound on the sample complexity of learning $\\\\ell_p$-distances for integer $p$.\\nFor any $p \\\\ge 1$ we show that $\\\\tilde \\\\Theta(\\\\min(nd,n^2))$ labeled tuples are\\nnecessary and sufficient for learning $d$-dimensional representations of\\n$n$-point datasets. Our results hold for an arbitrary distribution of the input\\nsamples and are based on giving the corresponding bounds on the\\nVapnik-Chervonenkis/Natarajan dimension of the associated problems. We further\\nshow that the theoretical bounds on sample complexity obtained via VC/Natarajan\\ndimension can have strong predictive power for experimental results, in\\ncontrast with the folklore belief about a substantial gap between the\\nstatistical learning theory and the practice of deep learning. Nonparametric estimation of nonlocal interaction kernels is crucial in\\nvarious applications involving interacting particle systems. The inference\\nchallenge, situated at the nexus of statistical learning and inverse problems,\\ncomes from the nonlocal dependency. A central question is whether the optimal\\nminimax rate of convergence for this problem aligns with the rate of\\n$M^{-\\\\frac{2\\\\beta}{2\\\\beta+1}}$ in classical nonparametric regression, where $M$\\nis the sample size and $\\\\beta$ represents the smoothness exponent of the radial\\nkernel. Our study confirms this alignment for systems with a finite number of\\nparticles.\\n  We introduce a tamed least squares estimator (tLSE) that attains the optimal\\nconvergence rate for a broad class of exchangeable distributions. The tLSE\\nbridges the smallest eigenvalue of random matrices and Sobolev embedding. This\\nestimator relies on nonasymptotic estimates for the left tail probability of\\nthe smallest eigenvalue of the normal matrix. The lower minimax rate is derived\\nusing the Fano-Tsybakov hypothesis testing method. Our findings reveal that\\nprovided the inverse problem in the large sample limit satisfies a coercivity\\ncondition, the left tail probability does not alter the bias-variance tradeoff,\\nand the optimal minimax rate remains intact. Our tLSE method offers a\\nstraightforward approach for establishing the optimal minimax rate for models\\nwith either local or nonlocal dependency. Although statistical learning theory provides a robust framework to\\nunderstand supervised learning, many theoretical aspects of deep learning\\nremain unclear, in particular how different architectures may lead to inductive\\nbias when trained using gradient based methods. The goal of these lectures is\\nto provide an overview of some of the main questions that arise when attempting\\nto understand deep learning from a learning theory perspective. After a brief\\nreminder on statistical learning theory and stochastic optimization, we discuss\\nimplicit bias in the context of benign overfitting. We then move to a general\\ndescription of the mirror descent algorithm, showing how we may go back and\\nforth between a parameter space and the corresponding function space for a\\ngiven learning problem, as well as how the geometry of the learning problem may\\nbe represented by a metric tensor. Building on this framework, we provide a\\ndetailed study of the implicit bias of gradient descent on linear diagonal\\nnetworks for various regression tasks, showing how the loss function, scale of\\nparameters at initialization and depth of the network may lead to various forms\\nof implicit bias, in particular transitioning between kernel or feature\\nlearning. Model averaging has received much attention in the past two decades, which\\nintegrates available information by averaging over potential models. Although\\nvarious model averaging methods have been developed, there are few literatures\\non the theoretical properties of model averaging from the perspective of\\nstability, and the majority of these methods constrain model weights to a\\nsimplex. The aim of this paper is to introduce stability from statistical\\nlearning theory into model averaging. Thus, we define the stability, asymptotic\\nempirical risk minimizer, generalization, and consistency of model averaging\\nand study the relationship among them. Our results indicate that stability can\\nensure that model averaging has good generalization performance and consistency\\nunder reasonable conditions, where consistency means model averaging estimator\\ncan asymptotically minimize the mean squared prediction error. We also propose\\na L2-penalty model averaging method without limiting model weights and prove\\nthat it has stability and consistency. In order to reduce the impact of tuning\\nparameter selection, we use 10-fold cross-validation to select a candidate set\\nof tuning parameters and perform a weighted average of the estimators of model\\nweights based on estimation errors. The Monte Carlo simulation and an\\nillustrative application demonstrate the usefulness of the proposed method. The parallel alternating direction method of multipliers (ADMM) algorithm is\\nwidely recognized for its effectiveness in handling large-scale datasets stored\\nin a distributed manner, making it a popular choice for solving statistical\\nlearning models. However, there is currently limited research on parallel\\nalgorithms specifically designed for high-dimensional regression with combined\\n(composite) regularization terms. These terms, such as elastic-net, sparse\\ngroup lasso, sparse fused lasso, and their nonconvex variants, have gained\\nsignificant attention in various fields due to their ability to incorporate\\nprior information and promote sparsity within specific groups or fused\\nvariables. The scarcity of parallel algorithms for combined regularizations can\\nbe attributed to the inherent nonsmoothness and complexity of these terms, as\\nwell as the absence of closed-form solutions for certain proximal operators\\nassociated with them. In this paper, we propose a unified constrained\\noptimization formulation based on the consensus problem for these types of\\nconvex and nonconvex regression problems and derive the corresponding parallel\\nADMM algorithms. Furthermore, we prove that the proposed algorithm not only has\\nglobal convergence but also exhibits linear convergence rate. Extensive\\nsimulation experiments, along with a financial example, serve to demonstrate\\nthe reliability, stability, and scalability of our algorithm. The R package for\\nimplementing the proposed algorithms can be obtained at\\nhttps://github.com/xfwu1016/CPADMM. We consider a deep neural network estimator based on empirical risk\\nminimization with l_1-regularization. We derive a general bound for its excess\\nrisk in regression and classification (including multiclass), and prove that it\\nis adaptively nearly-minimax (up to log-factors) simultaneously across the\\nentire range of various function classes. Mental health issues widely vary across individuals - the manifestations of\\nsigns and symptoms can be fairly heterogeneous. Recently, language-based\\ndepression and anxiety assessments have shown promise for capturing this\\nheterogeneous nature by evaluating a patient's own language, but such\\napproaches require a large sample of words per person to be accurate. In this\\nwork, we introduce adaptive language-based assessment - the task of iteratively\\nestimating an individual's psychological score based on limited language\\nresponses to questions that the model also decides to ask. To this end, we\\nexplore two statistical learning-based approaches for measurement/scoring:\\nclassical test theory (CTT) and item response theory (IRT). We find that using\\nadaptive testing in general can significantly reduce the number of questions\\nrequired to achieve high validity (r ~ 0.7) with standardized tests, bringing\\ndown from 11 total questions down to 3 for depression and 5 for anxiety. Given\\nthe combinatorial nature of the problem, we empirically evaluate multiple\\nstrategies for both the ordering and scoring objectives, introducing two new\\nmethods: a semi-supervised item response theory based method (ALIRT), and a\\nsupervised actor-critic based model. While both of the models achieve\\nsignificant improvements over random and fixed orderings, we find ALIRT to be a\\nscalable model that achieves the highest accuracy with lower numbers of\\nquestions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking\\nall 11 questions). Overall, ALIRT allows prompting a reduced number of\\nquestions without compromising accuracy or overhead computational costs. Current approaches for collision avoidance and space traffic management face\\nmany challenges, mainly due to the continuous increase in the number of objects\\nin orbit and the lack of scalable and automated solutions. To avoid\\ncatastrophic incidents, satellite owners/operators must be aware of their\\nassets' collision risk to decide whether a collision avoidance manoeuvre needs\\nto be performed. This process is typically executed through the use of warnings\\nissued in the form of CDMs which contain information about the event, such as\\nthe expected TCA and the probability of collision. Our previous work presented\\na statistical learning model that allowed us to answer two important questions:\\n(1) Will any new conjunctions be issued in the next specified time interval?\\n(2) When and with what uncertainty will the next CDM arrive? However, the model\\nwas based on an empirical Bayes homogeneous Poisson process, which assumes that\\nthe arrival rates of CDMs are constant over time. In fact, the rate at which\\nthe CDMs are issued depends on the behaviour of the objects as well as on the\\nscreening process performed by third parties. Thus, in this work, we extend the\\nprevious study and propose a Bayesian non-homogeneous Poisson process\\nimplemented with high precision using a Probabilistic Programming Language to\\nfully describe the underlying phenomena. We compare the proposed solution with\\na baseline model to demonstrate the added value of our approach. The results\\nshow that this problem can be successfully modelled by our Bayesian\\nnon-homogeneous Poisson Process with greater accuracy, contributing to the\\ndevelopment of automated collision avoidance systems and helping operators\\nreact timely but sparingly with satellite manoeuvres. Modern generative machine learning models demonstrate surprising ability to\\ncreate realistic outputs far beyond their training data, such as photorealistic\\nartwork, accurate protein structures, or conversational text. These successes\\nsuggest that generative models learn to effectively parametrize and sample\\narbitrarily complex distributions. Beginning half a century ago, foundational\\nworks in nonlinear dynamics used tools from information theory to infer\\nproperties of chaotic attractors from time series, motivating the development\\nof algorithms for parametrizing chaos in real datasets. In this perspective, we\\naim to connect these classical works to emerging themes in large-scale\\ngenerative statistical learning. We first consider classical attractor\\nreconstruction, which mirrors constraints on latent representations learned by\\nstate space models of time series. We next revisit early efforts to use\\nsymbolic approximations to compare minimal discrete generators underlying\\ncomplex processes, a problem relevant to modern efforts to distill and\\ninterpret black-box statistical models. Emerging interdisciplinary works bridge\\nnonlinear dynamics and learning theory, such as operator-theoretic methods for\\ncomplex fluid flows, or detection of broken detailed balance in biological\\ndatasets. We anticipate that future machine learning techniques may revisit\\nother classical concepts from nonlinear dynamics, such as transinformation\\ndecay and complexity-entropy tradeoffs. Conventional statistical wisdom established a well-understood relationship\\nbetween model complexity and prediction error, typically presented as a\\nU-shaped curve reflecting a transition between under- and overfitting regimes.\\nHowever, motivated by the success of overparametrized neural networks, recent\\ninfluential work has suggested this theory to be generally incomplete,\\nintroducing an additional regime that exhibits a second descent in test error\\nas the parameter count p grows past sample size n - a phenomenon dubbed double\\ndescent. While most attention has naturally been given to the deep-learning\\nsetting, double descent was shown to emerge more generally across non-neural\\nmodels: known cases include linear regression, trees, and boosting. In this\\nwork, we take a closer look at evidence surrounding these more classical\\nstatistical machine learning methods and challenge the claim that observed\\ncases of double descent truly extend the limits of a traditional U-shaped\\ncomplexity-generalization curve therein. We show that once careful\\nconsideration is given to what is being plotted on the x-axes of their double\\ndescent plots, it becomes apparent that there are implicitly multiple\\ncomplexity axes along which the parameter count grows. We demonstrate that the\\nsecond descent appears exactly (and only) when and where the transition between\\nthese underlying axes occurs, and that its location is thus not inherently tied\\nto the interpolation threshold p=n. We then gain further insight by adopting a\\nclassical nonparametric statistics perspective. We interpret the investigated\\nmethods as smoothers and propose a generalized measure for the effective number\\nof parameters they use on unseen examples, using which we find that their\\napparent double descent curves indeed fold back into more traditional convex\\nshapes - providing a resolution to tensions between double descent and\\nstatistical intuition. We develop a versatile framework for statistical learning in non-stationary\\nenvironments. In each time period, our approach applies a stability principle\\nto select a look-back window that maximizes the utilization of historical data\\nwhile keeping the cumulative bias within an acceptable range relative to the\\nstochastic error. Our theory showcases the adaptability of this approach to\\nunknown non-stationarity. The regret bound is minimax optimal up to logarithmic\\nfactors when the population losses are strongly convex, or Lipschitz only. At\\nthe heart of our analysis lie two novel components: a measure of similarity\\nbetween functions and a segmentation technique for dividing the non-stationary\\ndata sequence into quasi-stationary pieces. In many applications of machine learning, a large number of variables are\\nconsidered. Motivated by machine learning of interacting particle systems, we\\nconsider the situation when the number of input variables goes to infinity.\\nFirst, we continue the recent investigation of the mean field limit of kernels\\nand their reproducing kernel Hilbert spaces, completing the existing theory.\\nNext, we provide results relevant for approximation with such kernels in the\\nmean field limit, including a representer theorem. Finally, we use these\\nkernels in the context of statistical learning in the mean field limit,\\nfocusing on Support Vector Machines. In particular, we show mean field\\nconvergence of empirical and infinite-sample solutions as well as the\\nconvergence of the corresponding risks. On the one hand, our results establish\\nrigorous mean field limits in the context of kernel methods, providing new\\ntheoretical tools and insights for large-scale problems. On the other hand, our\\nsetting corresponds to a new form of limit of learning problems, which seems to\\nhave not been investigated yet in the statistical learning theory literature. With the help of massive data and rich computational resources, deep\\nQ-learning has been widely used in operations research and management science\\nand has contributed to great success in numerous applications, including\\nrecommender systems, supply chains, games, and robotic manipulation. However,\\nthe success of deep Q-learning lacks solid theoretical verification and\\ninterpretability. The aim of this paper is to theoretically verify the power of\\ndepth in deep Q-learning. Within the framework of statistical learning theory,\\nwe rigorously prove that deep Q-learning outperforms its traditional version by\\ndemonstrating its good generalization error bound. Our results reveal that the\\nmain reason for the success of deep Q-learning is the excellent performance of\\ndeep neural networks (deep nets) in capturing the special properties of rewards\\nnamely, spatial sparseness and piecewise constancy, rather than their large\\ncapacities. In this paper, we make fundamental contributions to the field of\\nreinforcement learning by answering to the following three questions: Why does\\ndeep Q-learning perform so well? When does deep Q-learning perform better than\\ntraditional Q-learning? How many samples are required to achieve a specific\\nprediction accuracy for deep Q-learning? Our theoretical assertions are\\nverified by applying deep Q-learning in the well-known beer game in supply\\nchain management and a simulated recommender system. Recently, machine learning of the branch and bound algorithm has shown\\npromise in approximating competent solutions to NP-hard problems. In this\\npaper, we utilize and comprehensively compare the outcomes of three neural\\nnetworks--graph convolutional neural network (GCNN), GraphSAGE, and graph\\nattention network (GAT)--to solve the capacitated vehicle routing problem. We\\ntrain these neural networks to emulate the decision-making process of the\\ncomputationally expensive Strong Branching strategy. The neural networks are\\ntrained on six instances with distinct topologies from the CVRPLIB and\\nevaluated on eight additional instances. Moreover, we reduced the minimum\\nnumber of vehicles required to solve a CVRP instance to a bin-packing problem,\\nwhich was addressed in a similar manner. Through rigorous experimentation, we\\nfound that this approach can match or improve upon the performance of the\\nbranch and bound algorithm with the Strong Branching strategy while requiring\\nsignificantly less computational time. The source code that corresponds to our\\nresearch findings and methodology is readily accessible and available for\\nreference at the following web address: https://isotlaboratory.github.io/ml4vrp Good posture and form are essential for safe and productive exercising. Even\\nin gym settings, trainers may not be readily available for feedback.\\nRehabilitation therapies and fitness workouts can thus benefit from recommender\\nsystems that provide real-time evaluation. In this paper, we present an\\nalgorithmic pipeline that can diagnose problems in exercise techniques and\\noffer corrective recommendations, with high sensitivity and specificity in\\nreal-time. We use MediaPipe for pose recognition, count repetitions using\\npeak-prominence detection, and use a learnable physics simulator to track\\nmotion evolution for each exercise. A test video is diagnosed based on\\ndeviations from the prototypical learned motion using statistical learning. The\\nsystem is evaluated on six full and upper body exercises. These real-time\\nrecommendations, counseled via low-cost equipment like smartphones, will allow\\nexercisers to rectify potential mistakes making self-practice feasible while\\nreducing the risk of workout injuries. Existing statistical learning guarantees for general kernel regressors often\\nyield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels\\nnaturally appear in several machine learning problems, e.g.\\\\ when fine-tuning a\\npre-trained deep neural network's last layer to adapt it to a novel task when\\nperforming transfer learning. We address this gap for finite-rank kernel ridge\\nregression (KRR) by deriving sharp non-asymptotic upper and lower bounds for\\nthe KRR test error of any finite-rank KRR. Our bounds are tighter than\\npreviously derived bounds on finite-rank KRR, and unlike comparable results,\\nthey also remain valid for any regularization parameters. This paper examines the problem of information routing in a large-scale\\ncommunication network, which can be formulated as a constrained statistical\\nlearning problem having access to only local information. We delineate a novel\\nState Augmentation (SA) strategy to maximize the aggregate information at\\nsource nodes using graph neural network (GNN) architectures, by deploying graph\\nconvolutions over the topological links of the communication network. The\\nproposed technique leverages only the local information available at each node\\nand efficiently routes desired information to the destination nodes. We\\nleverage an unsupervised learning procedure to convert the output of the GNN\\narchitecture to optimal information routing strategies. In the experiments, we\\nperform the evaluation on real-time network topologies to validate our\\nalgorithms. Numerical simulations depict the improved performance of the\\nproposed method in training a GNN parameterization as compared to baseline\\nalgorithms. Quantifying the dependence between high-dimensional random variables is\\ncentral to statistical learning and inference. Two classical methods are\\ncanonical correlation analysis (CCA), which identifies maximally correlated\\nprojected versions of the original variables, and Shannon's mutual information,\\nwhich is a universal dependence measure that also captures high-order\\ndependencies. However, CCA only accounts for linear dependence, which may be\\ninsufficient for certain applications, while mutual information is often\\ninfeasible to compute/estimate in high dimensions. This work proposes a middle\\nground in the form of a scalable information-theoretic generalization of CCA,\\ntermed max-sliced mutual information (mSMI). mSMI equals the maximal mutual\\ninformation between low-dimensional projections of the high-dimensional\\nvariables, which reduces back to CCA in the Gaussian case. It enjoys the best\\nof both worlds: capturing intricate dependencies in the data while being\\namenable to fast computation and scalable estimation from samples. We show that\\nmSMI retains favorable structural properties of Shannon's mutual information,\\nlike variational forms and identification of independence. We then study\\nstatistical estimation of mSMI, propose an efficiently computable neural\\nestimator, and couple it with formal non-asymptotic error bounds. We present\\nexperiments that demonstrate the utility of mSMI for several tasks,\\nencompassing independence testing, multi-view representation learning,\\nalgorithmic fairness, and generative modeling. We observe that mSMI\\nconsistently outperforms competing methods with little-to-no computational\\noverhead. The widely observed 'benign overfitting phenomenon' in the neural network\\nliterature raises the challenge to the 'bias-variance trade-off' doctrine in\\nthe statistical learning theory. Since the generalization ability of the 'lazy\\ntrained' over-parametrized neural network can be well approximated by that of\\nthe neural tangent kernel regression, the curve of the excess risk (namely, the\\nlearning curve) of kernel ridge regression attracts increasing attention\\nrecently. However, most recent arguments on the learning curve are heuristic\\nand are based on the 'Gaussian design' assumption. In this paper, under mild\\nand more realistic assumptions, we rigorously provide a full characterization\\nof the learning curve: elaborating the effect and the interplay of the choice\\nof the regularization parameter, the source condition and the noise. In\\nparticular, our results suggest that the 'benign overfitting phenomenon' exists\\nin very wide neural networks only when the noise level is small. Recent years have seen significant activity on the problem of using data for\\nthe purpose of learning properties of quantum systems or of processing\\nclassical or quantum data via quantum computing. As in classical learning,\\nquantum learning problems involve settings in which the mechanism generating\\nthe data is unknown, and the main goal of a learning algorithm is to ensure\\nsatisfactory accuracy levels when only given access to data and, possibly, side\\ninformation such as expert knowledge. This article reviews the complexity of\\nquantum learning using information-theoretic techniques by focusing on data\\ncomplexity, copy complexity, and model complexity. Copy complexity arises from\\nthe destructive nature of quantum measurements, which irreversibly alter the\\nstate to be processed, limiting the information that can be extracted about\\nquantum data. For example, in a quantum system, unlike in classical machine\\nlearning, it is generally not possible to evaluate the training loss\\nsimultaneously on multiple hypotheses using the same quantum data. To make the\\npaper self-contained and approachable by different research communities, we\\nprovide extensive background material on classical results from statistical\\nlearning theory, as well as on the distinguishability of quantum states.\\nThroughout, we highlight the differences between quantum and classical learning\\nby addressing both supervised and unsupervised learning, and we provide\\nextensive pointers to the literature. We investigate the problem of performing logistic regression on data\\ncollected from privacy-sensitive sellers. Since the data is private, sellers\\nmust be incentivized through payments to provide their data. Thus, the goal is\\nto design a mechanism that optimizes a weighted combination of test loss,\\nseller privacy, and payment, i.e., strikes a balance between multiple\\nobjectives of interest. We solve the problem by combining ideas from game\\ntheory, statistical learning theory, and differential privacy. The buyer's\\nobjective function can be highly non-convex. However, we show that, under\\ncertain conditions on the problem parameters, the problem can be convexified by\\nusing a change of variables. We also provide asymptotic results characterizing\\nthe buyer's test error and payments when the number of sellers becomes large.\\nFinally, we demonstrate our ideas by applying them to a real healthcare data\\nset. Domain adaptation (DA) is a statistical learning problem that arises when the\\ndistribution of the source data used to train a model differs from that of the\\ntarget data used to evaluate the model. While many DA algorithms have\\ndemonstrated considerable empirical success, blindly applying these algorithms\\ncan often lead to worse performance on new datasets. To address this, it is\\ncrucial to clarify the assumptions under which a DA algorithm has good target\\nperformance. In this work, we focus on the assumption of the presence of\\nconditionally invariant components (CICs), which are relevant for prediction\\nand remain conditionally invariant across the source and target data. We\\ndemonstrate that CICs, which can be estimated through conditional invariant\\npenalty (CIP), play three prominent roles in providing target risk guarantees\\nin DA. First, we propose a new algorithm based on CICs, importance-weighted\\nconditional invariant penalty (IW-CIP), which has target risk guarantees beyond\\nsimple settings such as covariate shift and label shift. Second, we show that\\nCICs help identify large discrepancies between source and target risks of other\\nDA algorithms. Finally, we demonstrate that incorporating CICs into the domain\\ninvariant projection (DIP) algorithm can address its failure scenario caused by\\nlabel-flipping features. We support our new algorithms and theoretical findings\\nvia numerical experiments on synthetic data, MNIST, CelebA, and Camelyon17\\ndatasets. In this work, we consider a sequence of stochastic optimization problems\\nfollowing a time-varying distribution via the lens of online optimization.\\nAssuming that the loss function satisfies the Polyak-{\\\\L}ojasiewicz condition,\\nwe apply online stochastic gradient descent and establish its dynamic regret\\nbound that is composed of cumulative distribution drifts and cumulative\\ngradient biases caused by stochasticity. The distribution metric we adopt here\\nis Wasserstein distance, which is well-defined without the absolute continuity\\nassumption or with a time-varying support set. We also establish a regret bound\\nof online stochastic proximal gradient descent when the objective function is\\nregularized. Moreover, we show that the above framework can be applied to the\\nConditional Value-at-Risk (CVaR) learning problem. Particularly, we improve an\\nexisting proof on the discovery of the PL condition of the CVaR problem,\\nresulting in a regret bound of online stochastic gradient descent. The swift progression of machine learning (ML) has not gone unnoticed in the\\nrealm of statistical mechanics. ML techniques have attracted attention by the\\nclassical density-functional theory (DFT) community, as they enable discovery\\nof free-energy functionals to determine the equilibrium-density profile of a\\nmany-particle system. Within DFT, the external potential accounts for the\\ninteraction of the many-particle system with an external field, thus, affecting\\nthe density distribution. In this context, we introduce a statistical-learning\\nframework to infer the external potential exerted on a many-particle system. We\\ncombine a Bayesian inference approach with the classical DFT apparatus to\\nreconstruct the external potential, yielding a probabilistic description of the\\nexternal potential functional form with inherent uncertainty quantification.\\nOur framework is exemplified with a grand-canonical one-dimensional particle\\nensemble with excluded volume interactions in a confined geometry. The required\\ntraining dataset is generated using a Monte Carlo (MC) simulation where the\\nexternal potential is applied to the grand-canonical ensemble. The resulting\\nparticle coordinates from the MC simulation are fed into the learning framework\\nto uncover the external potential. This eventually allows us to compute the\\nequilibrium density profile of the system by using the tools of DFT. Our\\napproach benchmarks the inferred density against the exact one calculated\\nthrough the DFT formulation with the true external potential. The proposed\\nBayesian procedure accurately infers the external potential and the density\\nprofile. We also highlight the external-potential uncertainty quantification\\nconditioned on the amount of available simulated data. The seemingly simple\\ncase study introduced in this work might serve as a prototype for studying a\\nwide variety of applications, including adsorption and capillarity. Predicting potential risks associated with the fatigue of key structural\\ncomponents is crucial in engineering design. However, fatigue often involves\\nentangled complexities of material microstructures and service conditions,\\nmaking diagnosis and prognosis of fatigue damage challenging. We report a\\nstatistical learning framework to predict the growth of fatigue cracks and the\\nlife-to-failure of the components under loading conditions with uncertainties.\\nDigital libraries of fatigue crack patterns and the remaining life are\\nconstructed by high-fidelity physical simulations. Dimensionality reduction and\\nneural network architectures are then used to learn the history dependence and\\nnonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques\\nare introduced to handle the statistical noises and rare events. The predicted\\nfatigue crack patterns are self-updated and self-corrected by the evolving\\ncrack patterns. The end-to-end approach is validated by representative examples\\nwith fatigue cracks in plates, which showcase the digital-twin scenario in\\nreal-time structural health monitoring and fatigue life prediction for\\nmaintenance management decision-making. In the last decade, developments in tropical geometry have provided a number\\nof uses directly applicable to problems in statistical learning. The TML\\npackage is the first R package which contains a comprehensive set of tools and\\nmethods used for basic computations related to tropical convexity,\\nvisualization of tropically convex sets, as well as supervised and unsupervised\\nlearning models using the tropical metric under the max-plus algebra over the\\ntropical projective torus. Primarily, the TML package employs a Hit and Run\\nMarkov chain Monte Carlo sampler in conjunction with the tropical metric as its\\nmain tool for statistical inference. In addition to basic computation and\\nvarious applications of the tropical HAR sampler, we also focus on several\\nsupervised and unsupervised methods incorporated in the TML package including\\ntropical principal component analysis, tropical logistic regression and\\ntropical kernel density estimation. These lecture notes provide an overview of existing methodologies and recent\\ndevelopments for estimation and inference with high dimensional time series\\nregression models. First, we present main limit theory results for high\\ndimensional dependent data which is relevant to covariance matrix structures as\\nwell as to dependent time series sequences. Second, we present main aspects of\\nthe asymptotic theory related to time series regression models with many\\ncovariates. Third, we discuss various applications of statistical learning\\nmethodologies for time series analysis purposes. Understanding the dynamics of large quantum systems is hindered by the curse\\nof dimensionality. Statistical learning offers new possibilities in this regime\\nby neural-network protocols and classical shadows, while both methods have\\nlimitations: the former is plagued by the predictive uncertainty and the latter\\nlacks the generalization ability. Here we propose a data-centric learning\\nparadigm combining the strength of these two approaches to facilitate diverse\\nquantum system learning (QSL) tasks. Particularly, our paradigm utilizes\\nclassical shadows along with other easily obtainable information of quantum\\nsystems to create the training dataset, which is then learnt by neural networks\\nto unveil the underlying mapping rule of the explored QSL problem. Capitalizing\\non the generalization power of neural networks, this paradigm can be trained\\noffline and excel at predicting previously unseen systems at the inference\\nstage, even with few state copies. Besides, it inherits the characteristic of\\nclassical shadows, enabling memory-efficient storage and faithful prediction.\\nThese features underscore the immense potential of the proposed data-centric\\napproach in discovering novel and large-scale quantum systems. For\\nconcreteness, we present the instantiation of our paradigm in quantum state\\ntomography and direct fidelity estimation tasks and conduct numerical analysis\\nup to 60 qubits. Our work showcases the profound prospects of data-centric\\nartificial intelligence to advance QSL in a faithful and generalizable manner. Interpreting a seemingly-simple function word like \\\"or\\\", \\\"behind\\\", or \\\"more\\\"\\ncan require logical, numerical, and relational reasoning. How are such words\\nlearned by children? Prior acquisition theories have often relied on positing a\\nfoundation of innate knowledge. Yet recent neural-network based visual question\\nanswering models apparently can learn to use function words as part of\\nanswering questions about complex visual scenes. In this paper, we study what\\nthese models learn about function words, in the hope of better understanding\\nhow the meanings of these words can be learnt by both models and children. We\\nshow that recurrent models trained on visually grounded language learn gradient\\nsemantics for function words requiring spacial and numerical reasoning.\\nFurthermore, we find that these models can learn the meanings of logical\\nconnectives \\\"and\\\" and \\\"or\\\" without any prior knowledge of logical reasoning, as\\nwell as early evidence that they are sensitive to alternative expressions when\\ninterpreting language. Finally, we show that word learning difficulty is\\ndependent on frequency in models' input. Our findings offer proof-of-concept\\nevidence that it is possible to learn the nuanced interpretations of function\\nwords in visually grounded context by using non-symbolic general statistical\\nlearning algorithms, without any prior knowledge of linguistic meaning. Online learning methods yield sequential regret bounds under minimal\\nassumptions and provide in-expectation risk bounds for statistical learning.\\nHowever, despite the apparent advantage of online guarantees over their\\nstatistical counterparts, recent findings indicate that in many important\\ncases, regret bounds may not guarantee tight high-probability risk bounds in\\nthe statistical setting. In this work we show that online to batch conversions\\napplied to general online learning algorithms can bypass this limitation. Via a\\ngeneral second-order correction to the loss function defining the regret, we\\nobtain nearly optimal high-probability risk bounds for several classical\\nstatistical estimation problems, such as discrete distribution estimation,\\nlinear regression, logistic regression, and conditional density estimation. Our\\nanalysis relies on the fact that many online learning algorithms are improper,\\nas they are not restricted to use predictors from a given reference class. The\\nimproper nature of our estimators enables significant improvements in the\\ndependencies on various problem parameters. Finally, we discuss some\\ncomputational advantages of our sequential algorithms over their existing batch\\ncounterparts. Despite the tremendous advances achieved over the past years by deep learning\\ntechniques, the latest risk prediction models for industrial applications still\\nrely on highly handtuned stage-wised statistical learning tools, such as\\ngradient boosting and random forest methods. Different from images or\\nlanguages, real-world financial data are high-dimensional, sparse, noisy and\\nextremely imbalanced, which makes deep neural network models particularly\\nchallenging to train and fragile in practice. In this work, we propose DeRisk,\\nan effective deep learning risk prediction framework for credit risk prediction\\non real-world financial data. DeRisk is the first deep risk prediction model\\nthat outperforms statistical learning approaches deployed in our company's\\nproduction system. We also perform extensive ablation studies on our method to\\npresent the most critical factors for the empirical success of DeRisk. Distribution shifts are a serious concern in modern statistical learning as\\nthey can systematically change the properties of the data away from the truth.\\nWe focus on Wasserstein distribution shifts, where every data point may undergo\\na slight perturbation, as opposed to the Huber contamination model where a\\nfraction of observations are outliers. We consider perturbations that are\\neither independent or coordinated joint shifts across data points. We analyze\\nseveral important statistical problems, including location estimation, linear\\nregression, and non-parametric density estimation. Under a squared loss for\\nmean estimation and prediction error in linear regression, we find the exact\\nminimax risk, a least favorable perturbation, and show that the sample mean and\\nleast squares estimators are respectively optimal. For other problems, we\\nprovide nearly optimal estimators and precise finite-sample bounds. We also\\nintroduce several tools for bounding the minimax risk under general\\ndistribution shifts, not just for Wasserstein perturbations, such as a\\nsmoothing technique for location families, and generalizations of classical\\ntools including least favorable sequences of priors, the modulus of continuity,\\nas well as Le Cam's, Fano's, and Assouad's methods. Multi-distribution learning is a natural generalization of PAC learning to\\nsettings with multiple data distributions. There remains a significant gap\\nbetween the known upper and lower bounds for PAC-learnable classes. In\\nparticular, though we understand the sample complexity of learning a VC\\ndimension d class on $k$ distributions to be $O(\\\\epsilon^{-2} \\\\ln(k)(d + k) +\\n\\\\min\\\\{\\\\epsilon^{-1} dk, \\\\epsilon^{-4} \\\\ln(k) d\\\\})$, the best lower bound is\\n$\\\\Omega(\\\\epsilon^{-2}(d + k \\\\ln(k)))$. We discuss recent progress on this\\nproblem and some hurdles that are fundamental to the use of game dynamics in\\nstatistical learning. We consider the general class of time-homogeneous stochastic dynamical\\nsystems, both discrete and continuous, and study the problem of learning a\\nrepresentation of the state that faithfully captures its dynamics. This is\\ninstrumental to learn the transfer operator of the system, that in turn can be\\nused for numerous tasks, such as forecasting and interpreting the system\\ndynamics. We show that the search for a good representation can be cast as an\\noptimization problem over neural networks. Our approach is supported by recent\\nresults in statistical learning theory, highlighting the role of approximation\\nerror and metric distortion in the context of transfer operator regression. The\\nobjective function we propose is associated with projection operators from the\\nrepresentation space to the data space, overcomes metric distortion, and can be\\nempirically estimated from data. In the discrete time setting, we further\\nderive a relaxed objective function that is differentiable and numerically\\nwell-conditioned. We compare our method against state-of-the-art approaches on\\ndifferent datasets, showing better performance across the board. As causal ground truth is incredibly rare, causal discovery algorithms are\\ncommonly only evaluated on simulated data. This is concerning, given that\\nsimulations reflect common preconceptions about generating processes regarding\\nnoise distributions, model classes, and more. In this work, we propose a novel\\nmethod for falsifying the output of a causal discovery algorithm in the absence\\nof ground truth. Our key insight is that while statistical learning seeks\\nstability across subsets of data points, causal learning should seek stability\\nacross subsets of variables. Motivated by this insight, our method relies on a\\nnotion of compatibility between causal graphs learned on different subsets of\\nvariables. We prove that detecting incompatibilities can falsify wrongly\\ninferred causal relations due to violation of assumptions or errors from finite\\nsample effects. Although passing such compatibility tests is only a necessary\\ncriterion for good performance, we argue that it provides strong evidence for\\nthe causal models whenever compatibility entails strong implications for the\\njoint distribution. We also demonstrate experimentally that detection of\\nincompatibilities can aid in causal model selection. Support Vector Machines (SVM) have gathered significant acclaim as\\nclassifiers due to their successful implementation of Statistical Learning\\nTheory. However, in the context of multiclass and multilabel settings, the\\nreliance on vector-based formulations in existing SVM-based models poses\\nlimitations regarding flexibility and ease of incorporating additional terms to\\nhandle specific challenges. To overcome these limitations, our research paper\\nfocuses on introducing a matrix formulation for SVM that effectively addresses\\nthese constraints. By employing the Accelerated Gradient Descent method in the\\ndual, we notably enhance the efficiency of solving the Matrix-SVM problem.\\nExperimental evaluations on multilabel and multiclass datasets demonstrate that\\nMatrix SVM achieves superior time efficacy while delivering similar results to\\nBinary Relevance SVM.\\n  Moreover, our matrix formulation unveils crucial insights and advantages that\\nmay not be readily apparent in traditional vector-based notations. We emphasize\\nthat numerous multilabel models can be viewed as extensions of SVM, with\\ncustomised modifications to meet specific requirements. The matrix formulation\\npresented in this paper establishes a solid foundation for developing more\\nsophisticated models capable of effectively addressing the distinctive\\nchallenges encountered in multilabel learning. In this paper, we provide a novel framework for the analysis of\\ngeneralization error of first-order optimization algorithms for statistical\\nlearning when the gradient can only be accessed through partial observations\\ngiven by an oracle. Our analysis relies on the regularity of the gradient\\nw.r.t. the data samples, and allows to derive near matching upper and lower\\nbounds for the generalization error of multiple learning problems, including\\nsupervised learning, transfer learning, robust learning, distributed learning\\nand communication efficient learning using gradient quantization. These results\\nhold for smooth and strongly-convex optimization problems, as well as smooth\\nnon-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In\\nparticular, our upper and lower bounds depend on a novel quantity that extends\\nthe notion of conditional standard deviation, and is a measure of the extent to\\nwhich the gradient can be approximated by having access to the oracle. As a\\nconsequence, our analysis provides a precise meaning to the intuition that\\noptimization of the statistical learning objective is as hard as the estimation\\nof its gradient. Finally, we show that, in the case of standard supervised\\nlearning, mini-batch gradient descent with increasing batch sizes and a warm\\nstart can reach a generalization error that is optimal up to a multiplicative\\nfactor, thus motivating the use of this optimization scheme in practical\\napplications. Many machine learning tasks can be formulated as a stochastic compositional\\noptimization (SCO) problem such as reinforcement learning, AUC maximization,\\nand meta-learning, where the objective function involves a nested composition\\nassociated with an expectation. While a significant amount of studies has been\\ndevoted to studying the convergence behavior of SCO algorithms, there is little\\nwork on understanding their generalization, i.e., how these learning algorithms\\nbuilt from training examples would behave on future test examples. In this\\npaper, we provide the stability and generalization analysis of stochastic\\ncompositional gradient descent algorithms through the lens of algorithmic\\nstability in the framework of statistical learning theory. Firstly, we\\nintroduce a stability concept called compositional uniform stability and\\nestablish its quantitative relation with generalization for SCO problems. Then,\\nwe establish the compositional uniform stability results for two popular\\nstochastic compositional gradient descent algorithms, namely SCGD and SCSC.\\nFinally, we derive dimension-independent excess risk bounds for SCGD and SCSC\\nby trade-offing their stability results and optimization errors. To the best of\\nour knowledge, these are the first-ever-known results on stability and\\ngeneralization analysis of stochastic compositional gradient descent\\nalgorithms. The recent advances in machine learning in various fields of applications can\\nbe largely attributed to the rise of deep learning (DL) methods and\\narchitectures. Despite being a key technology behind autonomous cars, image\\nprocessing, speech recognition, etc., a notorious problem remains the lack of\\ntheoretical understanding of DL and related interpretability and (adversarial)\\nrobustness issues. Understanding the specifics of DL, as compared to, say,\\nother forms of nonlinear regression methods or statistical learning, is\\ninteresting from a mathematical perspective, but at the same time it is of\\ncrucial importance in practice: treating neural networks as mere black boxes\\nmight be sufficient in certain cases, but many applications require waterproof\\nperformance guarantees and a deeper understanding of what could go wrong and\\nwhy it could go wrong. It is probably fair to say that, despite being\\nmathematically well founded as a method to approximate complicated functions,\\nDL is mostly still more like modern alchemy that is firmly in the hands of\\nengineers and computer scientists. Nevertheless, it is evident that certain\\nspecifics of DL that could explain its success in applications demands\\nsystematic mathematical approaches. In this work, we review robustness issues\\nof DL and particularly bridge concerns and attempts from approximation theory\\nto statistical learning theory. Further, we review Bayesian Deep Learning as a\\nmeans for uncertainty quantification and rigorous explainability. Cross-validation is a widely used technique for evaluating the performance of\\nprediction models. It helps avoid the optimism bias in error estimates, which\\ncan be significant for models built using complex statistical learning\\nalgorithms. However, since the cross-validation estimate is a random value\\ndependent on observed data, it is essential to accurately quantify the\\nuncertainty associated with the estimate. This is especially important when\\ncomparing the performance of two models using cross-validation, as one must\\ndetermine whether differences in error estimates are a result of chance\\nfluctuations. Although various methods have been developed for making\\ninferences on cross-validation estimates, they often have many limitations,\\nsuch as stringent model assumptions This paper proposes a fast bootstrap method\\nthat quickly estimates the standard error of the cross-validation estimate and\\nproduces valid confidence intervals for a population parameter measuring\\naverage model performance. Our method overcomes the computational challenge\\ninherent in bootstrapping the cross-validation estimate by estimating the\\nvariance component within a random effects model. It is just as flexible as the\\ncross-validation procedure itself. To showcase the effectiveness of our\\napproach, we employ comprehensive simulations and real data analysis across\\nthree diverse applications. The theory of statistical learning has focused on variational objectives\\nexpressed on functions. In this note, we discuss motivations to write similar\\nobjectives on measures, in particular to discuss out-of-distribution\\ngeneralization and weakly-supervised learning. It raises a natural question:\\ncan one cast usual statistical learning results to objectives expressed on\\nmeasures? Does the resulting construction lead to new algorithms of practical\\ninterest? Understanding the effect of a feature vector $x \\\\in \\\\mathbb{R}^d$ on the\\nresponse value (label) $y \\\\in \\\\mathbb{R}$ is the cornerstone of many\\nstatistical learning problems. Ideally, it is desired to understand how a set\\nof collected features combine together and influence the response value, but\\nthis problem is notoriously difficult, due to the high-dimensionality of data\\nand limited number of labeled data points, among many others. In this work, we\\ntake a new perspective on this problem, and we study the question of assessing\\nthe difference of influence that the two given features have on the response\\nvalue. We first propose a notion of closeness for the influence of features,\\nand show that our definition recovers the familiar notion of the magnitude of\\ncoefficients in the parametric model. We then propose a novel method to test\\nfor the closeness of influence in general model-free supervised learning\\nproblems. Our proposed test can be used with finite number of samples with\\ncontrol on type I error rate, no matter the ground truth conditional law\\n$\\\\mathcal{L}(Y |X)$. We analyze the power of our test for two general learning\\nproblems i) linear regression, and ii) binary classification under mixture of\\nGaussian models, and show that under the proper choice of score function, an\\ninternal component of our test, with sufficient number of samples will achieve\\nfull statistical power. We evaluate our findings through extensive numerical\\nsimulations, specifically we adopt the datamodel framework (Ilyas, et al.,\\n2022) for CIFAR-10 dataset to identify pairs of training samples with different\\ninfluence on the trained model via optional black box training mechanisms. We investigate the generalization error of statistical learning models in a\\nFederated Learning (FL) setting. Specifically, we study the evolution of the\\ngeneralization error with the number of communication rounds between the\\nclients and the parameter server, i.e., the effect on the generalization error\\nof how often the local models as computed by the clients are aggregated at the\\nparameter server. We establish PAC-Bayes and rate-distortion theoretic bounds\\non the generalization error that account explicitly for the effect of the\\nnumber of rounds, say $ R \\\\in \\\\mathbb{N}$, in addition to the number of\\nparticipating devices $K$ and individual datasets size $n$. The bounds, which\\napply in their generality for a large class of loss functions and learning\\nalgorithms, appear to be the first of their kind for the FL setting.\\nFurthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and\\nwe derive (more) explicit bounds on the generalization error in this case. In\\nparticular, we show that the generalization error of FSVM increases with $R$,\\nsuggesting that more frequent communication with the parameter server\\ndiminishes the generalization power of such learning algorithms. Combined with\\nthat the empirical risk generally decreases for larger values of $R$, this\\nindicates that $R$ might be a parameter to optimize in order to minimize the\\npopulation risk of FL algorithms. Moreover, specialized to the case $R=1$\\n(sometimes referred to as \\\"one-shot\\\" FL or distributed learning) our bounds\\nsuggest that the generalization error of the FL setting decreases faster than\\nthat of centralized learning by a factor of $\\\\mathcal{O}(\\\\sqrt{\\\\log(K)/K})$,\\nthereby generalizing recent findings in this direction to arbitrary loss\\nfunctions and algorithms. The results of this paper are also validated on some\\nexperiments. The theory of Koopman operators allows to deploy non-parametric machine\\nlearning algorithms to predict and analyze complex dynamical systems.\\nEstimators such as principal component regression (PCR) or reduced rank\\nregression (RRR) in kernel spaces can be shown to provably learn Koopman\\noperators from finite empirical observations of the system's time evolution.\\nScaling these approaches to very long trajectories is a challenge and requires\\nintroducing suitable approximations to make computations feasible. In this\\npaper, we boost the efficiency of different kernel-based Koopman operator\\nestimators using random projections (sketching). We derive, implement and test\\nthe new \\\"sketched\\\" estimators with extensive experiments on synthetic and\\nlarge-scale molecular dynamics datasets. Further, we establish non asymptotic\\nerror bounds giving a sharp characterization of the trade-offs between\\nstatistical learning rates and computational efficiency. Our empirical and\\ntheoretical analysis shows that the proposed estimators provide a sound and\\nefficient way to learn large scale dynamical systems. In particular our\\nexperiments indicate that the proposed estimators retain the same accuracy of\\nPCR or RRR, while being much faster. A fundamental feature of human intelligence is the ability to infer\\nhigh-level abstractions from low-level sensory data. An essential component of\\nsuch inference is the ability to discover modularized generative mechanisms.\\nDespite many efforts to use statistical learning and pattern recognition for\\nfinding disentangled factors, arguably human intelligence remains unmatched in\\nthis area.\\n  In this paper, we investigate a problem of learning, in a fully unsupervised\\nmanner, the inverse of a set of independent mechanisms from distorted data\\npoints. We postulate, and justify this claim with experimental results, that an\\nimportant weakness of existing machine learning solutions lies in the\\ninsufficiency of cross-module diversification. Addressing this crucial\\ndiscrepancy between human and machine intelligence is an important challenge\\nfor pattern recognition systems.\\n  To this end, our work proposes an unsupervised method that discovers and\\ndisentangles a set of independent mechanisms from unlabeled data, and learns\\nhow to invert them. A number of experts compete against each other for\\nindividual data points in an adversarial setting: one that best inverses the\\n(unknown) generative mechanism is the winner. We demonstrate that introducing\\nan orthogonalization layer into the expert architectures enforces additional\\ndiversity in the outputs, leading to significantly better separability.\\nMoreover, we propose a procedure for relocating data points between experts to\\nfurther prevent any one from claiming multiple mechanisms. We experimentally\\nillustrate that these techniques allow discovery and modularization of much\\nless pronounced transformations, in addition to considerably faster\\nconvergence. Statistical learning theory and high dimensional statistics have had a\\ntremendous impact on Machine Learning theory and have impacted a variety of\\ndomains including systems and control theory. Over the past few years we have\\nwitnessed a variety of applications of such theoretical tools to help answer\\nquestions such as: how many state-action pairs are needed to learn a static\\ncontrol policy to a given accuracy? Recent results have shown that continuously\\ndifferentiable and stabilizing control policies can be well-approximated using\\nneural networks with hard guarantees on performance, yet often even the\\nsimplest constrained control problems are not smooth. To address this void, in\\nthis paper we study smooth approximations of linear Model Predictive Control\\n(MPC) policies, in which hard constraints are replaced by barrier functions,\\na.k.a. barrier MPC. In particular, we show that barrier MPC inherits the\\nexponential stability properties of the original non-smooth MPC policy. Using a\\ncareful analysis of the proposed barrier MPC, we show that its smoothness\\nconstant can be carefully controlled, thereby paving the way for new sample\\ncomplexity results for approximating MPC policies from sampled state-action\\npairs. As people become more aware of their food choices, food computation models\\nhave become increasingly popular in assisting people in maintaining healthy\\neating habits. For example, food recommendation systems analyze recipe\\ninstructions to assess nutritional contents and provide recipe recommendations.\\nThe recent and remarkable successes of generative AI methods, such as\\nauto-regressive large language models, can lead to robust methods for a more\\ncomprehensive understanding of recipes for healthy food recommendations beyond\\nsurface-level nutrition content assessments. In this study, we explore the use\\nof generative AI methods to extend current food computation models, primarily\\ninvolving the analysis of nutrition and ingredients, to also incorporate\\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\\nCooking actions are notoriously hard to model using statistical learning\\nmethods due to irregular data patterns - significantly varying natural language\\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\\nfar more frequently than marinating the meat). The prototypical approach to\\nhandling irregular data patterns is to increase the volume of data that the\\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\\nthese problems are further compounded with larger data volumes presenting a\\nunique challenge that is not easily handled by simply scaling up. In this work,\\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\\nreliably generate cooking actions from recipes, despite difficulties with\\nirregular data patterns, while also outperforming Large Language Models and\\nother strong baselines. The performance of neural networks in content-based image retrieval (CBIR) is\\nhighly influenced by the chosen loss (objective) function. The majority of\\nobjective functions for neural models can be divided into metric learning and\\nstatistical learning. Metric learning approaches require a pair mining strategy\\nthat often lacks efficiency, while statistical learning approaches are not\\ngenerating highly compact features due to their indirect feature optimization.\\nTo this end, we propose a novel repeller-attractor loss that falls in the\\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\\nneed of generating pairs. Our loss is formed of three components. One leading\\nobjective ensures that the learned features are attracted to each designated\\nlearnable class anchor. The second loss component regulates the anchors and\\nforces them to be separable by a margin, while the third objective ensures that\\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\\ntwo-stage retrieval system by harnessing the learned class anchors during the\\nfirst stage of the retrieval process, eliminating the need of comparing the\\nquery with every image in the database. We establish a set of four datasets\\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\\nobjective in the context of few-shot and full-set training on the CBIR task, by\\nusing both convolutional and transformer architectures. Compared to existing\\nobjective functions, our empirical evidence shows that the proposed objective\\nis generating superior and more consistent results. We present a new framework for deriving bounds on the generalization bound of\\nstatistical learning algorithms from the perspective of online learning.\\nSpecifically, we construct an online learning game called the \\\"generalization\\ngame\\\", where an online learner is trying to compete with a fixed statistical\\nlearning algorithm in predicting the sequence of generalization gaps on a\\ntraining set of i.i.d. data points. We establish a connection between the\\nonline and statistical learning setting by showing that the existence of an\\nonline learning algorithm with bounded regret in this game implies a bound on\\nthe generalization error of the statistical learning algorithm, up to a\\nmartingale concentration term that is independent of the complexity of the\\nstatistical learning method. This technique allows us to recover several\\nstandard generalization bounds including a range of PAC-Bayesian and\\ninformation-theoretic guarantees, as well as generalizations thereof. Many machine learning approaches for decision making, such as reinforcement\\nlearning, rely on simulators or predictive models to forecast the\\ntime-evolution of quantities of interest, e.g., the state of an agent or the\\nreward of a policy. Forecasts of such complex phenomena are commonly described\\nby highly nonlinear dynamical systems, making their use in optimization-based\\ndecision-making challenging. Koopman operator theory offers a beneficial\\nparadigm for addressing this problem by characterizing forecasts via linear\\ntime-invariant (LTI) ODEs, turning multi-step forecasts into sparse matrix\\nmultiplication. Though there exists a variety of learning approaches, they\\nusually lack crucial learning-theoretic guarantees, making the behavior of the\\nobtained models with increasing data and dimensionality unclear. We address the\\naforementioned by deriving a universal Koopman-invariant reproducing kernel\\nHilbert space (RKHS) that solely spans transformations into LTI dynamical\\nsystems. The resulting Koopman Kernel Regression (KKR) framework enables the\\nuse of statistical learning tools from function approximation for novel\\nconvergence results and generalization error bounds under weaker assumptions\\nthan existing work. Our experiments demonstrate superior forecasting\\nperformance compared to Koopman operator and sequential data predictors in\\nRKHS. A core principle in statistical learning is that smoothness of target\\nfunctions allows to break the curse of dimensionality. However, learning a\\nsmooth function seems to require enough samples close to one another to get\\nmeaningful estimate of high-order derivatives, which would be hard in machine\\nlearning problems where the ratio between number of data and input dimension is\\nrelatively small. By deriving new lower bounds on the generalization error,\\nthis paper formalizes such an intuition, before investigating the role of\\nconstants and transitory regimes which are usually not depicted beyond\\nclassical learning theory statements while they play a dominant role in\\npractice. The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to\\napproximate Bayesian posterior distributions in statistical learning procedures\\nwith large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC)\\nalgorithms, SGLD is not stationary with respect to the posterior distribution;\\ntwo sources of error appear: The first error is introduced by an\\nEuler--Maruyama discretisation of a Langevin diffusion process, the second\\nerror comes from the data subsampling that enables its use in large-scale data\\nsettings. In this work, we consider an idealised version of SGLD to analyse the\\nmethod's pure subsampling error that we then see as a best-case error for\\ndiffusion-based subsampling MCMC methods. Indeed, we introduce and study the\\nStochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov\\nprocess that follows the Langevin diffusion corresponding to a data subset and\\nswitches this data subset after exponential waiting times. There, we show that\\nthe Wasserstein distance between the posterior and the limiting distribution of\\nSGLDiff is bounded above by a fractional power of the mean waiting time.\\nImportantly, this fractional power does not depend on the dimension of the\\nstate space. We bring our results into context with other analyses of SGLD. Anomaly detection is widely used in a broad range of domains from\\ncybersecurity to manufacturing, finance, and so on. Deep learning based anomaly\\ndetection has recently drawn much attention because of its superior capability\\nof recognizing complex data patterns and identifying outliers accurately.\\nHowever, deep learning models are typically iteratively optimized in a central\\nserver with input data gathered from edge devices, and such data transfer\\nbetween edge devices and the central server impose substantial overhead on the\\nnetwork and incur additional latency and energy consumption. To overcome this\\nproblem, we propose a fully-automated, lightweight, statistical learning based\\nanomaly detection framework called LightESD. It is an on-device learning method\\nwithout the need for data transfer between edge and server, and is extremely\\nlightweight that most low-end edge devices can easily afford with negligible\\ndelay, CPU/memory utilization, and power consumption. Yet, it achieves highly\\ncompetitive detection accuracy. Another salient feature is that it can\\nauto-adapt to probably any dataset without manually setting or configuring\\nmodel parameters or hyperparameters, which is a drawback of most existing\\nmethods. We focus on time series data due to its pervasiveness in edge\\napplications such as IoT. Our evaluation demonstrates that LightESD outperforms\\nother SOTA methods on detection accuracy, efficiency, and resource consumption.\\nAdditionally, its fully automated feature gives it another competitive\\nadvantage in terms of practical usability and generalizability. Statistical learning and logical reasoning are two major fields of AI\\nexpected to be unified for human-like machine intelligence. Most existing work\\nconsiders how to combine existing logical and statistical systems. However,\\nthere is no theory of inference so far explaining how basic approaches to\\nstatistical learning and logical reasoning stem from a common principle.\\nInspired by the fact that much empirical work in neuroscience suggests Bayesian\\n(or probabilistic generative) approaches to brain function including learning\\nand reasoning, we here propose a simple Bayesian model of logical reasoning and\\nstatistical learning. The theory is statistically correct as it satisfies\\nKolmogorov's axioms, is consistent with both Fenstad's representation theorem\\nand maximum likelihood estimation and performs exact Bayesian inference with a\\nlinear-time complexity. The theory is logically correct as it is a data-driven\\ngeneralisation of uncertain reasoning from consistency, possibility,\\ninconsistency and impossibility. The theory is correct in terms of machine\\nlearning as its solution to generation and prediction tasks on the MNIST\\ndataset is not only empirically reasonable but also theoretically correct\\nagainst the K nearest neighbour method. We simply model how data causes\\nsymbolic knowledge in terms of its satisfiability in formal logic. Symbolic\\nreasoning emerges as a result of the process of going the causality forwards\\nand backwards. The forward and backward processes correspond to an\\ninterpretation and inverse interpretation in formal logic, respectively. The\\ninverse interpretation differentiates our work from the mainstream often\\nreferred to as inverse entailment, inverse deduction or inverse resolution. The\\nperspective gives new insights into learning and reasoning towards human-like\\nmachine intelligence. With growing capabilities of large language models, prompting them has become\\nthe dominant way to access them. This has motivated the development of\\nstrategies for automatically selecting effective language prompts. In this\\npaper, we introduce prompt flatness, a new metric to quantify the expected\\nutility of a language prompt. This metric is inspired by flatness\\nregularization in statistical learning that quantifies the robustness of the\\nmodel towards its parameter perturbations. We provide theoretical foundations\\nfor this metric and its relationship with other prompt selection metrics,\\nproviding a comprehensive understanding of existing methods. Empirically, we\\nshow that combining prompt flatness with existing metrics improves both\\nperformance and sample efficiency. Our metric outperforms the previous prompt\\nselection metrics with an average increase of 5% in accuracy and 10% in Pearson\\ncorrelation across 6 classification benchmarks. One of the central questions in the theory of deep learning is to understand\\nhow neural networks learn hierarchical features. The ability of deep networks\\nto extract salient features is crucial to both their outstanding generalization\\nability and the modern deep learning paradigm of pretraining and finetuneing.\\nHowever, this feature learning process remains poorly understood from a\\ntheoretical perspective, with existing analyses largely restricted to two-layer\\nnetworks. In this work we show that three-layer neural networks have provably\\nricher feature learning capabilities than two-layer networks. We analyze the\\nfeatures learned by a three-layer network trained with layer-wise gradient\\ndescent, and present a general purpose theorem which upper bounds the sample\\ncomplexity and width needed to achieve low test error when the target has\\nspecific hierarchical structure. We instantiate our framework in specific\\nstatistical learning settings -- single-index models and functions of quadratic\\nfeatures -- and show that in the latter setting three-layer networks obtain a\\nsample complexity improvement over all existing guarantees for two-layer\\nnetworks. Crucially, this sample complexity improvement relies on the ability\\nof three-layer networks to efficiently learn nonlinear features. We then\\nestablish a concrete optimization-based depth separation by constructing a\\nfunction which is efficiently learnable via gradient descent on a three-layer\\nnetwork, yet cannot be learned efficiently by a two-layer network. Our work\\nmakes progress towards understanding the provable benefit of three-layer neural\\nnetworks over two-layer networks in the feature learning regime. In this paper I propose a generative model of supervised learning that\\nunifies two approaches to supervised learning, using a concept of a correct\\nloss function. Addressing two measurability problems, which have been ignored\\nin statistical learning theory, I propose to use convergence in outer\\nprobability to characterize the consistency of a learning algorithm. Building\\nupon these results, I extend a result due to Cucker-Smale, which addresses the\\nlearnability of a regression model, to the setting of a conditional probability\\nestimation problem. Additionally, I present a variant of Vapnik-Stefanuyk's\\nregularization method for solving stochastic ill-posed problems, and using it\\nto prove the generalizability of overparameterized supervised learning models. Optimal margin Distribution Machine (ODM) is a newly proposed statistical\\nlearning framework rooting in the novel margin theory, which demonstrates\\nbetter generalization performance than the traditional large margin based\\ncounterparts. Nonetheless, it suffers from the ubiquitous scalability problem\\nregarding both computation time and memory as other kernel methods. This paper\\nproposes a scalable ODM, which can achieve nearly ten times speedup compared to\\nthe original ODM training method. For nonlinear kernels, we propose a novel\\ndistribution-aware partition method to make the local ODM trained on each\\npartition be close and converge fast to the global one. When linear kernel is\\napplied, we extend a communication efficient SVRG method to accelerate the\\ntraining further. Extensive empirical studies validate that our proposed method\\nis highly computational efficient and almost never worsen the generalization. We study the problem of learning mixtures of Gaussians with censored data.\\nStatistical learning with censored data is a classical problem, with numerous\\npractical applications, however, finite-sample guarantees for even simple\\nlatent variable models such as Gaussian mixtures are missing. Formally, we are\\ngiven censored data from a mixture of univariate Gaussians $$\\n  \\\\sum_{i=1}^k w_i \\\\mathcal{N}(\\\\mu_i,\\\\sigma^2), $$ i.e. the sample is observed\\nonly if it lies inside a set $S$. The goal is to learn the weights $w_i$ and\\nthe means $\\\\mu_i$. We propose an algorithm that takes only\\n$\\\\frac{1}{\\\\varepsilon^{O(k)}}$ samples to estimate the weights $w_i$ and the\\nmeans $\\\\mu_i$ within $\\\\varepsilon$ error. Feature selection is one of the most relevant processes in any methodology\\nfor creating a statistical learning model. Usually, existing algorithms\\nestablish some criterion to select the most influential variables, discarding\\nthose that do not contribute to the model with any relevant information. This\\nmethodology makes sense in a static situation where the joint distribution of\\nthe data does not vary over time. However, when dealing with real data, it is\\ncommon to encounter the problem of the dataset shift and, specifically, changes\\nin the relationships between variables (concept shift). In this case, the\\ninfluence of a variable cannot be the only indicator of its quality as a\\nregressor of the model, since the relationship learned in the training phase\\nmay not correspond to the current situation. In tackling this problem, our\\napproach establishes a direct relationship between the Shapley values and\\nprediction errors, operating at a more local level to effectively detect the\\nindividual biases introduced by each variable. The proposed methodology is\\nevaluated through various examples, including synthetic scenarios mimicking\\nsudden and incremental shift situations, as well as two real-world cases\\ncharacterized by concept shifts. Additionally, we perform three analyses of\\nstandard situations to assess the algorithm's robustness in the absence of\\nshifts. The results demonstrate that our proposed algorithm significantly\\noutperforms state-of-the-art feature selection methods in concept shift\\nscenarios, while matching the performance of existing methodologies in static\\nsituations. Despite the huge recent breakthroughs in neural networks (NNs) for artificial\\nintelligence (specifically deep convolutional networks) such NNs do not achieve\\nhuman-level performance: they can be hacked by images that would fool no human\\nand lack `common sense'. It has been argued that a basis of human-level\\nintelligence is mankind's ability to perform relational reasoning: the\\ncomparison of different objects, measuring similarity, grasping of relations\\nbetween objects and the converse, figuring out the odd one out in a set of\\nobjects. Mankind can even do this with objects they have never seen before.\\nHere we show how ClusterFlow, a semi-supervised hierarchical clustering\\nframework can operate on trained NNs utilising the rich multi-dimensional class\\nand feature data found at the pre-SoftMax layer to build a hyperspacial map of\\nclasses/features and this adds more human-like functionality to modern deep\\nconvolutional neural networks. We demonstrate this with 3 tasks. 1. the\\nstatistical learning based `mistakes' made by infants when attending to images\\nof cats and dogs. 2. improving both the resilience to hacking images and the\\naccurate measure of certainty in deep-NNs. 3. Relational reasoning over sets of\\nimages, including those not known to the NN nor seen before. We also\\ndemonstrate that ClusterFlow can work on non-NN data and deal with missing data\\nby testing it on a Chemistry dataset. This work suggests that modern deep NNs\\ncan be made more human-like without re-training of the NNs. As it is known that\\nsome methods used in deep and convolutional NNs are not biologically plausible\\nor perhaps even the best approach: the ClusterFlow framework can sit on top of\\nany NN and will be a useful tool to add as NNs are improved in this regard. We study the generalization error of statistical learning models in a\\nFederated Learning (FL) setting. Specifically, there are $K$ devices or\\nclients, each holding an independent own dataset of size $n$. Individual\\nmodels, learned locally via Stochastic Gradient Descent, are aggregated\\n(averaged) by a central server into a global model and then sent back to the\\ndevices. We consider multiple (say $R \\\\in \\\\mathbb N^*$) rounds of model\\naggregation and study the effect of $R$ on the generalization error of the\\nfinal aggregated model. We establish an upper bound on the generalization error\\nthat accounts explicitly for the effect of $R$ (in addition to the number of\\nparticipating devices $K$ and dataset size $n$). It is observed that, for fixed\\n$(n, K)$, the bound increases with $R$, suggesting that the generalization of\\nsuch learning algorithms is negatively affected by more frequent communication\\nwith the parameter server. Combined with the fact that the empirical risk,\\nhowever, generally decreases for larger values of $R$, this indicates that $R$\\nmight be a parameter to optimize to reduce the population risk of FL\\nalgorithms. The results of this paper, which extend straightforwardly to the\\nheterogeneous data setting, are also illustrated through numerical examples. This work provides a comprehensive derivation of the parameter gradients for\\nGATv2 [4], a widely used implementation of Graph Attention Networks (GATs).\\nGATs have proven to be powerful frameworks for processing graph-structured data\\nand, hence, have been used in a range of applications. However, the achieved\\nperformance by these attempts has been found to be inconsistent across\\ndifferent datasets and the reasons for this remains an open research question.\\nAs the gradient flow provides valuable insights into the training dynamics of\\nstatistically learning models, this work obtains the gradients for the\\ntrainable model parameters of GATv2. The gradient derivations supplement the\\nefforts of [2], where potential pitfalls of GATv2 are investigated. In statistical learning theory, determining the sample complexity of\\nrealizable binary classification for VC classes was a long-standing open\\nproblem. The results of Simon and Hanneke established sharp upper bounds in\\nthis setting. However, the reliance of their argument on the uniform\\nconvergence principle limits its applicability to more general learning\\nsettings such as multiclass classification. In this paper, we address this\\nissue by providing optimal high probability risk bounds through a framework\\nthat surpasses the limitations of uniform convergence arguments.\\n  Our framework converts the leave-one-out error of permutation invariant\\npredictors into high probability risk bounds. As an application, by adapting\\nthe one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we\\npropose an algorithm that achieves an optimal PAC bound for binary\\nclassification. Specifically, our result shows that certain aggregations of\\none-inclusion graph algorithms are optimal, addressing a variant of a classic\\nquestion posed by Warmuth.\\n  We further instantiate our framework in three settings where uniform\\nconvergence is provably suboptimal. For multiclass classification, we prove an\\noptimal risk bound that scales with the one-inclusion hypergraph density of the\\nclass, addressing the suboptimality of the analysis of Daniely and\\nShalev-Shwartz. For partial hypothesis classification, we determine the optimal\\nsample complexity bound, resolving a question posed by Alon, Hanneke, Holzman,\\nand Moran. For realizable bounded regression with absolute loss, we derive an\\noptimal risk bound that relies on a modified version of the scale-sensitive\\ndimension, refining the results of Bartlett and Long. Our rates surpass\\nstandard uniform convergence-based results due to the smaller complexity\\nmeasure in our risk bound. Kernel methods provide a powerful framework for non parametric learning. They\\nare based on kernel functions and allow learning in a rich functional space\\nwhile applying linear statistical learning tools, such as Ridge Regression or\\nSupport Vector Machines. However, standard kernel methods suffer from a\\nquadratic time and memory complexity in the number of data points and thus have\\nlimited applications in large-scale learning. In this paper, we propose Snacks,\\na new large-scale solver for Kernel Support Vector Machines. Specifically,\\nSnacks relies on a Nystr\\\\\\\"om approximation of the kernel matrix and an\\naccelerated variant of the stochastic subgradient method. We demonstrate\\nformally through a detailed empirical evaluation, that it competes with other\\nSVM solvers on a variety of benchmark datasets. Nonsmooth composite optimization with orthogonality constraints has a broad\\nspectrum of applications in statistical learning and data science. However,\\nthis problem is generally challenging to solve due to its non-convex and\\nnon-smooth nature. Existing solutions are limited by one or more of the\\nfollowing restrictions: (i) they are full gradient methods that require high\\ncomputational costs in each iteration; (ii) they are not capable of solving\\ngeneral nonsmooth composite problems; (iii) they are infeasible methods and can\\nonly achieve the feasibility of the solution at the limit point; (iv) they lack\\nrigorous convergence guarantees; (v) they only obtain weak optimality of\\ncritical points. In this paper, we propose \\\\textit{\\\\textbf{OBCD}}, a new Block\\nCoordinate Descent method for solving general nonsmooth composite problems\\nunder Orthogonality constraints. \\\\textit{\\\\textbf{OBCD}} is a feasible method\\nwith low computation complexity footprints. In each iteration, our algorithm\\nupdates $k$ rows of the solution matrix ($k\\\\geq2$ is a parameter) to preserve\\nthe constraints. Then, it solves a small-sized nonsmooth composite optimization\\nproblem under orthogonality constraints either exactly or approximately. We\\ndemonstrate that any exact block-$k$ stationary point is always an approximate\\nblock-$k$ stationary point, which is equivalent to the critical stationary\\npoint. We are particularly interested in the case where $k=2$ as the resulting\\nsubproblem reduces to a one-dimensional nonconvex problem. We propose a\\nbreakpoint searching method and a fifth-order iterative method to solve this\\nproblem efficiently and effectively. We also propose two novel greedy\\nstrategies to find a good working set to further accelerate the convergence of\\n\\\\textit{\\\\textbf{OBCD}}. Finally, we have conducted extensive experiments on\\nseveral tasks to demonstrate the superiority of our approach. We study the problem of regression in a generalized linear model (GLM) with\\nmultiple signals and latent variables. This model, which we call a matrix GLM,\\ncovers many widely studied problems in statistical learning, including mixed\\nlinear regression, max-affine regression, and mixture-of-experts. In mixed\\nlinear regression, each observation comes from one of $L$ signal vectors\\n(regressors), but we do not know which one; in max-affine regression, each\\nobservation comes from the maximum of $L$ affine functions, each defined via a\\ndifferent signal vector. The goal in all these problems is to estimate the\\nsignals, and possibly some of the latent variables, from the observations. We\\npropose a novel approximate message passing (AMP) algorithm for estimation in a\\nmatrix GLM and rigorously characterize its performance in the high-dimensional\\nlimit. This characterization is in terms of a state evolution recursion, which\\nallows us to precisely compute performance measures such as the asymptotic\\nmean-squared error. The state evolution characterization can be used to tailor\\nthe AMP algorithm to take advantage of any structural information known about\\nthe signals. Using state evolution, we derive an optimal choice of AMP\\n`denoising' functions that minimizes the estimation error in each iteration.\\n  The theoretical results are validated by numerical simulations for mixed\\nlinear regression, max-affine regression, and mixture-of-experts. For\\nmax-affine regression, we propose an algorithm that combines AMP with\\nexpectation-maximization to estimate intercepts of the model along with the\\nsignals. The numerical results show that AMP significantly outperforms other\\nestimators for mixed linear regression and max-affine regression in most\\nparameter regimes. This paper provides a comprehensive tutorial for Bayesian practitioners in\\npharmacometrics using Pumas workflows. We start by giving a brief motivation of\\nBayesian inference for pharmacometrics highlighting limitations in existing\\nsoftware that Pumas addresses. We then follow by a description of all the steps\\nof a standard Bayesian workflow for pharmacometrics using code snippets and\\nexamples. This includes: model definition, prior selection, sampling from the\\nposterior, prior and posterior simulations and predictions, counter-factual\\nsimulations and predictions, convergence diagnostics, visual predictive checks,\\nand finally model comparison with cross-validation. Finally, the background and\\nintuition behind many advanced concepts in Bayesian statistics are explained in\\nsimple language. This includes many important ideas and precautions that users\\nneed to keep in mind when performing Bayesian analysis. Many of the algorithms,\\ncodes, and ideas presented in this paper are highly applicable to clinical\\nresearch and statistical learning at large but we chose to focus our\\ndiscussions on pharmacometrics in this paper to have a narrower scope in mind\\nand given the nature of Pumas as a software primarily for pharmacometricians. We propose an adjusted Wasserstein distributionally robust estimator -- based\\non a nonlinear transformation of the Wasserstein distributionally robust (WDRO)\\nestimator in statistical learning. The classic WDRO estimator is asymptotically\\nbiased, while our adjusted WDRO estimator is asymptotically unbiased, resulting\\nin a smaller asymptotic mean squared error. Meanwhile, the proposed adjusted\\nWDRO has an out-of-sample performance guarantee. Further, under certain\\nconditions, our proposed adjustment technique provides a general principle to\\nde-bias asymptotically biased estimators. Specifically, we will investigate how\\nthe adjusted WDRO estimator is developed in the generalized linear model,\\nincluding logistic regression, linear regression, and Poisson regression.\\nNumerical experiments demonstrate the favorable practical performance of the\\nadjusted estimator over the classic one. Space debris is a major problem in space exploration. International bodies\\ncontinuously monitor a large database of orbiting objects and emit warnings in\\nthe form of conjunction data messages. An important question for satellite\\noperators is to estimate when fresh information will arrive so that they can\\nreact timely but sparingly with satellite maneuvers. We propose a statistical\\nlearning model of the message arrival process, allowing us to answer two\\nimportant questions: (1) Will there be any new message in the next specified\\ntime interval? (2) When exactly and with what uncertainty will the next message\\narrive? The average prediction error for question (2) of our Bayesian Poisson\\nprocess model is smaller than the baseline in more than 4 hours in a test set\\nof 50k close encounter events. The spatial photonic Ising machine (SPIM) [D. Pierangeli et al., Phys. Rev.\\nLett. 122, 213902 (2019)] is a promising optical architecture utilizing spatial\\nlight modulation for solving large-scale combinatorial optimization problems\\nefficiently. The primitive version of the SPIM, however, can accommodate Ising\\nproblems with only rank-one interaction matrices. In this Letter, we propose a\\nnew computing model for the SPIM that can accommodate any Ising problem without\\nchanging its optical implementation. The proposed model is particularly\\nefficient for Ising problems with low-rank interaction matrices, such as\\nknapsack problems. Moreover, it acquires the learning ability of Boltzmann\\nmachines. We demonstrate that learning, classification, and sampling of the\\nMNIST handwritten digit images are achieved efficiently using the model with\\nlow-rank interactions. Thus, the proposed model exhibits higher practical\\napplicability to various problems of combinatorial optimization and statistical\\nlearning, without losing the scalability inherent in the SPIM architecture. As the issue of robustness in AI systems becomes vital, statistical learning\\ntechniques that are reliable even in presence of partly contaminated data have\\nto be developed. Preference data, in the form of (complete) rankings in the\\nsimplest situations, are no exception and the demand for appropriate concepts\\nand tools is all the more pressing given that technologies fed by or producing\\nthis type of data (e.g. search engines, recommending systems) are now massively\\ndeployed. However, the lack of vector space structure for the set of rankings\\n(i.e. the symmetric group $\\\\mathfrak{S}_n$) and the complex nature of\\nstatistics considered in ranking data analysis make the formulation of\\nrobustness objectives in this domain challenging. In this paper, we introduce\\nnotions of robustness, together with dedicated statistical methods, for\\nConsensus Ranking the flagship problem in ranking data analysis, aiming at\\nsummarizing a probability distribution on $\\\\mathfrak{S}_n$ by a median ranking.\\nPrecisely, we propose specific extensions of the popular concept of breakdown\\npoint, tailored to consensus ranking, and address the related computational\\nissues. Beyond the theoretical contributions, the relevance of the approach\\nproposed is supported by an experimental study. Neural network approaches to approximate the ground state of quantum\\nhamiltonians require the numerical solution of a highly nonlinear optimization\\nproblem. We introduce a statistical learning approach that makes the\\noptimization trivial by using kernel methods. Our scheme is an approximate\\nrealization of the power method, where supervised learning is used to learn the\\nnext step of the power iteration. We show that the ground state properties of\\narbitrary gapped quantum hamiltonians can be reached with polynomial resources\\nunder the assumption that the supervised learning is efficient. Using kernel\\nridge regression, we provide numerical evidence that the learning assumption is\\nverified by applying our scheme to find the ground states of several\\nprototypical interacting many-body quantum systems, both in one and two\\ndimensions, showing the flexibility of our approach. Cross-validation techniques for risk estimation and model selection are\\nwidely used in statistics and machine learning. However, the understanding of\\nthe theoretical properties of learning via model selection with\\ncross-validation risk estimation is quite low in face of its widespread use. In\\nthis context, this paper presents learning via model selection with\\ncross-validation risk estimation as a general systematic learning framework\\nwithin classical statistical learning theory and establishes distribution-free\\ndeviation bounds in terms of VC dimension, giving detailed proofs of the\\nresults and considering both bounded and unbounded loss functions. We also\\ndeduce conditions under which the deviation bounds of learning via model\\nselection are tighter than that of learning via empirical risk minimization in\\nthe whole hypotheses space, supporting the better performance of model\\nselection frameworks observed empirically in some instances. We consider a binary supervised learning classification problem where instead\\nof having data in a finite-dimensional Euclidean space, we observe measures on\\na compact space $\\\\mathcal{X}$. Formally, we observe data $D_N = (\\\\mu_1, Y_1),\\n\\\\ldots, (\\\\mu_N, Y_N)$ where $\\\\mu_i$ is a measure on $\\\\mathcal{X}$ and $Y_i$ is\\na label in $\\\\{0, 1\\\\}$. Given a set $\\\\mathcal{F}$ of base-classifiers on\\n$\\\\mathcal{X}$, we build corresponding classifiers in the space of measures. We\\nprovide upper and lower bounds on the Rademacher complexity of this new class\\nof classifiers that can be expressed simply in terms of corresponding\\nquantities for the class $\\\\mathcal{F}$. If the measures $\\\\mu_i$ are uniform\\nover a finite set, this classification task boils down to a multi-instance\\nlearning problem. However, our approach allows more flexibility and diversity\\nin the input data we can deal with. While such a framework has many possible\\napplications, this work strongly emphasizes on classifying data via topological\\ndescriptors called persistence diagrams. These objects are discrete measures on\\n$\\\\mathbb{R}^2$, where the coordinates of each point correspond to the range of\\nscales at which a topological feature exists. We will present several\\nclassifiers on measures and show how they can heuristically and theoretically\\nenable a good classification performance in various settings in the case of\\npersistence diagrams. Statistical learning methods have been growing in popularity in recent years.\\nMany of these procedures have parameters that must be tuned for models to\\nperform well. Research has been extensive in neural networks, but not for many\\nother learning methods. We looked at the behavior of tuning parameters for\\nsupport vector machines, gradient boosting machines, and adaboost in both a\\nclassification and regression setting. We used grid search to identify ranges\\nof tuning parameters where good models can be found across many different\\ndatasets. We then explored different optimization algorithms to select a model\\nacross the tuning parameter space. Models selected by the optimization\\nalgorithm were compared to the best models obtained through grid search to\\nselect well performing algorithms. This information was used to create an R\\npackage, EZtune, that automatically tunes support vector machines and boosted\\ntrees. Many planning and decision activities in logistics and supply chain\\nmanagement are based on forecasts of multiple time dependent factors.\\nTherefore, the quality of planning depends on the quality of the forecasts. We\\ncompare various forecasting methods in terms of out of the box forecasting\\nperformance on a broad set of simulated time series. We simulate various linear\\nand non-linear time series and look at the one step forecast performance of\\nstatistical learning methods. The National Football League (NFL) Scouting Combine serves as a tool to\\nevaluate the skills of prospective players and assess their readiness to play\\nin the NFL. The development of machine learning brings new opportunities in\\nassessing the utility of the Scouting Combine. Using machine and statistical\\nlearning, it may be possible to predict future success of prospective athletes,\\nas well as predict which Scouting Combine tests are the most important. Results\\nfrom statistical learning research have been contradicting whether the Scouting\\ncombine is a useful metric for player success. In this study, we investigate if\\nmachine learning can be used to determine matriculation and future success in\\nthe NFL. Using Scouting Combine data, we evaluate six different algorithms'\\nability to predict whether a potential draft pick will play a single NFL snap\\n(matriculation). If a player is drafted, we predict how many snaps they go on\\nto play (success). We are able to predict matriculation with 83% accuracy;\\nhowever, we are unable to predict later success. Our best performing algorithm\\nreturns large error and low explained variance (RMSE=1,210 snaps;\\n${R}^2$=0.17). These findings indicate that while the Scouting Combine can\\npredict NFL matriculation, it may not be a reliable predictor of long-term\\nplayer success. Constructing useful representations across a large number of tasks is a key\\nrequirement for sample-efficient intelligent systems. A traditional idea in\\nmultitask learning (MTL) is building a shared representation across tasks which\\ncan then be adapted to new tasks by tuning last layers. A desirable refinement\\nof using a shared one-fits-all representation is to construct task-specific\\nrepresentations. To this end, recent PathNet/muNet architectures represent\\nindividual tasks as pathways within a larger supernet. The subnetworks induced\\nby pathways can be viewed as task-specific representations that are composition\\nof modules within supernet's computation graph. This work explores the pathways\\nproposal from the lens of statistical learning: We first develop novel\\ngeneralization bounds for empirical risk minimization problems learning\\nmultiple tasks over multiple paths (Multipath MTL). In conjunction, we\\nformalize the benefits of resulting multipath representation when adapting to\\nnew downstream tasks. Our bounds are expressed in terms of Gaussian complexity,\\nlead to tangible guarantees for the class of linear representations, and\\nprovide novel insights into the quality and benefits of a multipath\\nrepresentation. When computation graph is a tree, Multipath MTL hierarchically\\nclusters the tasks and builds cluster-specific representations. We provide\\nfurther discussion and experiments for hierarchical MTL and rigorously identify\\nthe conditions under which Multipath MTL is provably superior to traditional\\nMTL approaches with shallow supernets. This paper deals with the scenario approach to robust optimization. This\\nrelies on a random sampling of the possibly infinite number of constraints\\ninduced by uncertainties in the parameters of an optimization problem. Solving\\nthe resulting random program yields a solution for which the quality is\\nmeasured in terms of the probability of violating the constraints for a random\\nvalue of the uncertainties, typically unseen before. Another central issue is\\nthe determination of the sample complexity, i.e., the number of random\\nconstraints (or scenarios) that one must consider in order to guarantee a\\ncertain level of reliability. In this paper, we introduce the notion of margin\\nto improve upon standard results in this field. In particular, using tools from\\nstatistical learning theory, we show that the sample complexity of a class of\\nrandom programs does not explicitly depend on the number of variables. In\\naddition, within the considered class, that includes polynomial constraints\\namong others, this result holds for both convex and nonconvex instances with\\nthe same level of guarantees. We also derive a posteriori bounds on the\\nprobability of violation and sketch a regularization approach that could be\\nused to improve the reliability of computed solutions on the basis of these\\nbounds. In the classic regression problem, the value of a real-valued random variable\\n$Y$ is to be predicted based on the observation of a random vector $X$, taking\\nits values in $\\\\mathbb{R}^d$ with $d\\\\geq 1$ say. The statistical learning\\nproblem consists in building a predictive function $\\\\hat{f}:\\\\mathbb{R}^d\\\\to\\n\\\\mathbb{R}$ based on independent copies of the pair $(X,Y)$ so that $Y$ is\\napproximated by $\\\\hat{f}(X)$ with minimum error in the mean-squared sense.\\nMotivated by various applications, ranging from environmental sciences to\\nfinance or insurance, special attention is paid here to the case of extreme\\n(i.e. very large) observations $X$. Because of their rarity, they contribute in\\na negligible manner to the (empirical) error and the predictive performance of\\nempirical quadratic risk minimizers can be consequently very poor in extreme\\nregions. In this paper, we develop a general framework for regression in the\\nextremes. It is assumed that $X$'s conditional distribution given $Y$ belongs\\nto a non parametric class of heavy-tailed probability distributions. It is then\\nshown that an asymptotic notion of risk can be tailored to summarize\\nappropriately predictive performance in extreme regions of the input space. It\\nis also proved that minimization of an empirical and non asymptotic version of\\nthis 'extreme risk', based on a fraction of the largest observations solely,\\nyields regression functions with good generalization capacity. In addition,\\nnumerical results providing strong empirical evidence of the relevance of the\\napproach proposed are displayed. Transport map methods offer a powerful statistical learning tool that can\\ncouple a target high-dimensional random variable with some reference random\\nvariable using invertible transformations. This paper presents new\\ncomputational techniques for building the Knothe--Rosenblatt (KR) rearrangement\\nbased on general separable functions. We first introduce a new construction of\\nthe KR rearrangement -- with guaranteed invertibility in its numerical\\nimplementation -- based on approximating the density of the target random\\nvariable using tensor-product spectral polynomials and downward closed sparse\\nindex sets. Compared to other constructions of KR arrangements based on either\\nmulti-linear approximations or nonlinear optimizations, our new construction\\nonly relies on a weighted least square approximation procedure. Then, inspired\\nby the recently developed deep tensor trains (Cui and Dolgov, Found. Comput.\\nMath. 22:1863--1922, 2022), we enhance the approximation power of sparse\\npolynomials by preconditioning the density approximation problem using\\ncompositions of maps. This is particularly suitable for high-dimensional and\\nconcentrated probability densities commonly seen in many applications. We\\napproximate the complicated target density by a composition of self-reinforced\\nKR rearrangements, in which previously constructed KR rearrangements -- based\\non the same approximation ansatz -- are used to precondition the density\\napproximation problem for building each new KR rearrangement. We demonstrate\\nthe efficiency of our proposed methods and the importance of using the\\ncomposite map on several inverse problems governed by ordinary differential\\nequations (ODEs) and partial differential equations (PDEs). Statistical learning models have been growing in popularity in recent years.\\nMany of these models have hyperparameters that must be tuned for models to\\nperform well. Tuning these parameters is not trivial. EZtune is an R package\\nwith a simple user interface that can tune support vector machines, adaboost,\\ngradient boosting machines, and elastic net. We first provide a brief summary\\nof the the models that EZtune can tune, including a discussion of each of their\\nhyperparameters. We then compare the ease of using EZtune, caret, and\\ntidymodels. This is followed with a comparison of the accuracy and computation\\ntimes for models tuned with EZtune and tidymodels. We conclude with a\\ndemonstration of how how EZtune can be used to help select a final model with\\noptimal predictive power. Our comparison shows that EZtune can tune support\\nvector machines and gradient boosting machines with EZtune also provides a user\\ninterface that is easy to use for a novice to statistical learning models or R. This paper studies the prediction of a target $\\\\mathbf{z}$ from a pair of\\nrandom variables $(\\\\mathbf{x},\\\\mathbf{y})$, where the ground-truth predictor is\\nadditive $\\\\mathbb{E}[\\\\mathbf{z} \\\\mid \\\\mathbf{x},\\\\mathbf{y}] =\\nf_\\\\star(\\\\mathbf{x}) +g_{\\\\star}(\\\\mathbf{y})$. We study the performance of\\nempirical risk minimization (ERM) over functions $f+g$, $f \\\\in F$ and $g \\\\in\\nG$, fit on a given training distribution, but evaluated on a test distribution\\nwhich exhibits covariate shift. We show that, when the class $F$ is \\\"simpler\\\"\\nthan $G$ (measured, e.g., in terms of its metric entropy), our predictor is\\nmore resilient to heterogeneous covariate shifts} in which the shift in\\n$\\\\mathbf{x}$ is much greater than that in $\\\\mathbf{y}$. Our analysis proceeds\\nby demonstrating that ERM behaves qualitatively similarly to orthogonal machine\\nlearning: the rate at which ERM recovers the $f$-component of the predictor has\\nonly a lower-order dependence on the complexity of the class $G$, adjusted for\\npartial non-indentifiability introduced by the additive structure. These\\nresults rely on a novel H\\\\\\\"older style inequality for the Dudley integral which\\nmay be of independent interest. Moreover, we corroborate our theoretical\\nfindings with experiments demonstrating improved resilience to shifts in\\n\\\"simpler\\\" features across numerous domains. The introduction of machine learning (ML) techniques to the field of survival\\nanalysis has increased the flexibility of modeling approaches, and ML based\\nmodels have become state-of-the-art. These models optimize their own cost\\nfunctions, and their performance is often evaluated using the concordance index\\n(C-index). From a statistical learning perspective, it is therefore an\\nimportant problem to analyze the relationship between the optimizers of the\\nC-index and those of the ML cost functions. We address this issue by providing\\nC-index Fisher-consistency results and excess risk bounds for several of the\\ncommonly used cost functions in survival analysis. We identify conditions under\\nwhich they are consistent, under the form of three nested families of survival\\nmodels. We also study the general case where no model assumption is made and\\npresent a new, off-the-shelf method that is shown to be consistent with the\\nC-index, although computationally expensive at inference. Finally, we perform\\nlimited numerical experiments with simulated data to illustrate our theoretical\\nfindings. Low-rank multivariate regression (LRMR) is an important statistical learning\\nmodel that combines highly correlated tasks as a multiresponse regression\\nproblem with low-rank priori on the coefficient matrix. In this paper, we study\\nquantized LRMR, a practical setting where the responses and/or the covariates\\nare discretized to finite precision. We focus on the estimation of the\\nunderlying coefficient matrix. To make consistent estimator that could achieve\\narbitrarily small error possible, we employ uniform quantization with random\\ndithering, i.e., we add appropriate random noise to the data before\\nquantization. Specifically, uniform dither and triangular dither are used for\\nresponses and covariates, respectively. Based on the quantized data, we propose\\nthe constrained Lasso and regularized Lasso estimators, and derive the\\nnon-asymptotic error bounds. With the aid of dithering, the estimators achieve\\nminimax optimal rate, while quantization only slightly worsens the\\nmultiplicative factor in the error rate. Moreover, we extend our results to a\\nlow-rank regression model with matrix responses. We corroborate and demonstrate\\nour theoretical results via simulations on synthetic data or image restoration. In this work, we further develop the Physics-informed Spectral Learning\\n(PiSL) by Espath et al. \\\\cite{Esp21} based on a discrete $L^2$ projection to\\nsolve the discrete Hodge--Helmholtz decomposition from sparse data. Within this\\nphysics-informed statistical learning framework, we adaptively build a sparse\\nset of Fourier basis functions with corresponding coefficients by solving a\\nsequence of minimization problems where the set of basis functions is augmented\\ngreedily at each optimization problem. Moreover, our PiSL computational\\nframework enjoys spectral (exponential) convergence. We regularize the\\nminimization problems with the seminorm of the fractional Sobolev space in a\\nTikhonov fashion. In the Fourier setting, the divergence- and curl-free\\nconstraints become a finite set of linear algebraic equations. The proposed\\ncomputational framework combines supervised and unsupervised learning\\ntechniques in that we use data concomitantly with the projection onto\\ndivergence- and curl-free spaces. We assess the capabilities of our method in\\nvarious numerical examples including the `Storm of the Century' with satellite\\ndata from 1993. We consider the problem of stochastic convex optimization with exp-concave\\nlosses using Empirical Risk Minimization in a convex class. Answering a\\nquestion raised in several prior works, we provide a $O( d / n + \\\\log( 1 /\\n\\\\delta) / n )$ excess risk bound valid for a wide class of bounded exp-concave\\nlosses, where $d$ is the dimension of the convex reference set, $n$ is the\\nsample size, and $\\\\delta$ is the confidence level. Our result is based on a\\nunified geometric assumption on the gradient of losses and the notion of local\\nnorms. In recent years, large amounts of electronic health records (EHRs) concerning\\nchronic diseases, such as cancer, diabetes, and mental disease, have been\\ncollected to facilitate medical diagnosis. Modeling the dynamic properties of\\nEHRs related to chronic diseases can be efficiently done using dynamic\\ntreatment regimes (DTRs), which are a set of sequential decision rules. While\\nReinforcement learning (RL) is a widely used method for creating DTRs, there is\\nongoing research in developing RL algorithms that can effectively handle large\\namounts of data. In this paper, we present a novel approach, a distributed\\nQ-learning algorithm, for generating DTRs. The novelties of our research are as\\nfollows: 1) From a methodological perspective, we present a novel and scalable\\napproach for generating DTRs by combining distributed learning with Q-learning.\\nThe proposed approach is specifically designed to handle large amounts of data\\nand effectively generate DTRs. 2) From a theoretical standpoint, we provide\\ngeneralization error bounds for the proposed distributed Q-learning algorithm,\\nwhich are derived within the framework of statistical learning theory. These\\nbounds quantify the relationships between sample size, prediction accuracy, and\\ncomputational burden, providing insights into the performance of the algorithm.\\n3) From an applied perspective, we demonstrate the effectiveness of our\\nproposed distributed Q-learning algorithm for DTRs by applying it to clinical\\ncancer treatments. The results show that our algorithm outperforms both\\ntraditional linear Q-learning and commonly used deep Q-learning in terms of\\nboth prediction accuracy and computation cost. Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\\nprogressing from research prototypes to commercial developer tools. As such,\\nunderstanding the capabilities and limitations of such models is becoming\\ncritical. However, the abilities of these models are typically measured using\\nautomated metrics that often only reveal a portion of their real-world\\nperformance. While, in general, the performance of NCMs appears promising,\\ncurrently much is unknown about how such models arrive at decisions. To this\\nend, this paper introduces $do_{code}$, a post-hoc interpretability methodology\\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\\nis based upon causal inference to enable programming language-oriented\\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\\nto exploring different model properties, we provide a concrete instantiation\\nthat aims to mitigate the impact of spurious correlations by grounding\\nexplanations of model behavior in properties of programming languages. To\\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\\nthat our framework can provide by performing a case study on two popular deep\\nlearning architectures and nine NCMs. The results of this case study illustrate\\nthat our studied NCMs are sensitive to changes in code syntax and statistically\\nlearn to predict tokens related to blocks of code (e.g., brackets, parenthesis,\\nsemicolon) with less confounding bias as compared to other programming language\\nconstructs. These insights demonstrate the potential of $do_{code}$ as a useful\\nmodel debugging mechanism that may aid in discovering biases and limitations in\\nNCMs. Providing generalization guarantees for modern neural networks has been a\\ncrucial task in statistical learning. Recently, several studies have attempted\\nto analyze the generalization error in such settings by using tools from\\nfractal geometry. While these works have successfully introduced new\\nmathematical tools to apprehend generalization, they heavily rely on a\\nLipschitz continuity assumption, which in general does not hold for neural\\nnetworks and might make the bounds vacuous. In this work, we address this issue\\nand prove fractal geometry-based generalization bounds without requiring any\\nLipschitz assumption. To achieve this goal, we build up on a classical covering\\nargument in learning theory and introduce a data-dependent fractal dimension.\\nDespite introducing a significant amount of technical complications, this new\\nnotion lets us control the generalization error (over either fixed or random\\nhypothesis spaces) along with certain mutual information (MI) terms. To provide\\na clearer interpretation to the newly introduced MI terms, as a next step, we\\nintroduce a notion of \\\"geometric stability\\\" and link our bounds to the prior\\nart. Finally, we make a rigorous connection between the proposed data-dependent\\ndimension and topological data analysis tools, which then enables us to compute\\nthe dimension in a numerically efficient way. We support our theory with\\nexperiments conducted on various settings. Heterogeneous treatment effects (HTE) based on patients' genetic or clinical\\nfactors are of significant interest to precision medicine. Simultaneously\\nmodeling HTE and corresponding main effects for randomized clinical trials with\\nhigh-dimensional predictive markers is challenging. Motivated by the modified\\ncovariates approach, we propose a two-stage statistical learning procedure for\\nestimating HTE with optimal efficiency augmentation, generalizing to arbitrary\\ninteraction model and exploiting powerful extreme gradient boosting trees\\n(XGBoost). Target estimands for HTE are defined in the scale of mean difference\\nfor quantitative outcomes, or risk ratio for binary outcomes, which are the\\nminimizers of specialized loss functions. The first stage is to estimate the\\nmain-effect equivalency of the baseline markers on the outcome, which is then\\nused as an augmentation term in the second stage estimation for HTE. The\\nproposed two-stage procedure is robust to model mis-specification of main\\neffects and improves efficiency for estimating HTE through nonparametric\\nfunction estimation, e.g., XGBoost. A permutation test is proposed for global\\nassessment of evidence for HTE. An analysis of a genetic study in Prostate\\nCancer Prevention Trial led by the SWOG Cancer Research Network, is conducted\\nto showcase the properties and the utilities of the two-stage method. This paper presents a novel mechanism design for multi-item auction settings\\nwith uncertain bidders' type distributions. Our proposed approach utilizes\\nnonparametric density estimation to accurately estimate bidders' types from\\nhistorical bids, and is built upon the Vickrey-Clarke-Groves (VCG) mechanism,\\nensuring satisfaction of Bayesian incentive compatibility (BIC) and\\n$\\\\delta$-individual rationality (IR). To further enhance the efficiency of our\\nmechanism, we introduce two novel strategies for query reduction: a filtering\\nmethod that screens potential winners' value regions within the confidence\\nintervals generated by our estimated distribution, and a classification\\nstrategy that designates the lower bound of an interval as the estimated type\\nwhen the length is below a threshold value. Simulation experiments conducted on\\nboth small-scale and large-scale data demonstrate that our mechanism\\nconsistently outperforms existing methods in terms of revenue maximization and\\nquery reduction, particularly in large-scale scenarios. This makes our proposed\\nmechanism a highly desirable and effective option for sellers in the realm of\\nmulti-item auctions. Slope failures possess destructive power that can cause significant damage to\\nboth life and infrastructure. Monitoring slopes prone to instabilities is\\ntherefore critical in mitigating the risk posed by their failure. The purpose\\nof slope monitoring is to detect precursory signs of stability issues, such as\\nchanges in the rate of displacement with which a slope is deforming. This\\ninformation can then be used to predict the timing or probability of an\\nimminent failure in order to provide an early warning. In this study, a more\\nobjective, statistical-learning algorithm is proposed to detect and\\ncharacterise the risk of a slope failure, based on spectral analysis of\\nserially correlated displacement time series data. The algorithm is applied to\\nsatellite-based interferometric synthetic radar (InSAR) displacement time\\nseries data to retrospectively analyse the risk of the 2019 Brumadinho tailings\\ndam collapse in Brazil. Two potential risk milestones are identified and signs\\nof a definitive but emergent risk (27 February 2018 to 26 August 2018) and\\nimminent risk of collapse of the tailings dam (27 June 2018 to 24 December\\n2018) are detected by the algorithm. Importantly, this precursory indication of\\nrisk of failure is detected as early as at least five months prior to the dam\\ncollapse on 25 January 2019. The results of this study demonstrate that the\\ncombination of spectral methods and second order statistical properties of\\nInSAR displacement time series data can reveal signs of a transition into an\\nunstable deformation regime, and that this algorithm can provide sufficient\\nearly warning that could help mitigate catastrophic slope failures. Software and System logs record runtime information about processes executing\\nwithin a system. These logs have become the most critical and ubiquitous forms\\nof observability data that help developers understand system behavior, monitor\\nsystem health and resolve issues. However, the volume of logs generated can be\\nhumongous (of the order of petabytes per day) especially for complex\\ndistributed systems, such as cloud, search engine, social media, etc. This has\\npropelled a lot of research on developing AI-based log based analytics and\\nintelligence solutions that can process huge volume of raw logs and generate\\ninsights. In order to enable users to perform multiple types of AI-based log\\nanalysis tasks in a uniform manner, we introduce LogAI\\n(https://github.com/salesforce/logai), a one-stop open source library for log\\nanalytics and intelligence. LogAI supports tasks such as log summarization, log\\nclustering and log anomaly detection. It adopts the OpenTelemetry data model,\\nto enable compatibility with different log management platforms. LogAI provides\\na unified model interface and provides popular time-series, statistical\\nlearning and deep learning models. Alongside this, LogAI also provides an\\nout-of-the-box GUI for users to conduct interactive analysis. With LogAI, we\\ncan also easily benchmark popular deep learning algorithms for log anomaly\\ndetection without putting in redundant effort to process the logs. We have\\nopensourced LogAI to cater to a wide range of applications benefiting both\\nacademic research and industrial prototyping. We analyze the Nystr\\\\\\\"om approximation of a positive definite kernel\\nassociated with a probability measure. We first prove an improved error bound\\nfor the conventional Nystr\\\\\\\"om approximation with i.i.d. sampling and\\nsingular-value decomposition in the continuous regime; the proof techniques are\\nborrowed from statistical learning theory. We further introduce a refined\\nselection of subspaces in Nystr\\\\\\\"om approximation with theoretical guarantees\\nthat is applicable to non-i.i.d. landmark points. Finally, we discuss their\\napplication to convex kernel quadrature and give novel theoretical guarantees\\nas well as numerical observations. We consider a class of learning problems in which an agent liquidates a risky\\nasset while creating both transient price impact driven by an unknown\\nconvolution propagator and linear temporary price impact with an unknown\\nparameter. We characterize the trader's performance as maximization of a\\nrevenue-risk functional, where the trader also exploits available information\\non a price predicting signal. We present a trading algorithm that alternates\\nbetween exploration and exploitation phases and achieves sublinear regrets with\\nhigh probability. For the exploration phase we propose a novel approach for\\nnon-parametric estimation of the price impact kernel by observing only the\\nvisible price process and derive sharp bounds on the convergence rate, which\\nare characterised by the singularity of the propagator. These kernel estimation\\nmethods extend existing methods from the area of Tikhonov regularisation for\\ninverse problems and are of independent interest. The bound on the regret in\\nthe exploitation phase is obtained by deriving stability results for the\\noptimizer and value function of the associated class of infinite-dimensional\\nstochastic control problems. As a complementary result we propose a\\nregression-based algorithm to estimate the conditional expectation of\\nnon-Markovian signals and derive its convergence rate. Data-driven approximations of ordinary differential equations offer a\\npromising alternative to classical methods of discovering a dynamical system\\nmodel, particularly in complex systems lacking explicit first principles. This\\npaper focuses on a complex system whose dynamics is described with a system of\\nsuch equations, coupled through a complex network. Numerous real-world systems,\\nincluding financial, social, and neural systems, belong to this class of\\ndynamical models. We propose essential elements for approximating these\\ndynamical systems using neural networks, including necessary biases and an\\nappropriate neural architecture. Emphasizing the differences from static\\nsupervised learning, we advocate for evaluating generalization beyond classical\\nassumptions of statistical learning theory. To estimate confidence in\\nprediction during inference time, we introduce a dedicated null model. By\\nstudying various complex network dynamics, we demonstrate that the neural\\napproximations of dynamics generalize across complex network structures, sizes,\\nand statistical properties of inputs. Our comprehensive framework enables\\naccurate and reliable deep learning approximations of high-dimensional,\\nnonlinear dynamical systems. In this paper, we develop two new randomized block-coordinate optimistic\\ngradient algorithms to approximate a solution of nonlinear equations in\\nlarge-scale settings, which are called root-finding problems. Our first\\nalgorithm is non-accelerated with constant stepsizes, and achieves\\n$\\\\mathcal{O}(1/k)$ best-iterate convergence rate on $\\\\mathbb{E}[ \\\\Vert\\nGx^k\\\\Vert^2]$ when the underlying operator $G$ is Lipschitz continuous and\\nsatisfies a weak Minty solution condition, where $\\\\mathbb{E}[\\\\cdot]$ is the\\nexpectation and $k$ is the iteration counter. Our second method is a new\\naccelerated randomized block-coordinate optimistic gradient algorithm. We\\nestablish both $\\\\mathcal{O}(1/k^2)$ and $o(1/k^2)$ last-iterate convergence\\nrates on both $\\\\mathbb{E}[ \\\\Vert Gx^k\\\\Vert^2]$ and $\\\\mathbb{E}[ \\\\Vert x^{k+1} -\\nx^{k}\\\\Vert^2]$ for this algorithm under the co-coerciveness of $G$. In\\naddition, we prove that the iterate sequence $\\\\{x^k\\\\}$ converges to a solution\\nalmost surely, and $\\\\Vert Gx^k\\\\Vert^2$ attains a $o(1/k)$ almost sure\\nconvergence rate. Then, we apply our methods to a class of large-scale\\nfinite-sum inclusions, which covers prominent applications in machine learning,\\nstatistical learning, and network optimization, especially in federated\\nlearning. We obtain two new federated learning-type algorithms and their\\nconvergence rate guarantees for solving this problem class. Merging satellite products and ground-based measurements is often required\\nfor obtaining precipitation datasets that simultaneously cover large regions\\nwith high density and are more accurate than pure satellite precipitation\\nproducts. Machine and statistical learning regression algorithms are regularly\\nutilized in this endeavour. At the same time, tree-based ensemble algorithms\\nare adopted in various fields for solving regression problems with high\\naccuracy and low computational cost. Still, information on which tree-based\\nensemble algorithm to select for correcting satellite precipitation products\\nfor the contiguous United States (US) at the daily time scale is missing from\\nthe literature. In this study, we worked towards filling this methodological\\ngap by conducting an extensive comparison between three algorithms of the\\ncategory of interest, specifically between random forests, gradient boosting\\nmachines (gbm) and extreme gradient boosting (XGBoost). We used daily data from\\nthe PERSIANN (Precipitation Estimation from Remotely Sensed Information using\\nArtificial Neural Networks) and the IMERG (Integrated Multi-satellitE\\nRetrievals for GPM) gridded datasets. We also used earth-observed precipitation\\ndata from the Global Historical Climatology Network daily (GHCNd) database. The\\nexperiments referred to the entire contiguous US and additionally included the\\napplication of the linear regression algorithm for benchmarking purposes. The\\nresults suggest that XGBoost is the best-performing tree-based ensemble\\nalgorithm among those compared... Machine learning is the dominant approach to artificial intelligence, through\\nwhich computers learn from data and experience. In the framework of supervised\\nlearning, a necessity for a computer to learn from data accurately and\\nefficiently is to be provided with auxiliary information about the data\\ndistribution and target function through the learning model. This notion of\\nauxiliary information relates to the concept of regularization in statistical\\nlearning theory. A common feature among real-world datasets is that data\\ndomains are multiscale and target functions are well-behaved and smooth. This\\npaper proposes an entropy-based learning model that exploits this data\\nstructure and discusses its statistical and computational benefits. The\\nhierarchical learning model is inspired by human beings' logical and\\nprogressive easy-to-hard learning mechanism and has interpretable levels. The\\nmodel apportions computational resources according to the complexity of data\\ninstances and target functions. This property can have multiple benefits,\\nincluding higher inference speed and computational savings in training a model\\nfor many users or when training is interrupted. We provide a statistical\\nanalysis of the learning mechanism using multiscale entropies and show that it\\ncan yield significantly stronger guarantees than uniform convergence bounds. This short study reformulates the statistical Bayesian learning problem using\\na quantum mechanics framework. Density operators representing ensembles of pure\\nstates of sample wave functions are used in place probability densities. We\\nshow that such representation allows to formulate the statistical Bayesian\\nlearning problem in different coordinate systems on the sample space. We\\nfurther show that such representation allows to learn projections of density\\noperators using a kernel trick. In particular, the study highlights that\\ndecomposing wave functions rather than probability densities, as it is done in\\nkernel embedding, allows to preserve the nature of probability operators.\\nResults are illustrated with a simple example using discrete orthogonal wavelet\\ntransform of density operators. Graph neural networks (GNNs) excel in modeling relational data such as\\nbiological, social, and transportation networks, but the underpinnings of their\\nsuccess are not well understood. Traditional complexity measures from\\nstatistical learning theory fail to account for observed phenomena like the\\ndouble descent or the impact of relational semantics on generalization error.\\nMotivated by experimental observations of ``transductive'' double descent in\\nkey networks and datasets, we use analytical tools from statistical physics and\\nrandom matrix theory to precisely characterize generalization in simple graph\\nconvolution networks on the contextual stochastic block model. Our results\\nilluminate the nuances of learning on homophilic versus heterophilic data and\\npredict double descent whose existence in GNNs has been questioned by recent\\nwork. We show how risk is shaped by the interplay between the graph noise,\\nfeature noise, and the number of training labels. Our findings apply beyond\\nstylized models, capturing qualitative trends in real-world GNNs and datasets.\\nAs a case in point, we use our analytic insights to improve performance of\\nstate-of-the-art graph convolution networks on heterophilic datasets. Fingerprints are key tools in climate change detection and attribution (D&A)\\nthat are used to determine whether changes in observations are different from\\ninternal climate variability (detection), and whether observed changes can be\\nassigned to specific external drivers (attribution). We propose a direct D&A\\napproach based on supervised learning to extract fingerprints that lead to\\nrobust predictions under relevant interventions on exogenous variables, i.e.,\\nclimate drivers other than the target. We employ anchor regression, a\\ndistributionally-robust statistical learning method inspired by causal\\ninference that extrapolates well to perturbed data under the interventions\\nconsidered. The residuals from the prediction achieve either uncorrelatedness\\nor mean independence with the exogenous variables, thus guaranteeing\\nrobustness. We define D&A as a unified hypothesis testing framework that relies\\non the same statistical model but uses different targets and test statistics.\\nIn the experiments, we first show that the CO2 forcing can be robustly\\npredicted from temperature spatial patterns under strong interventions on the\\nsolar forcing. Second, we illustrate attribution to the greenhouse gases and\\naerosols while protecting against interventions on the aerosols and CO2\\nforcing, respectively. Our study shows that incorporating robustness\\nconstraints against relevant interventions may significantly benefit detection\\nand attribution of climate change. Covariate distribution shifts and adversarial perturbations present\\nrobustness challenges to the conventional statistical learning framework: mild\\nshifts in the test covariate distribution can significantly affect the\\nperformance of the statistical model learned based on the training\\ndistribution. The model performance typically deteriorates when extrapolation\\nhappens: namely, covariates shift to a region where the training distribution\\nis scarce, and naturally, the learned model has little information. For\\nrobustness and regularization considerations, adversarial perturbation\\ntechniques are proposed as a remedy; however, careful study needs to be carried\\nout about what extrapolation region adversarial covariate shift will focus on,\\ngiven a learned model. This paper precisely characterizes the extrapolation\\nregion, examining both regression and classification in an infinite-dimensional\\nsetting. We study the implications of adversarial covariate shifts to\\nsubsequent learning of the equilibrium -- the Bayes optimal model -- in a\\nsequential game framework. We exploit the dynamics of the adversarial learning\\ngame and reveal the curious effects of the covariate shift to equilibrium\\nlearning and experimental design. In particular, we establish two directional\\nconvergence results that exhibit distinctive phenomena: (1) a blessing in\\nregression, the adversarial covariate shifts in an exponential rate to an\\noptimal experimental design for rapid subsequent learning, (2) a curse in\\nclassification, the adversarial covariate shifts in a subquadratic rate fast to\\nthe hardest experimental design trapping subsequent learning. Uncertainty is prevalent in engineering design, statistical learning, and\\ndecision making broadly. Due to inherent risk-averseness and ambiguity about\\nassumptions, it is common to address uncertainty by formulating and solving\\nconservative optimization models expressed using measures of risk and related\\nconcepts. We survey the rapid development of risk measures over the last\\nquarter century. From their beginning in financial engineering, we recount the\\nspread to nearly all areas of engineering and applied mathematics. Solidly\\nrooted in convex analysis, risk measures furnish a general framework for\\nhandling uncertainty with significant computational and theoretical advantages.\\nWe describe the key facts, list several concrete algorithms, and provide an\\nextensive list of references for further reading. The survey recalls\\nconnections with utility theory and distributionally robust optimization,\\npoints to emerging applications areas such as fair machine learning, and\\ndefines measures of reliability. Sparse reduced rank regression is an essential statistical learning method.\\nIn the contemporary literature, estimation is typically formulated as a\\nnonconvex optimization that often yields to a local optimum in numerical\\ncomputation. Yet, their theoretical analysis is always centered on the global\\noptimum, resulting in a discrepancy between the statistical guarantee and the\\nnumerical computation. In this research, we offer a new algorithm to address\\nthe problem and establish an almost optimal rate for the algorithmic solution.\\nWe also demonstrate that the algorithm achieves the estimation with a\\npolynomial number of iterations. In addition, we present a generalized\\ninformation criterion to simultaneously ensure the consistency of support set\\nrecovery and rank estimation. Under the proposed criterion, we show that our\\nalgorithm can achieve the oracle reduced rank estimation with a significant\\nprobability. The numerical studies and an application in the ovarian cancer\\ngenetic data demonstrate the effectiveness and scalability of our approach. A remarkable recent paper by Rubinfeld and Vasilyan (2022) initiated the\\nstudy of \\\\emph{testable learning}, where the goal is to replace hard-to-verify\\ndistributional assumptions (such as Gaussianity) with efficiently testable ones\\nand to require that the learner succeed whenever the unknown distribution\\npasses the corresponding test. In this model, they gave an efficient algorithm\\nfor learning halfspaces under testable assumptions that are provably satisfied\\nby Gaussians.\\n  In this paper we give a powerful new approach for developing algorithms for\\ntestable learning using tools from moment matching and metric distances in\\nprobability. We obtain efficient testable learners for any concept class that\\nadmits low-degree \\\\emph{sandwiching polynomials}, capturing most important\\nexamples for which we have ordinary agnostic learners. We recover the results\\nof Rubinfeld and Vasilyan as a corollary of our techniques while achieving\\nimproved, near-optimal sample complexity bounds for a broad range of concept\\nclasses and distributions.\\n  Surprisingly, we show that the information-theoretic sample complexity of\\ntestable learning is tightly characterized by the Rademacher complexity of the\\nconcept class, one of the most well-studied measures in statistical learning\\ntheory. In particular, uniform convergence is necessary and sufficient for\\ntestable learning. This leads to a fundamental separation from (ordinary)\\ndistribution-specific agnostic learning, where uniform convergence is\\nsufficient but not necessary. This paper studies the limits of language models' statistical learning in the\\ncontext of Zipf's law. First, we demonstrate that Zipf-law token distribution\\nemerges irrespective of the chosen tokenization. Second, we show that Zipf\\ndistribution is characterized by two distinct groups of tokens that differ both\\nin terms of their frequency and their semantics. Namely, the tokens that have a\\none-to-one correspondence with one semantic concept have different statistical\\nproperties than those with semantic ambiguity. Finally, we demonstrate how\\nthese properties interfere with statistical learning procedures motivated by\\ndistributional semantics. Always-valid concentration inequalities are increasingly used as performance\\nmeasures for online statistical learning, notably in the learning of generative\\nmodels and supervised learning. Such inequality advances the online learning\\nalgorithms design by allowing random, adaptively chosen sample sizes instead of\\na fixed pre-specified size in offline statistical learning. However,\\nestablishing such an always-valid type result for the task of matrix completion\\nis challenging and far from understood in the literature. Due to the importance\\nof such type of result, this work establishes and devises the always-valid risk\\nbound process for online matrix completion problems. Such theoretical advances\\nare made possible by a novel combination of non-asymptotic martingale\\nconcentration and regularized low-rank matrix regression. Our result enables a\\nmore sample-efficient online algorithm design and serves as a foundation to\\nevaluate online experiment policies on the task of online matrix completion. Group lasso is a commonly used regularization method in statistical learning\\nin which parameters are eliminated from the model according to predefined\\ngroups. However, when the groups overlap, optimizing the group lasso penalized\\nobjective can be time-consuming on large-scale problems because of the\\nnon-separability induced by the overlapping groups. This bottleneck has\\nseriously limited the application of overlapping group lasso regularization in\\nmany modern problems, such as gene pathway selection and graphical model\\nestimation. In this paper, we propose a separable penalty as an approximation\\nof the overlapping group lasso penalty. Thanks to the separability, the\\ncomputation of regularization based on our penalty is substantially faster than\\nthat of the overlapping group lasso, especially for large-scale and\\nhigh-dimensional problems. We show that the penalty is the tightest separable\\nrelaxation of the overlapping group lasso norm within the family of\\n$\\\\ell_{q_1}/\\\\ell_{q_2}$ norms. Moreover, we show that the estimator based on\\nthe proposed separable penalty is statistically equivalent to the one based on\\nthe overlapping group lasso penalty with respect to their error bounds and the\\nrate-optimal performance under the squared loss. We demonstrate the faster\\ncomputational time and statistical equivalence of our method compared with the\\noverlapping group lasso in simulation examples and a classification problem of\\ncancer tumors based on gene expression and multiple gene pathways. The fact that we can build models from data, and therefore refine our models\\nwith more data from experiments, is usually given for granted in scientific\\ninquiry. However, how much information can we extract, and how precise can we\\nexpect our learned model to be, if we have only a finite amount of data at our\\ndisposal? Nuclear physics demands an high degree of precision from models that\\nare inferred from the limited number of nuclei that can be possibly made in the\\nlaboratories.\\n  In manuscript I will introduce some concepts of computational science, such\\nas statistical theory of learning and Hamiltonian complexity, and use them to\\ncontextualise the results concerning the amount of data necessary to\\nextrapolate a mass model to a given precision. Modern statistical learning algorithms are capable of amazing flexibility,\\nbut struggle with interpretability. One possible solution is sparsity: making\\ninference such that many of the parameters are estimated as being identically\\n0, which may be imposed through the use of nonsmooth penalties such as the\\n$\\\\ell_1$ penalty. However, the $\\\\ell_1$ penalty introduces significant bias\\nwhen high sparsity is desired. In this article, we retain the $\\\\ell_1$ penalty,\\nbut define learnable penalty weights $\\\\lambda_p$ endowed with hyperpriors. We\\nstart the article by investigating the optimization problem this poses,\\ndeveloping a proximal operator associated with the $\\\\ell_1$ norm. We then study\\nthe theoretical properties of this variable-coefficient $\\\\ell_1$ penalty in the\\ncontext of penalized likelihood. Next, we investigate application of this\\npenalty to Variational Bayes, developing a model we call the Sparse Bayesian\\nLasso which allows for behavior qualitatively like Lasso regression to be\\napplied to arbitrary variational models. In simulation studies, this gives us\\nthe Uncertainty Quantification and low bias properties of simulation-based\\napproaches with an order of magnitude less computation. Finally, we apply our\\nmethodology to a Bayesian lagged spatiotemporal regression model of internal\\ndisplacement that occurred during the Iraqi Civil War of 2013-2017. Tournament procedures, recently introduced in Lugosi & Mendelson (2016),\\noffer an appealing alternative, from a theoretical perspective at least, to the\\nprinciple of Empirical Risk Minimization in machine learning. Statistical\\nlearning by Median-of-Means (MoM) basically consists in segmenting the training\\ndata into blocks of equal size and comparing the statistical performance of\\nevery pair of candidate decision rules on each data block: that with highest\\nperformance on the majority of the blocks is declared as the winner. In the\\ncontext of nonparametric regression, functions having won all their duels have\\nbeen shown to outperform empirical risk minimizers w.r.t. the mean squared\\nerror under minimal assumptions, while exhibiting robustness properties. It is\\nthe purpose of this paper to extend this approach in order to address other\\nlearning problems, in particular for which the performance criterion takes the\\nform of an expectation over pairs of observations rather than over one single\\nobservation, as may be the case in pairwise ranking, clustering or metric\\nlearning. Precisely, it is proved here that the bounds achieved by MoM are\\nessentially conserved when the blocks are built by means of independent\\nsampling without replacement schemes instead of a simple segmentation. These\\nresults are next extended to situations where the risk is related to a pairwise\\nloss function and its empirical counterpart is of the form of a $U$-statistic.\\nBeyond theoretical results guaranteeing the performance of the\\nlearning/estimation methods proposed, some numerical experiments provide\\nempirical evidence of their relevance in practice. Decision-making problems are commonly formulated as optimization problems,\\nwhich are then solved to make optimal decisions. In this work, we consider the\\ninverse problem where we use prior decision data to uncover the underlying\\ndecision-making process in the form of a mathematical optimization model. This\\nstatistical learning problem is referred to as data-driven inverse\\noptimization. We focus on problems where the underlying decision-making process\\nis modeled as a convex optimization problem whose parameters are unknown. We\\nformulate the inverse optimization problem as a bilevel program and propose an\\nefficient block coordinate descent-based algorithm to solve large problem\\ninstances. Numerical experiments on synthetic datasets demonstrate the\\ncomputational advantage of our method compared to standard commercial solvers.\\nMoreover, the real-world utility of the proposed approach is highlighted\\nthrough two realistic case studies in which we consider estimating risk\\npreferences and learning local constraint parameters of agents in a multiplayer\\nNash bargaining game. We discuss two solvable grokking (generalisation beyond overfitting) models\\nin a rule learning scenario. We show that grokking is a phase transition and\\nfind exact analytic expressions for the critical exponents, grokking\\nprobability, and grokking time distribution. Further, we introduce a\\ntensor-network map that connects the proposed grokking setup with the standard\\n(perceptron) statistical learning theory and show that grokking is a\\nconsequence of the locality of the teacher model. As an example, we analyse the\\ncellular automata learning task, numerically determine the critical exponent\\nand the grokking time distributions and compare them with the prediction of the\\nproposed grokking model. Finally, we numerically analyse the connection between\\nstructure formation and grokking. In this study, we propose a new numerical scheme for physics-informed neural\\nnetworks (PINNs) that enables precise and inexpensive solution for partial\\ndifferential equations (PDEs) in case of arbitrary geometries while strictly\\nenforcing Dirichlet boundary conditions. The proposed approach combines\\nadmissible NURBS parametrizations required to define the physical domain and\\nthe Dirichlet boundary conditions with a PINN solver. The fundamental boundary\\nconditions are automatically satisfied in this novel Deep NURBS framework. We\\nverified our new approach using two-dimensional elliptic PDEs when considering\\narbitrary geometries, including non-Lipschitz domains. Compared to the\\nclassical PINN solver, the Deep NURBS estimator has a remarkably high\\nconvergence rate for all the studied problems. Moreover, a desirable accuracy\\nwas realized for most of the studied PDEs using only one hidden layer of neural\\nnetworks. This novel approach is considered to pave the way for more effective\\nsolutions for high-dimensional problems by allowing for more realistic\\nphysics-informed statistical learning to solve PDE-based variational problems. Domain adaptation arises as an important problem in statistical learning\\ntheory when the data-generating processes differ between training and test\\nsamples, respectively called source and target domains. Recent theoretical\\nadvances show that the success of domain adaptation algorithms heavily relies\\non their ability to minimize the divergence between the probability\\ndistributions of the source and target domains. However, minimizing this\\ndivergence cannot be done independently of the minimization of other key\\ningredients such as the source risk or the combined error of the ideal joint\\nhypothesis. The trade-off between these terms is often ensured by algorithmic\\nsolutions that remain implicit and not directly reflected by the theoretical\\nguarantees. To get to the bottom of this issue, we propose in this paper a new\\ntheoretical framework for domain adaptation through hierarchical optimal\\ntransport. This framework provides more explicit generalization bounds and\\nallows us to consider the natural hierarchical organization of samples in both\\ndomains into classes or clusters. Additionally, we provide a new divergence\\nmeasure between the source and target domains called Hierarchical Wasserstein\\ndistance that indicates under mild assumptions, which structures have to be\\naligned to lead to a successful adaptation. We study the problem of multi-task non-smooth optimization that arises\\nubiquitously in statistical learning, decision-making and risk management. We\\ndevelop a data fusion approach that adaptively leverages commonalities among a\\nlarge number of objectives to improve sample efficiency while tackling their\\nunknown heterogeneities. We provide sharp statistical guarantees for our\\napproach. Numerical experiments on both synthetic and real data demonstrate\\nsignificant advantages of our approach over benchmarks. Supervised transfer learning has received considerable attention due to its\\npotential to boost the predictive power of machine learning in scenarios where\\ndata are scarce. Generally, a given set of source models and a dataset from a\\ntarget domain are used to adapt the pre-trained models to a target domain by\\nstatistically learning domain shift and domain-specific factors. While such\\nprocedurally and intuitively plausible methods have achieved great success in a\\nwide range of real-world applications, the lack of a theoretical basis hinders\\nfurther methodological development. This paper presents a general class of\\ntransfer learning regression called affine model transfer, following the\\nprinciple of expected-square loss minimization. It is shown that the affine\\nmodel transfer broadly encompasses various existing methods, including the most\\ncommon procedure based on neural feature extractors. Furthermore, the current\\npaper clarifies theoretical properties of the affine model transfer such as\\ngeneralization error and excess risk. Through several case studies, we\\ndemonstrate the practical benefits of modeling and estimating inter-domain\\ncommonality and domain-specific factors separately with the affine-type\\ntransfer models. This paper is focused on the study of entropic regularization in optimal\\ntransport as a smoothing method for Wasserstein estimators, through the prism\\nof the classical tradeoff between approximation and estimation errors in\\nstatistics. Wasserstein estimators are defined as solutions of variational\\nproblems whose objective function involves the use of an optimal transport cost\\nbetween probability measures. Such estimators can be regularized by replacing\\nthe optimal transport cost by its regularized version using an entropy penalty\\non the transport plan. The use of such a regularization has a potentially\\nsignificant smoothing effect on the resulting estimators. In this work, we\\ninvestigate its potential benefits on the approximation and estimation\\nproperties of regularized Wasserstein estimators. Our main contribution is to\\ndiscuss how entropic regularization may reach, at a lower computational cost,\\nstatistical performances that are comparable to those of un-regularized\\nWasserstein estimators in statistical learning problems involving\\ndistributional data analysis. To this end, we present new theoretical results\\non the convergence of regularized Wasserstein estimators. We also study their\\nnumerical performances using simulated and real data in the supervised learning\\nproblem of proportions estimation in mixture models using optimal transport. A characteristic feature of time-to-event data analysis is possible censoring\\nof the event time. Most of the statistical learning methods for handling\\ncensored data are limited by the assumption of independent censoring, even if\\nthis can lead to biased predictions when the assumption does not hold. This\\npaper introduces Clayton-boost, a boosting approach built upon the accelerated\\nfailure time model, which uses a Clayton copula to handle the dependency\\nbetween the event and censoring distributions. By taking advantage of a copula,\\nthe independent censoring assumption is not needed any more. During comparisons\\nwith commonly used methods, Clayton-boost shows a strong ability to remove\\nprediction bias at the presence of dependent censoring and outperforms the\\ncomparing methods either if the dependency strength or percentage censoring are\\nconsiderable. The encouraging performance of Clayton-boost shows that there is\\nindeed reasons to be critical about the independent censoring assumption, and\\nthat real-world data could highly benefit from modelling the potential\\ndependency. Recently, continual learning has received a lot of attention. One of the\\nsignificant problems is the occurrence of \\\\emph{concept drift}, which consists\\nof changing probabilistic characteristics of the incoming data. In the case of\\nthe classification task, this phenomenon destabilizes the model's performance\\nand negatively affects the achieved prediction quality. Most current methods\\napply statistical learning and similarity analysis over the raw data. However,\\nsimilarity analysis in streaming data remains a complex problem due to time\\nlimitation, non-precise values, fast decision speed, scalability, etc. This\\narticle introduces a novel method for monitoring changes in the probabilistic\\ndistribution of multi-dimensional data streams. As a measure of the rapidity of\\nchanges, we analyze the popular Kullback-Leibler divergence. During the\\nexperimental study, we show how to use this metric to predict the concept drift\\noccurrence and understand its nature. The obtained results encourage further\\nwork on the proposed methods and its application in the real tasks where the\\nprediction of the future appearance of concept drift plays a crucial role, such\\nas predictive maintenance. In medical settings, Individual Variation (IV) refers to variation that is\\ndue not to population differences or errors, but rather to within-subject\\nvariation, that is the intrinsic and characteristic patterns of variation\\npertaining to a given instance or the measurement process. While taking into\\naccount IV has been deemed critical for proper analysis of medical data, this\\nsource of uncertainty and its impact on robustness have so far been neglected\\nin Machine Learning (ML). To fill this gap, we look at how IV affects ML\\nperformance and generalization and how its impact can be mitigated.\\nSpecifically, we provide a methodological contribution to formalize the problem\\nof IV in the statistical learning framework and, through an experiment based on\\none of the largest real-world laboratory medicine datasets for the problem of\\nCOVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are\\nseverely impacted by the presence of IV in data; and 2) advanced learning\\nstrategies, based on data augmentation and data imprecisiation, and proper\\nstudy designs can be effective at improving robustness to IV. Our findings\\ndemonstrate the critical relevance of correctly accounting for IV to enable\\nsafe deployment of ML in clinical settings. We propose to minimize a generic differentiable objective with $L_1$\\nconstraint using a simple reparametrization and straightforward stochastic\\ngradient descent. Our proposal is the direct generalization of previous ideas\\nthat the $L_1$ penalty may be equivalent to a differentiable reparametrization\\nwith weight decay. We prove that the proposed method, \\\\textit{spred}, is an\\nexact differentiable solver of $L_1$ and that the reparametrization trick is\\ncompletely ``benign\\\" for a generic nonconvex function. Practically, we\\ndemonstrate the usefulness of the method in (1) training sparse neural networks\\nto perform gene selection tasks, which involves finding relevant features in a\\nvery high dimensional space, and (2) neural network compression task, to which\\nprevious attempts at applying the $L_1$-penalty have been unsuccessful.\\nConceptually, our result bridges the gap between the sparsity in deep learning\\nand conventional statistical learning. Deep learning has achieved remarkable success in many machine learning tasks\\nsuch as image classification, speech recognition, and game playing. However,\\nthese breakthroughs are often difficult to translate into real-world\\nengineering systems because deep learning models require a massive number of\\ntraining samples, which are costly to obtain in practice. To address labeled\\ndata scarcity, few-shot meta-learning optimizes learning algorithms that can\\nefficiently adapt to new tasks quickly. While meta-learning is gaining\\nsignificant interest in the machine learning literature, its working principles\\nand theoretic fundamentals are not as well understood in the engineering\\ncommunity.\\n  This review monograph provides an introduction to meta-learning by covering\\nprinciples, algorithms, theory, and engineering applications. After introducing\\nmeta-learning in comparison with conventional and joint learning, we describe\\nthe main meta-learning algorithms, as well as a general bilevel optimization\\nframework for the definition of meta-learning techniques. Then, we summarize\\nknown results on the generalization capabilities of meta-learning from a\\nstatistical learning viewpoint. Applications to communication systems,\\nincluding decoding and power allocation, are discussed next, followed by an\\nintroduction to aspects related to the integration of meta-learning with\\nemerging computing technologies, namely neuromorphic and quantum computing. The\\nmonograph is concluded with an overview of open research challenges. Inverse problems are paramount in Science and Engineering. In this paper, we\\nconsider the setup of Statistical Inverse Problem (SIP) and demonstrate how\\nStochastic Gradient Descent (SGD) algorithms can be used in the linear SIP\\nsetting. We provide consistency and finite sample bounds for the excess risk.\\nWe also propose a modification for the SGD algorithm where we leverage machine\\nlearning methods to smooth the stochastic gradients and improve empirical\\nperformance. We exemplify the algorithm in a setting of great interest\\nnowadays: the Functional Linear Regression model. In this case we consider a\\nsynthetic data example and examples with a real data classification problem. The problem of sampling constrained continuous distributions has frequently\\nappeared in many machine/statistical learning models. Many Monte Carlo Markov\\nChain (MCMC) sampling methods have been adapted to handle different types of\\nconstraints on the random variables. Among these methods, Hamilton Monte Carlo\\n(HMC) and the related approaches have shown significant advantages in terms of\\ncomputational efficiency compared to other counterparts. In this article, we\\nfirst review HMC and some extended sampling methods, and then we concretely\\nexplain three constrained HMC-based sampling methods, reflection,\\nreformulation, and spherical HMC. For illustration, we apply these methods to\\nsolve three well-known constrained sampling problems, truncated multivariate\\nnormal distributions, Bayesian regularized regression, and nonparametric\\ndensity estimation. In this review, we also connect constrained sampling with\\nanother similar problem in the statistical design of experiments of constrained\\ndesign space. Recently there is a large amount of work devoted to the study of Markov chain\\nstochastic gradient methods (MC-SGMs) which mainly focus on their convergence\\nanalysis for solving minimization problems. In this paper, we provide a\\ncomprehensive generalization analysis of MC-SGMs for both minimization and\\nminimax problems through the lens of algorithmic stability in the framework of\\nstatistical learning theory. For empirical risk minimization (ERM) problems, we\\nestablish the optimal excess population risk bounds for both smooth and\\nnon-smooth cases by introducing on-average argument stability. For minimax\\nproblems, we develop a quantitative connection between on-average argument\\nstability and generalization error which extends the existing results for\\nuniform stability \\\\cite{lei2021stability}. We further develop the first nearly\\noptimal convergence rates for convex-concave problems both in expectation and\\nwith high probability, which, combined with our stability results, show that\\nthe optimal generalization bounds can be attained for both smooth and\\nnon-smooth cases. To the best of our knowledge, this is the first\\ngeneralization analysis of SGMs when the gradients are sampled from a Markov\\nprocess. This tutorial survey provides an overview of recent non-asymptotic advances\\nin statistical learning theory as relevant to control and system\\nidentification. While there has been substantial progress across all areas of\\ncontrol, the theory is most well-developed when it comes to linear system\\nidentification and learning for the linear quadratic regulator, which are the\\nfocus of this manuscript. From a theoretical perspective, much of the labor\\nunderlying these advances has been in adapting tools from modern\\nhigh-dimensional statistics and learning theory. While highly relevant to\\ncontrol theorists interested in integrating tools from machine learning, the\\nfoundational material has not always been easily accessible. To remedy this, we\\nprovide a self-contained presentation of the relevant material, outlining all\\nthe key ideas and the technical machinery that underpin recent results. We also\\npresent a number of open problems and future directions. A variety of different performance metrics are commonly used in the machine\\nlearning literature for the evaluation of classification systems. Some of the\\nmost common ones for measuring quality of hard decisions are standard and\\nbalanced accuracy, standard and balanced error rate, F-beta score, and Matthews\\ncorrelation coefficient (MCC). In this document, we review the definition of\\nthese and other metrics and compare them with the expected cost (EC), a metric\\nintroduced in every statistical learning course but rarely used in the machine\\nlearning literature. We show that both the standard and balanced error rates\\nare special cases of the EC. Further, we show its relation with F-beta score\\nand MCC and argue that EC is superior to these traditional metrics for being\\nbased on first principles from statistics, and for being more general,\\ninterpretable, and adaptable to any application scenario. The metrics mentioned\\nabove measure the quality of hard decisions. Yet, most modern classification\\nsystems output continuous scores for the classes which we may want to evaluate\\ndirectly. Metrics for measuring the quality of system scores include the area\\nunder the ROC curve, equal error rate, cross-entropy, Brier score, and Bayes EC\\nor Bayes risk, among others. The last three metrics are special cases of a\\nfamily of metrics given by the expected value of proper scoring rules (PSRs).\\nWe review the theory behind these metrics, showing that they are a principled\\nway to measure the quality of the posterior probabilities produced by a system.\\nFinally, we show how to use these metrics to compute a system's calibration\\nloss and compare this metric with the widely-used expected calibration error\\n(ECE), arguing that calibration loss based on PSRs is superior to the ECE for\\nbeing more interpretable, more general, and directly applicable to the\\nmulti-class case, among other reasons. Subsampling or subdata selection is a useful approach in large-scale\\nstatistical learning. Most existing studies focus on model-based subsampling\\nmethods which significantly depend on the model assumption. In this paper, we\\nconsider the model-free subsampling strategy for generating subdata from the\\noriginal full data. In order to measure the goodness of representation of a\\nsubdata with respect to the original data, we propose a criterion, generalized\\nempirical F-discrepancy (GEFD), and study its theoretical properties in\\nconnection with the classical generalized L2-discrepancy in the theory of\\nuniform designs. These properties allow us to develop a kind of low-GEFD\\ndata-driven subsampling method based on the existing uniform designs. By\\nsimulation examples and a real case study, we show that the proposed\\nsubsampling method is superior to the random sampling method. Moreover, our\\nmethod keeps robust under diverse model specifications while other popular\\nsubsampling methods are under-performing. In practice, such a model-free\\nproperty is more appealing than the model-based subsampling methods, where the\\nlatter may have poor performance when the model is misspecified, as\\ndemonstrated in our simulation studies. Federated learning (FL) is a subfield of machine learning where multiple\\nclients try to collaboratively learn a model over a network under communication\\nconstraints. We consider finite-sum federated optimization under a second-order\\nfunction similarity condition and strong convexity, and propose two new\\nalgorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has\\ngrown popular recently, and is satisfied in many applications including\\ndistributed statistical learning and differentially private empirical risk\\nminimization. The first algorithm, SVRP, combines approximate stochastic\\nproximal point evaluations, client sampling, and variance reduction. We show\\nthat SVRP is communication efficient and achieves superior performance to many\\nexisting algorithms when function similarity is high enough. Our second\\nalgorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that\\nachieves even better performance and uniformly improves upon existing\\nalgorithms for federated optimization under second-order similarity and strong\\nconvexity. In the course of analyzing these algorithms, we provide a new\\nanalysis of the Stochastic Proximal Point Method (SPPM) that might be of\\nindependent interest. Our analysis of SPPM is simple, allows for approximate\\nproximal point evaluations, does not require any smoothness assumptions, and\\nshows a clear benefit in communication complexity over ordinary distributed\\nstochastic gradient descent. Humans can attribute beliefs to others. However, it is unknown to what extent\\nthis ability results from an innate biological endowment or from experience\\naccrued through child development, particularly exposure to language describing\\nothers' mental states. We test the viability of the language exposure\\nhypothesis by assessing whether models exposed to large quantities of human\\nlanguage display sensitivity to the implied knowledge states of characters in\\nwritten passages. In pre-registered analyses, we present a linguistic version\\nof the False Belief Task to both human participants and a Large Language Model,\\nGPT-3. Both are sensitive to others' beliefs, but while the language model\\nsignificantly exceeds chance behavior, it does not perform as well as the\\nhumans, nor does it explain the full extent of their behavior -- despite being\\nexposed to more language than a human would in a lifetime. This suggests that\\nwhile statistical learning from language exposure may in part explain how\\nhumans develop the ability to reason about the mental states of others, other\\nmechanisms are also responsible. With the mass construction of Gen III nuclear reactors, it is a popular trend\\nto use deep learning (DL) techniques for fast and effective diagnosis of\\npossible accidents. To overcome the common problems of previous work in\\ndiagnosing reactor accidents using deep learning theory, this paper proposes a\\ndiagnostic process that ensures robustness to noisy and crippled data and is\\ninterpretable. First, a novel Denoising Padded Autoencoder (DPAE) is proposed\\nfor representation extraction of monitoring data, with representation extractor\\nstill effective on disturbed data with signal-to-noise ratios up to 25.0 and\\nmonitoring data missing up to 40.0%. Secondly, a diagnostic framework using\\nDPAE encoder for extraction of representations followed by shallow statistical\\nlearning algorithms is proposed, and such stepwise diagnostic approach is\\ntested on disturbed datasets with 41.8% and 80.8% higher classification and\\nregression task evaluation metrics, in comparison with the end-to-end\\ndiagnostic approaches. Finally, a hierarchical interpretation algorithm using\\nSHAP and feature ablation is presented to analyze the importance of the input\\nmonitoring parameters and validate the effectiveness of the high importance\\nparameters. The outcomes of this study provide a referential method for\\nbuilding robust and interpretable intelligent reactor anomaly diagnosis systems\\nin scenarios with high safety requirements. Modern machine learning tasks often require considering not just one but\\nmultiple objectives. For example, besides the prediction quality, this could be\\nthe efficiency, robustness or fairness of the learned models, or any of their\\ncombinations. Multi-objective learning offers a natural framework for handling\\nsuch problems without having to commit to early trade-offs. Surprisingly,\\nstatistical learning theory so far offers almost no insight into the\\ngeneralization properties of multi-objective learning. In this work, we make\\nfirst steps to fill this gap: we establish foundational generalization bounds\\nfor the multi-objective setting as well as generalization and excess bounds for\\nlearning with scalarizations. We also provide the first theoretical analysis of\\nthe relation between the Pareto-optimal sets of the true objectives and the\\nPareto-optimal sets of their empirical approximations from training data. In\\nparticular, we show a surprising asymmetry: all Pareto-optimal solutions can be\\napproximated by empirically Pareto-optimal ones, but not vice versa. The currently dominating artificial intelligence and machine learning\\ntechnology, neural networks, builds on inductive statistical learning. Neural\\nnetworks of today are information processing systems void of understanding and\\nreasoning capabilities, consequently, they cannot explain promoted decisions in\\na humanly valid form. In this work, we revisit and use fundamental philosophy\\nof science theories as an analytical lens with the goal of revealing, what can\\nbe expected, and more importantly, not expected, from methods that aim to\\nexplain decisions promoted by a neural network. By conducting a case study we\\ninvestigate a selection of explainability method's performance over two mundane\\ndomains, animals and headgear. Through our study, we lay bare that the\\nusefulness of these methods relies on human domain knowledge and our ability to\\nunderstand, generalise and reason. The explainability methods can be useful\\nwhen the goal is to gain further insights into a trained neural network's\\nstrengths and weaknesses. If our aim instead is to use these explainability\\nmethods to promote actionable decisions or build trust in ML-models they need\\nto be less ambiguous than they are today. In this work, we conclude from our\\nstudy, that benchmarking explainability methods, is a central quest towards\\ntrustworthy artificial intelligence and machine learning. Machine and deep learning survival models demonstrate similar or even\\nimproved time-to-event prediction capabilities compared to classical\\nstatistical learning methods yet are too complex to be interpreted by humans.\\nSeveral model-agnostic explanations are available to overcome this issue;\\nhowever, none directly explain the survival function prediction. In this paper,\\nwe introduce SurvSHAP(t), the first time-dependent explanation that allows for\\ninterpreting survival black-box models. It is based on SHapley Additive\\nexPlanations with solid theoretical foundations and a broad adoption among\\nmachine learning practitioners. The proposed methods aim to enhance precision\\ndiagnostics and support domain experts in making decisions. Experiments on\\nsynthetic and medical data confirm that SurvSHAP(t) can detect variables with a\\ntime-dependent effect, and its aggregation is a better determinant of the\\nimportance of variables for a prediction than SurvLIME. SurvSHAP(t) is\\nmodel-agnostic and can be applied to all models with functional output. We\\nprovide an accessible implementation of time-dependent explanations in Python\\nat http://github.com/MI2DataLab/survshap. A rich supply of data and innovative algorithms have made data-driven\\nmodeling a popular technique in modern industry. Among various data-driven\\nmethods, latent variable models (LVMs) and their counterparts account for a\\nmajor share and play a vital role in many industrial modeling areas. LVM can be\\ngenerally divided into statistical learning-based classic LVM and neural\\nnetworks-based deep LVM (DLVM). We first discuss the definitions, theories and\\napplications of classic LVMs in detail, which serves as both a comprehensive\\ntutorial and a brief application survey on classic LVMs. Then we present a\\nthorough introduction to current mainstream DLVMs with emphasis on their\\ntheories and model architectures, soon afterwards provide a detailed survey on\\nindustrial applications of DLVMs. The aforementioned two types of LVM have\\nobvious advantages and disadvantages. Specifically, classic LVMs have concise\\nprinciples and good interpretability, but their model capacity cannot address\\ncomplicated tasks. Neural networks-based DLVMs have sufficient model capacity\\nto achieve satisfactory performance in complex scenarios, but it comes at\\nsacrifices in model interpretability and efficiency. Aiming at combining the\\nvirtues and mitigating the drawbacks of these two types of LVMs, as well as\\nexploring non-neural-network manners to build deep models, we propose a novel\\nconcept called lightweight deep LVM (LDLVM). After proposing this new idea, the\\narticle first elaborates the motivation and connotation of LDLVM, then provides\\ntwo novel LDLVMs, along with thorough descriptions on their principles,\\narchitectures and merits. Finally, outlooks and opportunities are discussed,\\nincluding important open questions and possible research directions. Estimating a Gibbs density function given a sample is an important problem in\\ncomputational statistics and statistical learning. Although the well\\nestablished maximum likelihood method is commonly used, it requires the\\ncomputation of the partition function (i.e., the normalization of the density).\\n  This function can be easily calculated for simple low-dimensional problems\\nbut its computation is difficult or even intractable for general densities and\\nhigh-dimensional problems. In this paper we propose an alternative approach\\nbased on Maximum A-Posteriori (MAP) estimators, we name Maximum Recovery MAP\\n(MR-MAP), to derive estimators that do not require the computation of the\\npartition function, and reformulate the problem as an optimization problem. We\\nfurther propose a least-action type potential that allows us to quickly solve\\nthe optimization problem as a feed-forward hyperbolic neural network. We\\ndemonstrate the effectiveness of our methods on some standard data sets. Time series are measured and analyzed across the sciences. One method of\\nquantifying the structure of time series is by calculating a set of summary\\nstatistics or `features', and then representing a time series in terms of its\\nproperties as a feature vector. The resulting feature space is interpretable\\nand informative, and enables conventional statistical learning approaches,\\nincluding clustering, regression, and classification, to be applied to\\ntime-series datasets. Many open-source software packages for computing sets of\\ntime-series features exist across multiple programming languages, including\\ncatch22 (22 features: Matlab, R, Python, Julia), feasts (42 features: R),\\ntsfeatures (63 features: R), Kats (40 features: Python), tsfresh (779 features:\\nPython), and TSFEL (390 features: Python). However, there are several issues:\\n(i) a singular access point to these packages is not currently available; (ii)\\nto access all feature sets, users must be fluent in multiple languages; and\\n(iii) these feature-extraction packages lack extensive accompanying\\nmethodological pipelines for performing feature-based time-series analysis,\\nsuch as applications to time-series classification. Here we introduce a\\nsolution to these issues in an R software package called theft: Tools for\\nHandling Extraction of Features from Time series. theft is a unified and\\nextendable framework for computing features from the six open-source\\ntime-series feature sets listed above. It also includes a suite of functions\\nfor processing and interpreting the performance of extracted features,\\nincluding extensive data-visualization templates, low-dimensional projections,\\nand time-series classification operations. With an increasing volume and\\ncomplexity of time-series datasets in the sciences and industry, theft provides\\na standardized framework for comprehensively quantifying and interpreting\\ninformative structure in time series. We address the consistency of a kernel ridge regression estimate of the\\nconditional mean embedding (CME), which is an embedding of the conditional\\ndistribution of $Y$ given $X$ into a target reproducing kernel Hilbert space\\n$\\\\mathcal{H}_Y$. The CME allows us to take conditional expectations of target\\nRKHS functions, and has been employed in nonparametric causal and Bayesian\\ninference. We address the misspecified setting, where the target CME is in the\\nspace of Hilbert-Schmidt operators acting from an input interpolation space\\nbetween $\\\\mathcal{H}_X$ and $L_2$, to $\\\\mathcal{H}_Y$. This space of operators\\nis shown to be isomorphic to a newly defined vector-valued interpolation space.\\nUsing this isomorphism, we derive a novel and adaptive statistical learning\\nrate for the empirical CME estimator under the misspecified setting. Our\\nanalysis reveals that our rates match the optimal $O(\\\\log n / n)$ rates without\\nassuming $\\\\mathcal{H}_Y$ to be finite dimensional. We further establish a lower\\nbound on the learning rate, which shows that the obtained upper bound is\\noptimal. We introduce a distance-based neural network model for regression, in which\\nprediction uncertainty is quantified by a belief function on the real line. The\\nmodel interprets the distances of the input vector to prototypes as pieces of\\nevidence represented by Gaussian random fuzzy numbers (GRFN's) and combined by\\nthe generalized product intersection rule, an operator that extends Dempster's\\nrule to random fuzzy sets. The network output is a GRFN that can be summarized\\nby three numbers characterizing the most plausible predicted value, variability\\naround this value, and epistemic uncertainty. Experiments with real datasets\\ndemonstrate the very good performance of the method as compared to\\nstate-of-the-art evidential and statistical learning algorithms.\\n\\\\keywords{Evidence theory, Dempster-Shafer theory, belief functions, machine\\nlearning, random fuzzy sets. Fake news detection has become a major task to solve as there has been an\\nincreasing number of fake news on the internet in recent years. Although many\\nclassification models have been proposed based on statistical learning methods\\nshowing good results, reasoning behind the classification performances may not\\nbe enough. In the self-supervised learning studies, it has been highlighted\\nthat a quality of representation (embedding) space matters and directly affects\\na downstream task performance. In this study, a quality of the representation\\nspace is analyzed visually and analytically in terms of linear separability for\\ndifferent classes on a real and fake news dataset. To further add\\ninterpretability to a classification model, a modification of Class Activation\\nMapping (CAM) is proposed. The modified CAM provides a CAM score for each word\\ntoken, where the CAM score on a word token denotes a level of focus on that\\nword token to make the prediction. Finally, it is shown that the naive BERT\\nmodel topped with a learnable linear layer is enough to achieve robust\\nperformance while being compatible with CAM. Machine learning methods have been lately used to solve partial differential\\nequations (PDEs) and dynamical systems. These approaches have been developed\\ninto a novel research field known as scientific machine learning in which\\ntechniques such as deep neural networks and statistical learning are applied to\\nclassical problems of applied mathematics. In this paper, we develop a novel\\nnumerical algorithm that incorporates machine learning and artificial\\nintelligence to solve PDEs. Based on the Legendre-Galerkin framework, we\\npropose the {\\\\it unsupervised machine learning} algorithm to learn {\\\\it\\nmultiple instances} of the solutions for different types of PDEs. Our approach\\novercomes the limitations of data-driven and physics-based methods. The\\nproposed neural network is applied to general 1D and 2D PDEs with various\\nboundary conditions as well as convection-dominated {\\\\it singularly perturbed\\nPDEs} that exhibit strong boundary layer behavior. This work in progress aims to provide a unified introduction to statistical\\nlearning, building up slowly from classical models like the GMM and HMM to\\nmodern neural networks like the VAE and diffusion models. There are today many\\ninternet resources that explain this or that new machine-learning algorithm in\\nisolation, but they do not (and cannot, in so brief a space) connect these\\nalgorithms with each other or with the classical literature on statistical\\nmodels, out of which the modern algorithms emerged. Also conspicuously lacking\\nis a single notational system which, although unfazing to those already\\nfamiliar with the material (like the authors of these posts), raises a\\nsignificant barrier to the novice's entry. Likewise, I have aimed to assimilate\\nthe various models, wherever possible, to a single framework for inference and\\nlearning, showing how (and why) to change one model into another with minimal\\nalteration (some of them novel, others from the literature).\\n  Some background is of course necessary. I have assumed the reader is familiar\\nwith basic multivariable calculus, probability and statistics, and linear\\nalgebra. The goal of this book is certainly not completeness, but rather to\\ndraw a more or less straight-line path from the basics to the extremely\\npowerful new models of the last decade. The goal then is to complement, not\\nreplace, such comprehensive texts as Bishop's \\\\emph{Pattern Recognition and\\nMachine Learning}, which is now 15 years old. The practical success of overparameterized neural networks has motivated the\\nrecent scientific study of interpolating methods, which perfectly fit their\\ntraining data. Certain interpolating methods, including neural networks, can\\nfit noisy training data without catastrophically bad test performance, in\\ndefiance of standard intuitions from statistical learning theory. Aiming to\\nexplain this, a body of recent work has studied benign overfitting, a\\nphenomenon where some interpolating methods approach Bayes optimality, even in\\nthe presence of noise. In this work we argue that while benign overfitting has\\nbeen instructive and fruitful to study, many real interpolating methods like\\nneural networks do not fit benignly: modest noise in the training set causes\\nnonzero (but non-infinite) excess risk at test time, implying these models are\\nneither benign nor catastrophic but rather fall in an intermediate regime. We\\ncall this intermediate regime tempered overfitting, and we initiate its\\nsystematic study. We first explore this phenomenon in the context of kernel\\n(ridge) regression (KR) by obtaining conditions on the ridge parameter and\\nkernel eigenspectrum under which KR exhibits each of the three behaviors. We\\nfind that kernels with powerlaw spectra, including Laplace kernels and ReLU\\nneural tangent kernels, exhibit tempered overfitting. We then empirically study\\ndeep neural networks through the lens of our taxonomy, and find that those\\ntrained to interpolation are tempered, while those stopped early are benign. We\\nhope our work leads to a more refined understanding of overfitting in modern\\nlearning. Initially introduced by Peter Hammer, Logical Analysis of Data is a\\nmethodology that aims at computing a logical justification for dividing a group\\nof data in two groups of observations, usually called the positive and negative\\ngroups. Consider this partition into positive and negative groups as the\\ndescription of a partially defined Boolean function; the data is then processed\\nto identify a subset of attributes, whose values may be used to characterize\\nthe observations of the positive groups against those of the negative group.\\n  LAD constitutes an interesting rule-based learning alternative to classic\\nstatistical learning techniques and has many practical applications.\\nNevertheless, the computation of group characterization may be costly,\\ndepending on the properties of the data instances. A major aim of our work is\\nto provide effective tools for speeding up the computations, by computing some\\n\\\\emph{a priori} probability that a given set of attributes does characterize\\nthe positive and negative groups. To this effect, we propose several models for\\nrepresenting the data set of observations, according to the information we have\\non it. These models, and the probabilities they allow us to compute, are also\\nhelpful for quickly assessing some properties of the real data at hand;\\nfurthermore they may help us to better analyze and understand the computational\\ndifficulties encountered by solving methods.\\n  Once our models have been established, the mathematical tools for computing\\nprobabilities come from Analytic Combinatorics. They allow us to express the\\ndesired probabilities as ratios of generating functions coefficients, which\\nthen provide a quick computation of their numerical values. A further,\\nlong-range goal of this paper is to show that the methods of Analytic\\nCombinatorics can help in analyzing the performance of various algorithms in\\nLAD and related fields. Since the 1980s, machine learning has been widely used for horse-racing\\npredictions, gradually expanding to where algorithms are now playing a huge\\nrole in the betting market. Machine learning has changed the horse-racing\\nbetting market over the last ten years, but main changes are still to come. The\\nparadigm shift of neural networks (deep learning) may not only improve our\\nability to simply predict the outcome of a race, but it will also certainly\\nshake our entire way of thinking about horse-racing - and maybe more generally\\nabout horses. Since 2012, deep learning provided more and more state-of-the-art\\nresults in computer vision and now statistical learning or game theory. We\\ndescribe how the convergence of the three machine learning fields (computer\\nvision, statistical learning, and game theory) will be game-changers in the\\nnext decade in our ability to predict and understand horse-racing. We consider\\nthat horse-racing is a real world laboratory where we can work on the\\nanimal-human interaction and build a non-anthropocentric Artificial\\nIntelligence. We believe that this will lead us to understand the horses better\\nand the interactions between animals and humans in general. A vast amount of location information exists in unstructured texts, such as\\nsocial media posts, news stories, scientific articles, web pages, travel blogs,\\nand historical archives. Geoparsing refers to the process of recognizing\\nlocation references from texts and identifying their geospatial\\nrepresentations. While geoparsing can benefit many domains, a summary of the\\nspecific applications is still missing. Further, there lacks a comprehensive\\nreview and comparison of existing approaches for location reference\\nrecognition, which is the first and a core step of geoparsing. To fill these\\nresearch gaps, this review first summarizes seven typical application domains\\nof geoparsing: geographic information retrieval, disaster management, disease\\nsurveillance, traffic management, spatial humanities, tourism management, and\\ncrime management. We then review existing approaches for location reference\\nrecognition by categorizing these approaches into four groups based on their\\nunderlying functional principle: rule-based, gazetteer matching-based,\\nstatistical learning-based, and hybrid approaches. Next, we thoroughly evaluate\\nthe correctness and computational efficiency of the 27 most widely used\\napproaches for location reference recognition based on 26 public datasets with\\ndifferent types of texts (e.g., social media posts and news stories) containing\\n39,736 location references across the world. Results from this thorough\\nevaluation can help inform future methodological developments for location\\nreference recognition, and can help guide the selection of proper approaches\\nbased on application needs. Meta learning has demonstrated tremendous success in few-shot learning with\\nlimited supervised data. In those settings, the meta model is usually\\noverparameterized. While the conventional statistical learning theory suggests\\nthat overparameterized models tend to overfit, empirical evidence reveals that\\noverparameterized meta learning methods still work well -- a phenomenon often\\ncalled \\\"benign overfitting.\\\" To understand this phenomenon, we focus on the\\nmeta learning settings with a challenging bilevel structure that we term the\\ngradient-based meta learning, and analyze its generalization performance under\\nan overparameterized meta linear regression model. While our analysis uses the\\nrelatively tractable linear models, our theory contributes to understanding the\\ndelicate interplay among data heterogeneity, model adaptation and benign\\noverfitting in gradient-based meta learning tasks. We corroborate our\\ntheoretical claims through numerical simulations. The integrity of training data, even when annotated by experts, is far from\\nguaranteed, especially for non-IID datasets comprising both in- and\\nout-of-distribution samples. In an ideal scenario, the majority of samples\\nwould be in-distribution, while samples that deviate semantically would be\\nidentified as out-of-distribution and excluded during the annotation process.\\nHowever, experts may erroneously classify these out-of-distribution samples as\\nin-distribution, assigning them labels that are inherently unreliable. This\\nmixture of unreliable labels and varied data types makes the task of learning\\nrobust neural networks notably challenging. We observe that both in- and\\nout-of-distribution samples can almost invariably be ruled out from belonging\\nto certain classes, aside from those corresponding to unreliable ground-truth\\nlabels. This opens the possibility of utilizing reliable complementary labels\\nthat indicate the classes to which a sample does not belong. Guided by this\\ninsight, we introduce a novel approach, termed \\\\textit{Gray Learning} (GL),\\nwhich leverages both ground-truth and complementary labels. Crucially, GL\\nadaptively adjusts the loss weights for these two label types based on\\nprediction confidence levels. By grounding our approach in statistical learning\\ntheory, we derive bounds for the generalization error, demonstrating that GL\\nachieves tight constraints even in non-IID settings. Extensive experimental\\nevaluations reveal that our method significantly outperforms alternative\\napproaches grounded in robust statistics. Whether the Millennials are less auto-centric than the previous generations\\nhas been widely discussed in the literature. Most existing studies use\\nregression models and assume that all factors are linear-additive in\\ncontributing to the young adults' driving behaviors. This study relaxes this\\nassumption by applying a non-parametric statistical learning method, namely the\\ngradient boosting decision trees (GBDT). Using U.S. nationwide travel surveys\\nfor 2001 and 2017, this study examines the non-linear dose-response effects of\\nlifecycle, socio-demographic and residential factors on daily driving distances\\nof Millennial and Gen-X young adults. Holding all other factors constant,\\nMillennial young adults had shorter predicted daily driving distances than\\ntheir Gen-X counterparts. Besides, residential and economic factors explain\\naround 50% of young adults' daily driving distances, while the collective\\ncontributions for life course events and demographics are about 33%. This study\\nalso identifies the density ranges for formulating effective land use policies\\naiming at reducing automobile travel demand. Statistical learning with a large number of rare binary features is commonly\\nencountered in analyzing electronic health records (EHR) data, especially in\\nthe modeling of disease onset with prior medical diagnoses and procedures.\\nDealing with the resulting highly sparse and large-scale binary feature matrix\\nis notoriously challenging as conventional methods may suffer from a lack of\\npower in testing and inconsistency in model fitting while machine learning\\nmethods may suffer from the inability of producing interpretable results or\\nclinically-meaningful risk factors. To improve EHR-based modeling and utilize\\nthe natural hierarchical structure of disease classification, we propose a\\ntree-guided feature selection and logic aggregation approach for large-scale\\nregression with rare binary features, in which dimension reduction is achieved\\nthrough not only a sparsity pursuit but also an aggregation promoter with the\\nlogic operator of ``or''. We convert the combinatorial problem into a convex\\nlinearly-constrained regularized estimation, which enables scalable computation\\nwith theoretical guarantees. In a suicide risk study with EHR data, our\\napproach is able to select and aggregate prior mental health diagnoses as\\nguided by the diagnosis hierarchy of the International Classification of\\nDiseases. By balancing the rarity and specificity of the EHR diagnosis records,\\nour strategy improves both prediction and model interpretation. We identify\\nimportant higher-level categories and subcategories of mental health conditions\\nand simultaneously determine the level of specificity needed for each of them\\nin predicting suicide risk. Application of machine learning may be understood as deriving new knowledge\\nfor practical use through explaining accumulated observations, training set.\\nPeirce used the term abduction for this kind of inference. Here I formalize the\\nconcept of abduction for real valued hypotheses, and show that 14 of the most\\npopular textbook ML learners (every learner I tested), covering classification,\\nregression and clustering, implement this concept of abduction inference. The\\napproach is proposed as an alternative to statistical learning theory, which\\nrequires an impractical assumption of indefinitely increasing training set for\\nits justification. Most machine learning techniques are based upon statistical learning theory,\\noften simplified for the sake of computing speed. This paper is focused on the\\nuncertainty aspect of mathematical modeling in machine learning. Regression\\nanalysis is chosen to further investigate the evaluation aspect of uncertainty\\nin model coefficients and, more importantly, in the output feature value\\npredictions. A survey demonstrates major stages in the conventional least\\nsquares approach to the creation of the regression model, along with its\\nuncertainty estimation. On the other hand, it is shown that in machine learning\\nthe model complexity and severe nonlinearity become serious obstacles to\\nuncertainty evaluation. Furthermore, the process of machine model training\\ndemands high computing power, not available at the level of personal computers.\\nThis is why so-called pre-trained models are widely used in such areas of\\nmachine learning as natural language processing. The latest example of a\\npre-trained model is the Generative Pre-trained Transformer 3 with hundreds of\\nbillions of parameters and a half-terabyte training dataset. Similarly,\\nmathematical models built from real data are growing in complexity which is\\naccompanied by the growing amount of training data. However, when machine\\nmodels and their predictions are used in decision-making, one needs to estimate\\nuncertainty and evaluate accompanying risks. This problem could be resolved\\nwith non-parametric techniques at the expense of greater demand for computing\\npower, which can be offered by modern supercomputers available, including those\\nutilizing graphical and tensor processing units along with the conventional\\ncentral processors. In this paper, we study a large-scale multi-agent minimax optimization\\nproblem, which models many interesting applications in statistical learning and\\ngame theory, including Generative Adversarial Networks (GANs). The overall\\nobjective is a sum of agents' private local objective functions. We first\\nanalyze an important special case, empirical minimax problem, where the overall\\nobjective approximates a true population minimax risk by statistical samples.\\nWe provide generalization bounds for learning with this objective through\\nRademacher complexity analysis. Then, we focus on the federated setting, where\\nagents can perform local computation and communicate with a central server.\\nMost existing federated minimax algorithms either require communication per\\niteration or lack performance guarantees with the exception of Local Stochastic\\nGradient Descent Ascent (SGDA), a multiple-local-update descent ascent\\nalgorithm which guarantees convergence under a diminishing stepsize. By\\nanalyzing Local SGDA under the ideal condition of no gradient noise, we show\\nthat generally it cannot guarantee exact convergence with constant stepsizes\\nand thus suffers from slow rates of convergence. To tackle this issue, we\\npropose FedGDA-GT, an improved Federated (Fed) Gradient Descent Ascent (GDA)\\nmethod based on Gradient Tracking (GT). When local objectives are Lipschitz\\nsmooth and strongly-convex-strongly-concave, we prove that FedGDA-GT converges\\nlinearly with a constant stepsize to global $\\\\epsilon$-approximation solution\\nwith $\\\\mathcal{O}(\\\\log (1/\\\\epsilon))$ rounds of communication, which matches\\nthe time complexity of centralized GDA method. Finally, we numerically show\\nthat FedGDA-GT outperforms Local SGDA. This paper proposes a new method to predict individual political ideology\\nfrom digital footprints on one of the world's largest online discussion forum.\\nWe compiled a unique data set from the online discussion forum reddit that\\ncontains information on the political ideology of around 91,000 users as well\\nas records of their comment frequency and the comments' text corpus in over\\n190,000 different subforums of interest. Applying a set of statistical learning\\napproaches, we show that information about activity in non-political discussion\\nforums alone, can very accurately predict a user's political ideology.\\nDepending on the model, we are able to predict the economic dimension of\\nideology with an accuracy of up to 90.63% and the social dimension with and\\naccuracy of up to 82.02%. In comparison, using the textual features from actual\\ncomments does not improve predictive accuracy. Our paper highlights the\\nimportance of revealed digital behaviour to complement stated preferences from\\ndigital communication when analysing human preferences and behaviour using\\nonline data. This work establishes that a physical system can perform statistical learning\\nwithout gradient computations, via an Agnostic Equilibrium Propagation\\n(Aeqprop) procedure that combines energy minimization, homeostatic control, and\\nnudging towards the correct response. In Aeqprop, the specifics of the system\\ndo not have to be known: the procedure is based only on external manipulations,\\nand produces a stochastic gradient descent without explicit gradient\\ncomputations. Thanks to nudging, the system performs a true, order-one gradient\\nstep for each training sample, in contrast with order-zero methods like\\nreinforcement or evolutionary strategies, which rely on trial and error. This\\nprocedure considerably widens the range of potential hardware for statistical\\nlearning to any system with enough controllable parameters, even if the details\\nof the system are poorly known. Aeqprop also establishes that in natural\\n(bio)physical systems, genuine gradient-based statistical learning may result\\nfrom generic, relatively simple mechanisms, without backpropagation and its\\nrequirement for analytic knowledge of partial derivatives. We study a class of dynamical systems modelled as Markov chains that admit an\\ninvariant distribution via the corresponding transfer, or Koopman, operator.\\nWhile data-driven algorithms to reconstruct such operators are well known,\\ntheir relationship with statistical learning is largely unexplored. We\\nformalize a framework to learn the Koopman operator from finite data\\ntrajectories of the dynamical system. We consider the restriction of this\\noperator to a reproducing kernel Hilbert space and introduce a notion of risk,\\nfrom which different estimators naturally arise. We link the risk with the\\nestimation of the spectral decomposition of the Koopman operator. These\\nobservations motivate a reduced-rank operator regression (RRR) estimator. We\\nderive learning bounds for the proposed estimator, holding both in i.i.d. and\\nnon i.i.d. settings, the latter in terms of mixing coefficients. Our results\\nsuggest RRR might be beneficial over other widely used estimators as confirmed\\nin numerical experiments both for forecasting and mode decomposition. Statistical learning theory provides bounds on the necessary number of\\ntraining samples needed to reach a prescribed accuracy in a learning problem\\nformulated over a given target class. This accuracy is typically measured in\\nterms of a generalization error, that is, an expected value of a given loss\\nfunction. However, for several applications -- for example in a\\nsecurity-critical context or for problems in the computational sciences --\\naccuracy in this sense is not sufficient. In such cases, one would like to have\\nguarantees for high accuracy on every input value, that is, with respect to the\\nuniform norm. In this paper we precisely quantify the number of training\\nsamples needed for any conceivable training algorithm to guarantee a given\\nuniform accuracy on any learning problem formulated over target classes\\ncontaining (or consisting of) ReLU neural networks of a prescribed\\narchitecture. We prove that, under very general assumptions, the minimal number\\nof training samples for this task scales exponentially both in the depth and\\nthe input dimension of the network architecture. Due to the drastic gap in complexity between sequential and batch statistical\\nlearning, recent work has studied a smoothed sequential learning setting, where\\nNature is constrained to select contexts with density bounded by 1/{\\\\sigma}\\nwith respect to a known measure {\\\\mu}. Unfortunately, for some function\\nclasses, there is an exponential gap between the statistically optimal regret\\nand that which can be achieved efficiently. In this paper, we give a\\ncomputationally efficient algorithm that is the first to enjoy the\\nstatistically optimal log(T/{\\\\sigma}) regret for realizable K-wise linear\\nclassification. We extend our results to settings where the true classifier is\\nlinear in an over-parameterized polynomial featurization of the contexts, as\\nwell as to a realizable piecewise-regression setting assuming access to an\\nappropriate ERM oracle. Somewhat surprisingly, standard disagreement-based\\nanalyses are insufficient to achieve regret logarithmic in 1/{\\\\sigma}. Instead,\\nwe develop a novel characterization of the geometry of the disagreement region\\ninduced by generalized linear classifiers. Along the way, we develop numerous\\ntechnical tools of independent interest, including a general anti-concentration\\nbound for the determinant of certain matrix averages. We study how well generative adversarial networks (GAN) learn probability\\ndistributions from finite samples by analyzing the convergence rates of these\\nmodels. Our analysis is based on a new oracle inequality that decomposes the\\nestimation error of GAN into the discriminator and generator approximation\\nerrors, generalization error and optimization error. To estimate the\\ndiscriminator approximation error, we establish error bounds on approximating\\nH\\\\\\\"older functions by ReLU neural networks, with explicit upper bounds on the\\nLipschitz constant of the network or norm constraint on the weights. For\\ngenerator approximation error, we show that neural network can approximately\\ntransform a low-dimensional source distribution to a high-dimensional target\\ndistribution and bound such approximation error by the width and depth of\\nneural network. Combining the approximation results with generalization bounds\\nof neural networks from statistical learning theory, we establish the\\nconvergence rates of GANs in various settings, when the error is measured by a\\ncollection of integral probability metrics defined through H\\\\\\\"older classes,\\nincluding the Wasserstein distance as a special case. In particular, for\\ndistributions concentrated around a low-dimensional set, we show that the\\nconvergence rates of GANs do not depend on the high ambient dimension, but on\\nthe lower intrinsic dimension. Distributed statistical learning has become a popular technique for\\nlarge-scale data analysis. Most existing work in this area focuses on dividing\\nthe observations, but we propose a new algorithm, DDAC-SpAM, which divides the\\nfeatures under a high-dimensional sparse additive model. Our approach involves\\nthree steps: divide, decorrelate, and conquer. The decorrelation operation\\nenables each local estimator to recover the sparsity pattern for each additive\\ncomponent without imposing strict constraints on the correlation structure\\namong variables. The effectiveness and efficiency of the proposed algorithm are\\ndemonstrated through theoretical analysis and empirical results on both\\nsynthetic and real data. The theoretical results include both the consistent\\nsparsity pattern recovery as well as statistical inference for each additive\\nfunctional component. Our approach provides a practical solution for fitting\\nsparse additive models, with promising applications in a wide range of domains. The Matrix-based Renyi's entropy enables us to directly measure information\\nquantities from given data without the costly probability density estimation of\\nunderlying distributions, thus has been widely adopted in numerous statistical\\nlearning and inference tasks. However, exactly calculating this new information\\nquantity requires access to the eigenspectrum of a semi-positive definite (SPD)\\nmatrix $A$ which grows linearly with the number of samples $n$, resulting in a\\n$O(n^3)$ time complexity that is prohibitive for large-scale applications. To\\naddress this issue, this paper takes advantage of stochastic trace\\napproximations for matrix-based Renyi's entropy with arbitrary $\\\\alpha \\\\in R^+$\\norders, lowering the complexity by converting the entropy approximation to a\\nmatrix-vector multiplication problem. Specifically, we develop random\\napproximations for integer order $\\\\alpha$ cases and polynomial series\\napproximations (Taylor and Chebyshev) for non-integer $\\\\alpha$ cases, leading\\nto a $O(n^2sm)$ overall time complexity, where $s,m \\\\ll n$ denote the number of\\nvector queries and the polynomial order respectively. We theoretically\\nestablish statistical guarantees for all approximation algorithms and give\\nexplicit order of s and m with respect to the approximation error\\n$\\\\varepsilon$, showing optimal convergence rate for both parameters up to a\\nlogarithmic factor. Large-scale simulations and real-world applications\\nvalidate the effectiveness of the developed approximations, demonstrating\\nremarkable speedup with negligible loss in accuracy. The Chernoff bound is a well-known tool for obtaining a high probability\\nbound on the expectation of a Bernoulli random variable in terms of its sample\\naverage. This bound is commonly used in statistical learning theory to upper\\nbound the generalisation risk of a hypothesis in terms of its empirical risk on\\nheld-out data, for the case of a binary-valued loss function. However, the\\nextension of this bound to the case of random variables taking values in the\\nunit interval is less well known in the community. In this note we provide a\\nproof of this extension for convenience and future reference. What explains the dramatic progress from 20th-century to 21st-century AI, and\\nhow can the remaining limitations of current AI be overcome? The widely\\naccepted narrative attributes this progress to massive increases in the\\nquantity of computational and data resources available to support statistical\\nlearning in deep artificial neural networks. We show that an additional crucial\\nfactor is the development of a new type of computation. Neurocompositional\\ncomputing adopts two principles that must be simultaneously respected to enable\\nhuman-level cognition: the principles of Compositionality and Continuity. These\\nhave seemed irreconcilable until the recent mathematical discovery that\\ncompositionality can be realized not only through discrete methods of symbolic\\ncomputing, but also through novel forms of continuous neural computing. The\\nrevolutionary recent progress in AI has resulted from the use of limited forms\\nof neurocompositional computing. New, deeper forms of neurocompositional\\ncomputing create AI systems that are more robust, accurate, and comprehensible. Orthogonal statistical learning and double machine learning have emerged as\\ngeneral frameworks for two-stage statistical prediction in the presence of a\\nnuisance component. We establish non-asymptotic bounds on the excess risk of\\northogonal statistical learning methods with a loss function satisfying a\\nself-concordance property. Our bounds improve upon existing bounds by a\\ndimension factor while lifting the assumption of strong convexity. We\\nillustrate the results with examples from multiple treatment effect estimation\\nand generalized partially linear modeling. The development of state-of-the-art (SOTA) Natural Language Processing (NLP)\\nsystems has steadily been establishing new techniques to absorb the statistics\\nof linguistic data. These techniques often trace well-known constructs from\\ntraditional theories, and we study these connections to close gaps around key\\nNLP methods as a means to orient future work. For this, we introduce an\\nanalytic model of the statistics learned by seminal algorithms (including GloVe\\nand Word2Vec), and derive insights for systems that use these algorithms and\\nthe statistics of co-occurrence, in general. In this work, we derive -- to the\\nbest of our knowledge -- the first known solution to Word2Vec's\\nsoftmax-optimized, skip-gram algorithm. This result presents exciting potential\\nfor future development as a direct solution to a deep learning (DL) language\\nmodel's (LM's) matrix factorization. However, we use the solution to\\ndemonstrate a seemingly-universal existence of a property that word vectors\\nexhibit and which allows for the prophylactic discernment of biases in data --\\nprior to their absorption by DL models. To qualify our work, we conduct an\\nanalysis of independence, i.e., on the density of statistical dependencies in\\nco-occurrence models, which in turn renders insights on the distributional\\nhypothesis' partial fulfillment by co-occurrence statistics. Conventionally, generation of natural language for dialogue agents may be\\nviewed as a statistical learning problem: determine the patterns in\\nhuman-provided data and generate appropriate responses with similar statistical\\nproperties. However, dialogue can also be regarded as a goal directed process,\\nwhere speakers attempt to accomplish a specific task. Reinforcement learning\\n(RL) algorithms are designed specifically for solving such goal-directed\\nproblems, but the most direct way to apply RL -- through trial-and-error\\nlearning in human conversations, -- is costly. In this paper, we study how\\noffline reinforcement learning can instead be used to train dialogue agents\\nentirely using static datasets collected from human speakers. Our experiments\\nshow that recently developed offline RL methods can be combined with language\\nmodels to yield realistic dialogue agents that better accomplish task goals. Knowledge graph (KG) representation learning aims to encode entities and\\nrelations into dense continuous vector spaces such that knowledge contained in\\na dataset could be consistently represented. Dense embeddings trained from KG\\ndatasets benefit a variety of downstream tasks such as KG completion and link\\nprediction. However, existing KG embedding methods fell short to provide a\\nsystematic solution for the global consistency of knowledge representation. We\\ndeveloped a mathematical language for KG based on an observation of their\\ninherent algebraic structure, which we termed as Knowledgebra. By analyzing\\nfive distinct algebraic properties, we proved that the semigroup is the most\\nreasonable algebraic structure for the relation embedding of a general\\nknowledge graph. We implemented an instantiation model, SemE, using simple\\nmatrix semigroups, which exhibits state-of-the-art performance on standard\\ndatasets. Moreover, we proposed a regularization-based method to integrate\\nchain-like logic rules derived from human knowledge into embedding training,\\nwhich further demonstrates the power of the developed language. As far as we\\nknow, by applying abstract algebra in statistical learning, this work develops\\nthe first formal language for general knowledge graphs, and also sheds light on\\nthe problem of neural-symbolic integration from an algebraic perspective. Applying deep learning to science is a new trend in recent years which leads\\nDL engineering to become an important problem. Although training data\\npreparation, model architecture design, and model training are the normal\\nprocesses to build DL models, all of them are complex and costly. Therefore,\\nreusing the open-sourced pre-trained model is a practical way to bypass this\\nhurdle for developers. Given a specific task, developers can collect massive\\npre-trained deep neural networks from public sources for re-using. However,\\ntesting the performance (e.g., accuracy and robustness) of multiple DNNs and\\nrecommending which model should be used is challenging regarding the scarcity\\nof labeled data and the demand for domain expertise. In this paper, we propose\\na labeling-free (LaF) model selection approach to overcome the limitations of\\nlabeling efforts for automated model reusing. The main idea is to statistically\\nlearn a Bayesian model to infer the models' specialty only based on predicted\\nlabels. We evaluate LaF using 9 benchmark datasets including image, text, and\\nsource code, and 165 DNNs, considering both the accuracy and robustness of\\nmodels. The experimental results demonstrate that LaF outperforms the baseline\\nmethods by up to 0.74 and 0.53 on Spearman's correlation and Kendall's $\\\\tau$,\\nrespectively. Nonconvex penalties are utilized for regularization in high-dimensional\\nstatistical learning algorithms primarily because they yield unbiased or nearly\\nunbiased estimators for the parameters in the model. Nonconvex penalties\\nexisting in the literature such as SCAD, MCP, Laplace and arctan have a\\nsingularity at origin which makes them useful also for variable selection.\\nHowever, in several high-dimensional frameworks such as deep learning, variable\\nselection is less of a concern. In this paper, we present a nonconvex penalty\\nwhich is smooth at origin. The paper includes asymptotic results for ordinary\\nleast squares estimators regularized with the new penalty function, showing\\nasymptotic bias that vanishes exponentially fast. We also conducted an\\nempirical study employing deep neural network architecture on three datasets\\nand convolutional neural network on four datasets. The empirical study showed\\nbetter performance for the new regularization approach in five out of the seven\\ndatasets. Private and public organizations regularly collect and analyze digitalized\\ndata about their associates, volunteers, clients, etc. However, because most\\npersonal data are sensitive, there is a key challenge in designing\\nprivacy-preserving systems. To tackle privacy concerns, research communities\\nhave proposed different methods to preserve privacy, with Differential privacy\\n(DP) standing out as a formal definition that allows quantifying the\\nprivacy-utility trade-off. Besides, with the local DP (LDP) model, users can\\nsanitize their data locally before transmitting it to the server. The objective\\nof this thesis is thus two-fold: O$_1$) To improve the utility and privacy in\\nmultiple frequency estimates under LDP guarantees, which is fundamental to\\nstatistical learning. And O$_2$) To assess the privacy-utility trade-off of\\nmachine learning (ML) models trained over differentially private data. For\\nO$_1$, we first tackled the problem from two \\\"multiple\\\" perspectives, i.e.,\\nmultiple attributes and multiple collections throughout time, while focusing on\\nutility. Secondly, we focused our attention on the multiple attributes aspect\\nonly, in which we proposed a solution focusing on privacy while preserving\\nutility. In both cases, we demonstrate through analytical and experimental\\nvalidations the advantages of our proposed solutions over state-of-the-art LDP\\nprotocols. For O$_2$, we empirically evaluated ML-based solutions designed to\\nsolve real-world problems while ensuring DP guarantees. Indeed, we mainly used\\nthe input data perturbation setting from the privacy-preserving ML literature.\\nThis is the situation in which the whole dataset is sanitized independently\\nand, thus, we implemented LDP algorithms from the perspective of the\\ncentralized data owner. In all cases, we concluded that differentially private\\nML models achieve nearly the same utility metrics as non-private ones. We describe basic ideas underlying research to build and understand\\nartificially intelligent systems: from symbolic approaches via statistical\\nlearning to interventional models relying on concepts of causality. Some of the\\nhard open problems of machine learning and AI are intrinsically related to\\ncausality, and progress may require advances in our understanding of how to\\nmodel and infer causality from data. We initiate a study of supervised learning from many independent sequences\\n(\\\"trajectories\\\") of non-independent covariates, reflecting tasks in sequence\\nmodeling, control, and reinforcement learning. Conceptually, our\\nmulti-trajectory setup sits between two traditional settings in statistical\\nlearning theory: learning from independent examples and learning from a single\\nauto-correlated sequence. Our conditions for efficient learning generalize the\\nformer setting--trajectories must be non-degenerate in ways that extend\\nstandard requirements for independent examples. Notably, we do not require that\\ntrajectories be ergodic, long, nor strictly stable.\\n  For linear least-squares regression, given $n$-dimensional examples produced\\nby $m$ trajectories, each of length $T$, we observe a notable change in\\nstatistical efficiency as the number of trajectories increases from a few\\n(namely $m \\\\lesssim n$) to many (namely $m \\\\gtrsim n$). Specifically, we\\nestablish that the worst-case error rate of this problem is $\\\\Theta(n / m T)$\\nwhenever $m \\\\gtrsim n$. Meanwhile, when $m \\\\lesssim n$, we establish a (sharp)\\nlower bound of $\\\\Omega(n^2 / m^2 T)$ on the worst-case error rate, realized by\\na simple, marginally unstable linear dynamical system. A key upshot is that, in\\ndomains where trajectories regularly reset, the error rate eventually behaves\\nas if all of the examples were independent, drawn from their marginals. As a\\ncorollary of our analysis, we also improve guarantees for the linear system\\nidentification problem. Morphologically rich languages pose difficulties to machine translation.\\nMachine translation engines that rely on statistical learning from parallel\\ntraining data, such as state-of-the-art neural systems, face challenges\\nespecially with rich morphology on the output language side. Key challenges of\\nrich target-side morphology in data-driven machine translation include: (1) A\\nlarge amount of differently inflected word surface forms entails a larger\\nvocabulary and thus data sparsity. (2) Some inflected forms of infrequent terms\\ntypically do not appear in the training corpus, which makes closed-vocabulary\\nsystems unable to generate these unobserved variants. (3) Linguistic agreement\\nrequires the system to correctly match the grammatical categories between\\ninflected word forms in the output sentence, both in terms of target-side\\nmorpho-syntactic wellformedness and semantic adequacy with respect to the\\ninput.\\n  In this paper, we re-investigate two target-side linguistic processing\\ntechniques: a lemma-tag strategy and a linguistically informed word\\nsegmentation strategy. Our experiments are conducted on a English-German\\ntranslation task under three training corpus conditions of different\\nmagnitudes. We find that a stronger Transformer baseline leaves less room for\\nimprovement than a shallow-RNN encoder-decoder model when translating\\nin-domain. However, we find that linguistic modeling of target-side morphology\\ndoes benefit the Transformer model when the same system is applied to\\nout-of-domain input text. We also successfully apply our approach to English to\\nCzech translation. Traditional statistical learning theory relies on the assumption that data\\nare identically and independently generated from a given distribution (i.i.d.).\\nThe independently distributed assumption, on the other hand, fails to hold in\\nmany real applications. In this survey, we consider learning settings in which\\nexamples are dependent and their dependence relationship can be characterized\\nby a graph. We collect various graph-dependent concentration bounds, which are\\nthen used to derive Rademacher and stability generalization bounds for learning\\nfrom graph-dependent data. We illustrate this paradigm with three learning\\ntasks and provide some research directions for future work. To the best of our\\nknowledge, this is the first survey on this subject. Learning from Label Proportions (LLP) is a weakly supervised learning method\\nthat aims to perform instance classification from training data consisting of\\npairs of bags containing multiple instances and the class label proportions\\nwithin the bags. Previous studies on multiclass LLP can be divided into two\\ncategories according to the learning task: per-instance label classification\\nand per-bag label proportion estimation. However, these methods often results\\nin high variance estimates of the risk when applied to complex models, or lack\\nstatistical learning theory arguments. To address this issue, we propose new\\nlearning methods based on statistical learning theory for both per-instance and\\nper-bag policies. We demonstrate that the proposed methods are respectively\\nrisk-consistent and classifier-consistent in an instance-wise manner, and\\nanalyze the estimation error bounds. Additionally, we present a heuristic\\napproximation method that utilizes an existing method for regressing label\\nproportions to reduce the computational complexity of the proposed methods.\\nThrough benchmark experiments, we demonstrated the effectiveness of the\\nproposed methods. A common approach in forecasting problems is to estimate a least-squares\\nregression (or other statistical learning models) from past data, which is then\\napplied to predict future outcomes. An underlying assumption is that the same\\ncorrelations that were observed in the past still hold for the future. We\\npropose a model for situations when this assumption is not met: adopting\\nmethods from the state space literature, we model how regression coefficients\\nchange over time. Our approach can shed light on the large uncertainties\\nassociated with forecasting the future, and how much of this is due to changing\\ndynamics of the past. Our simulation study shows that accurate estimates are\\nobtained when the outcome is continuous, but the procedure fails for binary\\noutcomes. In this paper, we analyze the generalization performance of the Iterative\\nHard Thresholding (IHT) algorithm widely used for sparse recovery problems. The\\nparameter estimation and sparsity recovery consistency of IHT has long been\\nknown in compressed sensing. From the perspective of statistical learning,\\nanother fundamental question is how well the IHT estimation would predict on\\nunseen data. This paper makes progress towards answering this open question by\\nintroducing a novel sparse generalization theory for IHT under the notion of\\nalgorithmic stability. Our theory reveals that: 1) under natural conditions on\\nthe empirical risk function over $n$ samples of dimension $p$, IHT with\\nsparsity level $k$ enjoys an $\\\\mathcal{\\\\tilde\\nO}(n^{-1/2}\\\\sqrt{k\\\\log(n)\\\\log(p)})$ rate of convergence in sparse excess risk;\\n2) a tighter $\\\\mathcal{\\\\tilde O}(n^{-1/2}\\\\sqrt{\\\\log(n)})$ bound can be\\nestablished by imposing an additional iteration stability condition on a\\nhypothetical IHT procedure invoked to the population risk; and 3) a fast rate\\nof order $\\\\mathcal{\\\\tilde O}\\\\left(n^{-1}k(\\\\log^3(n)+\\\\log(p))\\\\right)$ can be\\nderived for strongly convex risk function under proper strong-signal\\nconditions. The results have been substantialized to sparse linear regression\\nand sparse logistic regression models to demonstrate the applicability of our\\ntheory. Preliminary numerical evidence is provided to confirm our theoretical\\npredictions. Approximate learning machines have become popular in the era of small\\ndevices, including quantised, factorised, hashed, or otherwise compressed\\npredictors, and the quest to explain and guarantee good generalisation\\nabilities for such methods has just begun. In this paper we study the role of\\napproximability in learning, both in the full precision and the approximated\\nsettings of the predictor that is learned from the data, through a notion of\\nsensitivity of predictors to the action of the approximation operator at hand.\\nWe prove upper bounds on the generalisation of such predictors, yielding the\\nfollowing main findings, for any PAC-learnable class and any given\\napproximation operator. 1) We show that under mild conditions, approximable\\ntarget concepts are learnable from a smaller labelled sample, provided\\nsufficient unlabelled data. 2) We give algorithms that guarantee a good\\npredictor whose approximation also enjoys the same generalisation guarantees.\\n3) We highlight natural examples of structure in the class of sensitivities,\\nwhich reduce, and possibly even eliminate the otherwise abundant requirement of\\nadditional unlabelled data, and henceforth shed new light onto what makes one\\nproblem instance easier to learn than another. These results embed the scope of\\nmodern model compression approaches into the general goal of statistical\\nlearning theory, which in return suggests appropriate algorithms through\\nminimising uniform bounds. Semi-supervised learning (SSL) provides an effective means of leveraging\\nunlabelled data to improve a model performance. Even though the domain has\\nreceived a considerable amount of attention in the past years, most methods\\npresent the common drawback of lacking theoretical guarantees. Our starting\\npoint is to notice that the estimate of the risk that most discriminative SSL\\nmethods minimise is biased, even asymptotically. This bias impedes the use of\\nstandard statistical learning theory and can hurt empirical performance. We\\npropose a simple way of removing the bias. Our debiasing approach is\\nstraightforward to implement and applicable to most deep SSL methods. We\\nprovide simple theoretical guarantees on the trustworthiness of these modified\\nmethods, without having to rely on the strong assumptions on the data\\ndistribution that SSL theory usually requires. In particular, we provide\\ngeneralisation error bounds for the proposed methods. We evaluate debiased\\nversions of different existing SSL methods, such as the Pseudo-label method and\\nFixmatch, and show that debiasing can compete with classic deep SSL techniques\\nin various settings by providing better calibrated models. Additionally, we\\nprovide a theoretical explanation of the intuition of the popular SSL methods. Along with the proliferation of digital data collected using sensor\\ntechnologies and a boost of computing power, Deep Learning (DL) based\\napproaches have drawn enormous attention in the past decade due to their\\nimpressive performance in extracting complex relations from raw data and\\nrepresenting valuable information. Meanwhile, though, rooted in its notorious\\nblack-box nature, the appreciation of DL has been highly debated due to the\\nlack of interpretability. On the one hand, DL only utilizes statistical\\nfeatures contained in raw data while ignoring human knowledge of the underlying\\nsystem, which results in both data inefficiency and trust issues; on the other\\nhand, a trained DL model does not provide to researchers any extra insight\\nabout the underlying system beyond its output, which, however, is the essence\\nof most fields of science, e.g. physics and economics.\\n  This thesis addresses the issue of interpretability in general information\\nmodeling and endeavors to ease the problem from two scopes. Firstly, a\\nproblem-oriented perspective is applied to incorporate knowledge into modeling\\npractice, where interesting mathematical properties emerge naturally which cast\\nconstraints on modeling. Secondly, given a trained model, various methods could\\nbe applied to extract further insights about the underlying system. These two\\npathways are termed as guided model design and secondary measurements.\\nRemarkably, a novel scheme emerges for the modeling practice in statistical\\nlearning: Algebraic Learning (AgLr). Instead of being restricted to the\\ndiscussion of any specific model, AgLr starts from idiosyncrasies of a learning\\ntask itself and studies the structure of a legitimate model class. This novel\\nscheme demonstrates the noteworthy value of abstract algebra for general AI,\\nwhich has been overlooked in recent progress, and could shed further light on\\ninterpretable information modeling. Predicting sets of outcomes -- instead of unique outcomes -- is a promising\\nsolution to uncertainty quantification in statistical learning. Despite a rich\\nliterature on constructing prediction sets with statistical guarantees,\\nadapting to unknown covariate shift -- a prevalent issue in practice -- poses a\\nserious unsolved challenge. In this paper, we show that prediction sets with\\nfinite-sample coverage guarantee are uninformative and propose a novel flexible\\ndistribution-free method, PredSet-1Step, to efficiently construct prediction\\nsets with an asymptotic coverage guarantee under unknown covariate shift. We\\nformally show that our method is \\\\textit{asymptotically probably approximately\\ncorrect}, having well-calibrated coverage error with high confidence for large\\nsamples. We illustrate that it achieves nominal coverage in a number of\\nexperiments and a data set concerning HIV risk prediction in a South African\\ncohort study. Our theory hinges on a new bound for the convergence rate of the\\ncoverage of Wald confidence intervals based on general asymptotically linear\\nestimators. Nonnegative (linear) least square problems are a fundamental class of\\nproblems that is well-studied in statistical learning and for which solvers\\nhave been implemented in many of the standard programming languages used within\\nthe machine learning community. The existing off-the-shelf solvers view the\\nnon-negativity constraint in these problems as an obstacle and, compared to\\nunconstrained least squares, perform additional effort to address it. However,\\nin many of the typical applications, the data itself is nonnegative as well,\\nand we show that the nonnegativity in this case makes the problem easier. In\\nparticular, while the oracle complexity of unconstrained least squares problems\\nnecessarily scales with one of the data matrix constants (typically the\\nspectral norm) and these problems are solved to additive error, we show that\\nnonnegative least squares problems with nonnegative data are solvable to\\nmultiplicative error and with complexity that is independent of any matrix\\nconstants. The algorithm we introduce is accelerated and based on a primal-dual\\nperspective. We further show how to provably obtain linear convergence using\\nadaptive restart coupled with our method and demonstrate its effectiveness on\\nlarge-scale data via numerical experiments. Understanding generalization in modern machine learning settings has been one\\nof the major challenges in statistical learning theory. In this context, recent\\nyears have witnessed the development of various generalization bounds\\nsuggesting different complexity notions such as the mutual information between\\nthe data sample and the algorithm output, compressibility of the hypothesis\\nspace, and the fractal dimension of the hypothesis space. While these bounds\\nhave illuminated the problem at hand from different angles, their suggested\\ncomplexity notions might appear seemingly unrelated, thereby restricting their\\nhigh-level impact. In this study, we prove novel generalization bounds through\\nthe lens of rate-distortion theory, and explicitly relate the concepts of\\nmutual information, compressibility, and fractal dimensions in a single\\nmathematical framework. Our approach consists of (i) defining a generalized\\nnotion of compressibility by using source coding concepts, and (ii) showing\\nthat the `compression error rate' can be linked to the generalization error\\nboth in expectation and with high probability. We show that in the `lossless\\ncompression' setting, we recover and improve existing mutual information-based\\nbounds, whereas a `lossy compression' scheme allows us to link generalization\\nto the rate-distortion dimension -- a particular notion of fractal dimension.\\nOur results bring a more unified perspective on generalization and open up\\nseveral future research directions. As a fundamental problem in machine learning, dataset shift induces a\\nparadigm to learn and transfer knowledge under changing environment. Previous\\nmethods assume the changes are induced by covariate, which is less practical\\nfor complex real-world data. We consider the Generalized Label Shift (GLS),\\nwhich provides an interpretable insight into the learning and transfer of\\ndesirable knowledge. Current GLS methods: 1) are not well-connected with the\\nstatistical learning theory; 2) usually assume the shifting conditional\\ndistributions will be matched with an implicit transformation, but its explicit\\nmodeling is unexplored. In this paper, we propose a conditional adaptation\\nframework to deal with these challenges. From the perspective of learning\\ntheory, we prove that the generalization error of conditional adaptation is\\nlower than previous covariate adaptation. Following the theoretical results, we\\npropose the minimum uncertainty principle to learn conditional invariant\\ntransformation via discrepancy optimization. Specifically, we propose the\\n\\\\textit{conditional metric operator} on Hilbert space to characterize the\\ndistinctness of conditional distributions. For finite observations, we prove\\nthat the empirical estimation is always well-defined and will converge to\\nunderlying truth as sample size increases. The results of extensive experiments\\ndemonstrate that the proposed model achieves competitive performance under\\ndifferent GLS scenarios. We study the problem of learning causal models from observational data\\nthrough the lens of interpolation and its counterpart -- regularization. A\\nlarge volume of recent theoretical, as well as empirical work, suggests that,\\nin highly complex model classes, interpolating estimators can have good\\nstatistical generalization properties and can even be optimal for statistical\\nlearning. Motivated by an analogy between statistical and causal learning\\nrecently highlighted by Janzing (2019), we investigate whether interpolating\\nestimators can also learn good causal models. To this end, we consider a simple\\nlinearly confounded model and derive precise asymptotics for the *causal risk*\\nof the min-norm interpolator and ridge-regularized regressors in the\\nhigh-dimensional regime. Under the principle of independent causal mechanisms,\\na standard assumption in causal learning, we find that interpolators cannot be\\noptimal and causal learning requires stronger regularization than statistical\\nlearning. This resolves a recent conjecture in Janzing (2019). Beyond this\\nassumption, we find a larger range of behavior that can be precisely\\ncharacterized with a new measure of *confounding strength*. If the confounding\\nstrength is negative, causal learning requires weaker regularization than\\nstatistical learning, interpolators can be optimal, and the optimal\\nregularization can even be negative. If the confounding strength is large, the\\noptimal regularization is infinite, and learning from observational data is\\nactively harmful. In the Big Data era, with the ubiquity of geolocation sensors in particular,\\nmassive datasets exhibiting a possibly complex spatial dependence structure are\\nbecoming increasingly available. In this context, the standard probabilistic\\ntheory of statistical learning does not apply directly and guarantees of the\\ngeneralization capacity of predictive rules learned from such data are left to\\nestablish. We analyze here the simple Kriging task from a statistical learning\\nperspective, i.e. by carrying out a nonparametric finite-sample predictive\\nanalysis. Given $d\\\\geq 1$ values taken by a realization of a square integrable\\nrandom field $X=\\\\{X_s\\\\}_{s\\\\in S}$, $S\\\\subset \\\\mathbb{R}^2$, with unknown\\ncovariance structure, at sites $s_1,\\\\; \\\\ldots,\\\\; s_d$ in $S$, the goal is to\\npredict the unknown values it takes at any other location $s\\\\in S$ with minimum\\nquadratic risk. The prediction rule being derived from a training spatial\\ndataset: a single realization $X'$ of $X$, independent from those to be\\npredicted, observed at $n\\\\geq 1$ locations $\\\\sigma_1,\\\\; \\\\ldots,\\\\; \\\\sigma_n$ in\\n$S$. Despite the connection of this minimization problem with kernel ridge\\nregression, establishing the generalization capacity of empirical risk\\nminimizers is far from straightforward, due to the non independent and\\nidentically distributed nature of the training data $X'_{\\\\sigma_1},\\\\; \\\\ldots,\\\\;\\nX'_{\\\\sigma_n}$ involved in the learning procedure. In this article,\\nnon-asymptotic bounds of order $O_{\\\\mathbb{P}}(1/\\\\sqrt{n})$ are proved for the\\nexcess risk of a plug-in predictive rule mimicking the true minimizer in the\\ncase of isotropic stationary Gaussian processes, observed at locations forming\\na regular grid in the learning stage. These theoretical results are illustrated\\nby various numerical experiments, on simulated data and on real-world datasets. Approximating distributions from their samples is a canonical\\nstatistical-learning problem. One of its most powerful and successful\\nmodalities approximates every distribution to an $\\\\ell_1$ distance essentially\\nat most a constant times larger than its closest $t$-piece degree-$d$\\npolynomial, where $t\\\\ge1$ and $d\\\\ge0$. Letting $c_{t,d}$ denote the smallest\\nsuch factor, clearly $c_{1,0}=1$, and it can be shown that $c_{t,d}\\\\ge 2$ for\\nall other $t$ and $d$. Yet current computationally efficient algorithms show\\nonly $c_{t,1}\\\\le 2.25$ and the bound rises quickly to $c_{t,d}\\\\le 3$ for $d\\\\ge\\n9$. We derive a near-linear-time and essentially sample-optimal estimator that\\nestablishes $c_{t,d}=2$ for all $(t,d)\\\\ne(1,0)$. Additionally, for many\\npractical distributions, the lowest approximation distance is achieved by\\npolynomials with vastly varying number of pieces. We provide a method that\\nestimates this number near-optimally, hence helps approach the best possible\\napproximation. Experiments combining the two techniques confirm improved\\nperformance over existing methodologies. We consider a batch active learning scenario where the learner adaptively\\nissues batches of points to a labeling oracle. Sampling labels in batches is\\nhighly desirable in practice due to the smaller number of interactive rounds\\nwith the labeling oracle (often human beings). However, batch active learning\\ntypically pays the price of a reduced adaptivity, leading to suboptimal\\nresults. In this paper we propose a solution which requires a careful trade off\\nbetween the informativeness of the queried points and their diversity. We\\ntheoretically investigate batch active learning in the practically relevant\\nscenario where the unlabeled pool of data is available beforehand ({\\\\em\\npool-based} active learning). We analyze a novel stage-wise greedy algorithm\\nand show that, as a function of the label complexity, the excess risk of this\\nalgorithm matches the known minimax rates in standard statistical learning\\nsettings. Our results also exhibit a mild dependence on the batch size. These\\nare the first theoretical results that employ careful trade offs between\\ninformativeness and diversity to rigorously quantify the statistical\\nperformance of batch active learning in the pool-based scenario. The problem of human trust in artificial intelligence is one of the most\\nfundamental problems in applied machine learning. Our processes for evaluating\\nAI trustworthiness have substantial ramifications for ML's impact on science,\\nhealth, and humanity, yet confusion surrounds foundational concepts. What does\\nit mean to trust an AI, and how do humans assess AI trustworthiness? What are\\nthe mechanisms for building trustworthy AI? And what is the role of\\ninterpretable ML in trust? Here, we draw from statistical learning theory and\\nsociological lenses on human-automation trust to motivate an AI-as-tool\\nframework, which distinguishes human-AI trust from human-AI-human trust.\\nEvaluating an AI's contractual trustworthiness involves predicting future model\\nbehavior using behavior certificates (BCs) that aggregate behavioral evidence\\nfrom diverse sources including empirical out-of-distribution and out-of-task\\nevaluation and theoretical proofs linking model architecture to behavior. We\\nclarify the role of interpretability in trust with a ladder of model access.\\nInterpretability (level 3) is not necessary or even sufficient for trust, while\\nthe ability to run a black-box model at-will (level 2) is necessary and\\nsufficient. While interpretability can offer benefits for trust, it can also\\nincur costs. We clarify ways interpretability can contribute to trust, while\\nquestioning the perceived centrality of interpretability to trust in popular\\ndiscourse. How can we empower people with tools to evaluate trust? Instead of\\ntrying to understand how a model works, we argue for understanding how a model\\nbehaves. Instead of opening up black boxes, we should create more behavior\\ncertificates that are more correct, relevant, and understandable. We discuss\\nhow to build trusted and trustworthy AI responsibly. Much of modern learning theory has been split between two regimes: the\\nclassical offline setting, where data arrive independently, and the online\\nsetting, where data arrive adversarially. While the former model is often both\\ncomputationally and statistically tractable, the latter requires no\\ndistributional assumptions. In an attempt to achieve the best of both worlds,\\nprevious work proposed the smooth online setting where each sample is drawn\\nfrom an adversarially chosen distribution, which is smooth, i.e., it has a\\nbounded density with respect to a fixed dominating measure. We provide tight\\nbounds on the minimax regret of learning a nonparametric function class, with\\nnearly optimal dependence on both the horizon and smoothness parameters.\\nFurthermore, we provide the first oracle-efficient, no-regret algorithms in\\nthis setting. In particular, we propose an oracle-efficient improper algorithm\\nwhose regret achieves optimal dependence on the horizon and a proper algorithm\\nrequiring only a single oracle call per round whose regret has the optimal\\nhorizon dependence in the classification setting and is sublinear in general.\\nBoth algorithms have exponentially worse dependence on the smoothness parameter\\nof the adversary than the minimax rate. We then prove a lower bound on the\\noracle complexity of any proper learning algorithm, which matches the\\noracle-efficient upper bounds up to a polynomial factor, thus demonstrating the\\nexistence of a statistical-computational gap in smooth online learning.\\nFinally, we apply our results to the contextual bandit setting to show that if\\na function class is learnable in the classical setting, then there is an\\noracle-efficient, no-regret algorithm for contextual bandits in the case that\\ncontexts arrive in a smooth manner. When using recurrent neural networks (RNNs) it is common practice to apply\\ntrained models to sequences longer than those seen in training. This\\n\\\"extrapolating\\\" usage deviates from the traditional statistical learning setup\\nwhere guarantees are provided under the assumption that train and test\\ndistributions are identical. Here we set out to understand when RNNs can\\nextrapolate, focusing on a simple case where the data generating distribution\\nis memoryless. We first show that even with infinite training data, there exist\\nRNN models that interpolate perfectly (i.e., they fit the training data) yet\\nextrapolate poorly to longer sequences. We then show that if gradient descent\\nis used for training, learning will converge to perfect extrapolation under\\ncertain assumptions on initialization. Our results complement recent studies on\\nthe implicit bias of gradient descent, showing that it plays a key role in\\nextrapolation when learning temporal prediction models. We consider information-theoretic bounds on expected generalization error for\\nstatistical learning problems in a networked setting. In this setting, there\\nare $K$ nodes, each with its own independent dataset, and the models from each\\nnode have to be aggregated into a final centralized model. We consider both\\nsimple averaging of the models as well as more complicated multi-round\\nalgorithms. We give upper bounds on the expected generalization error for a\\nvariety of problems, such as those with Bregman divergence or Lipschitz\\ncontinuous losses, that demonstrate an improved dependence of $1/K$ on the\\nnumber of nodes. These \\\"per node\\\" bounds are in terms of the mutual information\\nbetween the training dataset and the trained weights at each node, and are\\ntherefore useful in describing the generalization properties inherent to having\\ncommunication or privacy constraints at each node. This paper considers the problem of supervised learning with linear methods\\nwhen both features and labels can be corrupted, either in the form of heavy\\ntailed data and/or corrupted rows. We introduce a combination of coordinate\\ngradient descent as a learning algorithm together with robust estimators of the\\npartial derivatives. This leads to robust statistical learning methods that\\nhave a numerical complexity nearly identical to non-robust ones based on\\nempirical risk minimization. The main idea is simple: while robust learning\\nwith gradient descent requires the computational cost of robustly estimating\\nthe whole gradient to update all parameters, a parameter can be updated\\nimmediately using a robust estimator of a single partial derivative in\\ncoordinate gradient descent. We prove upper bounds on the generalization error\\nof the algorithms derived from this idea, that control both the optimization\\nand statistical errors with and without a strong convexity assumption of the\\nrisk. Finally, we propose an efficient implementation of this approach in a new\\npython library called linlearn, and demonstrate through extensive numerical\\nexperiments that our approach introduces a new interesting compromise between\\nrobustness, statistical performance and numerical efficiency for this problem. It has been recognized that heavily overparameterized deep neural networks\\n(DNNs) exhibit surprisingly good generalization performance in various\\nmachine-learning tasks. Although benefits of depth have been investigated from\\ndifferent perspectives such as the approximation theory and the statistical\\nlearning theory, existing theories do not adequately explain the empirical\\nsuccess of overparameterized DNNs. In this work, we report a remarkable\\ninterplay between depth and locality of a target function. We introduce\\n$k$-local and $k$-global functions, and find that depth is beneficial for\\nlearning local functions but detrimental to learning global functions. This\\ninterplay is not properly captured by the neural tangent kernel, which\\ndescribes an infinitely wide neural network within the lazy learning regime. Cross-validation (CV) is one of the most widely used techniques in\\nstatistical learning for estimating the test error of a model, but its behavior\\nis not yet fully understood. It has been shown that standard confidence\\nintervals for test error using estimates from CV may have coverage below\\nnominal levels. This phenomenon occurs because each sample is used in both the\\ntraining and testing procedures during CV and as a result, the CV estimates of\\nthe errors become correlated. Without accounting for this correlation, the\\nestimate of the variance is smaller than it should be. One way to mitigate this\\nissue is by estimating the mean squared error of the prediction error instead\\nusing nested CV. This approach has been shown to achieve superior coverage\\ncompared to intervals derived from standard CV. In this work, we generalize the\\nnested CV idea to the Cox proportional hazards model and explore various\\nchoices of test error for this setting. We present a principled data-driven strategy for learning deterministic\\nhydrodynamic models directly from stochastic non-equilibrium active particle\\ntrajectories. We apply our method to learning a hydrodynamic model for the\\npropagating density lanes observed in self-propelled particle systems and to\\nlearning a continuum description of cell dynamics in epithelial tissues. We\\nalso infer from stochastic particle trajectories the latent phoretic fields\\ndriving chemotaxis. This demonstrates that statistical learning theory combined\\nwith physical priors can enable discovery of multi-scale models of\\nnon-equilibrium stochastic processes characteristic of collective movement in\\nliving systems. The concept of median/consensus has been widely investigated in order to\\nprovide a statistical summary of ranking data, i.e. realizations of a random\\npermutation $\\\\Sigma$ of a finite set, $\\\\{1,\\\\; \\\\ldots,\\\\; n\\\\}$ with $n\\\\geq 1$\\nsay. As it sheds light onto only one aspect of $\\\\Sigma$'s distribution $P$, it\\nmay neglect other informative features. It is the purpose of this paper to\\ndefine analogs of quantiles, ranks and statistical procedures based on such\\nquantities for the analysis of ranking data by means of a metric-based notion\\nof depth function on the symmetric group. Overcoming the absence of vector\\nspace structure on $\\\\mathfrak{S}_n$, the latter defines a center-outward\\nordering of the permutations in the support of $P$ and extends the classic\\nmetric-based formulation of consensus ranking (medians corresponding then to\\nthe deepest permutations). The axiomatic properties that ranking depths should\\nideally possess are listed, while computational and generalization issues are\\nstudied at length. Beyond the theoretical analysis carried out, the relevance\\nof the novel concepts and methods introduced for a wide variety of statistical\\ntasks are also supported by numerous numerical experiments. We establish a high-dimensional statistical learning framework for\\nindividualized asset allocation. Our proposed methodology addresses\\ncontinuous-action decision-making with a large number of characteristics. We\\ndevelop a discretization approach to model the effect of continuous actions and\\nallow the discretization frequency to be large and diverge with the number of\\nobservations. The value function of continuous-action is estimated using\\npenalized regression with our proposed generalized penalties that are imposed\\non linear transformations of the model coefficients. We show that our proposed\\nDiscretization and Regression with generalized fOlded concaVe penalty on Effect\\ndiscontinuity (DROVE) approach enjoys desirable theoretical properties and\\nallows for statistical inference of the optimal value associated with optimal\\ndecision-making. Empirically, the proposed framework is exercised with the\\nHealth and Retirement Study data in finding individualized optimal asset\\nallocation. The results show that our individualized optimal strategy improves\\nthe population financial well-being. Unlike the classical linear model, nonlinear generative models have been\\naddressed sparsely in the literature of statistical learning. This work aims to\\nbringing attention to these models and their secrecy potential. To this end, we\\ninvoke the replica method to derive the asymptotic normalized cross entropy in\\nan inverse probability problem whose generative model is described by a\\nGaussian random field with a generic covariance function. Our derivations\\nfurther demonstrate the asymptotic statistical decoupling of the Bayesian\\nestimator and specify the decoupled setting for a given nonlinear model.\\n  The replica solution depicts that strictly nonlinear models establish an\\nall-or-nothing phase transition: There exists a critical load at which the\\noptimal Bayesian inference changes from perfect to an uncorrelated learning.\\nBased on this finding, we design a new secure coding scheme which achieves the\\nsecrecy capacity of the wiretap channel. This interesting result implies that\\nstrictly nonlinear generative models are perfectly secured without any secure\\ncoding. We justify this latter statement through the analysis of an\\nillustrative model for perfectly secure and reliable inference. Tree-based ensembles such as the Random Forest are modern classics among\\nstatistical learning methods. In particular, they are used for predicting\\nunivariate responses. In case of multiple outputs the question arises whether\\nwe separately fit univariate models or directly follow a multivariate approach.\\nFor the latter, several possibilities exist that are, e.g. based on modified\\nsplitting or stopping rules for multi-output regression. In this work we\\ncompare these methods in extensive simulations to help in answering the primary\\nquestion when to use multivariate ensemble techniques. The classical statistical learning theory implies that fitting too many\\nparameters leads to overfitting and poor performance. That modern deep neural\\nnetworks generalize well despite a large number of parameters contradicts this\\nfinding and constitutes a major unsolved problem towards explaining the success\\nof deep learning. While previous work focuses on the implicit regularization\\ninduced by stochastic gradient descent (SGD), we study here how the local\\ngeometry of the energy landscape around local minima affects the statistical\\nproperties of SGD with Gaussian gradient noise. We argue that under reasonable\\nassumptions, the local geometry forces SGD to stay close to a low dimensional\\nsubspace and that this induces another form of implicit regularization and\\nresults in tighter bounds on the generalization error for deep neural networks.\\nTo derive generalization error bounds for neural networks, we first introduce a\\nnotion of stagnation sets around the local minima and impose a local essential\\nconvexity property of the population risk. Under these conditions, lower bounds\\nfor SGD to remain in these stagnation sets are derived. If stagnation occurs,\\nwe derive a bound on the generalization error of deep neural networks involving\\nthe spectral norms of the weight matrices but not the number of network\\nparameters. Technically, our proofs are based on controlling the change of\\nparameter values in the SGD iterates and local uniform convergence of the\\nempirical loss functions based on the entropy of suitable neighborhoods around\\nlocal minima. There has been a surge of interest in developing robust estimators for models\\nwith heavy-tailed and bounded variance data in statistics and machine learning,\\nwhile few works impose unbounded variance. This paper proposes two type of\\nrobust estimators, the ridge log-truncated M-estimator and the elastic net\\nlog-truncated M-estimator. The first estimator is applied to convex regressions\\nsuch as quantile regression and generalized linear models, while the other one\\nis applied to high dimensional non-convex learning problems such as regressions\\nvia\\n  deep neural networks. Simulations and real data analysis demonstrate the\\n{robustness} of log-truncated estimations over standard estimations. Rankings and scores are two common data types used by judges to express\\npreferences and/or perceptions of quality in a collection of objects. Numerous\\nmodels exist to study data of each type separately, but no unified statistical\\nmodel captures both data types simultaneously without first performing data\\nconversion. We propose the Mallows-Binomial model to close this gap, which\\ncombines a Mallows' $\\\\phi$ ranking model with Binomial score models through\\nshared parameters that quantify object quality, a consensus ranking, and the\\nlevel of consensus between judges. We propose an efficient tree-search\\nalgorithm to calculate the exact MLE of model parameters, study statistical\\nproperties of the model both analytically and through simulation, and apply our\\nmodel to real data from an instance of grant panel review that collected both\\nscores and partial rankings. Furthermore, we demonstrate how model outputs can\\nbe used to rank objects with confidence. The proposed model is shown to\\nsensibly combine information from both scores and rankings to quantify object\\nquality and measure consensus with appropriate levels of statistical\\nuncertainty. A fundamental challenge in interactive learning and decision making, ranging\\nfrom bandit problems to reinforcement learning, is to provide sample-efficient,\\nadaptive learning algorithms that achieve near-optimal regret. This question is\\nanalogous to the classical problem of optimal (supervised) statistical\\nlearning, where there are well-known complexity measures (e.g., VC dimension\\nand Rademacher complexity) that govern the statistical complexity of learning.\\nHowever, characterizing the statistical complexity of interactive learning is\\nsubstantially more challenging due to the adaptive nature of the problem. The\\nmain result of this work provides a complexity measure, the Decision-Estimation\\nCoefficient, that is proven to be both necessary and sufficient for\\nsample-efficient interactive learning. In particular, we provide:\\n  1. a lower bound on the optimal regret for any interactive decision making\\nproblem, establishing the Decision-Estimation Coefficient as a fundamental\\nlimit.\\n  2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which\\ntransforms any algorithm for supervised estimation into an online algorithm for\\ndecision making. E2D attains a regret bound that matches our lower bound up to\\ndependence on a notion of estimation performance, thereby achieving optimal\\nsample-efficient learning as characterized by the Decision-Estimation\\nCoefficient.\\n  Taken together, these results constitute a theory of learnability for\\ninteractive decision making. When applied to reinforcement learning settings,\\nthe Decision-Estimation Coefficient recovers essentially all existing hardness\\nresults and lower bounds. More broadly, the approach can be viewed as a\\ndecision-theoretic analogue of the classical Le Cam theory of statistical\\nestimation; it also unifies a number of existing approaches -- both Bayesian\\nand frequentist. Recent advances in neuroimaging along with algorithmic innovations in\\nstatistical learning from network data offer a unique pathway to integrate\\nbrain structure and function, and thus facilitate revealing some of the brain's\\norganizing principles at the system level. In this direction, we develop a\\nsupervised graph representation learning framework to model the relationship\\nbetween brain structural connectivity (SC) and functional connectivity (FC) via\\na graph encoder-decoder system, where the SC is used as input to predict\\nempirical FC. A trainable graph convolutional encoder captures direct and\\nindirect interactions between brain regions-of-interest that mimic actual\\nneural communications, as well as to integrate information from both the\\nstructural network topology and nodal (i.e., region-specific) attributes. The\\nencoder learns node-level SC embeddings which are combined to generate (whole\\nbrain) graph-level representations for reconstructing empirical FC networks.\\nThe proposed end-to-end model utilizes a multi-objective loss function to\\njointly reconstruct FC networks and learn discriminative graph representations\\nof the SC-to-FC mapping for downstream subject (i.e., graph-level)\\nclassification. Comprehensive experiments demonstrate that the learnt\\nrepresentations of said relationship capture valuable information from the\\nintrinsic properties of the subject's brain networks and lead to improved\\naccuracy in classifying a large population of heavy drinkers and non-drinkers\\nfrom the Human Connectome Project. Our work offers new insights on the\\nrelationship between brain networks that support the promising prospect of\\nusing graph representation learning to discover more about human brain activity\\nand function. Modern statistical applications often involve minimizing an objective\\nfunction that may be nonsmooth and/or nonconvex. This paper focuses on a broad\\nBregman-surrogate algorithm framework including the local linear approximation,\\nmirror descent, iterative thresholding, DC programming and many others as\\nparticular instances. The recharacterization via generalized Bregman functions\\nenables us to construct suitable error measures and establish global\\nconvergence rates for nonconvex and nonsmooth objectives in possibly high\\ndimensions. For sparse learning problems with a composite objective, under some\\nregularity conditions, the obtained estimators as the surrogate's fixed points,\\nthough not necessarily local minimizers, enjoy provable statistical guarantees,\\nand the sequence of iterates can be shown to approach the statistical truth\\nwithin the desired accuracy geometrically fast. The paper also studies how to\\ndesign adaptive momentum based accelerations without assuming convexity or\\nsmoothness by carefully controlling stepsize and relaxation parameters. In this paper, an unsupervised and cognitively driven weighted-entropy method\\nfor embedding semantic categories in hyperbolic geometry is proposed. The model\\nis driven by two fields of research in cognitive linguistics: the first is the\\nstatistical learning theory of language acquisition and the proposal of using\\nhigh-dimensional networks to represent semantic knowledge in cognition, and the\\nsecond is the domain-specific approach to semantic communication. Weighted\\nconditional entropy of word co-occurrence is proposed as the embedding metric,\\nand the two weighting parameters are collocation diversity and conditional\\nprobability ranking in the corresponding statistical distribution. The\\nBoltzmann distribution is then used on the weighted-entropy metric and embedded\\ninto a hyperbolic Poincare disk model. Testing has been in particular performed\\nin the domains of basic color and kinship words, which belong to the classes\\nthat domain-specificity focused research in cognitive semantics has most\\nintensively investigated. Results show that this new approach can successfully\\nmodel and map the semantic relationships of popularity and similarity for most\\nof the basic color and kinship words in English and have potential to be\\ngeneralized to other semantic domains and different languages. Generally, this\\npaper contributes to both computational cognitive semantics and the research on\\nnetwork and geometry-driven language embedding in computational linguistics and\\nNLP. The training and test data for deep-neural-network-based classifiers are\\nusually assumed to be sampled from the same distribution. When part of the test\\nsamples are drawn from a distribution that is sufficiently far away from that\\nof the training samples (a.k.a. out-of-distribution (OOD) samples), the trained\\nneural network has a tendency to make high confidence predictions for these OOD\\nsamples. Detection of the OOD samples is critical when training a neural\\nnetwork used for image classification, object detection, etc. It can enhance\\nthe classifier's robustness to irrelevant inputs, and improve the system\\nresilience and security under different forms of attacks. Detection of OOD\\nsamples has three main challenges: (i) the proposed OOD detection method should\\nbe compatible with various architectures of classifiers (e.g., DenseNet,\\nResNet), without significantly increasing the model complexity and requirements\\non computational resources; (ii) the OOD samples may come from multiple\\ndistributions, whose class labels are commonly unavailable; (iii) a score\\nfunction needs to be defined to effectively separate OOD samples from\\nin-distribution (InD) samples. To overcome these challenges, we propose a\\nWasserstein-based out-of-distribution detection (WOOD) method. The basic idea\\nis to define a Wasserstein-distance-based score that evaluates the\\ndissimilarity between a test sample and the distribution of InD samples. An\\noptimization problem is then formulated and solved based on the proposed score\\nfunction. The statistical learning bound of the proposed method is investigated\\nto guarantee that the loss value achieved by the empirical optimizer\\napproximates the global optimum. The comparison study results demonstrate that\\nthe proposed WOOD consistently outperforms other existing OOD detection\\nmethods. In recent years, several results in the supervised learning setting suggested\\nthat classical statistical learning-theoretic measures, such as VC dimension,\\ndo not adequately explain the performance of deep learning models which\\nprompted a slew of work in the infinite-width and iteration regimes. However,\\nthere is little theoretical explanation for the success of neural networks\\nbeyond the supervised setting. In this paper we argue that, under some\\ndistributional assumptions, classical learning-theoretic measures can\\nsufficiently explain generalization for graph neural networks in the\\ntransductive setting. In particular, we provide a rigorous analysis of the\\nperformance of neural networks in the context of transductive inference,\\nspecifically by analysing the generalisation properties of graph convolutional\\nnetworks for the problem of node classification. While VC Dimension does result\\nin trivial generalisation error bounds in this setting as well, we show that\\ntransductive Rademacher complexity can explain the generalisation properties of\\ngraph convolutional networks for stochastic block models. We further use the\\ngeneralisation error bounds based on transductive Rademacher complexity to\\ndemonstrate the role of graph convolutions and network architectures in\\nachieving smaller generalisation error and provide insights into when the graph\\nstructure can help in learning. The findings of this paper could re-new the\\ninterest in studying generalisation in neural networks in terms of\\nlearning-theoretic measures, albeit in specific problems. It is important to forecast dam inflow for flood damage mitigation. The\\nhydrograph provides critical information such as the start time, peak level,\\nand volume. Particularly, dam management requires a 6-h lead time of the dam\\ninflow forecast based on a future hydrograph. The authors propose novel target\\ninflow weights to create an ocean feature vector extracted from the analyzed\\nimages of the sea surface. We extracted 4,096 elements of the dimension vector\\nin the fc6 layer of the pre-trained VGG16 network. Subsequently, we reduced it\\nto three dimensions of t-SNE. Furthermore, we created the principal component\\nof the sea temperature weights using PCA. We found that these weights\\ncontribute to the stability of predictor importance by numerical experiments.\\nAs base regression models, we calibrate the least squares with kernel\\nexpansion, the quantile random forest minimized out-of bag error, and the\\nsupport vector regression with a polynomial kernel. When we compute the\\npredictor importance, we visualize the stability of each variable importance\\nintroduced by our proposed weights, compared with other results without\\nweights. We apply our method to a dam at Kanto region in Japan and focus on the\\ntrained term from 2007 to 2018, with a limited flood term from June to October.\\nWe test the accuracy over the 2019 flood term. Finally, we present the applied\\nresults and further statistical learning for unknown flood forecast. The science of causality explains/determines 'cause-effect' relationship\\nbetween the entities of a system by providing mathematical tools for the\\npurpose. In spite of all the success and widespread applications of\\nmachine-learning (ML) algorithms, these algorithms are based on statistical\\nlearning alone. Currently, they are nowhere close to 'human-like' intelligence\\nas they fail to answer and learn based on the important \\\"Why?\\\" questions.\\nHence, researchers are attempting to integrate ML with the science of\\ncausality. Among the many causal learning issues encountered by ML, one is that\\nthese algorithms are dumb to the temporal order or structure in data. In this\\nwork we develop a machine learning pipeline based on a recently proposed\\n'neurochaos' feature learning technique (ChaosFEX feature extractor), that\\nhelps us to learn generalized causal-structure in given time-series data. Comparing probability distributions is at the crux of many machine learning\\nalgorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two\\nclasses of distances between probability distributions that have attracted\\nabundant attention in past years. This paper establishes some conditions under\\nwhich the Wasserstein distance can be controlled by MMD norms. Our work is\\nmotivated by the compressive statistical learning (CSL) theory, a general\\nframework for resource-efficient large scale learning in which the training\\ndata is summarized in a single vector (called sketch) that captures the\\ninformation relevant to the considered learning task. Inspired by existing\\nresults in CSL, we introduce the H\\\\\\\"older Lower Restricted Isometric Property\\nand show that this property comes with interesting guarantees for compressive\\nstatistical learning. Based on the relations between the MMD and the\\nWasserstein distances, we provide guarantees for compressive statistical\\nlearning by introducing and studying the concept of Wasserstein regularity of\\nthe learning task, that is when some task-specific metric between probability\\ndistributions can be bounded by a Wasserstein distance. Disobeying the classical wisdom of statistical learning theory, modern deep\\nneural networks generalize well even though they typically contain millions of\\nparameters. Recently, it has been shown that the trajectories of iterative\\noptimization algorithms can possess fractal structures, and their\\ngeneralization error can be formally linked to the complexity of such fractals.\\nThis complexity is measured by the fractal's intrinsic dimension, a quantity\\nusually much smaller than the number of parameters in the network. Even though\\nthis perspective provides an explanation for why overparametrized networks\\nwould not overfit, computing the intrinsic dimension (e.g., for monitoring\\ngeneralization during training) is a notoriously difficult task, where existing\\nmethods typically fail even in moderate ambient dimensions. In this study, we\\nconsider this problem from the lens of topological data analysis (TDA) and\\ndevelop a generic computational tool that is built on rigorous mathematical\\nfoundations. By making a novel connection between learning theory and TDA, we\\nfirst illustrate that the generalization error can be equivalently bounded in\\nterms of a notion called the 'persistent homology dimension' (PHD), where,\\ncompared with prior work, our approach does not require any additional\\ngeometrical or statistical assumptions on the training dynamics. Then, by\\nutilizing recently established theoretical results and TDA tools, we develop an\\nefficient algorithm to estimate PHD in the scale of modern deep neural networks\\nand further provide visualization tools to help understand generalization in\\ndeep learning. Our experiments show that the proposed approach can efficiently\\ncompute a network's intrinsic dimension in a variety of settings, which is\\npredictive of the generalization error. Developing shopping experiences that delight the customer requires businesses\\nto understand customer taste. This work reports a method to learn the shopping\\npreferences of frequent shoppers to an online gift store by combining ideas\\nfrom retail analytics and statistical learning with sparsity. Shopping activity\\nis represented as a bipartite graph. This graph is refined by applying\\nsparsity-based statistical learning methods. These methods are interpretable\\nand reveal insights about customers' preferences as well as products driving\\nrevenue to the store. Advancing lithium-ion batteries (LIBs) in both design and usage is key to\\npromoting electrification in the coming decades to mitigate human-caused\\nclimate change. Inadequate understanding of LIB degradation is an important\\nbottleneck that limits battery durability and safety. Here, we propose hybrid\\nphysics-based and data-driven modeling for online diagnosis and prognosis of\\nbattery degradation. Compared to existing battery modeling efforts, we aim to\\nbuild a model with physics as its backbone and statistical learning techniques\\nas enhancements. Such a hybrid model has better generalizability and\\ninterpretability together with a well-calibrated uncertainty associated with\\nits prediction, rendering it more valuable and relevant to safety-critical\\napplications under realistic usage scenarios. Maximum correntropy criterion regression (MCCR) models have been well studied\\nwithin the frame of statistical learning when the scale parameters take fixed\\nvalues or go to infinity. This paper studies the MCCR models with\\ntending-to-zero scale parameters. It is revealed that the optimal learning rate\\nof MCCR models is ${\\\\mathcal{O}}(n^{-1})$ in the asymptotic sense when the\\nsample size $n$ goes to infinity. In the case of finite samples, the\\nperformances on robustness of MCCR, Huber and the least square regression\\nmodels are compared. The applications of these three methods on real data are\\nalso displayed. Merging the two cultures of deep and statistical learning provides insights\\ninto structured high-dimensional data. Traditional statistical modeling is\\nstill a dominant strategy for structured tabular data. Deep learning can be\\nviewed through the lens of generalized linear models (GLMs) with composite link\\nfunctions. Sufficient dimensionality reduction (SDR) and sparsity performs\\nnonlinear feature engineering. We show that prediction, interpolation and\\nuncertainty quantification can be achieved using probabilistic methods at the\\noutput layer of the model. Thus a general framework for machine learning arises\\nthat first generates nonlinear features (a.k.a factors) via sparse\\nregularization and stochastic gradient optimisation and second uses a\\nstochastic output layer for predictive uncertainty. Rather than using shallow\\nadditive architectures as in many statistical models, deep learning uses layers\\nof semi affine input transformations to provide a predictive rule. Applying\\nthese layers of transformations leads to a set of attributes (a.k.a features)\\nto which predictive statistical methods can be applied. Thus we achieve the\\nbest of both worlds: scalability and fast predictive rule construction together\\nwith uncertainty quantification. Sparse regularisation with un-supervised or\\nsupervised learning finds the features. We clarify the duality between shallow\\nand wide models such as PCA, PPR, RRR and deep but skinny architectures such as\\nautoencoders, MLPs, CNN, and LSTM. The connection with data transformations is\\nof practical importance for finding good network architectures. By\\nincorporating probabilistic components at the output level we allow for\\npredictive uncertainty. For interpolation we use deep Gaussian process and ReLU\\ntrees for classification. We provide applications to regression, classification\\nand interpolation. Finally, we conclude with directions for future research. Understanding when and why interpolating methods generalize well has recently\\nbeen a topic of interest in statistical learning theory. However,\\nsystematically connecting interpolating methods to achievable notions of\\noptimality has only received partial attention. In this paper, we investigate\\nthe question of what is the optimal way to interpolate in linear regression\\nusing functions that are linear in the response variable (as the case for the\\nBayes optimal estimator in ridge regression) and depend on the data, the\\npopulation covariance of the data, the signal-to-noise ratio and the covariance\\nof the prior for the signal, but do not depend on the value of the signal\\nitself nor the noise vector in the training data. We provide a closed-form\\nexpression for the interpolator that achieves this notion of optimality and\\nshow that it can be derived as the limit of preconditioned gradient descent\\nwith a specific initialization. We identify a regime where the minimum-norm\\ninterpolator provably generalizes arbitrarily worse than the optimal\\nresponse-linear achievable interpolator that we introduce, and validate with\\nnumerical experiments that the notion of optimality we consider can be achieved\\nby interpolating methods that only use the training data as input in the case\\nof an isotropic prior. Finally, we extend the notion of optimal response-linear\\ninterpolation to random features regression under a linear data-generating\\nmodel that has been previously studied in the literature. Aggregated predictors are obtained by making a set of basic predictors vote\\naccording to some weights, that is, to some probability distribution.\\n  Randomized predictors are obtained by sampling in a set of basic predictors,\\naccording to some prescribed probability distribution.\\n  Thus, aggregated and randomized predictors have in common that they are not\\ndefined by a minimization problem, but by a probability distribution on the set\\nof predictors. In statistical learning theory, there is a set of tools designed\\nto understand the generalization ability of such procedures: PAC-Bayesian or\\nPAC-Bayes bounds.\\n  Since the original PAC-Bayes bounds of D. McAllester, these tools have been\\nconsiderably improved in many directions (we will for example describe a\\nsimplified version of the localization technique of O. Catoni that was missed\\nby the community, and later rediscovered as \\\"mutual information bounds\\\"). Very\\nrecently, PAC-Bayes bounds received a considerable attention: for example there\\nwas workshop on PAC-Bayes at NIPS 2017, \\\"(Almost) 50 Shades of Bayesian\\nLearning: PAC-Bayesian trends and insights\\\", organized by B. Guedj, F. Bach and\\nP. Germain. One of the reason of this recent success is the successful\\napplication of these bounds to neural networks by G. Dziugaite and D. Roy.\\n  An elementary introduction to PAC-Bayes theory is still missing. This is an\\nattempt to provide such an introduction. A knowledge graph (KG) is a data structure which represents entities and\\nrelations as the vertices and edges of a directed graph with edge types. KGs\\nare an important primitive in modern machine learning and artificial\\nintelligence. Embedding-based models, such as the seminal TransE [Bordes et\\nal., 2013] and the recent PairRE [Chao et al., 2020] are among the most popular\\nand successful approaches for representing KGs and inferring missing edges\\n(link completion). Their relative success is often credited in the literature\\nto their ability to learn logical rules between the relations.\\n  In this work, we investigate whether learning rules between relations is\\nindeed what drives the performance of embedding-based methods. We define motif\\nlearning and two alternative mechanisms, network learning (based only on the\\nconnectivity of the KG, ignoring the relation types), and unstructured\\nstatistical learning (ignoring the connectivity of the graph). Using\\nexperiments on synthetic KGs, we show that KG models can learn motifs and how\\nthis ability is degraded by non-motif (noise) edges. We propose tests to\\ndistinguish the contributions of the three mechanisms to performance, and apply\\nthem to popular KG benchmarks. We also discuss an issue with the standard\\nperformance testing protocol and suggest an improvement.\\n  To appear in the proceedings of Complex Networks 2021. Short Term Load Forecast (STLF) is necessary for effective scheduling,\\noperation optimization trading, and decision-making for electricity consumers.\\nModern and efficient machine learning methods are recalled nowadays to manage\\ncomplicated structural big datasets, which are characterized by having a\\nnonlinear temporal dependence structure. We propose different statistical\\nnonlinear models to manage these challenges of hard type datasets and forecast\\n15-min frequency electricity load up to 2-days ahead. We show that the\\nLong-short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) models applied\\nto the production line of a chemical production facility outperform several\\nother predictive models in terms of out-of-sample forecasting accuracy by the\\nDiebold-Mariano (DM) test with several metrics. The predictive information is\\nfundamental for the risk and production management of electricity consumers. Active learning theories and methods have been extensively studied in\\nclassical statistical learning settings. However, deep active learning, i.e.,\\nactive learning with deep learning models, is usually based on empirical\\ncriteria without solid theoretical justification, thus suffering from heavy\\ndoubts when some of those fail to provide benefits in real applications. In\\nthis paper, by exploring the connection between the generalization performance\\nand the training dynamics, we propose a theory-driven deep active learning\\nmethod (dynamicAL) which selects samples to maximize training dynamics. In\\nparticular, we prove that the convergence speed of training and the\\ngeneralization performance are positively correlated under the ultra-wide\\ncondition and show that maximizing the training dynamics leads to better\\ngeneralization performance. Furthermore, to scale up to large deep neural\\nnetworks and data sets, we introduce two relaxations for the subset selection\\nproblem and reduce the time complexity from polynomial to constant. Empirical\\nresults show that dynamicAL not only outperforms the other baselines\\nconsistently but also scales well on large deep learning models. We hope our\\nwork would inspire more attempts on bridging the theoretical findings of deep\\nnetworks and practical impacts of deep active learning in real applications. Compressive learning forms the exciting intersection between compressed\\nsensing and statistical learning where one exploits forms of sparsity and\\nstructure to reduce the memory and/or computational complexity of the learning\\ntask. In this paper, we look at the independent component analysis (ICA) model\\nthrough the compressive learning lens. In particular, we show that solutions to\\nthe cumulant based ICA model have particular structure that induces a low\\ndimensional model set that resides in the cumulant tensor space. By showing a\\nrestricted isometry property holds for random cumulants e.g. Gaussian\\nensembles, we prove the existence of a compressive ICA scheme. Thereafter, we\\npropose two algorithms of the form of an iterative projection gradient (IPG)\\nand an alternating steepest descent (ASD) algorithm for compressive ICA, where\\nthe order of compression asserted from the restricted isometry property is\\nrealised through empirical results. We provide analysis of the CICA algorithms\\nincluding the effects of finite samples. The effects of compression are\\ncharacterised by a trade-off between the sketch size and the statistical\\nefficiency of the ICA estimates. By considering synthetic and real datasets, we\\nshow the substantial memory gains achieved over well-known ICA algorithms by\\nusing one of the proposed CICA algorithms. Finally, we conclude the paper with\\nopen problems including interesting challenges from the emerging field of\\ncompressive learning. We study the problem of synthesizing programs that include machine learning\\ncomponents such as deep neural networks (DNNs). We focus on statistical\\nproperties, which are properties expected to hold with high probability --\\ne.g., that an image classification model correctly identifies people in images\\nwith high probability. We propose novel algorithms for sketching and\\nsynthesizing such programs by leveraging ideas from statistical learning theory\\nto provide statistical soundness guarantees. We evaluate our approach on\\nsynthesizing list processing programs that include DNN components used to\\nprocess image inputs, as well as case studies on image classification and on\\nprecision medicine. Our results demonstrate that our approach can be used to\\nsynthesize programs with probabilistic guarantees. The striking fractal geometry of strange attractors underscores the\\ngenerative nature of chaos: like probability distributions, chaotic systems can\\nbe repeatedly measured to produce arbitrarily-detailed information about the\\nunderlying attractor. Chaotic systems thus pose a unique challenge to modern\\nstatistical learning techniques, while retaining quantifiable mathematical\\nproperties that make them controllable and interpretable as benchmarks. Here,\\nwe present a growing database currently comprising 131 known chaotic dynamical\\nsystems spanning fields such as astrophysics, climatology, and biochemistry.\\nEach system is paired with precomputed multivariate and univariate time series.\\nOur dataset has comparable scale to existing static time series databases;\\nhowever, our systems can be re-integrated to produce additional datasets of\\narbitrary length and granularity. Our dataset is annotated with known\\nmathematical properties of each system, and we perform feature analysis to\\nbroadly categorize the diverse dynamics present across the collection. Chaotic\\nsystems inherently challenge forecasting models, and across extensive\\nbenchmarks we correlate forecasting performance with the degree of chaos\\npresent. We also exploit the unique generative properties of our dataset in\\nseveral proof-of-concept experiments: surrogate transfer learning to improve\\ntime series classification, importance sampling to accelerate model training,\\nand benchmarking symbolic regression algorithms. Contextual multi-armed bandits (CMAB) have been widely used for learning to\\nfilter and prioritize information according to a user's interest. In this work,\\nwe analyze top-K ranking under the CMAB framework where the top-K arms are\\nchosen iteratively to maximize a reward. The context, which represents a set of\\nobservable factors related to the user, is used to increase prediction accuracy\\ncompared to a standard multi-armed bandit. Contextual bandit methods have\\nmostly been studied under strict linearity assumptions, but we drop that\\nassumption and learn non-linear stochastic reward functions with deep neural\\nnetworks. We introduce a novel algorithm called the Deep Upper Confidence Bound\\n(UCB) algorithm. Deep UCB balances exploration and exploitation with a separate\\nneural network to model the learning convergence. We compare the performance of\\nmany bandit algorithms varying K over real-world data sets with\\nhigh-dimensional data and non-linear reward functions. Empirical results show\\nthat the performance of Deep UCB often outperforms though it is sensitive to\\nthe problem and reward setup. Additionally, we prove theoretical regret bounds\\non Deep UCB giving convergence to optimality for the weak class of CMAB\\nproblems. Current deep neural networks are highly overparameterized (up to billions of\\nconnection weights) and nonlinear. Yet they can fit data almost perfectly\\nthrough variants of gradient descent algorithms and achieve unexpected levels\\nof prediction accuracy without overfitting. These are formidable results that\\ndefy predictions of statistical learning and pose conceptual challenges for\\nnon-convex optimization. In this paper, we use methods from statistical physics\\nof disordered systems to analytically study the computational fallout of\\noverparameterization in non-convex binary neural network models, trained on\\ndata generated from a structurally simpler but \\\"hidden\\\" network. As the number\\nof connection weights increases, we follow the changes of the geometrical\\nstructure of different minima of the error loss function and relate them to\\nlearning and generalization performance. A first transition happens at the\\nso-called interpolation point, when solutions begin to exist (perfect fitting\\nbecomes possible). This transition reflects the properties of typical\\nsolutions, which however are in sharp minima and hard to sample. After a gap, a\\nsecond transition occurs, with the discontinuous appearance of a different kind\\nof \\\"atypical\\\" structures: wide regions of the weight space that are\\nparticularly solution-dense and have good generalization properties. The two\\nkinds of solutions coexist, with the typical ones being exponentially more\\nnumerous, but empirically we find that efficient algorithms sample the\\natypical, rare ones. This suggests that the atypical phase transition is the\\nrelevant one for learning. The results of numerical tests with realistic\\nnetworks on observables suggested by the theory are consistent with this\\nscenario. Estimating Kullback Leibler (KL) divergence from samples of two distributions\\nis essential in many machine learning problems. Variational methods using\\nneural network discriminator have been proposed to achieve this task in a\\nscalable manner. However, we noted that most of these methods using neural\\nnetwork discriminators suffer from high fluctuations (variance) in estimates\\nand instability in training. In this paper, we look at this issue from\\nstatistical learning theory and function space complexity perspective to\\nunderstand why this happens and how to solve it. We argue that the cause of\\nthese pathologies is lack of control over the complexity of the neural network\\ndiscriminator function and could be mitigated by controlling it. To achieve\\nthis objective, we 1) present a novel construction of the discriminator in the\\nReproducing Kernel Hilbert Space (RKHS), 2) theoretically relate the error\\nprobability bound of the KL estimates to the complexity of the discriminator in\\nthe RKHS space, 3) present a scalable way to control the complexity (RKHS norm)\\nof the discriminator for a reliable estimation of KL divergence, and 4) prove\\nthe consistency of the proposed estimator. In three different applications of\\nKL divergence : estimation of KL, estimation of mutual information and\\nVariational Bayes, we show that by controlling the complexity as developed in\\nthe theory, we are able to reduce the variance of KL estimates and stabilize\\nthe training Decades of research has studied how language learning infants learn to\\ndiscriminate speech sounds, segment words, and associate words with their\\nmeanings. While gradual development of such capabilities is unquestionable, the\\nexact nature of these skills and the underlying mental representations yet\\nremains unclear. In parallel, computational studies have shown that basic\\ncomprehension of speech can be achieved by statistical learning between speech\\nand concurrent referentially ambiguous visual input. These models can operate\\nwithout prior linguistic knowledge such as representations of linguistic units,\\nand without learning mechanisms specifically targeted at such units. This has\\nraised the question of to what extent knowledge of linguistic units, such as\\nphone(me)s, syllables, and words, could actually emerge as latent\\nrepresentations supporting the translation between speech and representations\\nin other modalities, and without the units being proximal learning targets for\\nthe learner. In this study, we formulate this idea as the so-called latent\\nlanguage hypothesis (LLH), connecting linguistic representation learning to\\ngeneral predictive processing within and across sensory modalities. We review\\nthe extent that the audiovisual aspect of LLH is supported by the existing\\ncomputational studies. We then explore LLH further in extensive learning\\nsimulations with different neural network models for audiovisual\\ncross-situational learning, and comparing learning from both synthetic and real\\nspeech data. We investigate whether the latent representations learned by the\\nnetworks reflect phonetic, syllabic, or lexical structure of input speech by\\nutilizing an array of complementary evaluation metrics related to linguistic\\nselectivity and temporal characteristics of the representations. As a result,\\nwe find that representations associated... Children's health studies support an association between maternal\\nenvironmental exposures and children's birth outcomes. A common goal is to\\nidentify critical windows of susceptibility--periods during gestation with\\nincreased association between maternal exposures and a future outcome. The\\ntiming of the critical windows and magnitude of the associations are likely\\nheterogeneous across different levels of individual, family, and neighborhood\\ncharacteristics. Using an administrative Colorado birth cohort we estimate the\\nindividualized relationship between weekly exposures to fine particulate matter\\n(PM$_{2.5}$) during gestation and birth weight. To achieve this goal, we\\npropose a statistical learning method combining distributed lag models and\\nBayesian additive regression trees to estimate critical windows at the\\nindividual level and identify characteristics that induce heterogeneity from a\\nhigh-dimensional set of potential modifying factors. We find evidence of\\nheterogeneity in the PM$_{2.5}$-birth weight relationship, with some\\nmother-child dyads showing a 3 times larger decrease in birth weight for an IQR\\nincrease in exposure (5.9 to 8.5 $\\\\mu g/m^3$ PM$_{2.5}$) compared to the\\npopulation average. Specifically, we find increased susceptibility for\\nnon-Hispanic mothers who are either younger, have higher body mass index or\\nlower educational attainment. Our case study is the first precision health\\nstudy of critical windows. Random forests are a popular class of algorithms used for regression and\\nclassification. The algorithm introduced by Breiman in 2001 and many of its\\nvariants are ensembles of randomized decision trees built from axis-aligned\\npartitions of the feature space. One such variant, called Mondrian forests, was\\nproposed to handle the online setting and is the first class of random forests\\nfor which minimax rates were obtained in arbitrary dimension. However, the\\nrestriction to axis-aligned splits fails to capture dependencies between\\nfeatures, and random forests that use oblique splits have shown improved\\nempirical performance for many tasks. In this work, we show that a large class\\nof random forests with general split directions also achieve minimax optimal\\nconvergence rates in arbitrary dimension. This class includes STIT forests, a\\ngeneralization of Mondrian forests to arbitrary split directions, as well as\\nrandom forests derived from Poisson hyperplane tessellations. These are the\\nfirst results showing that random forest variants with oblique splits can\\nobtain minimax optimality in arbitrary dimension. Our proof technique relies on\\nthe novel application of the theory of stationary random tessellations in\\nstochastic geometry to statistical learning theory. It is increasingly common to encounter prediction tasks in the biomedical\\nsciences for which multiple datasets are available for model training. Common\\napproaches such as pooling datasets and applying standard statistical learning\\nmethods can result in poor out-of-study prediction performance when datasets\\nare heterogeneous. Theoretical and applied work has shown $\\\\textit{multi-study\\nensembling}$ to be a viable alternative that leverages the variability across\\ndatasets in a manner that promotes model generalizability. Multi-study\\nensembling uses a two-stage $\\\\textit{stacking}$ strategy which fits\\nstudy-specific models and estimates ensemble weights separately. This approach\\nignores, however, the ensemble properties at the model-fitting stage,\\npotentially resulting in a loss of efficiency. We therefore propose\\n$\\\\textit{optimal ensemble construction}$, an $\\\\textit{all-in-one}$ approach to\\nmulti-study stacking whereby we jointly estimate ensemble weights as well as\\nparameters associated with each study-specific model. We prove that limiting\\ncases of our approach yield existing methods such as multi-study stacking and\\npooling datasets before model fitting. We propose an efficient block coordinate\\ndescent algorithm to optimize the proposed loss function. We compare our\\napproach to standard methods by applying it to a multi-country COVID-19 dataset\\nfor baseline mortality prediction. We show that when little data is available\\nfor a country before the onset of the pandemic, leveraging data from other\\ncountries can substantially improve prediction accuracy. Importantly, our\\napproach outperforms multi-study stacking and other standard methods in this\\napplication. We further characterize the method's performance in simulations.\\nOur method remains competitive with or outperforms multi-study stacking and\\nother earlier methods across a range of between-study heterogeneity levels. In this paper, we study the contextual dynamic pricing problem where the\\nmarket value of a product is linear in its observed features plus some market\\nnoise. Products are sold one at a time, and only a binary response indicating\\nsuccess or failure of a sale is observed. Our model setting is similar to\\nJavanmard and Nazerzadeh [2019] except that we expand the demand curve to a\\nsemiparametric model and need to learn dynamically both parametric and\\nnonparametric components. We propose a dynamic statistical learning and\\ndecision-making policy that combines semiparametric estimation from a\\ngeneralized linear model with an unknown link and online decision-making to\\nminimize regret (maximize revenue). Under mild conditions, we show that for a\\nmarket noise c.d.f. $F(\\\\cdot)$ with $m$-th order derivative ($m\\\\geq 2$), our\\npolicy achieves a regret upper bound of $\\\\tilde{O}_{d}(T^{\\\\frac{2m+1}{4m-1}})$,\\nwhere $T$ is time horizon and $\\\\tilde{O}_{d}$ is the order that hides\\nlogarithmic terms and the dimensionality of feature $d$. The upper bound is\\nfurther reduced to $\\\\tilde{O}_{d}(\\\\sqrt{T})$ if $F$ is super smooth whose\\nFourier transform decays exponentially. In terms of dependence on the horizon\\n$T$, these upper bounds are close to $\\\\Omega(\\\\sqrt{T})$, the lower bound where\\n$F$ belongs to a parametric class. We further generalize these results to the\\ncase with dynamically dependent product features under the strong mixing\\ncondition. An interesting observation in artificial neural networks is their favorable\\ngeneralization error despite typically being extremely overparameterized. It is\\nwell known that the classical statistical learning methods often result in\\nvacuous generalization errors in the case of overparameterized neural networks.\\nAdopting the recently developed Neural Tangent (NT) kernel theory, we prove\\nuniform generalization bounds for overparameterized neural networks in kernel\\nregimes, when the true data generating model belongs to the reproducing kernel\\nHilbert space (RKHS) corresponding to the NT kernel. Importantly, our bounds\\ncapture the exact error rates depending on the differentiability of the\\nactivation functions. In order to establish these bounds, we propose the\\ninformation gain of the NT kernel as a measure of complexity of the learning\\nproblem. Our analysis uses a Mercer decomposition of the NT kernel in the basis\\nof spherical harmonics and the decay rate of the corresponding eigenvalues. As\\na byproduct of our results, we show the equivalence between the RKHS\\ncorresponding to the NT kernel and its counterpart corresponding to the\\nMat\\\\'ern family of kernels, showing the NT kernels induce a very general class\\nof models. We further discuss the implications of our analysis for some recent\\nresults on the regret bounds for reinforcement learning and bandit algorithms,\\nwhich use overparameterized neural networks. Statistical learning additions to physically derived mathematical models are\\ngaining traction in the literature. A recent approach has been to augment the\\nunderlying physics of the governing equations with data driven Bayesian\\nstatistical methodology. Coined statFEM, the method acknowledges a priori model\\nmisspecification, by embedding stochastic forcing within the governing\\nequations. Upon receipt of additional data, the posterior distribution of the\\ndiscretised finite element solution is updated using classical Bayesian\\nfiltering techniques. The resultant posterior jointly quantifies uncertainty\\nassociated with the ubiquitous problem of model misspecification and the data\\nintended to represent the true process of interest. Despite this appeal,\\ncomputational scalability is a challenge to statFEM's application to\\nhigh-dimensional problems typically experienced in physical and industrial\\ncontexts. This article overcomes this hurdle by embedding a low-rank\\napproximation of the underlying dense covariance matrix, obtained from the\\nleading order modes of the full-rank alternative. Demonstrated on a series of\\nreaction-diffusion problems of increasing dimension, using experimental and\\nsimulated data, the method reconstructs the sparsely observed data-generating\\nprocesses with minimal loss of information, in both the posterior mean and\\nvariance, paving the way for further integration of physical and probabilistic\\napproaches to complex systems. Difference-of-Convex (DC) minimization, referring to the problem of\\nminimizing the difference of two convex functions, has been found rich\\napplications in statistical learning and studied extensively for decades.\\nHowever, existing methods are primarily based on multi-stage convex relaxation,\\nonly leading to weak optimality of critical points. This paper proposes a\\ncoordinate descent method for minimizing a class of DC functions based on\\nsequential nonconvex approximation. Our approach iteratively solves a nonconvex\\none-dimensional subproblem globally, and it is guaranteed to converge to a\\ncoordinate-wise stationary point. We prove that this new optimality condition\\nis always stronger than the standard critical point condition and directional\\npoint condition under a mild \\\\textit{locally bounded nonconvexity assumption}.\\nFor comparisons, we also include a naive variant of coordinate descent methods\\nbased on sequential convex approximation in our study. When the objective\\nfunction satisfies a \\\\textit{globally bounded nonconvexity assumption} and\\n\\\\textit{Luo-Tseng error bound assumption}, coordinate descent methods achieve\\n\\\\textit{Q-linear} convergence rate. Also, for many applications of interest, we\\nshow that the nonconvex one-dimensional subproblem can be computed exactly and\\nefficiently using a breakpoint searching method. Finally, we have conducted\\nextensive experiments on several statistical learning tasks to show the\\nsuperiority of our approach.\\n  Keywords: Coordinate Descent, DC Minimization, DC Programming,\\nDifference-of-Convex Programs, Nonconvex Optimization, Sparse Optimization,\\nBinary Optimization. In practice, and especially when training deep neural networks, visual\\nrecognition rules are often learned based on various sources of information. On\\nthe other hand, the recent deployment of facial recognition systems with uneven\\nperformances on different population segments has highlighted the\\nrepresentativeness issues induced by a naive aggregation of the datasets. In\\nthis paper, we show how biasing models can remedy these problems. Based on the\\n(approximate) knowledge of the biasing mechanisms at work, our approach\\nconsists in reweighting the observations, so as to form a nearly debiased\\nestimator of the target distribution. One key condition is that the supports of\\nthe biased distributions must partly overlap, and cover the support of the\\ntarget distribution. In order to meet this requirement in practice, we propose\\nto use a low dimensional image representation, shared across the image\\ndatabases. Finally, we provide numerical experiments highlighting the relevance\\nof our approach. The rapid recent progress in machine learning (ML) has raised a number of\\nscientific questions that challenge the longstanding dogma of the field. One of\\nthe most important riddles is the good empirical generalization of\\noverparameterized models. Overparameterized models are excessively complex with\\nrespect to the size of the training dataset, which results in them perfectly\\nfitting (i.e., interpolating) the training data, which is usually noisy. Such\\ninterpolation of noisy data is traditionally associated with detrimental\\noverfitting, and yet a wide range of interpolating models -- from simple linear\\nmodels to deep neural networks -- have recently been observed to generalize\\nextremely well on fresh test data. Indeed, the recently discovered double\\ndescent phenomenon has revealed that highly overparameterized models often\\nimprove over the best underparameterized model in test performance.\\n  Understanding learning in this overparameterized regime requires new theory\\nand foundational empirical studies, even for the simplest case of the linear\\nmodel. The underpinnings of this understanding have been laid in very recent\\nanalyses of overparameterized linear regression and related statistical\\nlearning tasks, which resulted in precise analytic characterizations of double\\ndescent. This paper provides a succinct overview of this emerging theory of\\noverparameterized ML (henceforth abbreviated as TOPML) that explains these\\nrecent findings through a statistical signal processing perspective. We\\nemphasize the unique aspects that define the TOPML research area as a subfield\\nof modern ML theory and outline interesting open questions that remain. Embedding approaches have become one of the most pervasive techniques for\\nmulti-label classification. However, the training process of embedding methods\\nusually involves a complex quadratic or semidefinite programming problem, or\\nthe model may even involve an NP-hard problem. Thus, such methods are\\nprohibitive on large-scale applications. More importantly, much of the\\nliterature has already shown that the binary relevance (BR) method is usually\\ngood enough for some applications. Unfortunately, BR runs slowly due to its\\nlinear dependence on the size of the input data. The goal of this paper is to\\nprovide a simple method, yet with provable guarantees, which can achieve\\ncompetitive performance without a complex training process. To achieve our\\ngoal, we provide a simple stochastic sketch strategy for multi-label\\nclassification and present theoretical results from both algorithmic and\\nstatistical learning perspectives. Our comprehensive empirical studies\\ncorroborate our theoretical findings and demonstrate the superiority of the\\nproposed methods. This monograph develops a comprehensive statistical learning framework that\\nis robust to (distributional) perturbations in the data using Distributionally\\nRobust Optimization (DRO) under the Wasserstein metric. Beginning with\\nfundamental properties of the Wasserstein metric and the DRO formulation, we\\nexplore duality to arrive at tractable formulations and develop finite-sample,\\nas well as asymptotic, performance guarantees. We consider a series of learning\\nproblems, including (i) distributionally robust linear regression; (ii)\\ndistributionally robust regression with group structure in the predictors;\\n(iii) distributionally robust multi-output regression and multiclass\\nclassification, (iv) optimal decision making that combines distributionally\\nrobust regression with nearest-neighbor estimation; (v) distributionally robust\\nsemi-supervised learning, and (vi) distributionally robust reinforcement\\nlearning. A tractable DRO relaxation for each problem is being derived,\\nestablishing a connection between robustness and regularization, and obtaining\\nbounds on the prediction and estimation errors of the solution. Beyond theory,\\nwe include numerical experiments and case studies using synthetic and real\\ndata. The real data experiments are all associated with various health\\ninformatics problems, an application area which provided the initial impetus\\nfor this work. Data is expanding at an unimaginable rate, and with this development comes\\nthe responsibility of the quality of data. Data Quality refers to the relevance\\nof the information present and helps in various operations like decision making\\nand planning in a particular organization. Mostly data quality is measured on\\nan ad-hoc basis, and hence none of the developed concepts provide any practical\\napplication. The current empirical study was undertaken to formulate a concrete\\nautomated data quality platform to assess the quality of incoming dataset and\\ngenerate a quality label, score and comprehensive report. We utilize various\\ndatasets from healthdata.gov, opendata.nhs and Demographics and Health Surveys\\n(DHS) Program to observe the variations in the quality score and formulate a\\nlabel using Principal Component Analysis(PCA). The results of the current\\nempirical study revealed a metric that encompasses nine quality ingredients,\\nnamely provenance, dataset characteristics, uniformity, metadata coupling,\\npercentage of missing cells and duplicate rows, skewness of data, the ratio of\\ninconsistencies of categorical columns, and correlation between these\\nattributes. The study also provides an illustrative case study and validation\\nof the metric following Mutation Testing approaches. This research study\\nprovides an automated platform which takes an incoming dataset and metadata to\\nprovide the DQ score, report and label. The results of this study would be\\nuseful to data scientists as the value of this quality label would instill\\nconfidence before deploying the data for his/her respective practical\\napplication. Models with dominant advection always posed a difficult challenge for\\nprojection-based reduced order modelling. Many methodologies that have recently\\nbeen proposed are based on the pre-processing of the full-order solutions to\\naccelerate the Kolmogorov N-width decay thereby obtaining smaller linear\\nsubspaces with improved accuracy. These methods however must rely on the\\nknowledge of the characteristic speeds in phase space of the solution, limiting\\ntheir range of applicability to problems with explicit functional form for the\\nadvection field. In this work we approach the problem of automatically\\ndetecting the correct pre-processing transformation in a statistical learning\\nframework by implementing a deep-learning architecture. The purely data-driven\\nmethod allowed us to generalise the existing approaches of linear subspace\\nmanipulation to non-linear hyperbolic problems with unknown advection fields.\\nThe proposed algorithm has been validated against simple test cases to\\nbenchmark its performances and later successfully applied to a multiphase\\nsimulation. Lung X-ray images, if processed using statistical and computational methods,\\ncan distinguish pneumonia from COVID-19. The present work shows that it is\\npossible to extract lung X-ray characteristics to improve the methods of\\nexamining and diagnosing patients with suspected COVID-19, distinguishing them\\nfrom malaria, dengue, H1N1, tuberculosis, and Streptococcus pneumonia. More\\nprecisely, an intelligent computational model was developed to process lung\\nX-ray images and classify whether the image is of a patient with COVID-19. The\\nimages were processed and extracted their characteristics. These\\ncharacteristics were the input data for an unsupervised statistical learning\\nmethod, PCA, and clustering, which identified specific attributes of X-ray\\nimages with Covid-19. The introduction of statistical models allowed a fast\\nalgorithm, which used the X-means clustering method associated with the\\nBayesian Information Criterion (CIB). The developed algorithm efficiently\\ndistinguished each pulmonary pathology from X-ray images. The method exhibited\\nexcellent sensitivity. The average recognition accuracy of COVID-19 was 0.93\\nand 0.051. We investigate the functioning of a classifying biological neural network\\nfrom the perspective of statistical learning theory, modelled, in a simplified\\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\\nidentity activation function. In the purely stochastic (robust) regime, we give\\na generalisation error bound that holds with high probability, thus showing\\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\\nRNNs retain a partial signature of the paths they are fed as the unique\\ninformation exploited for training and classification tasks. We argue that\\nthese RNNs are easy to train and robust and back these observations with\\nnumerical experiments on both synthetic and real data. We also exhibit a\\ntrade-off phenomenon between accuracy and robustness. We develop a statistical method to learn a molecular Hamiltonian matrix from\\na time-series of electron density matrices. We extend our previous method to\\nlarger molecular systems by incorporating physical properties to reduce\\ndimensionality, while also exploiting regularization techniques like ridge\\nregression for addressing multicollinearity. With the learned Hamiltonian we\\ncan solve the Time-Dependent Hartree-Fock (TDHF) equation to propagate the\\nelectron density in time, and predict its dynamics for field-free and field-on\\nscenarios. We observe close quantitative agreement between the predicted\\ndynamics and ground truth for both field-off trajectories similar to the\\ntraining data, and field-on trajectories outside of the training data. The smooth 1-Wasserstein distance (SWD) $W_1^\\\\sigma$ was recently proposed as\\na means to mitigate the curse of dimensionality in empirical approximation\\nwhile preserving the Wasserstein structure. Indeed, SWD exhibits parametric\\nconvergence rates and inherits the metric and topological structure of the\\nclassic Wasserstein distance. Motivated by the above, this work conducts a\\nthorough statistical study of the SWD, including a high-dimensional limit\\ndistribution result for empirical $W_1^\\\\sigma$, bootstrap consistency,\\nconcentration inequalities, and Berry-Esseen type bounds. The derived\\nnondegenerate limit stands in sharp contrast with the classic empirical $W_1$,\\nfor which a similar result is known only in the one-dimensional case. We also\\nexplore asymptotics and characterize the limit distribution when the smoothing\\nparameter $\\\\sigma$ is scaled with $n$, converging to $0$ at a sufficiently slow\\nrate. The dimensionality of the sampled distribution enters empirical SWD\\nconvergence bounds only through the prefactor (i.e., the constant). We provide\\na sharp characterization of this prefactor's dependence on the smoothing\\nparameter and the intrinsic dimension. This result is then used to derive new\\nempirical convergence rates for classic $W_1$ in terms of the intrinsic\\ndimension. As applications of the limit distribution theory, we study\\ntwo-sample testing and minimum distance estimation (MDE) under $W_1^\\\\sigma$. We\\nestablish asymptotic validity of SWD testing, while for MDE, we prove\\nmeasurability, almost sure convergence, and limit distributions for optimal\\nestimators and their corresponding $W_1^\\\\sigma$ error. Our results suggest that\\nthe SWD is well suited for high-dimensional statistical learning and inference. Prior beliefs are central to Bayesian accounts of cognition, but many of\\nthese accounts do not directly measure priors. More specifically, initial\\nstates of belief heavily influence how new information is assumed to be\\nutilized when updating a particular model. Despite this, prior and posterior\\nbeliefs are either inferred from sequential participant actions or elicited\\nthrough impoverished means. We had participants play a version of the game\\n\\\"Plinko\\\", to first elicit individual participant priors in a theoretically\\nagnostic manner. Subsequent learning and updating of participant beliefs was\\nthen directly measured. We show that participants hold a variety of priors that\\ncluster around prototypical probability distributions that in turn influence\\nlearning. In follow-up experiments we show that participant priors are stable\\nover time and that the ability to update beliefs is influenced by a simple\\nenvironmental manipulation (i.e. a short break). This data reveals the\\nimportance of directly measuring participant beliefs rather than assuming or\\ninferring them as has been widely done in the literature to date. The Plinko\\ngame provides a flexible and fecund means for examining statistical learning\\nand mental model updating. The US Census Bureau will deliberately corrupt data sets derived from the\\n2020 US Census in an effort to maintain privacy, suggesting a painful trade-off\\nbetween the privacy of respondents and the precision of economic analysis. To\\ninvestigate whether this trade-off is inevitable, we formulate a semiparametric\\nmodel of causal inference with high dimensional corrupted data. We propose a\\nprocedure for data cleaning, estimation, and inference with data\\ncleaning-adjusted confidence intervals. We prove consistency, Gaussian\\napproximation, and semiparametric efficiency by finite sample arguments, with a\\nrate of $n^{-1/2}$ for semiparametric estimands that degrades gracefully for\\nnonparametric estimands. Our key assumption is that the true covariates are\\napproximately low rank, which we interpret as approximate repeated measurements\\nand validate in the Census. In our analysis, we provide nonasymptotic\\ntheoretical contributions to matrix completion, statistical learning, and\\nsemiparametric statistics. Calibrated simulations verify the coverage of our\\ndata cleaning-adjusted confidence intervals and demonstrate the relevance of\\nour results for 2020 Census data. A recurring theme in statistical learning, online learning, and beyond is\\nthat faster convergence rates are possible for problems with low noise, often\\nquantified by the performance of the best hypothesis; such results are known as\\nfirst-order or small-loss guarantees. While first-order guarantees are\\nrelatively well understood in statistical and online learning, adapting to low\\nnoise in contextual bandits (and more broadly, decision making) presents major\\nalgorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy,\\nLangford, Luo, and Schapire asked whether first-order guarantees are even\\npossible for contextual bandits and -- if so -- whether they can be attained by\\nefficient algorithms. We give a resolution to this question by providing an\\noptimal and efficient reduction from contextual bandits to online regression\\nwith the logarithmic (or, cross-entropy) loss. Our algorithm is simple and\\npractical, readily accommodates rich function classes, and requires no\\ndistributional assumptions beyond realizability. In a large-scale empirical\\nevaluation, we find that our approach typically outperforms comparable\\nnon-first-order methods.\\n  On the technical side, we show that the logarithmic loss and an\\ninformation-theoretic quantity called the triangular discrimination play a\\nfundamental role in obtaining first-order guarantees, and we combine this\\nobservation with new refinements to the regression oracle reduction framework\\nof Foster and Rakhlin. The use of triangular discrimination yields novel\\nresults even for the classical statistical learning model, and we anticipate\\nthat it will find broader use. Multi-modal distributions are commonly used to model clustered data in\\nstatistical learning tasks. In this paper, we consider the Mixed Linear\\nRegression (MLR) problem. We propose an optimal transport-based framework for\\nMLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the\\nWasserstein distance between the learned and target mixture regression models.\\nThrough a model-based duality analysis, WMLR reduces the underlying MLR task to\\na nonconvex-concave minimax optimization problem, which can be provably solved\\nto find a minimax stationary point by the Gradient Descent Ascent (GDA)\\nalgorithm. In the special case of mixtures of two linear regression models, we\\nshow that WMLR enjoys global convergence and generalization guarantees. We\\nprove that WMLR's sample complexity grows linearly with the dimension of data.\\nFinally, we discuss the application of WMLR to the federated learning task\\nwhere the training samples are collected by multiple agents in a network.\\nUnlike the Expectation Maximization algorithm, WMLR directly extends to the\\ndistributed, federated learning setting. We support our theoretical results\\nthrough several numerical experiments, which highlight our framework's ability\\nto handle the federated learning setting with mixture models. Modern multivariate machine learning and statistical methodologies estimate\\nparameters of interest while leveraging prior knowledge of the association\\nbetween outcome variables. The methods that do allow for estimation of\\nrelationships do so typically through an error covariance matrix in\\nmultivariate regression which does not scale to other types of models. In this\\narticle we proposed the MinPEN framework to simultaneously estimate regression\\ncoefficients associated with the multivariate regression model and the\\nrelationships between outcome variables using mild assumptions. The MinPen\\nframework utilizes a novel penalty based on the minimum function to exploit\\ndetected relationships between responses. An iterative algorithm that\\ngeneralizes current state of the art methods is proposed as a solution to the\\nnon-convex optimization that is required to obtain estimates. Theoretical\\nresults such as high dimensional convergence rates, model selection\\nconsistency, and a framework for post selection inference are provided. We\\nextend the proposed MinPen framework to other exponential family loss\\nfunctions, with a specific focus on multiple binomial responses. Tuning\\nparameter selection is also addressed. Finally, simulations and two data\\nexamples are presented to show the finite sample properties of this framework. Understanding generalization in deep learning has been one of the major\\nchallenges in statistical learning theory over the last decade. While recent\\nwork has illustrated that the dataset and the training algorithm must be taken\\ninto account in order to obtain meaningful generalization bounds, it is still\\ntheoretically not clear which properties of the data and the algorithm\\ndetermine the generalization performance. In this study, we approach this\\nproblem from a dynamical systems theory perspective and represent stochastic\\noptimization algorithms as random iterated function systems (IFS). Well studied\\nin the dynamical systems literature, under mild assumptions, such IFSs can be\\nshown to be ergodic with an invariant measure that is often supported on sets\\nwith a fractal structure. As our main contribution, we prove that the\\ngeneralization error of a stochastic optimization algorithm can be bounded\\nbased on the `complexity' of the fractal structure that underlies its invariant\\nmeasure. Leveraging results from dynamical systems theory, we show that the\\ngeneralization error can be explicitly linked to the choice of the algorithm\\n(e.g., stochastic gradient descent -- SGD), algorithm hyperparameters (e.g.,\\nstep-size, batch-size), and the geometry of the problem (e.g., Hessian of the\\nloss). We further specialize our results to specific problems (e.g.,\\nlinear/logistic regression, one hidden-layered neural networks) and algorithms\\n(e.g., SGD and preconditioned variants), and obtain analytical estimates for\\nour bound.For modern neural networks, we develop an efficient algorithm to\\ncompute the developed bound and support our theory with various experiments on\\nneural networks. Boosting methods are widely used in statistical learning to deal with\\nhigh-dimensional data due to their variable selection feature. However, those\\nmethods lack straightforward ways to construct estimators for the precision of\\nthe parameters such as variance or confidence interval, which can be achieved\\nby conventional statistical methods like Bayesian inference. In this paper, we\\npropose a new inference method \\\"BayesBoost\\\" that combines boosting and Bayesian\\nfor linear mixed models to make the uncertainty estimation for the random\\neffects possible on the one hand. On the other hand, the new method overcomes\\nthe shortcomings of Bayesian inference in giving precise and unambiguous\\nguidelines for the selection of covariates by benefiting from boosting\\ntechniques. The implementation of Bayesian inference leads to the randomness of\\nmodel selection criteria like the conditional AIC (cAIC), so we also propose a\\ncAIC-based model selection criteria that focus on the stabilized regions\\ninstead of the global minimum. The effectiveness of the new approach can be\\nobserved via simulation and in a data example from the field of neurophysiology\\nfocussing on the mechanisms in the brain while listening to unpleasant sounds. We present a novel offline-online method to mitigate the computational burden\\nof the characterization of posterior random variables in statistical learning.\\nIn the offline phase, the proposed method learns the joint law of the parameter\\nrandom variables and the observable random variables in the tensor-train (TT)\\nformat. In the online phase, the resulting order-preserving conditional\\ntransport can characterize the posterior random variables given newly observed\\ndata in real time. Compared with the state-of-the-art normalizing flow\\ntechniques, the proposed method relies on function approximation and is\\nequipped with a thorough performance analysis. The function approximation\\nperspective also allows us to further extend the capability of transport maps\\nin challenging problems with high-dimensional observations and high-dimensional\\nparameters. On the one hand, we present novel heuristics to reorder and/or\\nreparametrize the variables to enhance the approximation power of TT. On the\\nother hand, we integrate the TT-based transport maps and the parameter\\nreordering/reparametrization into layered compositions to further improve the\\nperformance of the resulting transport maps. We demonstrate the efficiency of\\nthe proposed method on various statistical learning tasks in ordinary\\ndifferential equations (ODEs) and partial differential equations (PDEs). A large body of recent work has begun to explore the potential of\\nparametrized quantum circuits (PQCs) as machine learning models, within the\\nframework of hybrid quantum-classical optimization. In particular, theoretical\\nguarantees on the out-of-sample performance of such models, in terms of\\ngeneralization bounds, have emerged. However, none of these generalization\\nbounds depend explicitly on how the classical input data is encoded into the\\nPQC. We derive generalization bounds for PQC-based models that depend\\nexplicitly on the strategy used for data-encoding. These imply bounds on the\\nperformance of trained PQC-based models on unseen data. Moreover, our results\\nfacilitate the selection of optimal data-encoding strategies via structural\\nrisk minimization, a mathematically rigorous framework for model selection. We\\nobtain our generalization bounds by bounding the complexity of PQC-based models\\nas measured by the Rademacher complexity and the metric entropy, two complexity\\nmeasures from statistical learning theory. To achieve this, we rely on a\\nrepresentation of PQC-based models via trigonometric functions. Our\\ngeneralization bounds emphasize the importance of well-considered data-encoding\\nstrategies for PQC-based models. Generalised linear models for multi-class classification problems are one of\\nthe fundamental building blocks of modern machine learning tasks. In this\\nmanuscript, we characterise the learning of a mixture of $K$ Gaussians with\\ngeneric means and covariances via empirical risk minimisation (ERM) with any\\nconvex loss and regularisation. In particular, we prove exact asymptotics\\ncharacterising the ERM estimator in high-dimensions, extending several previous\\nresults about Gaussian mixture classification in the literature. We exemplify\\nour result in two tasks of interest in statistical learning: a) classification\\nfor a mixture with sparse means, where we study the efficiency of $\\\\ell_1$\\npenalty with respect to $\\\\ell_2$; b) max-margin multi-class classification,\\nwhere we characterise the phase transition on the existence of the multi-class\\nlogistic maximum likelihood estimator for $K>2$. Finally, we discuss how our\\ntheory can be applied beyond the scope of synthetic data, showing that in\\ndifferent cases Gaussian mixtures capture closely the learning curve of\\nclassification tasks in real data sets. Background modeling and subtraction is a promising research area with a\\nvariety of applications for video surveillance. Recent years have witnessed a\\nproliferation of effective learning-based deep neural networks in this area.\\nHowever, the techniques have only provided limited descriptions of scenes'\\nproperties while requiring heavy computations, as their single-valued mapping\\nfunctions are learned to approximate the temporal conditional averages of\\nobserved target backgrounds and foregrounds. On the other hand, statistical\\nlearning in imagery domains has been a prevalent approach with high adaptation\\nto dynamic context transformation, notably using Gaussian Mixture Models (GMM)\\nwith its generalization capabilities. By leveraging both, we propose a novel\\nmethod called CDN-MEDAL-net for background modeling and subtraction with two\\nconvolutional neural networks. The first architecture, CDN-GM, is grounded on\\nan unsupervised GMM statistical learning strategy to describe observed scenes'\\nsalient features. The second one, MEDAL-net, implements a light-weighted\\npipeline of online video background subtraction. Our two-stage architecture is\\nsmall, but it is very effective with rapid convergence to representations of\\nintricate motion patterns. Our experiments show that the proposed approach is\\nnot only capable of effectively extracting regions of moving objects in unseen\\ncases, but it is also very efficient. We investigate the problem of active learning in the streaming setting in\\nnon-parametric regimes, where the labels are stochastically generated from a\\nclass of functions on which we make no assumptions whatsoever. We rely on\\nrecently proposed Neural Tangent Kernel (NTK) approximation tools to construct\\na suitable neural embedding that determines the feature space the algorithm\\noperates on and the learned model computed atop. Since the shape of the label\\nrequesting threshold is tightly related to the complexity of the function to be\\nlearned, which is a-priori unknown, we also derive a version of the algorithm\\nwhich is agnostic to any prior knowledge. This algorithm relies on a regret\\nbalancing scheme to solve the resulting online model selection problem, and is\\ncomputationally efficient. We prove joint guarantees on the cumulative regret\\nand number of requested labels which depend on the complexity of the labeling\\nfunction at hand. In the linear case, these guarantees recover known minimax\\nresults of the generalization error as a function of the label complexity in a\\nstandard statistical learning setting. This paper argues that continual learning methods can benefit by splitting\\nthe capacity of the learner across multiple models. We use statistical learning\\ntheory and experimental analysis to show how multiple tasks can interact with\\neach other in a non-trivial fashion when a single model is trained on them. The\\ngeneralization error on a particular task can improve when it is trained with\\nsynergistic tasks, but can also deteriorate when trained with competing tasks.\\nThis theory motivates our method named Model Zoo which, inspired from the\\nboosting literature, grows an ensemble of small models, each of which is\\ntrained during one episode of continual learning. We demonstrate that Model Zoo\\nobtains large gains in accuracy on a variety of continual learning benchmark\\nproblems. Code is available at\\nhttps://github.com/grasp-lyrl/modelzoo_continual. Unlike the conventional wisdom in statistical learning theory, the test error\\nof a deep neural network (DNN) often demonstrates double descent: as the model\\ncomplexity increases, it first follows a classical U-shaped curve and then\\nshows a second descent. Through bias-variance decomposition, recent studies\\nrevealed that the bell-shaped variance is the major cause of model-wise double\\ndescent (when the DNN is widened gradually). This paper investigates epoch-wise\\ndouble descent, i.e., the test error of a DNN also shows double descent as the\\nnumber of training epoches increases. By extending the bias-variance analysis\\nto epoch-wise double descent of the zero-one loss, we surprisingly find that\\nthe variance itself, without the bias, varies consistently with the test error.\\nInspired by this result, we propose a novel metric, optimization variance (OV),\\nto measure the diversity of model updates caused by the stochastic gradients of\\nrandom training batches drawn in the same iteration. OV can be estimated using\\nsamples from the training set only but correlates well with the (unknown)\\n\\\\emph{test} error, and hence early stopping may be achieved without using a\\nvalidation set. To understand better good generalization performance in state-of-the-art\\nneural network (NN) models, and in particular the success of the ALPHAHAT\\nmetric based on Heavy-Tailed Self-Regularization (HT-SR) theory, we analyze of\\na corpus of models that was made publicly-available for a contest to predict\\nthe generalization accuracy of NNs. These models include a wide range of\\nqualities and were trained with a range of architectures and regularization\\nhyperparameters. We break ALPHAHAT into its two subcomponent metrics: a\\nscale-based metric; and a shape-based metric. We identify what amounts to a\\nSimpson's paradox: where \\\"scale\\\" metrics (from traditional statistical learning\\ntheory) perform well in aggregate, but can perform poorly on subpartitions of\\nthe data of a given depth, when regularization hyperparameters are varied; and\\nwhere \\\"shape\\\" metrics (from HT-SR theory) perform well on each subpartition of\\nthe data, when hyperparameters are varied for models of a given depth, but can\\nperform poorly overall when models with varying depths are aggregated. Our\\nresults highlight the subtlety of comparing models when both architectures and\\nhyperparameters are varied; the complementary role of implicit scale versus\\nimplicit shape parameters in understanding NN model quality; and the need to go\\nbeyond one-size-fits-all metrics based on upper bounds from generalization\\ntheory to describe the performance of NN models. Our results also clarify\\nfurther why the ALPHAHAT metric from HT-SR theory works so well at predicting\\ngeneralization across a broad range of CV and NLP models. The availability of large datasets composed of graphs creates an\\nunprecedented need to invent novel tools in statistical learning for\\ngraph-valued random variables. To characterize the average of a sample of\\ngraphs, one can compute the sample Frechet mean and median graphs. In this\\npaper, we address the following foundational question: does a mean or median\\ngraph inherit the structural properties of the graphs in the sample? An\\nimportant graph property is the edge density; we establish that edge density is\\nan hereditary property, which can be transmitted from a graph sample to its\\nsample Frechet mean or median graphs, irrespective of the method used to\\nestimate the mean or the median. Because of the prominence of the Frechet mean\\nin graph-valued machine learning, this novel theoretical result has some\\nsignificant practical consequences. Offset Rademacher complexities have been shown to provide tight upper bounds\\nfor the square loss in a broad class of problems including improper statistical\\nlearning and online learning. We show that the offset complexity can be\\ngeneralized to any loss that satisfies a certain general convexity condition.\\nFurther, we show that this condition is closely related to both exponential\\nconcavity and self-concordance, unifying apparently disparate results. By a\\nnovel geometric argument, many of our bounds translate to improper learning in\\na non-convex class with Audibert's star algorithm. Thus, the offset complexity\\nprovides a versatile analytic tool that covers both convex empirical risk\\nminimization and improper learning under entropy conditions. Applying the\\nmethod, we recover the optimal rates for proper and improper learning with the\\n$p$-loss for $1 < p < \\\\infty$, and show that improper variants of empirical\\nrisk minimization can attain fast rates for logistic regression and other\\ngeneralized linear models. We review the role of information and learning in the stability and\\noptimization of queueing systems. In recent years, techniques from supervised\\nlearning, bandit learning and reinforcement learning have been applied to\\nqueueing systems supported by increasing role of information in decision\\nmaking. We present observations and new results that help rationalize the\\napplication of these areas to queueing systems.\\n  We prove that the MaxWeight and BackPressure policies are an application of\\nBlackwell's Approachability Theorem. This connects queueing theoretic results\\nwith adversarial learning. We then discuss the requirements of statistical\\nlearning for service parameter estimation. As an example, we show how queue\\nsize regret can be bounded when applying a perceptron algorithm to classify\\nservice. Next, we discuss the role of state information in improved decision\\nmaking. Here we contrast the roles of epistemic information (information on\\nuncertain parameters) and aleatoric information (information on an uncertain\\nstate). Finally we review recent advances in the theory of reinforcement\\nlearning and queueing, as well as, provide discussion on current research\\nchallenges. Between the years 2015 and 2019, members of the Horizon 2020-funded\\nInnovative Training Network named \\\"AMVA4NewPhysics\\\" studied the customization\\nand application of advanced multivariate analysis methods and statistical\\nlearning tools to high-energy physics problems, as well as developed entirely\\nnew ones. Many of those methods were successfully used to improve the\\nsensitivity of data analyses performed by the ATLAS and CMS experiments at the\\nCERN Large Hadron Collider; several others, still in the testing phase, promise\\nto further improve the precision of measurements of fundamental physics\\nparameters and the reach of searches for new phenomena. In this paper, the most\\nrelevant new tools, among those studied and developed, are presented along with\\nthe evaluation of their performances. Space debris is a major problem in space exploration. International bodies\\ncontinuously monitor a large database of orbiting objects and emit warnings in\\nthe form of conjunction data messages. An important question for satellite\\noperators is to estimate when fresh information will arrive so that they can\\nreact timely but sparingly with satellite maneuvers. We propose a statistical\\nlearning model of the message arrival process, allowing us to answer two\\nimportant questions: (1) Will there be any new message in the next specified\\ntime interval? (2) When exactly and with what uncertainty will the next message\\narrive? The average prediction error for question (2) of our Bayesian Poisson\\nprocess model is smaller than the baseline in more than 4 hours in a test set\\nof 50k close encounter events. This paper applies statistical learning techniques to an observational\\nQuantified-Self (QS) study to build a descriptive model of sleep quality. A\\ntotal of 472 days of my sleep data was collected with an Oura ring and combined\\nwith lifestyle, environmental, and psychological data. Such n-of-1 QS projects\\npose a number of challenges: heterogeneous data sources; missing values; high\\ndimensionality; dynamic feedback loops; human biases. This paper directly\\naddresses these challenges with an end-to-end QS pipeline that produces robust\\ndescriptive models. Sleep quality is one of the most difficult modelling\\ntargets in QS research, due to high noise and a large number of\\nweakly-contributing factors. Sleep quality was selected so that approaches from\\nthis paper would generalise to most other n-of-1 QS projects. Techniques are\\npresented for combining and engineering features for the different classes of\\ndata types, sample frequencies, and schema - including event logs, weather, and\\ngeo-spatial data. Statistical analyses for outliers, normality,\\n(auto)correlation, stationarity, and missing data are detailed, along with a\\nproposed method for hierarchical clustering to identify correlated groups of\\nfeatures. The missing data was overcome using a combination of knowledge-based\\nand statistical techniques, including several multivariate imputation\\nalgorithms. \\\"Markov unfolding\\\" is presented for collapsing the time series into\\na collection of independent observations, whilst incorporating historical\\ninformation. The final model was interpreted in two ways: by inspecting the\\ninternal $\\\\beta$-parameters, and using the SHAP framework. These two\\ninterpretation techniques were combined to produce a list of the 16\\nmost-predictive features, demonstrating that an observational study can greatly\\nnarrow down the number of features that need to be considered when designing\\ninterventional QS studies. In this dissertation, we study the intersection of quantum computing and\\nsupervised machine learning algorithms, which means that we investigate quantum\\nalgorithms for supervised machine learning that operate on classical data. This\\narea of research falls under the umbrella of quantum machine learning, a\\nresearch area of computer science which has recently received wide attention.\\nIn particular, we investigate to what extent quantum computers can be used to\\naccelerate supervised machine learning algorithms. The aim of this is to\\ndevelop a clear understanding of the promises and limitations of the current\\nstate of the art of quantum algorithms for supervised machine learning, but\\nalso to define directions for future research in this exciting field. We start\\nby looking at supervised quantum machine learning (QML) algorithms through the\\nlens of statistical learning theory. In this framework, we derive novel bounds\\non the computational complexities of a large set of supervised QML algorithms\\nunder the requirement of optimal learning rates. Next, we give a new bound for\\nHamiltonian simulation of dense Hamiltonians, a major subroutine of most known\\nsupervised QML algorithms, and then derive a classical algorithm with nearly\\nthe same complexity. We then draw the parallels to recent \\\"quantum-inspired\\\"\\nresults, and will explain the implications of these results for quantum machine\\nlearning applications. Looking for areas which might bear larger advantages for\\nQML algorithms, we finally propose a novel algorithm for Quantum Boltzmann\\nmachines, and argue that quantum algorithms for quantum data are one of the\\nmost promising applications for QML with potentially exponential advantage over\\nclassical approaches. This paper focuses on understanding how the generalization error scales with\\nthe amount of the training data for deep neural networks (DNNs). Existing\\ntechniques in statistical learning require computation of capacity measures,\\nsuch as VC dimension, to provably bound this error. It is however unclear how\\nto extend these measures to DNNs and therefore the existing analyses are\\napplicable to simple neural networks, which are not used in practice, e.g.,\\nlinear or shallow ones or otherwise multi-layer perceptrons. Moreover, many\\ntheoretical error bounds are not empirically verifiable. We derive estimates of\\nthe generalization error that hold for deep networks and do not rely on\\nunattainable capacity measures. The enabling technique in our approach hinges\\non two major assumptions: i) the network achieves zero training error, ii) the\\nprobability of making an error on a test point is proportional to the distance\\nbetween this point and its nearest training point in the feature space and at a\\ncertain maximal distance (that we call radius) it saturates. Based on these\\nassumptions we estimate the generalization error of DNNs. The obtained estimate\\nscales as O(1/(\\\\delta N^{1/d})), where N is the size of the training data and\\nis parameterized by two quantities, the effective dimensionality of the data as\\nperceived by the network (d) and the aforementioned radius (\\\\delta), both of\\nwhich we find empirically. We show that our estimates match with the\\nexperimentally obtained behavior of the error on multiple learning tasks using\\nbenchmark data-sets and realistic models. Estimating training data requirements\\nis essential for deployment of safety critical applications such as autonomous\\ndriving etc. Furthermore, collecting and annotating training data requires a\\nhuge amount of financial, computational and human resources. Our empirical\\nestimates will help to efficiently allocate resources. Real-world data typically contain a large number of features that are often\\nheterogeneous in nature, relevance, and also units of measure. When assessing\\nthe similarity between data points, one can build various distance measures\\nusing subsets of these features. Using the fewest features but still retaining\\nsufficient information about the system is crucial in many statistical learning\\napproaches, particularly when data are sparse. We introduce a statistical test\\nthat can assess the relative information retained when using two different\\ndistance measures, and determine if they are equivalent, independent, or if one\\nis more informative than the other. This in turn allows finding the most\\ninformative distance measure out of a pool of candidates. The approach is\\napplied to find the most relevant policy variables for controlling the Covid-19\\nepidemic and to find compact yet informative representations of atomic\\nstructures, but its potential applications are wide ranging in many branches of\\nscience. We address an inherent difficulty in welfare-theoretic fair machine learning\\nby proposing an equivalently axiomatically-justified alternative and studying\\nthe resulting computational and statistical learning questions. Welfare metrics\\nquantify overall wellbeing across a population of one or more groups, and\\nwelfare-based objectives and constraints have recently been proposed to\\nincentivize fair machine learning methods to produce satisfactory solutions\\nthat consider the diverse needs of multiple groups. Unfortunately, many\\nmachine-learning problems are more naturally cast as loss minimization tasks,\\nrather than utility maximization, which complicates direct application of\\nwelfare-centric methods to fair machine learning. In this work, we define a\\ncomplementary measure, termed malfare, measuring overall societal harm (rather\\nthan wellbeing), with axiomatic justification via the standard axioms of\\ncardinal welfare. We then cast fair machine learning as malfare minimization\\nover the risk values (expected losses) of each group. Surprisingly, the axioms\\nof cardinal welfare (malfare) dictate that this is not equivalent to simply\\ndefining utility as negative loss. Building upon these concepts, we define\\nfair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an\\n$\\\\varepsilon$-$\\\\delta$ malfare-optimal model with bounded sample complexity,\\nfor any data distribution, and for any (axiomatically justified) malfare\\nconcept. Finally, we show broad conditions under which, with appropriate\\nmodifications, standard PAC-learners may be converted to FPAC learners. This\\nplaces FPAC learning on firm theoretical ground, as it yields statistical and\\ncomputational efficiency guarantees for many well-studied machine-learning\\nmodels, and is also practically relevant, as it democratizes fair ML by\\nproviding concrete training algorithms and rigorous generalization guarantees\\nfor these models Few-shot and one-shot learning have been the subject of active and intensive\\nresearch in recent years, with mounting evidence pointing to successful\\nimplementation and exploitation of few-shot learning algorithms in practice.\\nClassical statistical learning theories do not fully explain why few- or\\none-shot learning is at all possible since traditional generalisation bounds\\nnormally require large training and testing samples to be meaningful. This\\nsharply contrasts with numerous examples of successful one- and few-shot\\nlearning systems and applications.\\n  In this work we present mathematical foundations for a theory of one-shot and\\nfew-shot learning and reveal conditions specifying when such learning schemes\\nare likely to succeed. Our theory is based on intrinsic properties of\\nhigh-dimensional spaces. We show that if the ambient or latent decision space\\nof a learning machine is sufficiently high-dimensional than a large class of\\nobjects in this space can indeed be easily learned from few examples provided\\nthat certain data non-concentration conditions are met. The modelling of Earth observation data is a challenging problem, typically\\napproached by either purely mechanistic or purely data-driven methods.\\nMechanistic models encode the domain knowledge and physical rules governing the\\nsystem. Such models, however, need the correct specification of all\\ninteractions between variables in the problem and the appropriate\\nparameterization is a challenge in itself. On the other hand, machine learning\\napproaches are flexible data-driven tools, able to approximate arbitrarily\\ncomplex functions, but lack interpretability and struggle when data is scarce\\nor in extrapolation regimes. In this paper, we argue that hybrid learning\\nschemes that combine both approaches can address all these issues efficiently.\\nWe introduce Gaussian process (GP) convolution models for hybrid modelling in\\nEarth observation (EO) problems. We specifically propose the use of a class of\\nGP convolution models called latent force models (LFMs) for EO time series\\nmodelling, analysis and understanding. LFMs are hybrid models that incorporate\\nphysical knowledge encoded in differential equations into a multioutput GP\\nmodel. LFMs can transfer information across time-series, cope with missing\\nobservations, infer explicit latent functions forcing the system, and learn\\nparameterizations which are very helpful for system analysis and\\ninterpretability. We consider time series of soil moisture from active (ASCAT)\\nand passive (SMOS, AMSR2) microwave satellites. We show how assuming a first\\norder differential equation as governing equation, the model automatically\\nestimates the e-folding time or decay rate related to soil moisture persistence\\nand discovers latent forces related to precipitation. The proposed hybrid\\nmethodology reconciles the two main approaches in remote sensing parameter\\nestimation by blending statistical learning and mechanistic modeling. Machine and statistical learning algorithms can be reliably automated and\\napplied at scale. Therefore, they can constitute a considerable asset for\\ndesigning practical forecasting systems, such as those related to urban water\\ndemand. Quantile regression algorithms are statistical and machine learning\\nalgorithms that can provide probabilistic forecasts in a straightforward way,\\nand have not been applied so far for urban water demand forecasting. In this\\nwork, we aim to fill this gap by automating and extensively comparing several\\nquantile-regression-based practical systems for probabilistic one-day ahead\\nurban water demand forecasting. For designing the practical systems, we use\\nfive individual algorithms (i.e., the quantile regression, linear boosting,\\ngeneralized random forest, gradient boosting machine and quantile regression\\nneural network algorithms), their mean combiner and their median combiner. The\\ncomparison is conducted by exploiting a large urban water flow dataset, as well\\nas several types of hydrometeorological time series (which are considered as\\nexogenous predictor variables in the forecasting setting). The results mostly\\nfavour the practical systems designed using the linear boosting algorithm,\\nprobably due to the presence of trends in the urban water flow time series. The\\nforecasts of the mean and median combiners are also found to be skilful in\\ngeneral terms. The angular measure on the unit sphere characterizes the first-order\\ndependence structure of the components of a random vector in extreme regions\\nand is defined in terms of standardized margins. Its statistical recovery is an\\nimportant step in learning problems involving observations far away from the\\ncenter. In the common situation that the components of the vector have\\ndifferent distributions, the rank transformation offers a convenient and robust\\nway of standardizing data in order to build an empirical version of the angular\\nmeasure based on the most extreme observations. However, the study of the\\nsampling distribution of the resulting empirical angular measure is\\nchallenging. It is the purpose of the paper to establish finite-sample bounds\\nfor the maximal deviations between the empirical and true angular measures,\\nuniformly over classes of Borel sets of controlled combinatorial complexity.\\nThe bounds are valid with high probability and, up to logarithmic factors,\\nscale as the square root of the effective sample size. The bounds are applied\\nto provide performance guarantees for two statistical learning procedures\\ntailored to extreme regions of the input space and built upon the empirical\\nangular measure: binary classification in extreme regions through empirical\\nrisk minimization and unsupervised anomaly detection through minimum-volume\\nsets of the sphere. This paper addresses the problem of online learning in metric spaces using\\nexponential weights. We extend the analysis of the exponentially weighted\\naverage forecaster, traditionally studied in a Euclidean settings, to a more\\nabstract framework. Our results rely on the notion of barycenters, a suitable\\nversion of Jensen's inequality and a synthetic notion of lower curvature bound\\nin metric spaces known as the measure contraction property. We also adapt the\\nonline-to-batch conversion principle to apply our results to a statistical\\nlearning framework. This paper considers batch Reinforcement Learning (RL) with general value\\nfunction approximation. Our study investigates the minimal assumptions to\\nreliably estimate/minimize Bellman error, and characterizes the generalization\\nperformance by (local) Rademacher complexities of general function classes,\\nwhich makes initial steps in bridging the gap between statistical learning\\ntheory and batch RL. Concretely, we view the Bellman error as a surrogate loss\\nfor the optimality gap, and prove the followings: (1) In double sampling\\nregime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the\\nRademacher complexity of the function class. (2) In the single sampling regime,\\nsample-efficient risk minimization is not possible without further assumptions,\\nregardless of algorithms. However, with completeness assumptions, the excess\\nrisk of FQI and a minimax style algorithm can be again bounded by the\\nRademacher complexity of the corresponding function classes. (3) Fast\\nstatistical rates can be achieved by using tools of local Rademacher\\ncomplexity. Our analysis covers a wide range of function classes, including\\nfinite classes, linear spaces, kernel spaces, sparse linear features, etc. We discuss how over the last 30 to 50 years, Artificial Intelligence (AI)\\nsystems that focused only on data have been handicapped, and how knowledge has\\nbeen critical in developing smarter, intelligent, and more effective systems.\\nIn fact, the vast progress in AI can be viewed in terms of the three waves of\\nAI as identified by DARPA. During the first wave, handcrafted knowledge has\\nbeen at the center-piece, while during the second wave, the data-driven\\napproaches supplanted knowledge. Now we see a strong role and resurgence of\\nknowledge fueling major breakthroughs in the third wave of AI underpinning\\nfuture intelligent systems as they attempt human-like decision making, and seek\\nto become trusted assistants and companions for humans. We find a wider\\navailability of knowledge created from diverse sources, using manual to\\nautomated means both by repurposing as well as by extraction. Using knowledge\\nwith statistical learning is becoming increasingly indispensable to help make\\nAI systems more transparent and auditable. We will draw a parallel with the\\nrole of knowledge and experience in human intelligence based on cognitive\\nscience, and discuss emerging neuro-symbolic or hybrid AI systems in which\\nknowledge is the critical enabler for combining capabilities of the\\ndata-intensive statistical AI systems with those of symbolic AI systems,\\nresulting in more capable AI systems that support more human-like intelligence. Markov models are often used to capture the temporal patterns of sequential\\ndata for statistical learning applications. While the Hidden Markov\\nmodeling-based learning mechanisms are well studied in literature, we analyze a\\nsymbolic-dynamics inspired approach. Under this umbrella, Markov modeling of\\ntime-series data consists of two major steps -- discretization of continuous\\nattributes followed by estimating the size of temporal memory of the\\ndiscretized sequence. These two steps are critical for the accurate and concise\\nrepresentation of time-series data in the discrete space. Discretization\\ngoverns the information content of the resultant discretized sequence. On the\\nother hand, memory estimation of the symbolic sequence helps to extract the\\npredictive patterns in the discretized data. Clearly, the effectiveness of\\nsignal representation as a discrete Markov process depends on both these steps.\\nIn this paper, we will review the different techniques for discretization and\\nmemory estimation for discrete stochastic processes. In particular, we will\\nfocus on the individual problems of discretization and order estimation for\\ndiscrete stochastic process. We will present some results from literature on\\npartitioning from dynamical systems theory and order estimation using concepts\\nof information theory and statistical learning. The paper also presents some\\nrelated problem formulations which will be useful for machine learning and\\nstatistical learning application using the symbolic framework of data analysis.\\nWe present some results of statistical analysis of a complex thermoacoustic\\ninstability phenomenon during lean-premixed combustion in jet-turbine engines\\nusing the proposed Markov modeling method. Though learning has become a core component of modern information processing,\\nthere is now ample evidence that it can lead to biased, unsafe, and prejudiced\\nsystems. The need to impose requirements on learning is therefore paramount,\\nespecially as it reaches critical applications in social, industrial, and\\nmedical domains. However, the non-convexity of most modern statistical problems\\nis only exacerbated by the introduction of constraints. Whereas good\\nunconstrained solutions can often be learned using empirical risk minimization,\\neven obtaining a model that satisfies statistical constraints can be\\nchallenging. All the more so, a good one. In this paper, we overcome this issue\\nby learning in the empirical dual domain, where constrained statistical\\nlearning problems become unconstrained and deterministic. We analyze the\\ngeneralization properties of this approach by bounding the empirical duality\\ngap -- i.e., the difference between our approximate, tractable solution and the\\nsolution of the original (non-convex) statistical problem -- and provide a\\npractical constrained learning algorithm. These results establish a constrained\\ncounterpart to classical learning theory, enabling the explicit use of\\nconstraints in learning. We illustrate this theory and algorithm in\\nrate-constrained learning applications arising in fairness and adversarial\\nrobustness. In data-driven SHM, the signals recorded from systems in operation can be\\nnoisy and incomplete. Data corresponding to each of the operational,\\nenvironmental, and damage states are rarely available a priori; furthermore,\\nlabelling to describe the measurements is often unavailable. In consequence,\\nthe algorithms used to implement SHM should be robust and adaptive, while\\naccommodating for missing information in the training-data -- such that new\\ninformation can be included if it becomes available. By reviewing novel\\ntechniques for statistical learning (introduced in previous work), it is argued\\nthat probabilistic algorithms offer a natural solution to the modelling of SHM\\ndata in practice. In three case-studies, probabilistic methods are adapted for\\napplications to SHM signals -- including semi-supervised learning, active\\nlearning, and multi-task learning. This paper presents the first general (supervised) statistical learning\\nframework for point processes in general spaces. Our approach is based on the\\ncombination of two new concepts, which we define in the paper: i) bivariate\\ninnovations, which are measures of discrepancy/prediction-accuracy between two\\npoint processes, and ii) point process cross-validation (CV), which we here\\ndefine through point process thinning. The general idea is to carry out the\\nfitting by predicting CV-generated validation sets using the corresponding\\ntraining sets; the prediction error, which we minimise, is measured by means of\\nbivariate innovations. Having established various theoretical properties of our\\nbivariate innovations, we study in detail the case where the CV procedure is\\nobtained through independent thinning and we apply our statistical learning\\nmethodology to three typical spatial statistical settings, namely parametric\\nintensity estimation, non-parametric intensity estimation and Papangelou\\nconditional intensity fitting. Aside from deriving theoretical properties\\nrelated to these cases, in each of them we numerically show that our\\nstatistical learning approach outperforms the state of the art in terms of mean\\n(integrated) squared error. Quantile regression is a fundamental problem in statistical learning\\nmotivated by a need to quantify uncertainty in predictions, or to model a\\ndiverse population without being overly reductive. For instance,\\nepidemiological forecasts, cost estimates, and revenue predictions all benefit\\nfrom being able to quantify the range of possible values accurately. As such,\\nmany models have been developed for this problem over many years of research in\\nstatistics, machine learning, and related fields. Rather than proposing yet\\nanother (new) algorithm for quantile regression we adopt a meta viewpoint: we\\ninvestigate methods for aggregating any number of conditional quantile models,\\nin order to improve accuracy and robustness. We consider weighted ensembles\\nwhere weights may vary over not only individual models, but also over quantile\\nlevels, and feature values. All of the models we consider in this paper can be\\nfit using modern deep learning toolkits, and hence are widely accessible (from\\nan implementation point of view) and scalable. To improve the accuracy of the\\npredicted quantiles (or equivalently, prediction intervals), we develop tools\\nfor ensuring that quantiles remain monotonically ordered, and apply conformal\\ncalibration methods. These can be used without any modification of the original\\nlibrary of base models. We also review some basic theory surrounding quantile\\naggregation and related scoring rules, and contribute a few new results to this\\nliterature (for example, the fact that post sorting or post isotonic regression\\ncan only improve the weighted interval score). Finally, we provide an extensive\\nsuite of empirical comparisons across 34 data sets from two different benchmark\\nrepositories. Cyclic block coordinate methods are a fundamental class of optimization\\nmethods widely used in practice and implemented as part of standard software\\npackages for statistical learning. Nevertheless, their convergence is generally\\nnot well understood and so far their good practical performance has not been\\nexplained by existing convergence analyses. In this work, we introduce a new\\nblock coordinate method that applies to the general class of variational\\ninequality (VI) problems with monotone operators. This class includes composite\\nconvex optimization problems and convex-concave min-max optimization problems\\nas special cases and has not been addressed by the existing work. The resulting\\nconvergence bounds match the optimal convergence bounds of full gradient\\nmethods, but are provided in terms of a novel gradient Lipschitz condition\\nw.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient\\nLipschitz constant in our bounds is never larger than a factor $\\\\sqrt{m}$\\ncompared to the traditional Euclidean Lipschitz constant, while it is possible\\nfor it to be much smaller. Further, for the case when the operator in the VI\\nhas finite-sum structure, we propose a variance reduced variant of our method\\nwhich further decreases the per-iteration cost and has better convergence rates\\nin certain regimes. To obtain these results, we use a gradient extrapolation\\nstrategy that allows us to view a cyclic collection of block coordinate-wise\\ngradients as one implicit gradient. Efficient simulation of SDEs is essential in many applications, particularly\\nfor ergodic systems that demand efficient simulation of both short-time\\ndynamics and large-time statistics. However, locally Lipschitz SDEs often\\nrequire special treatments such as implicit schemes with small time-steps to\\naccurately simulate the ergodic measure. We introduce a framework to construct\\ninference-based schemes adaptive to large time-steps (ISALT) from data,\\nachieving a reduction in time by several orders of magnitudes. The key is the\\nstatistical learning of an approximation to the infinite-dimensional\\ndiscrete-time flow map. We explore the use of numerical schemes (such as the\\nEuler-Maruyama, a hybrid RK4, and an implicit scheme) to derive informed basis\\nfunctions, leading to a parameter inference problem. We introduce a scalable\\nalgorithm to estimate the parameters by least squares, and we prove the\\nconvergence of the estimators as data size increases.\\n  We test the ISALT on three non-globally Lipschitz SDEs: the 1D double-well\\npotential, a 2D multi-scale gradient system, and the 3D stochastic Lorenz\\nequation with degenerate noise. Numerical results show that ISALT can tolerate\\ntime-step magnitudes larger than plain numerical schemes. It reaches optimal\\naccuracy in reproducing the invariant measure when the time-step is\\nmedium-large. In 2001, Leo Breiman wrote of a divide between \\\"data modeling\\\" and\\n\\\"algorithmic modeling\\\" cultures. Twenty years later this division feels far\\nmore ephemeral, both in terms of assigning individuals to camps, and in terms\\nof intellectual boundaries. We argue that this is largely due to the \\\"data\\nmodelers\\\" incorporating algorithmic methods into their toolbox, particularly\\ndriven by recent developments in the statistical understanding of Breiman's own\\nRandom Forest methods. While this can be simplistically described as \\\"Breiman\\nwon\\\", these same developments also expose the limitations of the\\nprediction-first philosophy that he espoused, making careful statistical\\nanalysis all the more important. This paper outlines these exciting recent\\ndevelopments in the random forest literature which, in our view, occurred as a\\nresult of a necessary blending of the two ways of thinking Breiman originally\\ndescribed. We also ask what areas statistics and statisticians might currently\\noverlook. Despite remarkable success in a variety of applications, it is well-known\\nthat deep learning can fail catastrophically when presented with\\nout-of-distribution data. Toward addressing this challenge, we consider the\\ndomain generalization problem, wherein predictors are trained using data drawn\\nfrom a family of related training domains and then evaluated on a distinct and\\nunseen test domain. We show that under a natural model of data generation and a\\nconcomitant invariance condition, the domain generalization problem is\\nequivalent to an infinite-dimensional constrained statistical learning problem;\\nthis problem forms the basis of our approach, which we call Model-Based Domain\\nGeneralization. Due to the inherent challenges in solving constrained\\noptimization problems in deep learning, we exploit nonconvex duality theory to\\ndevelop unconstrained relaxations of this statistical problem with tight bounds\\non the duality gap. Based on this theoretical motivation, we propose a novel\\ndomain generalization algorithm with convergence guarantees. In our\\nexperiments, we report improvements of up to 30 percentage points over\\nstate-of-the-art domain generalization baselines on several benchmarks\\nincluding ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS. Many physical phenomena are described by Hamiltonian mechanics using an\\nenergy function (the Hamiltonian). Recently, the Hamiltonian neural network,\\nwhich approximates the Hamiltonian as a neural network, and its extensions have\\nattracted much attention. This is a very powerful method, but its use in\\ntheoretical studies remains limited. In this study, by combining the\\nstatistical learning theory and Kolmogorov-Arnold-Moser (KAM) theory, we\\nprovide a theoretical analysis of the behavior of Hamiltonian neural networks\\nwhen the learning error is not completely zero. A Hamiltonian neural network\\nwith non-zero errors can be considered as a perturbation from the true\\ndynamics, and the perturbation theory of the Hamilton equation is widely known\\nas the KAM theory. To apply the KAM theory, we provide a generalization error\\nbound for Hamiltonian neural networks by deriving an estimate of the covering\\nnumber of the gradient of the multi-layer perceptron, which is the key\\ningredient of the model. This error bound gives an $L^\\\\infty$ bound on the\\nHamiltonian that is required in the application of the KAM theory. This paper investigates the theory of robustness against adversarial attacks.\\nWe focus on randomized classifiers (\\\\emph{i.e.} classifiers that output random\\nvariables) and provide a thorough analysis of their behavior through the lens\\nof statistical learning theory and information theory. To this aim, we\\nintroduce a new notion of robustness for randomized classifiers, enforcing\\nlocal Lipschitzness using probability metrics. Equipped with this definition,\\nwe make two new contributions. The first one consists in devising a new upper\\nbound on the adversarial generalization gap of randomized classifiers. More\\nprecisely, we devise bounds on the generalization gap and the adversarial gap\\n(\\\\emph{i.e.} the gap between the risk and the worst-case risk under attack) of\\nrandomized classifiers. The second contribution presents a yet simple but\\nefficient noise injection method to design robust randomized classifiers. We\\nshow that our results are applicable to a wide range of machine learning models\\nunder mild hypotheses. We further corroborate our findings with experimental\\nresults using deep neural networks on standard image datasets, namely CIFAR-10\\nand CIFAR-100. All robust models we trained models can simultaneously achieve\\nstate-of-the-art accuracy (over $0.82$ clean accuracy on CIFAR-10) and enjoy\\n\\\\emph{guaranteed} robust accuracy bounds ($0.45$ against $\\\\ell_2$ adversaries\\nwith magnitude $0.5$ on CIFAR-10). In statistical learning and analysis from shared data, which is increasingly\\nwidely adopted in platforms such as federated learning and meta-learning, there\\nare two major concerns: privacy and robustness. Each participating individual\\nshould be able to contribute without the fear of leaking one's sensitive\\ninformation. At the same time, the system should be robust in the presence of\\nmalicious participants inserting corrupted data. Recent algorithmic advances in\\nlearning from shared data focus on either one of these threats, leaving the\\nsystem vulnerable to the other. We bridge this gap for the canonical problem of\\nestimating the mean from i.i.d. samples. We introduce PRIME, which is the first\\nefficient algorithm that achieves both privacy and robustness for a wide range\\nof distributions. We further complement this result with a novel exponential\\ntime algorithm that improves the sample complexity of PRIME, achieving a\\nnear-optimal guarantee and matching a known lower bound for (non-robust)\\nprivate mean estimation. This proves that there is no extra statistical cost to\\nsimultaneously guaranteeing privacy and robustness. Statistical learning theory provides the foundation to applied machine\\nlearning, and its various successful applications in computer vision, natural\\nlanguage processing and other scientific domains. The theory, however, does not\\ntake into account the unique challenges of performing statistical learning in\\ngeospatial settings. For instance, it is well known that model errors cannot be\\nassumed to be independent and identically distributed in geospatial (a.k.a.\\nregionalized) variables due to spatial correlation; and trends caused by\\ngeophysical processes lead to covariate shifts between the domain where the\\nmodel was trained and the domain where it will be applied, which in turn harm\\nthe use of classical learning methodologies that rely on random samples of the\\ndata. In this work, we introduce the geostatistical (transfer) learning\\nproblem, and illustrate the challenges of learning from geospatial data by\\nassessing widely-used methods for estimating generalization error of learning\\nmodels, under covariate shift and spatial correlation. Experiments with\\nsynthetic Gaussian process data as well as with real data from geophysical\\nsurveys in New Zealand indicate that none of the methods are adequate for model\\nselection in a geospatial context. We provide general guidelines regarding the\\nchoice of these methods in practice while new methods are being actively\\nresearched. A novel framework for statistical learning is introduced which combines ideas\\nfrom regularization and ensembling. This framework is applied to learn an\\nensemble of logistic regression models for high-dimensional binary\\nclassification. In the new framework the models in the ensemble are learned\\nsimultaneously by optimizing a multi-convex objective function. To enforce\\ndiversity between the models the objective function penalizes overlap between\\nthe models in the ensemble. Measures of diversity in classifier ensembles are\\nused to show how our method learns the ensemble by exploiting the\\naccuracy-diversity trade-off for ensemble models. In contrast to other\\nensembling approaches, the resulting ensemble model is fully interpretable as a\\nlogistic regression model, asymptotically consistent, and at the same time\\nyields excellent prediction accuracy as demonstrated in an extensive simulation\\nstudy and gene expression data applications. The models found by the proposed\\nensemble methodology can also reveal alternative mechanisms that can explain\\nthe relationship between the predictors and the response variable. An\\nopen-source compiled software library implementing the proposed method is\\nbriefly discussed. In the context of statistical learning, the Information Bottleneck method\\nseeks a right balance between accuracy and generalization capability through a\\nsuitable tradeoff between compression complexity, measured by minimum\\ndescription length, and distortion evaluated under logarithmic loss measure. In\\nthis paper, we study a variation of the problem, called scalable information\\nbottleneck, in which the encoder outputs multiple descriptions of the\\nobservation with increasingly richer features. The model, which is of\\nsuccessive-refinement type with degraded side information streams at the\\ndecoders, is motivated by some application scenarios that require varying\\nlevels of accuracy depending on the allowed (or targeted) level of complexity.\\nWe establish an analytic characterization of the optimal relevance-complexity\\nregion for vector Gaussian sources. Then, we derive a variational inference\\ntype algorithm for general sources with unknown distribution; and show means of\\nparametrizing it using neural networks. Finally, we provide experimental\\nresults on the MNIST dataset which illustrate that the proposed method\\ngeneralizes better to unseen data during the training phase. We provide a new adaptive method for online convex optimization, MetaGrad,\\nthat is robust to general convex losses but achieves faster rates for a broad\\nclass of special functions, including exp-concave and strongly convex\\nfunctions, but also various types of stochastic and non-stochastic functions\\nwithout any curvature. We prove this by drawing a connection to the Bernstein\\ncondition, which is known to imply fast rates in offline statistical learning.\\nMetaGrad further adapts automatically to the size of the gradients. Its main\\nfeature is that it simultaneously considers multiple learning rates, which are\\nweighted directly proportional to their empirical performance on the data using\\na new meta-algorithm. We provide three versions of MetaGrad. The full matrix\\nversion maintains a full covariance matrix and is applicable to learning tasks\\nfor which we can afford update time quadratic in the dimension. The other two\\nversions provide speed-ups for high-dimensional learning tasks with an update\\ntime that is linear in the dimension: one is based on sketching, the other on\\nrunning a separate copy of the basic algorithm per coordinate. We evaluate all\\nversions of MetaGrad on benchmark online classification and regression tasks,\\non which they consistently outperform both online gradient descent and AdaGrad. We analyse prior risk factors for severe, critical or fatal courses of\\nCovid-19 based on a retrospective cohort using claims data of the AOK Bayern.\\nAs our main methodological contribution, we avoid prior grouping and\\npre-selection of candidate risk factors. Instead, fine-grained hierarchical\\ninformation from medical classification systems for diagnoses, pharmaceuticals\\nand procedures are used, resulting in more than 33,000 covariates. Our approach\\nhas better predictive ability than well-specified morbidity groups but does not\\nneed prior subject-matter knowledge. The methodology and estimated coefficients\\nare made available to decision makers to prioritize protective measures towards\\nvulnerable subpopulations and to researchers who like to adjust for a large set\\nof confounders in studies of individual risk factors. Consider a regression problem where the learner is given a large collection\\nof $d$-dimensional data points, but can only query a small subset of the\\nreal-valued labels. How many queries are needed to obtain a $1+\\\\epsilon$\\nrelative error approximation of the optimum? While this problem has been\\nextensively studied for least squares regression, little is known for other\\nlosses. An important example is least absolute deviation regression ($\\\\ell_1$\\nregression) which enjoys superior robustness to outliers compared to least\\nsquares. We develop a new framework for analyzing importance sampling methods\\nin regression problems, which enables us to show that the query complexity of\\nleast absolute deviation regression is $\\\\Theta(d/\\\\epsilon^2)$ up to logarithmic\\nfactors. We further extend our techniques to show the first bounds on the query\\ncomplexity for any $\\\\ell_p$ loss with $p\\\\in(1,2)$. As a key novelty in our\\nanalysis, we introduce the notion of robust uniform convergence, which is a new\\napproximation guarantee for the empirical loss. While it is inspired by uniform\\nconvergence in statistical learning, our approach additionally incorporates a\\ncorrection term to avoid unnecessary variance due to outliers. This can be\\nviewed as a new connection between statistical learning theory and variance\\nreduction techniques in stochastic optimization, which should be of independent\\ninterest. Of all the characteristics of a violin, those that concern its shape are\\nprobably the most important ones, as the violin maker has complete control over\\nthem. Contemporary violin making, however, is still based more on tradition\\nthan understanding, and a definitive scientific study of the specific relations\\nthat exist between shape and vibrational properties is yet to come and sorely\\nmissed. In this article, using standard statistical learning tools, we show\\nthat the modal frequencies of violin tops can, in fact, be predicted from\\ngeometric parameters, and that artificial intelligence can be successfully\\napplied to traditional violin making. We also study how modal frequencies vary\\nwith the thicknesses of the plate (a process often referred to as {\\\\em plate\\ntuning}) and discuss the complexity of this dependency. Finally, we propose a\\npredictive tool for plate tuning, which takes into account material and\\ngeometric parameters. Knowledge distillation is an effective approach to leverage a well-trained\\nnetwork or an ensemble of them, named as the teacher, to guide the training of\\na student network. The outputs from the teacher network are used as soft labels\\nfor supervising the training of a new network. Recent studies\\n\\\\citep{muller2019does,yuan2020revisiting} revealed an intriguing property of\\nthe soft labels that making labels soft serves as a good regularization to the\\nstudent network. From the perspective of statistical learning, regularization\\naims to reduce the variance, however how bias and variance change is not clear\\nfor training with soft labels. In this paper, we investigate the bias-variance\\ntradeoff brought by distillation with soft labels. Specifically, we observe\\nthat during training the bias-variance tradeoff varies sample-wisely. Further,\\nunder the same distillation temperature setting, we observe that the\\ndistillation performance is negatively associated with the number of some\\nspecific samples, which are named as regularization samples since these samples\\nlead to bias increasing and variance decreasing. Nevertheless, we empirically\\nfind that completely filtering out regularization samples also deteriorates\\ndistillation performance. Our discoveries inspired us to propose the novel\\nweighted soft labels to help the network adaptively handle the sample-wise\\nbias-variance tradeoff. Experiments on standard evaluation benchmarks validate\\nthe effectiveness of our method. Our code is available at\\n\\\\url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}. Distributed learning provides an attractive framework for scaling the\\nlearning task by sharing the computational load over multiple nodes in a\\nnetwork. Here, we investigate the performance of distributed learning for\\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\\nare distributed over the network. We adopt a statistical learning approach. In\\ncontrast to works that focus on the performance on the training data, we focus\\non the generalization error, i.e., the performance on unseen data. We provide\\nhigh-probability bounds on the generalization error for both isotropic and\\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\\ndependence of the generalization performance on the partitioning of the model\\nover the network. In particular, our results show that the generalization error\\nof the distributed solution can be substantially higher than that of the\\ncentralized solution even when the error on the training data is at the same\\nlevel for both the centralized and distributed approaches. Our numerical\\nresults illustrate the performance with both real-world image data as well as\\nsynthetic data. Data-driven methods for battery lifetime prediction are attracting increasing\\nattention for applications in which the degradation mechanisms are poorly\\nunderstood and suitable training sets are available. However, while advanced\\nmachine learning and deep learning methods promise high performance with\\nminimal data preprocessing, simpler linear models with engineered features\\noften achieve comparable performance, especially for small training sets, while\\nalso providing physical and statistical interpretability. In this work, we use\\na previously published dataset to develop simple, accurate, and interpretable\\ndata-driven models for battery lifetime prediction. We first present the\\n\\\"capacity matrix\\\" concept as a compact representation of battery\\nelectrochemical cycling data, along with a series of feature representations.\\nWe then create a number of univariate and multivariate models, many of which\\nachieve comparable performance to the highest-performing models previously\\npublished for this dataset. These models also provide insights into the\\ndegradation of these cells. Our approaches can be used both to quickly train\\nmodels for a new dataset and to benchmark the performance of more advanced\\nmachine learning methods. We study the continuity property of the generalized entropy as a function of\\nthe underlying probability distribution, defined with an action space and a\\nloss function, and use this property to answer the basic questions in\\nstatistical learning theory: the excess risk analyses for various learning\\nmethods. We first derive upper and lower bounds for the entropy difference of\\ntwo distributions in terms of several commonly used f-divergences, the\\nWasserstein distance, a distance that depends on the action space and the loss\\nfunction, and the Bregman divergence generated by the entropy, which also\\ninduces bounds in terms of the Euclidean distance between the two\\ndistributions. Examples are given along with the discussion of each general\\nresult, comparisons are made with the existing entropy difference bounds, and\\nnew mutual information upper bounds are derived based on the new results. We\\nthen apply the entropy difference bounds to the theory of statistical learning.\\nIt is shown that the excess risks in the two popular learning paradigms, the\\nfrequentist learning and the Bayesian learning, both can be studied with the\\ncontinuity property of different forms of the generalized entropy. The analysis\\nis then extended to the continuity of generalized conditional entropy. The\\nextension provides performance bounds for Bayes decision making with mismatched\\ndistributions. It also leads to excess risk bounds for a third paradigm of\\nlearning, where the decision rule is optimally designed under the projection of\\nthe empirical distribution to a predefined family of distributions. We thus\\nestablish a unified method of excess risk analysis for the three major\\nparadigms of statistical learning, through the continuity of generalized\\nentropy. A central issue of many statistical learning problems is to select an\\nappropriate model from a set of candidate models. Large models tend to inflate\\nthe variance (or overfitting), while small models tend to cause biases (or\\nunderfitting) for a given fixed dataset. In this work, we address the critical\\nchallenge of model selection to strike a balance between model fitting and\\nmodel complexity, thus gaining reliable predictive power. We consider the task\\nof approaching the theoretical limit of statistical learning, meaning that the\\nselected model has the predictive performance that is as good as the best\\npossible model given a class of potentially misspecified candidate models. We\\npropose a generalized notion of Takeuchi's information criterion and prove that\\nthe proposed method can asymptotically achieve the optimal out-sample\\nprediction loss under reasonable assumptions. It is the first proof of the\\nasymptotic property of Takeuchi's information criterion to our best knowledge.\\nOur proof applies to a wide variety of nonlinear models, loss functions, and\\nhigh dimensionality (in the sense that the models' complexity can grow with\\nsample size). The proposed method can be used as a computationally efficient\\nsurrogate for leave-one-out cross-validation. Moreover, for modeling streaming\\ndata, we propose an online algorithm that sequentially expands the model\\ncomplexity to enhance selection stability and reduce computation cost.\\nExperimental studies show that the proposed method has desirable predictive\\npower and significantly less computational cost than some popular methods. Understanding generalization in deep learning is arguably one of the most\\nimportant questions in deep learning. Deep learning has been successfully\\nadopted to a large number of problems ranging from pattern recognition to\\ncomplex decision making, but many recent researchers have raised many concerns\\nabout deep learning, among which the most important is generalization. Despite\\nnumerous attempts, conventional statistical learning approaches have yet been\\nable to provide a satisfactory explanation on why deep learning works. A recent\\nline of works aims to address the problem by trying to predict the\\ngeneralization performance through complexity measures. In this competition, we\\ninvite the community to propose complexity measures that can accurately predict\\ngeneralization of models. A robust and general complexity measure would\\npotentially lead to a better understanding of deep learning's underlying\\nmechanism and behavior of deep models on unseen data, or shed light on better\\ngeneralization bounds. All these outcomes will be important for making deep\\nlearning more robust and reliable. Conditional Value-at-Risk ($\\\\mathrm{CV@R}$) is one of the most popular\\nmeasures of risk, which has been recently considered as a performance criterion\\nin supervised statistical learning, as it is related to desirable operational\\nfeatures in modern applications, such as safety, fairness, distributional\\nrobustness, and prediction error stability. However, due to its variational\\ndefinition, $\\\\mathrm{CV@R}$ is commonly believed to result in difficult\\noptimization problems, even for smooth and strongly convex loss functions. We\\ndisprove this statement by establishing noisy (i.e., fixed-accuracy) linear\\nconvergence of stochastic gradient descent for sequential $\\\\mathrm{CV@R}$\\nlearning, for a large class of not necessarily strongly-convex (or even convex)\\nloss functions satisfying a set-restricted Polyak-Lojasiewicz inequality. This\\nclass contains all smooth and strongly convex losses, confirming that classical\\nproblems, such as linear least squares regression, can be solved efficiently\\nunder the $\\\\mathrm{CV@R}$ criterion, just as their risk-neutral versions. Our\\nresults are illustrated numerically on such a risk-aware ridge regression task,\\nalso verifying their validity in practice. We study high-dimensional least-squares regression within a subgaussian\\nstatistical learning framework with heterogeneous noise. It includes $s$-sparse\\nand $r$-low-rank least-squares regression when a fraction $\\\\epsilon$ of the\\nlabels are adversarially contaminated. We also present a novel theory of\\ntrace-regression with matrix decomposition based on a new application of the\\nproduct process. For these problems, we show novel near-optimal \\\"subgaussian\\\"\\nestimation rates of the form\\n$r(n,d_{e})+\\\\sqrt{\\\\log(1/\\\\delta)/n}+\\\\epsilon\\\\log(1/\\\\epsilon)$, valid with\\nprobability at least $1-\\\\delta$. Here, $r(n,d_{e})$ is the optimal\\nuncontaminated rate as a function of the effective dimension $d_{e}$ but\\nindependent of the failure probability $\\\\delta$. These rates are valid\\nuniformly on $\\\\delta$, i.e., the estimators' tuning do not depend on $\\\\delta$.\\nLastly, we consider noisy robust matrix completion with non-uniform sampling.\\nIf only the low-rank matrix is of interest, we present a novel near-optimal\\nrate that is independent of the corruption level $a$. Our estimators are\\ntractable and based on a new \\\"sorted\\\" Huber-type loss. No information on\\n$(s,r,\\\\epsilon,a)$ are needed to tune these estimators. Our analysis makes use\\nof novel $\\\\delta$-optimal concentration inequalities for the multiplier and\\nproduct processes which could be useful elsewhere. For instance, they imply\\nnovel sharp oracle inequalities for Lasso and Slope with optimal dependence on\\n$\\\\delta$. Numerical simulations confirm our theoretical predictions. In\\nparticular, \\\"sorted\\\" Huber regression can outperform classical Huber\\nregression. We propose a statistical learning framework based on group-sparse regression\\nthat can be used to 1) enforce conservation laws, 2) ensure model equivalence,\\nand 3) guarantee symmetries when learning or inferring differential-equation\\nmodels from measurement data. Directly learning $\\\\textit{interpretable}$\\nmathematical models from data has emerged as a valuable modeling approach.\\nHowever, in areas like biology, high noise levels, sensor-induced correlations,\\nand strong inter-system variability can render data-driven models nonsensical\\nor physically inconsistent without additional constraints on the model\\nstructure. Hence, it is important to leverage $\\\\textit{prior}$ knowledge from\\nphysical principles to learn \\\"biologically plausible and physically consistent\\\"\\nmodels rather than models that simply fit the data best. We present a novel\\ngroup Iterative Hard Thresholding (gIHT) algorithm and use stability selection\\nto infer physically consistent models with minimal parameter tuning. We show\\nseveral applications from systems biology that demonstrate the benefits of\\nenforcing $\\\\textit{priors}$ in data-driven modeling. The history of AI has included several \\\"waves\\\" of ideas. The first wave, from\\nthe mid-1950s to the 1980s, focused on logic and symbolic hand-encoded\\nrepresentations of knowledge, the foundations of so-called \\\"expert systems\\\".\\nThe second wave, starting in the 1990s, focused on statistics and machine\\nlearning, in which, instead of hand-programming rules for behavior, programmers\\nconstructed \\\"statistical learning algorithms\\\" that could be trained on large\\ndatasets. In the most recent wave research in AI has largely focused on deep\\n(i.e., many-layered) neural networks, which are loosely inspired by the brain\\nand trained by \\\"deep learning\\\" methods. However, while deep neural networks\\nhave led to many successes and new capabilities in computer vision, speech\\nrecognition, language processing, game-playing, and robotics, their potential\\nfor broad application remains limited by several factors.\\n  A concerning limitation is that even the most successful of today's AI\\nsystems suffer from brittleness-they can fail in unexpected ways when faced\\nwith situations that differ sufficiently from ones they have been trained on.\\nThis lack of robustness also appears in the vulnerability of AI systems to\\nadversarial attacks, in which an adversary can subtly manipulate data in a way\\nto guarantee a specific wrong answer or action from an AI system. AI systems\\nalso can absorb biases-based on gender, race, or other factors-from their\\ntraining data and further magnify these biases in their subsequent\\ndecision-making. Taken together, these various limitations have prevented AI\\nsystems such as automatic medical diagnosis or autonomous vehicles from being\\nsufficiently trustworthy for wide deployment. The massive proliferation of AI\\nacross society will require radically new ideas to yield technology that will\\nnot sacrifice our productivity, our quality of life, or our values. There are currently many barriers that prevent non-experts from exploiting\\nmachine learning solutions ranging from the lack of intuition on statistical\\nlearning techniques to the trickiness of hyperparameter tuning. Such barriers\\nhave led to an explosion of interest in automated machine learning (AutoML),\\nwhereby an off-the-shelf system can take care of many of the steps for\\nend-users without the need for expertise in machine learning. This paper\\npresents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the\\nresults of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits\\nthe diversity of existing AutoML systems by leveraging the differences in their\\nmodel search space and heuristics. Empirically, we show that diversity of each\\nAutoML system is sufficient to justify ensembling at the AutoML system level.\\nIn demonstrating this, we also establish new state-of-the-art AutoML results on\\nthe OpenML tabular classification benchmark. Dealing with land cover classification of the new image sources has also\\nturned to be a complex problem requiring large amount of memory and processing\\ntime. In order to cope with these problems, statistical learning has greatly\\nhelped in the last years to develop statistical retrieval and classification\\nmodels that can ingest large amounts of Earth observation data. Kernel methods\\nconstitute a family of powerful machine learning algorithms, which have found\\nwide use in remote sensing and geosciences. However, kernel methods are still\\nnot widely adopted because of the high computational cost when dealing with\\nlarge scale problems, such as the inversion of radiative transfer models or the\\nclassification of high spatial-spectral-temporal resolution data. This paper\\nintroduces an efficient kernel method for fast statistical retrieval of\\nbio-geo-physical parameters and image classification problems. The method\\nallows to approximate a kernel matrix with a set of projections on random bases\\nsampled from the Fourier domain. The method is simple, computationally very\\nefficient in both memory and processing costs, and easily parallelizable. We\\nshow that kernel regression and classification is now possible for datasets\\nwith millions of examples and high dimensionality. Examples on atmospheric\\nparameter retrieval from hyperspectral infrared sounders like IASI/Metop; large\\nscale emulation and inversion of the familiar PROSAIL radiative transfer model\\non Sentinel-2 data; and the identification of clouds over landmarks in time\\nseries of MSG/Seviri images show the efficiency and effectiveness of the\\nproposed technique. Computationally expensive Radiative Transfer Models (RTMs) are widely used}\\nto realistically reproduce the light interaction with the Earth surface and\\natmosphere. Because these models take long processing time, the common practice\\nis to first generate a sparse look-up table (LUT) and then make use of\\ninterpolation methods to sample the multi-dimensional LUT input variable space.\\nHowever, the question arise whether common interpolation methods perform most\\naccurate. As an alternative to interpolation, this work proposes to use\\nemulation, i.e., approximating the RTM output by means of statistical learning.\\nTwo experiments were conducted to assess the accuracy in delivering spectral\\noutputs using interpolation and emulation: (1) at canopy level, using PROSAIL;\\nand (2) at top-of-atmosphere level, using MODTRAN. Various interpolation\\n(nearest-neighbour, inverse distance weighting, piece-wice linear) and\\nemulation (Gaussian process regression (GPR), kernel ridge regression, neural\\nnetworks) methods were evaluated against a dense reference LUT. In all\\nexperiments, the emulation methods clearly produced more accurate output\\nspectra than classical interpolation methods. GPR emulation performed up to ten\\ntimes more accurately than the best performing interpolation method, and this\\nwith a speed that is competitive with the faster interpolation methods. It is\\nconcluded that emulation can function as a fast and more accurate alternative\\nto commonly used interpolation methods for reconstructing RTM spectral data. By transferring knowledge learned from seen/previous tasks, meta learning\\naims to generalize well to unseen/future tasks. Existing meta-learning\\napproaches have shown promising empirical performance on various multiclass\\nclassification problems, but few provide theoretical analysis on the\\nclassifiers' generalization ability on future tasks. In this paper, under the\\nassumption that all classification tasks are sampled from the same\\nmeta-distribution, we leverage margin theory and statistical learning theory to\\nestablish three margin-based transfer bounds for meta-learning based multiclass\\nclassification (MLMC). These bounds reveal that the expected error of a given\\nclassification algorithm for a future task can be estimated with the average\\nempirical error on a finite number of previous tasks, uniformly over a class of\\npreprocessing feature maps/deep neural networks (i.e. deep feature embeddings).\\nTo validate these bounds, instead of the commonly-used cross-entropy loss, a\\nmulti-margin loss is employed to train a number of representative MLMC models.\\nExperiments on three benchmarks show that these margin-based models still\\nachieve competitive performance, validating the practical value of our\\nmargin-based theoretical analysis. Acquisition of data is a difficult task in many applications of machine\\nlearning, and it is only natural that one hopes and expects the population risk\\nto decrease (better performance) monotonically with increasing data points. It\\nturns out, somewhat surprisingly, that this is not the case even for the most\\nstandard algorithms that minimize the empirical risk. Non-monotonic behavior of\\nthe risk and instability in training have manifested and appeared in the\\npopular deep learning paradigm under the description of double descent. These\\nproblems highlight the current lack of understanding of learning algorithms and\\ngeneralization. It is, therefore, crucial to pursue this concern and provide a\\ncharacterization of such behavior. In this paper, we derive the first\\nconsistent and risk-monotonic (in high probability) algorithms for a general\\nstatistical learning setting under weak assumptions, consequently answering\\nsome questions posed by Viering et al. 2019 on how to avoid non-monotonic\\nbehavior of risk curves. We further show that risk monotonicity need not\\nnecessarily come at the price of worse excess risk rates. To achieve this, we\\nderive new empirical Bernstein-like concentration inequalities of independent\\ninterest that hold for certain non-i.i.d.~processes such as Martingale\\nDifference Sequences. The goal of these lecture notes is to review the problem of free energy\\nminimization as a unified framework underlying the definition of maximum\\nentropy modelling, generalized Bayesian inference, learning with latent\\nvariables, statistical learning analysis of generalization,and local\\noptimization. Free energy minimization is first introduced, here and\\nhistorically, as a thermodynamic principle. Then, it is described\\nmathematically in the context of Fenchel duality. Finally, the mentioned\\napplications to modelling, inference, learning, and optimization are covered\\nstarting from basic principles. In recent years, generative adversarial networks (GANs) have demonstrated\\nimpressive experimental results while there are only a few works that foster\\nstatistical learning theory for GANs. In this work, we propose an infinite\\ndimensional theoretical framework for generative adversarial learning. We\\nassume that the probability density functions of the underlying measure are\\nuniformly bounded, $k$-times $\\\\alpha$-H\\\\\\\"{o}lder differentiable\\n($C^{k,\\\\alpha}$) and uniformly bounded away from zero. Under these assumptions,\\nwe show that the Rosenblatt transformation induces an optimal generator, which\\nis realizable in the hypothesis space of $C^{k,\\\\alpha}$-generators. With a\\nconsistent definition of the hypothesis space of discriminators, we further\\nshow that the Jensen-Shannon divergence between the distribution induced by the\\ngenerator from the adversarial learning procedure and the data generating\\ndistribution converges to zero. Under certain regularity assumptions on the\\ndensity of the data generating process, we also provide rates of convergence\\nbased on chaining and concentration. The rapid progress in artificial intelligence (AI) and machine learning has\\nopened unprecedented analytics possibilities in various team and individual\\nsports, including baseball, basketball, and tennis. More recently, AI\\ntechniques have been applied to football, due to a huge increase in data\\ncollection by professional teams, increased computational power, and advances\\nin machine learning, with the goal of better addressing new scientific\\nchallenges involved in the analysis of both individual players' and coordinated\\nteams' behaviors. The research challenges associated with predictive and\\nprescriptive football analytics require new developments and progress at the\\nintersection of statistical learning, game theory, and computer vision. In this\\npaper, we provide an overarching perspective highlighting how the combination\\nof these fields, in particular, forms a unique microcosm for AI research, while\\noffering mutual benefits for professional teams, spectators, and broadcasters\\nin the years to come. We illustrate that this duality makes football analytics\\na game changer of tremendous value, in terms of not only changing the game of\\nfootball itself, but also in terms of what this domain can mean for the field\\nof AI. We review the state-of-the-art and exemplify the types of analysis\\nenabled by combining the aforementioned fields, including illustrative examples\\nof counterfactual analysis using predictive models, and the combination of\\ngame-theoretic analysis of penalty kicks with statistical learning of player\\nattributes. We conclude by highlighting envisioned downstream impacts,\\nincluding possibilities for extensions to other sports (real and virtual). We study problem-dependent rates, i.e., generalization errors that scale\\nnear-optimally with the variance, the effective loss, or the gradient norms\\nevaluated at the \\\"best hypothesis.\\\" We introduce a principled framework dubbed\\n\\\"uniform localized convergence,\\\" and characterize sharp problem-dependent rates\\nfor central statistical learning problems. From a methodological viewpoint, our\\nframework resolves several fundamental limitations of existing uniform\\nconvergence and localization analysis approaches. It also provides improvements\\nand some level of unification in the study of localized complexities, one-sided\\nuniform inequalities, and sample-based iterative algorithms. In the so-called\\n\\\"slow rate\\\" regime, we provides the first (moment-penalized) estimator that\\nachieves the optimal variance-dependent rate for general \\\"rich\\\" classes; we\\nalso establish improved loss-dependent rate for standard empirical risk\\nminimization. In the \\\"fast rate\\\" regime, we establish finite-sample\\nproblem-dependent bounds that are comparable to precise asymptotics. In\\naddition, we show that iterative algorithms like gradient descent and\\nfirst-order Expectation-Maximization can achieve optimal generalization error\\nin several representative problems across the areas of non-convex learning,\\nstochastic optimization, and learning with missing data. We develop a rigorous and general framework for constructing\\ninformation-theoretic divergences that subsume both $f$-divergences and\\nintegral probability metrics (IPMs), such as the $1$-Wasserstein distance. We\\nprove under which assumptions these divergences, hereafter referred to as\\n$(f,\\\\Gamma)$-divergences, provide a notion of `distance' between probability\\nmeasures and show that they can be expressed as a two-stage\\nmass-redistribution/mass-transport process. The $(f,\\\\Gamma)$-divergences\\ninherit features from IPMs, such as the ability to compare distributions which\\nare not absolutely continuous, as well as from $f$-divergences, namely the\\nstrict concavity of their variational representations and the ability to\\ncontrol heavy-tailed distributions for particular choices of $f$. When\\ncombined, these features establish a divergence with improved properties for\\nestimation, statistical learning, and uncertainty quantification applications.\\nUsing statistical learning as an example, we demonstrate their advantage in\\ntraining generative adversarial networks (GANs) for heavy-tailed,\\nnot-absolutely continuous sample distributions. We also show improved\\nperformance and stability over gradient-penalized Wasserstein GAN in image\\ngeneration. Complex systems which can be represented in the form of static and dynamic\\ngraphs arise in different fields, e.g. communication, engineering and industry.\\nOne of the interesting problems in analysing dynamic network structures is to\\nmonitor changes in their development. Statistical learning, which encompasses\\nboth methods based on artificial intelligence and traditional statistics, can\\nbe used to progress in this research area. However, the majority of approaches\\napply only one or the other framework. In this paper, we discuss the\\npossibility of bringing together both disciplines in order to create enhanced\\nnetwork monitoring procedures focussing on the example of combining statistical\\nprocess control and deep learning algorithms. Together with the presentation of\\nchange point and anomaly detection in network data, we propose to monitor the\\nresponse times of ambulance services, applying jointly the control chart for\\nquantile function values and a graph convolutional network. We present a deep learning algorithm for the numerical solution of parametric\\nfamilies of high-dimensional linear Kolmogorov partial differential equations\\n(PDEs). Our method is based on reformulating the numerical approximation of a\\nwhole family of Kolmogorov PDEs as a single statistical learning problem using\\nthe Feynman-Kac formula. Successful numerical experiments are presented, which\\nempirically confirm the functionality and efficiency of our proposed algorithm\\nin the case of heat equations and Black-Scholes option pricing models\\nparametrized by affine-linear coefficient functions. We show that a single deep\\nneural network trained on simulated data is capable of learning the solution\\nfunctions of an entire family of PDEs on a full space-time region. Most\\nnotably, our numerical observations and theoretical results also demonstrate\\nthat the proposed method does not suffer from the curse of dimensionality,\\ndistinguishing it from almost all standard numerical methods for PDEs. Classical machine learning implicitly assumes that labels of the training\\ndata are sampled from a clean distribution, which can be too restrictive for\\nreal-world scenarios. However, statistical-learning-based methods may not train\\ndeep learning models robustly with these noisy labels. Therefore, it is urgent\\nto design Label-Noise Representation Learning (LNRL) methods for robustly\\ntraining deep models with noisy labels. To fully understand LNRL, we conduct a\\nsurvey study. We first clarify a formal definition for LNRL from the\\nperspective of machine learning. Then, via the lens of learning theory and\\nempirical study, we figure out why noisy labels affect deep models'\\nperformance. Based on the theoretical guidance, we categorize different LNRL\\nmethods into three directions. Under this unified taxonomy, we provide a\\nthorough discussion of the pros and cons of different categories. More\\nimportantly, we summarize the essential components of robust LNRL, which can\\nspark new directions. Lastly, we propose possible research directions within\\nLNRL, such as new datasets, instance-dependent LNRL, and adversarial LNRL. We\\nalso envision potential directions beyond LNRL, such as learning with\\nfeature-noise, preference-noise, domain-noise, similarity-noise, graph-noise\\nand demonstration-noise. Various iterative reconstruction algorithms for inverse problems can be\\nunfolded as neural networks. Empirically, this approach has often led to\\nimproved results, but theoretical guarantees are still scarce. While some\\nprogress on generalization properties of neural networks have been made, great\\nchallenges remain. In this chapter, we discuss and combine these topics to\\npresent a generalization error analysis for a class of neural networks suitable\\nfor sparse reconstruction from few linear measurements. The hypothesis class\\nconsidered is inspired by the classical iterative soft-thresholding algorithm\\n(ISTA). The neural networks in this class are obtained by unfolding iterations\\nof ISTA and learning some of the weights. Based on training samples, we aim at\\nlearning the optimal network parameters via empirical risk minimization and\\nthereby the optimal network that reconstructs signals from their compressive\\nlinear measurements. In particular, we may learn a sparsity basis that is\\nshared by all of the iterations/layers and thereby obtain a new approach for\\ndictionary learning. For this class of networks, we present a generalization\\nbound, which is based on bounding the Rademacher complexity of hypothesis\\nclasses consisting of such deep networks via Dudley's integral. Remarkably,\\nunder realistic conditions, the generalization error scales only\\nlogarithmically in the number of layers, and at most linear in number of\\nmeasurements. To date, there has been no formal study of the statistical cost of\\ninterpretability in machine learning. As such, the discourse around potential\\ntrade-offs is often informal and misconceptions abound. In this work, we aim to\\ninitiate a formal study of these trade-offs. A seemingly insurmountable\\nroadblock is the lack of any agreed upon definition of interpretability.\\nInstead, we propose a shift in perspective. Rather than attempt to define\\ninterpretability, we propose to model the \\\\emph{act} of \\\\emph{enforcing}\\ninterpretability. As a starting point, we focus on the setting of empirical\\nrisk minimization for binary classification, and view interpretability as a\\nconstraint placed on learning. That is, we assume we are given a subset of\\nhypothesis that are deemed to be interpretable, possibly depending on the data\\ndistribution and other aspects of the context. We then model the act of\\nenforcing interpretability as that of performing empirical risk minimization\\nover the set of interpretable hypotheses. This model allows us to reason about\\nthe statistical implications of enforcing interpretability, using known results\\nin statistical learning theory. Focusing on accuracy, we perform a case\\nanalysis, explaining why one may or may not observe a trade-off between\\naccuracy and interpretability when the restriction to interpretable classifiers\\ndoes or does not come at the cost of some excess statistical risk. We close\\nwith some worked examples and some open problems, which we hope will spur\\nfurther theoretical development around the tradeoffs involved in\\ninterpretability. Neural networks have achieved remarkable successes in machine learning tasks.\\nThis has recently been extended to graph learning using neural networks.\\nHowever, there is limited theoretical work in understanding how and when they\\nperform well, especially relative to established statistical learning\\ntechniques such as spectral embedding. In this short paper, we present a simple\\ngenerative model where unsupervised graph convolutional network fails, while\\nthe adjacency spectral embedding succeeds. Specifically, unsupervised graph\\nconvolutional network is unable to look beyond the first eigenvector in certain\\napproximately regular graphs, thus missing inference signals in non-leading\\neigenvectors. The phenomenon is demonstrated by visual illustrations and\\ncomprehensive simulations. The theoretical analysis of deep neural networks (DNN) is arguably among the\\nmost challenging research directions in machine learning (ML) right now, as it\\nrequires from scientists to lay novel statistical learning foundations to\\nexplain their behaviour in practice. While some success has been achieved\\nrecently in this endeavour, the question on whether DNNs can be analyzed using\\nthe tools from other scientific fields outside the ML community has not\\nreceived the attention it may well have deserved. In this paper, we explore the\\ninterplay between DNNs and game theory (GT), and show how one can benefit from\\nthe classic readily available results from the latter when analyzing the\\nformer. In particular, we consider the widely studied class of congestion\\ngames, and illustrate their intrinsic relatedness to both linear and non-linear\\nDNNs and to the properties of their loss surface. Beyond retrieving the\\nstate-of-the-art results from the literature, we argue that our work provides a\\nvery promising novel tool for analyzing the DNNs and support this claim by\\nproposing concrete open problems that can advance significantly our\\nunderstanding of DNNs when solved. We consider bounds on the generalization performance of the least-norm linear\\nregressor, in the over-parameterized regime where it can interpolate the data.\\nWe describe a sense in which any generalization bound of a type that is\\ncommonly proved in statistical learning theory must sometimes be very loose\\nwhen applied to analyze the least-norm interpolant. In particular, for a\\nvariety of natural joint distributions on training examples, any valid\\ngeneralization bound that depends only on the output of the learning algorithm,\\nthe number of training examples, and the confidence parameter, and that\\nsatisfies a mild condition (substantially weaker than monotonicity in sample\\nsize), must sometimes be very loose -- it can be bounded below by a constant\\nwhen the true excess risk goes to zero. In this work we addressed the problem of capturing sequential information\\ncontained in longitudinal electronic health records (EHRs). Clinical notes,\\nwhich is a particular type of EHR data, are a rich source of information and\\npractitioners often develop clever solutions how to maximise the sequential\\ninformation contained in free-texts. We proposed a systematic methodology for\\nlearning from chronological events available in clinical notes. The proposed\\nmethodological {\\\\it path signature} framework creates a non-parametric\\nhierarchical representation of sequential events of any type and can be used as\\nfeatures for downstream statistical learning tasks. The methodology was\\ndeveloped and externally validated using the largest in the UK secondary care\\nmental health EHR data on a specific task of predicting survival risk of\\npatients diagnosed with Alzheimer's disease. The signature-based model was\\ncompared to a common survival random forest model. Our results showed a\\n15.4$\\\\%$ increase of risk prediction AUC at the time point of 20 months after\\nthe first admission to a specialist memory clinic and the signature method\\noutperformed the baseline mixed-effects model by 13.2 $\\\\%$. Cardio/cerebrovascular diseases (CVD) have become one of the major health\\nissue in our societies. But recent studies show that the present pathology\\ntests to detect CVD are ineffectual as they do not consider different stages of\\nplatelet activation or the molecular dynamics involved in platelet interactions\\nand are incapable to consider inter-individual variability. Here we propose a\\nstochastic platelet deposition model and an inferential scheme to estimate the\\nbiologically meaningful model parameters using approximate Bayesian computation\\nwith a summary statistic that maximally discriminates between different types\\nof patients. Inferred parameters from data collected on healthy volunteers and\\ndifferent patient types help us to identify specific biological parameters and\\nhence biological reasoning behind the dysfunction for each type of patients.\\nThis work opens up an unprecedented opportunity of personalized pathology test\\nfor CVD detection and medical treatment. Networked control systems have gained considerable attention over the last\\ndecade as a result of the trend towards decentralised control applications and\\nthe emergence of cyber-physical system applications. However, real-world\\nwireless networked control systems suffer from limited communication\\nbandwidths, reliability issues, and a lack of awareness of network dynamics due\\nto the complex nature of wireless networks. Combining machine learning and\\nevent-triggered control has the potential to alleviate some of these issues.\\nFor example, machine learning can be used to overcome the problem of a lack of\\nnetwork models by learning system behavior or adapting to dynamically changing\\nmodels by continuously learning model dynamics. Event-triggered control can\\nhelp to conserve communication bandwidth by transmitting control information\\nonly when necessary or when resources are available. The purpose of this\\narticle is to conduct a review of the literature on the use of machine learning\\nin combination with event-triggered control. Machine learning techniques such\\nas statistical learning, neural networks, and reinforcement learning-based\\napproaches such as deep reinforcement learning are being investigated in\\ncombination with event-triggered control. We discuss how these learning\\nalgorithms can be used for different applications depending on the purpose of\\nthe machine learning use. Following the review and discussion of the\\nliterature, we highlight open research questions and challenges associated with\\nmachine learning-based event-triggered control and suggest potential solutions. As one of the triumphs and milestones of robust statistics, Huber regression\\nplays an important role in robust inference and estimation. It has also been\\nfinding a great variety of applications in machine learning. In a parametric\\nsetup, it has been extensively studied. However, in the statistical learning\\ncontext where a function is typically learned in a nonparametric way, there is\\nstill a lack of theoretical understanding of how Huber regression estimators\\nlearn the conditional mean function and why it works in the absence of\\nlight-tailed noise assumptions. To address these fundamental questions, we\\nconduct an assessment of Huber regression from a statistical learning\\nviewpoint. First, we show that the usual risk consistency property of Huber\\nregression estimators, which is usually pursued in machine learning, cannot\\nguarantee their learnability in mean regression. Second, we argue that Huber\\nregression should be implemented in an adaptive way to perform mean regression,\\nimplying that one needs to tune the scale parameter in accordance with the\\nsample size and the moment condition of the noise. Third, with an adaptive\\nchoice of the scale parameter, we demonstrate that Huber regression estimators\\ncan be asymptotic mean regression calibrated under $(1+\\\\epsilon)$-moment\\nconditions ($\\\\epsilon>0$). Last but not least, under the same moment\\nconditions, we establish almost sure convergence rates for Huber regression\\nestimators. Note that the $(1+\\\\epsilon)$-moment conditions accommodate the\\nspecial case where the response variable possesses infinite variance and so the\\nestablished convergence rates justify the robustness feature of Huber\\nregression estimators. In the above senses, the present study provides a\\nsystematic statistical learning assessment of Huber regression estimators and\\njustifies their merits in terms of robustness from a theoretical viewpoint. Nesterov's accelerated gradient (AG) is a popular technique to optimize\\nobjective functions comprising two components: a convex loss and a penalty\\nfunction. While AG methods perform well for convex penalties, such as the\\nLASSO, convergence issues may arise when it is applied to nonconvex penalties,\\nsuch as SCAD. A recent proposal generalizes Nesterov's AG method to the\\nnonconvex setting. The proposed algorithm requires specification of several\\nhyperparameters for its practical application. Aside from some general\\nconditions, there is no explicit rule for selecting the hyperparameters, and\\nhow different selection can affect convergence of the algorithm. In this\\narticle, we propose a hyperparameter setting based on the complexity upper\\nbound to accelerate convergence, and consider the application of this nonconvex\\nAG algorithm to high-dimensional linear and logistic sparse learning problems.\\nWe further establish the rate of convergence and present a simple and useful\\nbound to characterize our proposed optimal damping sequence. Simulation studies\\nshow that convergence can be made, on average, considerably faster than that of\\nthe conventional proximal gradient algorithm. Our experiments also show that\\nthe proposed method generally outperforms the current state-of-the-art methods\\nin terms of signal recovery. This paper makes a selective survey on the recent development of the factor\\nmodel and its application on statistical learnings. We focus on the perspective\\nof the low-rank structure of factor models, and particularly draws attentions\\nto estimating the model from the low-rank recovery point of view. The survey\\nmainly consists of three parts: the first part is a review on new factor\\nestimations based on modern techniques on recovering low-rank structures of\\nhigh-dimensional models. The second part discusses statistical inferences of\\nseveral factor-augmented models and applications in econometric learning\\nmodels. The final part summarizes new developments dealing with unbalanced\\npanels from the matrix completion perspective. The Spatio-Temporal Traffic Prediction (STTP) problem is a classical problem\\nwith plenty of prior research efforts that benefit from traditional statistical\\nlearning and recent deep learning approaches. While STTP can refer to many\\nreal-world problems, most existing studies focus on quite specific\\napplications, such as the prediction of taxi demand, ridesharing order, traffic\\nspeed, and so on. This hinders the STTP research as the approaches designed for\\ndifferent applications are hardly comparable, and thus how an\\napplication-driven approach can be generalized to other scenarios is unclear.\\nTo fill in this gap, this paper makes three efforts: (i) we propose an analytic\\nframework, called STAnalytic, to qualitatively investigate STTP approaches\\nregarding their design considerations on various spatial and temporal factors,\\naiming to make different application-driven approaches comparable; (ii) we\\ndesign a spatio-temporal meta-model, called STMeta, which can flexibly\\nintegrate generalizable temporal and spatial knowledge identified by\\nSTAnalytic, (iii) we build an STTP benchmark platform including ten real-life\\ndatasets with five scenarios to quantitatively measure the generalizability of\\nSTTP approaches. In particular, we implement STMeta with different deep\\nlearning techniques, and STMeta demonstrates better generalizability than\\nstate-of-the-art approaches by achieving lower prediction error on average\\nacross all the datasets. Neural networks have been achieving high generalization performance on many\\ntasks despite being highly over-parameterized. Since classical statistical\\nlearning theory struggles to explain this behavior, much effort has recently\\nbeen focused on uncovering the mechanisms behind it, in the hope of developing\\na more adequate theoretical framework and having a better control over the\\ntrained models. In this work, we adopt an alternate perspective, viewing the\\nneural network as a dynamical system displacing input particles over time. We\\nconduct a series of experiments and, by analyzing the network's behavior\\nthrough its displacements, we show the presence of a low kinetic energy\\ndisplacement bias in the transport map of the network, and link this bias with\\ngeneralization performance. From this observation, we reformulate the learning\\nproblem as follows: finding neural networks which solve the task while\\ntransporting the data as efficiently as possible. This offers a novel\\nformulation of the learning problem which allows us to provide regularity\\nresults for the solution network, based on Optimal Transport theory. From a\\npractical viewpoint, this allows us to propose a new learning algorithm, which\\nautomatically adapts to the complexity of the given task, and leads to networks\\nwith a high generalization ability even in low data regimes. Many statistical learning models hold an assumption that the training data\\nand the future unlabeled data are drawn from the same distribution. However,\\nthis assumption is difficult to fulfill in real-world scenarios and creates\\nbarriers in reusing existing labels from similar application domains. Transfer\\nLearning is intended to relax this assumption by modeling relationships between\\ndomains, and is often applied in deep learning applications to reduce the\\ndemand for labeled data and training time. Despite recent advances in exploring\\ndeep learning models with visual analytics tools, little work has explored the\\nissue of explaining and diagnosing the knowledge transfer process between deep\\nlearning models. In this paper, we present a visual analytics framework for the\\nmulti-level exploration of the transfer learning processes when training deep\\nneural networks. Our framework establishes a multi-aspect design to explain how\\nthe learned knowledge from the existing model is transferred into the new\\nlearning task when training deep neural networks. Based on a comprehensive\\nrequirement and task analysis, we employ descriptive visualization with\\nperformance measures and detailed inspections of model behaviors from the\\nstatistical, instance, feature, and model structure levels. We demonstrate our\\nframework through two case studies on image classification by fine-tuning\\nAlexNets to illustrate how analysts can utilize our framework. Large observational data are increasingly available in disciplines such as\\nhealth, economic and social sciences, where researchers are interested in\\ncausal questions rather than prediction. In this paper, we examine the problem\\nof estimating heterogeneous treatment effects using non-parametric\\nregression-based methods, starting from an empirical study aimed at\\ninvestigating the effect of participation in school meal programs on health\\nindicators. Firstly, we introduce the setup and the issues related to\\nconducting causal inference with observational or non-fully randomized data,\\nand how these issues can be tackled with the help of statistical learning\\ntools. Then, we review and develop a unifying taxonomy of the existing\\nstate-of-the-art frameworks that allow for individual treatment effects\\nestimation via non-parametric regression models. After presenting a brief\\noverview on the problem of model selection, we illustrate the performance of\\nsome of the methods on three different simulated studies. We conclude by\\ndemonstrating the use of some of the methods on an empirical analysis of the\\nschool meal program data. In several research problems we deal with probabilistic sequences of inputs\\n(e.g., sequence of stimuli) from which an agent generates a corresponding\\nsequence of responses and it is of interest to model the relation between them.\\nA new class of stochastic processes, namely \\\\textit{sequences of random objects\\ndriven by context tree models}, has been introduced to model such relation in\\nthe context of auditory statistical learning. This paper introduces a freely\\navailable Matlab toolbox (SeqROCTM) that implements this new class of\\nstochastic processes and three model selection procedures to make inference on\\nit. Besides, due to the close relation of the new mathematical framework with\\ncontext tree models, the toolbox also implements several existing model\\nselection algorithms for context tree models. For a learning task, Gaussian process (GP) is interested in learning the\\nstatistical relationship between inputs and outputs, since it offers not only\\nthe prediction mean but also the associated variability. The vanilla GP however\\nstruggles to learn complicated distribution with the property of, e.g.,\\nheteroscedastic noise, multi-modality and non-stationarity, from massive data\\ndue to the Gaussian marginal and the cubic complexity. To this end, this\\narticle studies new scalable GP paradigms including the non-stationary\\nheteroscedastic GP, the mixture of GPs and the latent GP, which introduce\\nadditional latent variables to modulate the outputs or inputs in order to learn\\nricher, non-Gaussian statistical representation. We further resort to different\\nvariational inference strategies to arrive at analytical or tighter evidence\\nlower bounds (ELBOs) of the marginal likelihood for efficient and effective\\nmodel training. Extensive numerical experiments against state-of-the-art GP and\\nneural network (NN) counterparts on various tasks verify the superiority of\\nthese scalable modulated GPs, especially the scalable latent GP, for learning\\ndiverse data distributions. The fundamental result of Li, Long, and Srinivasan on approximations of set\\nsystems has become a key tool across several communities such as learning\\ntheory, algorithms, computational geometry, combinatorics and data analysis.\\n  The goal of this paper is to give a modular, self-contained, intuitive proof\\nof this result for finite set systems. The only ingredient we assume is the\\nstandard Chernoff's concentration bound. This makes the proof accessible to a\\nwider audience, readers not familiar with techniques from statistical learning\\ntheory, and makes it possible to be covered in a single self-contained lecture\\nin a geometry, algorithms or combinatorics course. Understanding the complex structure of multivariate extremes is a major\\nchallenge in various fields from portfolio monitoring and environmental risk\\nmanagement to insurance. In the framework of multivariate Extreme Value Theory,\\na common characterization of extremes' dependence structure is the angular\\nmeasure. It is a suitable measure to work in extreme regions as it provides\\nmeaningful insights concerning the subregions where extremes tend to\\nconcentrate their mass. The present paper develops a novel optimization-based\\napproach to assess the dependence structure of extremes. This support\\nidentification scheme rewrites as estimating clusters of features which best\\ncapture the support of extremes. The dimension reduction technique we provide\\nis applied to statistical learning tasks such as feature clustering and anomaly\\ndetection. Numerical experiments provide strong empirical evidence of the\\nrelevance of our approach. We developed MLHO (pronounced as melo), an end-to-end Machine Learning\\nframework that leverages iterative feature and algorithm selection to predict\\nHealth Outcomes. MLHO implements iterative sequential representation mining,\\nand feature and model selection, for predicting the patient-level risk of\\nhospitalization, ICU admission, need for mechanical ventilation, and death. It\\nbases this prediction on data from patients' past medical records (before their\\nCOVID-19 infection). MLHO's architecture enables a parallel and\\noutcome-oriented model calibration, in which different statistical learning\\nalgorithms and vectors of features are simultaneously tested to improve the\\nprediction of health outcomes. Using clinical and demographic data from a large\\ncohort of over 13,000 COVID-19-positive patients, we modeled the four adverse\\noutcomes utilizing about 600 features representing patients' pre-COVID health\\nrecords and demographics. The mean AUC ROC for mortality prediction was 0.91,\\nwhile the prediction performance ranged between 0.80 and 0.81 for the ICU,\\nhospitalization, and ventilation. We broadly describe the clusters of features\\nthat were utilized in modeling and their relative influence for predicting each\\noutcome. Our results demonstrated that while demographic variables (namely age)\\nare important predictors of adverse outcomes after a COVID-19 infection, the\\nincorporation of the past clinical records are vital for a reliable prediction\\nmodel. As the COVID-19 pandemic unfolds around the world, adaptable and\\ninterpretable machine learning frameworks (like MLHO) are crucial to improve\\nour readiness for confronting the potential future waves of COVID-19, as well\\nas other novel infectious diseases that may emerge. To understand the black-box characteristics of deep networks, counterfactual\\nexplanation that deduces not only the important features of an input space but\\nalso how those features should be modified to classify input as a target class\\nhas gained an increasing interest. The patterns that deep networks have learned\\nfrom a training dataset can be grasped by observing the feature variation among\\nvarious classes. However, current approaches perform the feature modification\\nto increase the classification probability for the target class irrespective of\\nthe internal characteristics of deep networks. This often leads to unclear\\nexplanations that deviate from real-world data distributions. To address this\\nproblem, we propose a counterfactual explanation method that exploits the\\nstatistics learned from a training dataset. Especially, we gradually construct\\nan explanation by iterating over masking and composition steps. The masking\\nstep aims to select an important feature from the input data to be classified\\nas a target class. Meanwhile, the composition step aims to optimize the\\npreviously selected feature by ensuring that its output score is close to the\\nlogit space of the training data that are classified as the target class.\\nExperimental results show that our method produces human-friendly\\ninterpretations on various classification datasets and verify that such\\ninterpretations can be achieved with fewer feature modification. In this paper, we show that, in vector-to-vector regression utilizing deep\\nneural networks (DNNs), a generalized loss of mean absolute error (MAE) between\\nthe predicted and expected feature vectors is upper bounded by the sum of an\\napproximation error, an estimation error, and an optimization error. Leveraging\\nupon error decomposition techniques in statistical learning theory and\\nnon-convex optimization theory, we derive upper bounds for each of the three\\naforementioned errors and impose necessary constraints on DNN models. Moreover,\\nwe assess our theoretical results through a set of image de-noising and speech\\nenhancement experiments. Our proposed upper bounds of MAE for DNN based\\nvector-to-vector regression are corroborated by the experimental results and\\nthe upper bounds are valid with and without the \\\"over-parametrization\\\"\\ntechnique. We propose a robust parameter estimation method for dynamical systems based\\non Statistical Learning techniques which aims to estimate a set of parameters\\nthat well fit the dynamics in order to obtain robust evidences about the\\nqualitative behaviour of its trajectory. The method is quite general and\\nflexible, since it does not rely on any specific property of the dynamical\\nsystem, and represents a reinterpretation of Approximate Bayesian Computation\\nmethods through the lens of Statistical Learning. The method is specially\\nuseful for estimating parameters in epidemiological compartmental models in\\norder to obtain qualitative properties of a disease evolution. We apply it to\\nsimulated and real data about COVID-19 spread in the US in order to evaluate\\nqualitatively its evolution over time, showing how one may assess the\\neffectiveness of measures implemented to slow the spread and some qualitative\\nfeatures of the disease current and future evolution. Characterising intractable high-dimensional random variables is one of the\\nfundamental challenges in stochastic computation. The recent surge of transport\\nmaps offers a mathematical foundation and new insights for tackling this\\nchallenge by coupling intractable random variables with tractable reference\\nrandom variables. This paper generalises the functional tensor-train\\napproximation of the inverse Rosenblatt transport recently developed by Dolgov\\net al. (Stat Comput 30:603--625, 2020) to a wide class of high-dimensional\\nnon-negative functions, such as unnormalised probability density functions.\\nFirst, we extend the inverse Rosenblatt transform to enable the transport to\\ngeneral reference measures other than the uniform measure. We develop an\\nefficient procedure to compute this transport from a squared tensor-train\\ndecomposition which preserves the monotonicity. More crucially, we integrate\\nthe proposed order-preserving functional tensor-train transport into a nested\\nvariable transformation framework inspired by the layered structure of deep\\nneural networks. The resulting deep inverse Rosenblatt transport significantly\\nexpands the capability of tensor approximations and transport maps to random\\nvariables with complicated nonlinear interactions and concentrated density\\nfunctions. We demonstrate the efficiency of the proposed approach on a range of\\napplications in statistical learning and uncertainty quantification, including\\nparameter estimation for dynamical systems and inverse problems constrained by\\npartial differential equations. As machine learning is increasingly used in essential systems, it is\\nimportant to reduce or eliminate the incidence of serious bugs. A growing body\\nof research has developed machine learning algorithms with formal guarantees\\nabout performance, robustness, or fairness. Yet, the analysis of these\\nalgorithms is often complex, and implementing such systems in practice\\nintroduces room for error. Proof assistants can be used to formally verify\\nmachine learning systems by constructing machine checked proofs of correctness\\nthat rule out such bugs. However, reasoning about probabilistic claims inside\\nof a proof assistant remains challenging. We show how a probabilistic program\\ncan be automatically represented in a theorem prover using the concept of\\n\\\\emph{reparameterization}, and how some of the tedious proofs of measurability\\ncan be generated automatically from the probabilistic program. To demonstrate\\nthat this approach is broad enough to handle rather different types of machine\\nlearning systems, we verify both a classic result from statistical learning\\ntheory (PAC-learnability of decision stumps) and prove that the null model used\\nin a Bayesian hypothesis test satisfies a fairness criterion called demographic\\nparity. Devising dynamic pricing policy with always valid online statistical learning\\nprocedure is an important and as yet unresolved problem. Most existing dynamic\\npricing policy, which focus on the faithfulness of adopted customer choice\\nmodels, exhibit a limited capability for adapting the online uncertainty of\\nlearned statistical model during pricing process. In this paper, we propose a\\nnovel approach for designing dynamic pricing policy based regularized online\\nstatistical learning with theoretical guarantees. The new approach overcomes\\nthe challenge of continuous monitoring of online Lasso procedure and possesses\\nseveral appealing properties. In particular, we make the decisive observation\\nthat the always-validity of pricing decisions builds and thrives on the online\\nregularization scheme. Our proposed online regularization scheme equips the\\nproposed optimistic online regularized maximum likelihood pricing (OORMLP)\\npricing policy with three major advantages: encode market noise knowledge into\\npricing process optimism; empower online statistical learning with\\nalways-validity over all decision points; envelop prediction error process with\\ntime-uniform non-asymptotic oracle inequalities. This type of non-asymptotic\\ninference results allows us to design more sample-efficient and robust dynamic\\npricing algorithms in practice. In theory, the proposed OORMLP algorithm\\nexploits the sparsity structure of high-dimensional models and secures a\\nlogarithmic regret in a decision horizon. These theoretical advances are made\\npossible by proposing an optimistic online Lasso procedure that resolves\\ndynamic pricing problems at the process level, based on a novel use of\\nnon-asymptotic martingale concentration. In experiments, we evaluate OORMLP in\\ndifferent synthetic and real pricing problem settings, and demonstrate that\\nOORMLP advances the state-of-the-art methods. This paper investigates the optimal resource allocation in free space optical\\n(FSO) fronthaul networks. The optimal allocation maximizes an average weighted\\nsum-capacity subject to power limitation and data congestion constraints. Both\\nadaptive power assignment and node selection are considered based on the\\ninstantaneous channel state information (CSI) of the links. By parameterizing\\nthe resource allocation policy, we formulate the problem as an unsupervised\\nstatistical learning problem. We consider the graph neural network (GNN) for\\nthe policy parameterization to exploit the FSO network structure with\\nsmall-scale training parameters. The GNN is shown to retain the permutation\\nequivariance that matches with the permutation equivariance of resource\\nallocation policy in networks. The primal-dual learning algorithm is developed\\nto train the GNN in a model-free manner, where the knowledge of system models\\nis not required. Numerical simulations present the strong performance of the\\nGNN relative to a baseline policy with equal power assignment and random node\\nselection. Empirical risk minimization is perhaps the most influential idea in\\nstatistical learning, with applications to nearly all scientific and technical\\ndomains in the form of regression and classification models. To analyze massive\\nstreaming datasets in distributed computing environments, practitioners\\nincreasingly prefer to deploy regression models on edge rather than in the\\ncloud. By keeping data on edge devices, we minimize the energy, communication,\\nand data security risk associated with the model. Although it is equally\\nadvantageous to train models at the edge, a common assumption is that the model\\nwas originally trained in the cloud, since training typically requires\\nsubstantial computation and memory. To this end, we propose STORM, an online\\nsketch for empirical risk minimization. STORM compresses a data stream into a\\ntiny array of integer counters. This sketch is sufficient to estimate a variety\\nof surrogate losses over the original dataset. We provide rigorous theoretical\\nanalysis and show that STORM can estimate a carefully chosen surrogate loss for\\nthe least-squares objective. In an exhaustive experimental comparison for\\nlinear regression models on real-world datasets, we find that STORM allows\\naccurate regression models to be trained. Concept Drift (CD) detection intends to continuously identify changes in data\\nstream behaviors, supporting researchers in the study and modeling of\\nreal-world phenomena. Motivated by the lack of learning guarantees in current\\nCD algorithms, we decided to take advantage of the Statistical Learning Theory\\n(SLT) to formalize the necessary requirements to ensure probabilistic learning\\nbounds, so drifts would refer to actual changes in data rather than by chance.\\nAs discussed along this paper, a set of mathematical assumptions must be held\\nin order to rely on SLT bounds, which are especially controversial in CD\\nscenarios. Based on this issue, we propose a methodology to address those\\nassumptions in CD scenarios and therefore ensure learning guarantees.\\nComplementary, we assessed a set of relevant and known CD algorithms from the\\nliterature in light of our methodology. As main contribution, we expect this\\nwork to support researchers while designing and evaluating CD algorithms on\\ndifferent domains. Within the machine learning community, the widely-used uniform convergence\\nframework has been used to answer the question of how complex,\\nover-parameterized models can generalize well to new data. This approach bounds\\nthe test error of the worst-case model one could have fit to the data, but it\\nhas fundamental limitations. Inspired by the statistical mechanics approach to\\nlearning, we formally define and develop a methodology to compute precisely the\\nfull distribution of test errors among interpolating classifiers from several\\nmodel classes. We apply our method to compute this distribution for several\\nreal and synthetic datasets, with both linear and random feature classification\\nmodels. We find that test errors tend to concentrate around a small typical\\nvalue $\\\\varepsilon^*$, which deviates substantially from the test error of the\\nworst-case interpolating model on the same datasets, indicating that \\\"bad\\\"\\nclassifiers are extremely rare. We provide theoretical results in a simple\\nsetting in which we characterize the full asymptotic distribution of test\\nerrors, and we show that these indeed concentrate around a value\\n$\\\\varepsilon^*$, which we also identify exactly. We then formalize a more\\ngeneral conjecture supported by our empirical findings. Our results show that\\nthe usual style of analysis in statistical learning theory may not be\\nfine-grained enough to capture the good generalization performance observed in\\npractice, and that approaches based on the statistical mechanics of learning\\nmay offer a promising alternative. Stemming from information-theoretic learning, the correntropy criterion and\\nits applications to machine learning tasks have been extensively explored and\\nstudied. Its application to regression problems leads to the robustness\\nenhanced regression paradigm -- namely, correntropy based regression. Having\\ndrawn a great variety of successful real-world applications, its theoretical\\nproperties have also been investigated recently in a series of studies from a\\nstatistical learning viewpoint. The resulting big picture is that correntropy\\nbased regression regresses towards the conditional mode function or the\\nconditional mean function robustly under certain conditions. Continuing this\\ntrend and going further, in the present study, we report some new insights into\\nthis problem. First, we show that under the additive noise regression model,\\nsuch a regression paradigm can be deduced from minimum distance estimation,\\nimplying that the resulting estimator is essentially a minimum distance\\nestimator and thus possesses robustness properties. Second, we show that the\\nregression paradigm, in fact, provides a unified approach to regression\\nproblems in that it approaches the conditional mean, the conditional mode, as\\nwell as the conditional median functions under certain conditions. Third, we\\npresent some new results when it is utilized to learn the conditional mean\\nfunction by developing its error bounds and exponential convergence rates under\\nconditional $(1+\\\\epsilon)$-moment assumptions. The saturation effect on the\\nestablished convergence rates, which was observed under $(1+\\\\epsilon)$-moment\\nassumptions, still occurs, indicating the inherent bias of the regression\\nestimator. These novel insights deepen our understanding of correntropy based\\nregression, help cement the theoretic correntropy framework, and also enable us\\nto investigate learning schemes induced by general bounded nonconvex loss\\nfunctions. In statistical learning, algorithms for model selection allow the learner to\\nadapt to the complexity of the best hypothesis class in a sequence. We ask\\nwhether similar guarantees are possible for contextual bandit learning. Complexity is a fundamental concept underlying statistical learning theory\\nthat aims to inform generalization performance. Parameter count, while\\nsuccessful in low-dimensional settings, is not well-justified for\\noverparameterized settings when the number of parameters is more than the\\nnumber of training samples. We revisit complexity measures based on Rissanen's\\nprinciple of minimum description length (MDL) and define a novel MDL-based\\ncomplexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP\\nis defined via an optimality criterion over the encodings induced by a good\\nRidge estimator class. We provide an extensive theoretical characterization of\\nMDL-COMP for linear models and kernel methods and show that it is not just a\\nfunction of parameter count, but rather a function of the singular values of\\nthe design or the kernel matrix and the signal-to-noise ratio. For a linear\\nmodel with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors,\\nMDL-COMP scales linearly with $d$ when $d<n$, but the scaling is exponentially\\nsmaller -- $\\\\log d$ for $d>n$. For kernel methods, we show that MDL-COMP\\ninforms minimax in-sample error, and can decrease as the dimensionality of the\\ninput increases. We also prove that MDL-COMP upper bounds the in-sample mean\\nsquared error (MSE). Via an array of simulations and real-data experiments, we\\nshow that a data-driven Prac-MDL-COMP informs hyper-parameter tuning for\\noptimizing test MSE with ridge regression in limited data settings, sometimes\\nimproving upon cross-validation and (always) saving computational costs.\\nFinally, our findings also suggest that the recently observed double decent\\nphenomenons in overparameterized models might be a consequence of the choice of\\nnon-ideal estimators. The main question is: why and how can we ever predict based on a finite\\nsample? The question is not answered by statistical learning theory. Here, I\\nsuggest that prediction requires belief in \\\"predictability\\\" of the underlying\\ndependence, and learning involves search for a hypothesis where these beliefs\\nare violated the least given the observations. The measure of these violations\\n(\\\"errors\\\") for given data, hypothesis and particular type of predictability\\nbeliefs is formalized as concept of incongruity in modal Logic of Observations\\nand Hypotheses (LOH). I show on examples of many popular textbook learners\\n(from hierarchical clustering to k-NN and SVM) that each of them minimizes its\\nown version of incongruity. In addition, the concept of incongruity is shown to\\nbe flexible enough for formalization of some important data analysis problems,\\nnot considered as part of ML. Variational representations of divergences and distances between\\nhigh-dimensional probability distributions offer significant theoretical\\ninsights and practical advantages in numerous research areas. Recently, they\\nhave gained popularity in machine learning as a tractable and scalable approach\\nfor training probabilistic models and for statistically differentiating between\\ndata distributions. Their advantages include: 1) They can be estimated from\\ndata as statistical averages. 2) Such representations can leverage the ability\\nof neural networks to efficiently approximate optimal solutions in function\\nspaces. However, a systematic and practical approach to improving the tightness\\nof such variational formulas, and accordingly accelerate statistical learning\\nand estimation from data, is currently lacking. Here we develop such a\\nmethodology for building new, tighter variational representations of\\ndivergences. Our approach relies on improved objective functionals constructed\\nvia an auxiliary optimization problem. Furthermore, the calculation of the\\nfunctional Hessian of objective functionals unveils the local curvature\\ndifferences around the common optimal variational solution; this quantifies and\\norders the tightness gains between different variational representations.\\nFinally, numerical simulations utilizing neural network optimization\\ndemonstrate that tighter representations can result in significantly faster\\nlearning and more accurate estimation of divergences in both synthetic and real\\ndatasets (of more than 1000 dimensions), often accelerated by nearly an order\\nof magnitude. We develop an approach for estimating models described via conditional moment\\nrestrictions, with a prototypical application being non-parametric instrumental\\nvariable regression. We introduce a min-max criterion function, under which the\\nestimation problem can be thought of as solving a zero-sum game between a\\nmodeler who is optimizing over the hypothesis space of the target model and an\\nadversary who identifies violating moments over a test function space. We\\nanalyze the statistical estimation rate of the resulting estimator for\\narbitrary hypothesis spaces, with respect to an appropriate analogue of the\\nmean squared error metric, for ill-posed inverse problems. We show that when\\nthe minimax criterion is regularized with a second moment penalty on the test\\nfunction and the test function space is sufficiently rich, then the estimation\\nrate scales with the critical radius of the hypothesis and test function\\nspaces, a quantity which typically gives tight fast rates. Our main result\\nfollows from a novel localized Rademacher analysis of statistical learning\\nproblems defined via minimax objectives. We provide applications of our main\\nresults for several hypothesis spaces used in practice such as: reproducing\\nkernel Hilbert spaces, high dimensional sparse linear functions, spaces defined\\nvia shape constraints, ensemble estimators such as random forests, and neural\\nnetworks. For each of these applications we provide computationally efficient\\noptimization methods for solving the corresponding minimax problem (e.g.\\nstochastic first-order heuristics for neural networks). In several\\napplications, we show how our modified mean squared error rate, combined with\\nconditions that bound the ill-posedness of the inverse problem, lead to mean\\nsquared error rates. We conclude with an extensive experimental analysis of the\\nproposed methods. From the statistical learning perspective, complexity control via explicit\\nregularization is a necessity for improving the generalization of\\nover-parameterized models. However, the impressive generalization performance\\nof neural networks with only implicit regularization may be at odds with this\\nconventional wisdom. In this work, we revisit the importance of explicit\\nregularization for obtaining well-calibrated predictive uncertainty.\\nSpecifically, we introduce a probabilistic measure of calibration performance,\\nwhich is lower bounded by the log-likelihood. We then explore explicit\\nregularization techniques for improving the log-likelihood on unseen samples,\\nwhich provides well-calibrated predictive uncertainty. Our findings present a\\nnew direction to improve the predictive probability quality of deterministic\\nneural networks, which can be an efficient and scalable alternative to Bayesian\\nneural networks and ensemble methods. Machine learning techniques are used in a wide range of domains. However,\\nmachine learning models often suffer from the problem of over-fitting. Many\\ndata augmentation methods have been proposed to tackle such a problem, and one\\nof them is called mixup. Mixup is a recently proposed regularization procedure,\\nwhich linearly interpolates a random pair of training examples. This\\nregularization method works very well experimentally, but its theoretical\\nguarantee is not adequately discussed. In this study, we aim to discover why\\nmixup works well from the aspect of the statistical learning theory. In classical statistical learning theory, one of the most well studied\\nproblems is that of binary classification. The information-theoretic sample\\ncomplexity of this task is tightly characterized by the Vapnik-Chervonenkis\\n(VC) dimension. A quantum analog of this task, with training data given as a\\nquantum state has also been intensely studied and is now known to have the same\\nsample complexity as its classical counterpart.\\n  We propose a novel quantum version of the classical binary classification\\ntask by considering maps with classical input and quantum output and\\ncorresponding classical-quantum training data. We discuss learning strategies\\nfor the agnostic and for the realizable case and study their performance to\\nobtain sample complexity upper bounds. Moreover, we provide sample complexity\\nlower bounds which show that our upper bounds are essentially tight for pure\\noutput states. In particular, we see that the sample complexity is the same as\\nin the classical binary classification task w.r.t. its dependence on accuracy,\\nconfidence and the VC-dimension. Much progress has been made in semi-supervised learning (SSL) by combining\\nmethods that exploit different aspects of the data distribution, e.g.\\nconsistency regularisation relies on properties of $p(x)$, whereas entropy\\nminimisation pertains to the label distribution $p(y|x)$. Focusing on the\\nlatter, we present a probabilistic model for discriminative SSL, that mirrors\\nits classical generative counterpart. Under the assumption $y|x$ is\\ndeterministic, the prior over latent variables becomes discrete. We show that\\nseveral well-known SSL methods can be interpreted as approximating this prior,\\nand can be improved upon. We extend the discriminative model to neuro-symbolic\\nSSL, where label features satisfy logical rules, by showing such rules relate\\ndirectly to the above prior, thus justifying a family of methods that link\\nstatistical learning and logical reasoning, and unifying them with regular SSL. We propose a new class of algorithms for randomly scheduling network\\ntransmissions. The idea is to use (discrete) determinantal point processes\\n(subsets) to randomly assign medium access to various {\\\\em repulsive} subsets\\nof potential transmitters. This approach can be seen as a natural extension of\\n(spatial) Aloha, which schedules transmissions independently. Under a general\\npath loss model and Rayleigh fading, we show that, similarly to Aloha, they are\\nalso subject to elegant analysis of the coverage probabilities and transmission\\nattempts (also known as local delay). This is mainly due to the explicit,\\ndeterminantal form of the conditional (Palm) distribution and closed-form\\nexpressions for the Laplace functional of determinantal processes.\\nInterestingly, the derived performance characteristics of the network are\\namenable to various optimizations of the scheduling parameters, which are\\ndeterminantal kernels, allowing the use of techniques developed for statistical\\nlearning with determinantal processes. Well-established sampling algorithms for\\ndeterminantal processes can be used to cope with implementation issues, which\\nis is beyond the scope of this paper, but it creates paths for further\\nresearch. We consider a distributionally robust formulation of stochastic optimization\\nproblems arising in statistical learning, where robustness is with respect to\\nuncertainty in the underlying data distribution. Our formulation builds on\\nrisk-averse optimization techniques and the theory of coherent risk measures.\\nIt uses semi-deviation risk for quantifying uncertainty, allowing us to compute\\nsolutions that are robust against perturbations in the population data\\ndistribution. We consider a large family of loss functions that can be\\nnon-convex and non-smooth and develop an efficient stochastic subgradient\\nmethod. We prove that it converges to a point satisfying the optimality\\nconditions. To our knowledge, this is the first method with rigorous\\nconvergence guarantees in the context of non-convex non-smooth distributionally\\nrobust stochastic optimization. Our method can achieve any desired level of\\nrobustness with little extra computational cost compared to population risk\\nminimization. We also illustrate the performance of our algorithm on real\\ndatasets arising in convex and non-convex supervised learning problems. The notion of generalization in classical Statistical Learning is often\\nattached to the postulate that data points are independent and identically\\ndistributed (IID) random variables. While relevant in many applications, this\\npostulate may not hold in general, encouraging the development of learning\\nframeworks that are robust to non-IID data. In this work, we consider the\\nregression problem from an Optimal Recovery perspective. Relying on a model\\nassumption comparable to choosing a hypothesis class, a learner aims at\\nminimizing the worst-case error, without recourse to any probabilistic\\nassumption on the data. We first develop a semidefinite program for calculating\\nthe worst-case error of any recovery map in finite-dimensional Hilbert spaces.\\nThen, for any Hilbert space, we show that Optimal Recovery provides a formula\\nwhich is user-friendly from an algorithmic point-of-view, as long as the\\nhypothesis class is linear. Interestingly, this formula coincides with kernel\\nridgeless regression in some cases, proving that minimizing the average error\\nand worst-case error can yield the same solution. We provide numerical\\nexperiments in support of our theoretical findings. In a landmark paper published in 2001, Leo Breiman described the tense\\nstandoff between two cultures of data modeling: parametric statistical and\\nalgorithmic machine learning. The cultural division between these two\\nstatistical learning frameworks has been growing at a steady pace in recent\\nyears. What is the way forward? It has become blatantly obvious that this\\nwidening gap between \\\"the two cultures\\\" cannot be averted unless we find a way\\nto blend them into a coherent whole. This article presents a solution by\\nestablishing a link between the two cultures. Through examples, we describe the\\nchallenges and potential gains of this new integrated statistical thinking. A complex combination of simultaneous supervised-unsupervised learning is\\nbelieved to be the key to humans performing tasks seamlessly across multiple\\ndomains or tasks. This phenomenon of cross-domain learning has been very well\\nstudied in domain adaptation literature. Recent domain adaptation works rely on\\nan indirect way of first aligning the source and target domain distributions\\nand then train a classifier on the labeled source domain to classify the target\\ndomain. However, this approach has the main drawback that obtaining a\\nnear-perfect alignment of the domains in itself might be difficult/impossible\\n(e.g., language domains). To address this, we follow Vapnik's imperative of\\nstatistical learning that states any desired problem should be solved in the\\nmost direct way rather than solving a more general intermediate task and\\npropose a direct approach to domain adaptation that does not require domain\\nalignment. We propose a model referred Contradistinguisher that learns\\ncontrastive features and whose objective is to jointly learn to\\ncontradistinguish the unlabeled target domain in an unsupervised way and\\nclassify in a supervised way on the source domain. We achieve the\\nstate-of-the-art on Office-31 and VisDA-2017 datasets in both single-source and\\nmulti-source settings. We also notice that the contradistinguish loss improves\\nthe model performance by increasing the shape bias. In the context of regression, we consider the fundamental question of making\\nan estimator fair while preserving its prediction accuracy as much as possible.\\nTo that end, we define its projection to fairness as its closest fair estimator\\nin a sense that reflects prediction accuracy. Our methodology leverages tools\\nfrom optimal transport to construct efficiently the projection to fairness of\\nany given estimator as a simple post-processing step. Moreover, our approach\\nprecisely quantifies the cost of fairness, measured in terms of prediction\\naccuracy. In this paper, novel gradient-based online learning algorithms are developed\\nto investigate an important environmental application: real-time river\\npollution source identification, which aims at estimating the released mass,\\nlocation, and time of a river pollution source based on downstream sensor data\\nmonitoring the pollution concentration. The pollution is assumed to be\\ninstantaneously released once. The problem can be formulated as a non-convex\\nloss minimization problem in statistical learning, and our online algorithms\\nhave vectorized and adaptive step sizes to ensure high estimation accuracy in\\nthree dimensions which have different magnitudes. In order to keep the\\nalgorithm from stucking to the saddle points of non-convex loss, the escaping\\nfrom saddle points module and multi-start setting are derived to further\\nimprove the estimation accuracy by searching for the global minimizer of the\\nloss functions. This can be shown theoretically and experimentally as the\\n$O(N)$ local regret of the algorithms and the high probability cumulative\\nregret bound $O(N)$ under a particular error bound condition in loss functions.\\nA real-life river pollution source identification example shows the superior\\nperformance of our algorithms compared to existing methods in terms of\\nestimation accuracy. Managerial insights for the decision maker to use the\\nalgorithms are also provided. Statistical learning theory under independent and identically distributed\\n(iid) sampling and online learning theory for worst case individual sequences\\nare two of the best developed branches of learning theory. Statistical learning\\nunder general non-iid stochastic processes is less mature. We provide two\\nnatural notions of learnability of a function class under a general stochastic\\nprocess. We show that both notions are in fact equivalent to online\\nlearnability. Our results hold for both binary classification and regression. Indoor localization has become an important issue for wireless sensor\\nnetworks. This paper presents a zoning-based localization technique that uses\\nWiFi signals and works efficiently in indoor environments. The targeted area is\\ncomposed of several zones, the objective being to determine the zone of the\\nsensor using an observation model based on statistical learning. Inference capabilities of machine learning (ML) systems skyrocketed in recent\\nyears, now playing a pivotal role in various aspect of society. The goal in\\nstatistical learning is to use data to obtain simple algorithms for predicting\\na random variable $Y$ from a correlated observation $X$. Since the dimension of\\n$X$ is typically huge, computationally feasible solutions should summarize it\\ninto a lower-dimensional feature vector $T$, from which $Y$ is predicted. The\\nalgorithm will successfully make the prediction if $T$ is a good proxy of $Y$,\\ndespite the said dimensionality-reduction. A myriad of ML algorithms (mostly\\nemploying deep learning (DL)) for finding such representations $T$ based on\\nreal-world data are now available. While these methods are often effective in\\npractice, their success is hindered by the lack of a comprehensive theory to\\nexplain it. The information bottleneck (IB) theory recently emerged as a bold\\ninformation-theoretic paradigm for analyzing DL systems. Adopting mutual\\ninformation as the figure of merit, it suggests that the best representation\\n$T$ should be maximally informative about $Y$ while minimizing the mutual\\ninformation with $X$. In this tutorial we survey the information-theoretic\\norigins of this abstract principle, and its recent impact on DL. For the\\nlatter, we cover implications of the IB problem on DL theory, as well as\\npractical algorithms inspired by it. Our goal is to provide a unified and\\ncohesive description. A clear view of current knowledge is particularly\\nimportant for further leveraging IB and other information-theoretic ideas to\\nstudy DL models. All famous machine learning algorithms that comprise both supervised and\\nsemi-supervised learning work well only under a common assumption: the training\\nand test data follow the same distribution. When the distribution changes, most\\nstatistical models must be reconstructed from newly collected data, which for\\nsome applications can be costly or impossible to obtain. Therefore, it has\\nbecome necessary to develop approaches that reduce the need and the effort to\\nobtain new labeled samples by exploiting data that are available in related\\nareas, and using these further across similar fields. This has given rise to a\\nnew machine learning framework known as transfer learning: a learning setting\\ninspired by the capability of a human being to extrapolate knowledge across\\ntasks to learn more efficiently. Despite a large amount of different transfer\\nlearning scenarios, the main objective of this survey is to provide an overview\\nof the state-of-the-art theoretical results in a specific, and arguably the\\nmost popular, sub-field of transfer learning, called domain adaptation. In this\\nsub-field, the data distribution is assumed to change across the training and\\nthe test data, while the learning task remains the same. We provide a first\\nup-to-date description of existing results related to domain adaptation problem\\nthat cover learning bounds based on different statistical learning frameworks. This paper is concerned by the problem of selecting an optimal sampling set\\nof sensors over a network of time series for the purpose of signal recovery at\\nnon-observed sensors with a minimal reconstruction error. The problem is\\nmotivated by applications where time-dependent graph signals are collected over\\nredundant networks. In this setting, one may wish to only use a subset of\\nsensors to predict data streams over the whole collection of nodes in the\\nunderlying graph. A typical application is the possibility to reduce the power\\nconsumption in a network of sensors that may have limited battery supplies. We\\npropose and compare various data-driven strategies to turn off a fixed number\\nof sensors or equivalently to select a sampling set of nodes. We also relate\\nour approach to the existing literature on sensor selection from multivariate\\ndata with a (possibly) underlying graph structure. Our methodology combines\\ntools from multivariate time series analysis, graph signal processing,\\nstatistical learning in high-dimension and deep learning. To illustrate the\\nperformances of our approach, we report numerical experiments on the analysis\\nof real data from bike sharing networks in different cities. This thesis contributes to the mathematical foundation of domain adaptation\\nas emerging field in machine learning. In contrast to classical statistical\\nlearning, the framework of domain adaptation takes into account deviations\\nbetween probability distributions in the training and application setting.\\nDomain adaptation applies for a wider range of applications as future samples\\noften follow a distribution that differs from the ones of the training samples.\\nA decisive point is the generality of the assumptions about the similarity of\\nthe distributions. Therefore, in this thesis we study domain adaptation\\nproblems under as weak similarity assumptions as can be modelled by finitely\\nmany moments. We provide statistical learning guarantees for two unsupervised learning\\ntasks in the context of compressive statistical learning, a general framework\\nfor resource-efficient large-scale learning that we introduced in a companion\\npaper.The principle of compressive statistical learning is to compress a\\ntraining collection, in one pass, into a low-dimensional sketch (a vector of\\nrandom empirical generalized moments) that captures the information relevant to\\nthe considered learning task. We explicitly describe and analyze random feature\\nfunctions which empirical averages preserve the needed information for\\ncompressive clustering and compressive Gaussian mixture modeling with fixed\\nknown variance, and establish sufficient sketch sizes given the problem\\ndimensions. There is a clear need for efficient algorithms to tune hyperparameters for\\nstatistical learning schemes, since the commonly applied search methods (such\\nas grid search with N-fold cross-validation) are inefficient and/or\\napproximate. Previously existing algorithms that efficiently search for\\nhyperparameters relying on the smoothness of the cost function cannot be\\napplied in problems such as Lasso regression.\\n  In this contribution, we develop a hyperparameter optimization method that\\nrelies on the structure of proximal gradient methods and does not require a\\nsmooth cost function. Such a method is applied to Leave-one-out (LOO)-validated\\nLasso and Group Lasso to yield efficient, data-driven, hyperparameter\\noptimization algorithms.\\n  Numerical experiments corroborate the convergence of the proposed method to a\\nlocal optimum of the LOO validation error curve, and the efficiency of its\\napproximations. Stochastic optimization lies at the core of most statistical learning models.\\nThe recent great development of stochastic algorithmic tools focused\\nsignificantly onto proximal gradient iterations, in order to find an efficient\\napproach for nonsmooth (composite) population risk functions. The complexity of\\nfinding optimal predictors by minimizing regularized risk is largely understood\\nfor simple regularizations such as $\\\\ell_1/\\\\ell_2$ norms. However, more complex\\nproperties desired for the predictor necessitates highly difficult regularizers\\nas used in grouped lasso or graph trend filtering. In this chapter we develop\\nand analyze minibatch variants of stochastic proximal gradient algorithm for\\ngeneral composite objective functions with stochastic nonsmooth components. We\\nprovide iteration complexity for constant and variable stepsize policies\\nobtaining that, for minibatch size $N$, after\\n$\\\\mathcal{O}(\\\\frac{1}{N\\\\epsilon})$ iterations $\\\\epsilon-$suboptimality is\\nattained in expected quadratic distance to optimal solution. The numerical\\ntests on $\\\\ell_2-$regularized SVMs and parametric sparse representation\\nproblems confirm the theoretical behaviour and surpasses minibatch SGD\\nperformance. Design-consistent model-assisted estimation has become the standard practice\\nin survey sampling. However, a general theory is lacking so far, which allows\\none to incorporate modern machine-learning techniques that can lead to\\npotentially much more powerful assisting models. We propose a subsampling\\nRao-Blackwell method, and develop a statistical learning theory for exactly\\ndesign-unbiased estimation with the help of linear or non-linear prediction\\nmodels. Our approach makes use of classic ideas from Statistical Science as\\nwell as the rapidly growing field of Machine Learning. Provided rich auxiliary\\ninformation, it can yield considerable efficiency gains over standard linear\\nmodel-assisted methods, while ensuring valid estimation for the given target\\npopulation, which is robust against potential mis-specifications of the\\nassisting model at the individual level. The sparse factorization of a large matrix is fundamental in modern\\nstatistical learning. In particular, the sparse singular value decomposition\\nand its variants have been utilized in multivariate regression, factor\\nanalysis, biclustering, vector time series modeling, among others. The appeal\\nof this factorization is owing to its power in discovering a\\nhighly-interpretable latent association network, either between samples and\\nvariables or between responses and predictors. However, many existing methods\\nare either ad hoc without a general performance guarantee, or are\\ncomputationally intensive, rendering them unsuitable for large-scale studies.\\nWe formulate the statistical problem as a sparse factor regression and tackle\\nit with a divide-and-conquer approach. In the first stage of division, we\\nconsider both sequential and parallel approaches for simplifying the task into\\na set of co-sparse unit-rank estimation (CURE) problems, and establish the\\nstatistical underpinnings of these commonly-adopted and yet poorly understood\\ndeflation methods. In the second stage of division, we innovate a contended\\nstagewise learning technique, consisting of a sequence of simple incremental\\nupdates, to efficiently trace out the whole solution paths of CURE. Our\\nalgorithm has a much lower computational complexity than alternating convex\\nsearch, and the choice of the step size enables a flexible and principled\\ntradeoff between statistical accuracy and computational efficiency. Our work is\\namong the first to enable stagewise learning for non-convex problems, and the\\nidea can be applicable in many multi-convex problems. Extensive simulation\\nstudies and an application in genetics demonstrate the effectiveness and\\nscalability of our approach. Estimation of distribution algorithms (EDA) are stochastic optimization\\nalgorithms. EDA establishes a probability model to describe the distribution of\\nsolution from the perspective of population macroscopically by statistical\\nlearning method, and then randomly samples the probability model to generate a\\nnew population. EDA can better solve multi-objective optimal problems (MOPs).\\nHowever, the performance of EDA decreases in solving many-objective optimal\\nproblems (MaOPs), which contains more than three objectives. Reference Vector\\nGuided Evolutionary Algorithm (RVEA), based on the EDA framework, can better\\nsolve MaOPs. In our paper, we use the framework of RVEA. However, we generate\\nthe new population by Wasserstein Generative Adversarial Networks-Gradient\\nPenalty (WGAN-GP) instead of using crossover and mutation. WGAN-GP have\\nadvantages of fast convergence, good stability and high sample quality. WGAN-GP\\nlearn the mapping relationship from standard normal distribution to given data\\nset distribution based on a given data set subject to the same distribution. It\\ncan quickly generate populations with high diversity and good convergence. To\\nmeasure the performance, RM-MEDA, MOPSO and NSGA-II are selected to perform\\ncomparison experiments over DTLZ and LSMOP test suites with 3-, 5-, 8-, 10- and\\n15-objective. Estimating the parameters of mathematical models is a common problem in\\nalmost all branches of science. However, this problem can prove notably\\ndifficult when processes and model descriptions become increasingly complex and\\nan explicit likelihood function is not available. With this work, we propose a\\nnovel method for globally amortized Bayesian inference based on invertible\\nneural networks which we call BayesFlow. The method uses simulation to learn a\\nglobal estimator for the probabilistic mapping from observed data to underlying\\nmodel parameters. A neural network pre-trained in this way can then, without\\nadditional training or optimization, infer full posteriors on arbitrary many\\nreal datasets involving the same model family. In addition, our method\\nincorporates a summary network trained to embed the observed data into\\nmaximally informative summary statistics. Learning summary statistics from data\\nmakes the method applicable to modeling scenarios where standard inference\\ntechniques with hand-crafted summary statistics fail. We demonstrate the\\nutility of BayesFlow on challenging intractable models from population\\ndynamics, epidemiology, cognitive science and ecology. We argue that BayesFlow\\nprovides a general framework for building amortized Bayesian parameter\\nestimation machines for any forward model from which data can be simulated. This paper addresses the problem of model compression via knowledge\\ndistillation. To this end, we propose a new knowledge distillation method based\\non transferring feature statistics, specifically the channel-wise mean and\\nvariance, from the teacher to the student. Our method goes beyond the standard\\nway of enforcing the mean and variance of the student to be similar to those of\\nthe teacher through an $L_2$ loss, which we found it to be of limited\\neffectiveness. Specifically, we propose a new loss based on adaptive instance\\nnormalization to effectively transfer the feature statistics. The main idea is\\nto transfer the learned statistics back to the teacher via adaptive instance\\nnormalization (conditioned on the student) and let the teacher network\\n\\\"evaluate\\\" via a loss whether the statistics learned by the student are\\nreliably transferred. We show that our distillation method outperforms other\\nstate-of-the-art distillation methods over a large set of experimental settings\\nincluding different (a) network architectures, (b) teacher-student capacities,\\n(c) datasets, and (d) domains. Distributionally Robust Optimization (DRO) has enabled to prove the\\nequivalence between robustness and regularization in classification and\\nregression, thus providing an analytical reason why regularization generalizes\\nwell in statistical learning. Although DRO's extension to sequential\\ndecision-making overcomes $\\\\textit{external uncertainty}$ through the robust\\nMarkov Decision Process (MDP) setting, the resulting formulation is hard to\\nsolve, especially on large domains. On the other hand, existing regularization\\nmethods in reinforcement learning only address $\\\\textit{internal uncertainty}$\\ndue to stochasticity. Our study aims to facilitate robust reinforcement\\nlearning by establishing a dual relation between robust MDPs and\\nregularization. We introduce Wasserstein distributionally robust MDPs and prove\\nthat they hold out-of-sample performance guarantees. Then, we introduce a new\\nregularizer for empirical value functions and show that it lower bounds the\\nWasserstein distributionally robust value function. We extend the result to\\nlinear value function approximation for large state spaces. Our approach\\nprovides an alternative formulation of robustness with guaranteed finite-sample\\nperformance. Moreover, it suggests using regularization as a practical tool for\\ndealing with $\\\\textit{external uncertainty}$ in reinforcement learning methods. Panel count data describes aggregated counts of recurrent events observed at\\ndiscrete time points. To understand dynamics of health behaviors, the field of\\nquantitative behavioral research has evolved to increasingly rely upon panel\\ncount data collected via multiple self reports, for example, about frequencies\\nof smoking using in-the-moment surveys on mobile devices. However, missing\\nreports are common and present a major barrier to downstream statistical\\nlearning. As a first step, under a missing completely at random assumption\\n(MCAR), we propose a simple yet widely applicable functional EM algorithm to\\nestimate the counting process mean function, which is of central interest to\\nbehavioral scientists. The proposed approach wraps several popular panel count\\ninference methods, seamlessly deals with incomplete counts and is robust to\\nmisspecification of the Poisson process assumption. Theoretical analysis of the\\nproposed algorithm provides finite-sample guarantees by expanding parametric EM\\ntheory to our general non-parametric setting. We illustrate the utility of the\\nproposed algorithm through numerical experiments and an analysis of smoking\\ncessation data. We also discuss useful extensions to address deviations from\\nthe MCAR assumption and covariate effects. Boosting is a widely used machine learning approach based on the idea of\\naggregating weak learning rules. While in statistical learning numerous\\nboosting methods exist both in the realizable and agnostic settings, in online\\nlearning they exist only in the realizable case. In this work we provide the\\nfirst agnostic online boosting algorithm; that is, given a weak learner with\\nonly marginally-better-than-trivial regret guarantees, our algorithm boosts it\\nto a strong learner with sublinear regret.\\n  Our algorithm is based on an abstract (and simple) reduction to online convex\\noptimization, which efficiently converts an arbitrary online convex optimizer\\nto an online booster.\\n  Moreover, this reduction extends to the statistical as well as the online\\nrealizable settings, thus unifying the 4 cases of statistical/online and\\nagnostic/realizable boosting. Most data in genome-wide phylogenetic analysis (phylogenomics) is essentially\\nmultidimensional, posing a major challenge to human comprehension and\\ncomputational analysis. Also, we can not directly apply statistical learning\\nmodels in data science to a set of phylogenetic trees since the space of\\nphylogenetic trees is not Euclidean. In fact, the space of phylogenetic trees\\nis a tropical Grassmannian in terms of max-plus algebra. Therefore, to classify\\nmulti-locus data sets for phylogenetic analysis, we propose tropical support\\nvector machines (SVMs). Like classical SVMs, a tropical SVM is a discriminative\\nclassifier defined by the tropical hyperplane which maximizes the minimum\\ntropical distance from data points to itself in order to separate these data\\npoints into sectors (half-spaces) in the tropical projective torus. Both hard\\nmargin tropical SVMs and soft margin tropical SVMs can be formulated as linear\\nprogramming problems. We focus on classifying two categories of data, and we\\nstudy a simpler case by assuming the data points from the same category ideally\\nstay in the same sector of a tropical separating hyperplane. For hard margin\\ntropical SVMs, we prove the necessary and sufficient conditions for two\\ncategories of data points to be separated, and we show an explicit formula for\\nthe optimal value of the feasible linear programming problem. For soft margin\\ntropical SVMs, we develop novel methods to compute an optimal tropical\\nseparating hyperplane. Computational experiments show our methods work well. We\\nend this paper with open problems. In the era of clinical information explosion, a good strategy for clinical\\ntext summarization is helpful to improve the clinical workflow. The ideal\\nsummarization strategy can preserve important information in the informative\\nbut less organized, ill-structured clinical narrative texts. Instead of using\\npure statistical learning approaches, which are difficult to interpret and\\nexplain, we utilized knowledge of computational linguistics with human\\nexperts-curated biomedical knowledge base to achieve the interpretable and\\nmeaningful clinical text summarization. Our research objective is to use the\\nbiomedical ontology with semantic information, and take the advantage from the\\nlanguage hierarchical structure, the constituency tree, in order to identify\\nthe correct clinical concepts and the corresponding negation information, which\\nis critical for summarizing clinical concepts from narrative text. We achieved\\nthe clinically acceptable performance for both negation detection and concept\\nidentification, and the clinical concepts with common negated patterns can be\\nidentified and negated by the proposed method. Intensive algorithmic efforts have been made to enable the rapid improvements\\nof certificated robustness for complex ML models recently. However, current\\nrobustness certification methods are only able to certify under a limited\\nperturbation radius. Given that existing pure data-driven statistical\\napproaches have reached a bottleneck, in this paper, we propose to integrate\\nstatistical ML models with knowledge (expressed as logical rules) as a\\nreasoning component using Markov logic networks (MLN, so as to further improve\\nthe overall certified robustness. This opens new research questions about\\ncertifying the robustness of such a paradigm, especially the reasoning\\ncomponent (e.g., MLN). As the first step towards understanding these questions,\\nwe first prove that the computational complexity of certifying the robustness\\nof MLN is #P-hard. Guided by this hardness result, we then derive the first\\ncertified robustness bound for MLN by carefully analyzing different model\\nregimes. Finally, we conduct extensive experiments on five datasets including\\nboth high-dimensional images and natural language texts, and we show that the\\ncertified robustness with knowledge-based logical reasoning indeed\\nsignificantly outperforms that of the state-of-the-arts. Domain adaptation in imitation learning represents an essential step towards\\nimproving generalizability. However, even in the restricted setting of\\nthird-person imitation where transfer is between isomorphic Markov Decision\\nProcesses, there are no strong guarantees on the performance of transferred\\npolicies. We present problem-dependent, statistical learning guarantees for\\nthird-person imitation from observation in an offline setting, and a lower\\nbound on performance in the online setting. We study the linear ill-posed inverse problem with noisy data in the\\nstatistical learning setting. Approximate reconstructions from random noisy\\ndata are sought with general regularization schemes in Hilbert scale. We\\ndiscuss the rates of convergence for the regularized solution under the prior\\nassumptions and a certain link condition. We express the error in terms of\\ncertain distance functions. For regression functions with smoothness given in\\nterms of source conditions the error bound can then be explicitly established. In multiclass classification, the goal is to learn how to predict a random\\nlabel $Y$, valued in $\\\\mathcal{Y}=\\\\{1,\\\\; \\\\ldots,\\\\; K \\\\}$ with $K\\\\geq 3$, based\\nupon observing a r.v. $X$, taking its values in $\\\\mathbb{R}^q$ with $q\\\\geq 1$\\nsay, by means of a classification rule $g:\\\\mathbb{R}^q\\\\to \\\\mathcal{Y}$ with\\nminimum probability of error $\\\\mathbb{P}\\\\{Y\\\\neq g(X) \\\\}$. However, in a wide\\nvariety of situations, the task targeted may be more ambitious, consisting in\\nsorting all the possible label values $y$ that may be assigned to $X$ by\\ndecreasing order of the posterior probability $\\\\eta_y(X)=\\\\mathbb{P}\\\\{Y=y \\\\mid X\\n\\\\}$. This article is devoted to the analysis of this statistical learning\\nproblem, halfway between multiclass classification and posterior probability\\nestimation (regression) and referred to as label ranking here. We highlight the\\nfact that it can be viewed as a specific variant of ranking median regression\\n(RMR), where, rather than observing a random permutation $\\\\Sigma$ assigned to\\nthe input vector $X$ and drawn from a Bradley-Terry-Luce-Plackett model with\\nconditional preference vector $(\\\\eta_1(X),\\\\; \\\\ldots,\\\\; \\\\eta_K(X))$, the sole\\ninformation available for training a label ranking rule is the label $Y$ ranked\\non top, namely $\\\\Sigma^{-1}(1)$. Inspired by recent results in RMR, we prove\\nthat under appropriate noise conditions, the One-Versus-One (OVO) approach to\\nmulticlassification yields, as a by-product, an optimal ranking of the labels\\nwith overwhelming probability. Beyond theoretical guarantees, the relevance of\\nthe approach to label ranking promoted in this article is supported by\\nexperimental results. We propose a learning model called the quantum statistical learning QSQ\\nmodel, which extends the SQ learning model introduced by Kearns to the quantum\\nsetting. Our model can be also seen as a restriction of the quantum PAC\\nlearning model: here, the learner does not have direct access to quantum\\nexamples, but can only obtain estimates of measurement statistics on them.\\nTheoretically, this model provides a simple yet expressive setting to explore\\nthe power of quantum examples in machine learning. From a practical\\nperspective, since simpler operations are required, learning algorithms in the\\nQSQ model are more feasible for implementation on near-term quantum devices. We\\nprove a number of results about the QSQ learning model. We first show that\\nparity functions, (log n)-juntas and polynomial-sized DNF formulas are\\nefficiently learnable in the QSQ model, in contrast to the classical setting\\nwhere these problems are provably hard. This implies that many of the\\nadvantages of quantum PAC learning can be realized even in the more restricted\\nquantum SQ learning model. It is well-known that weak statistical query\\ndimension, denoted by WSQDIM(C), characterizes the complexity of learning a\\nconcept class C in the classical SQ model. We show that log(WSQDIM(C)) is a\\nlower bound on the complexity of QSQ learning, and furthermore it is tight for\\ncertain concept classes C. Additionally, we show that this quantity provides\\nstrong lower bounds for the small-bias quantum communication model under\\nproduct distributions. Finally, we introduce the notion of private quantum PAC\\nlearning, in which a quantum PAC learner is required to be differentially\\nprivate. We show that learnability in the QSQ model implies learnability in the\\nquantum private PAC model. Additionally, we show that in the private PAC\\nlearning setting, the classical and quantum sample complexities are equal, up\\nto constant factors. Pediatric obstructive sleep apnea affects an estimated 1-5% of\\nelementary-school aged children and can lead to other detrimental health\\nproblems. Swift diagnosis and treatment are critical to a child's growth and\\ndevelopment, but the variability of symptoms and the complexity of the\\navailable data make this a challenge. We take a first step in streamlining the\\nprocess by focusing on inexpensive data from questionnaires and craniofacial\\nmeasurements. We apply correlation networks, the Mapper algorithm from\\ntopological data analysis, and singular value decomposition in a process of\\nexploratory data analysis. We then apply a variety of supervised and\\nunsupervised learning techniques from statistics, machine learning, and\\ntopology, ranging from support vector machines to Bayesian classifiers and\\nmanifold learning. Finally, we analyze the results of each of these methods and\\ndiscuss the implications for a multi-data-sourced algorithm moving forward. We propose a risk-averse statistical learning framework wherein the\\nperformance of a learning algorithm is evaluated by the conditional\\nvalue-at-risk (CVaR) of losses rather than the expected loss. We devise\\nalgorithms based on stochastic gradient descent for this framework. While\\nexisting studies of CVaR optimization require direct access to the underlying\\ndistribution, our algorithms make a weaker assumption that only i.i.d.\\\\ samples\\nare given. For convex and Lipschitz loss functions, we show that our algorithm\\nhas $O(1/\\\\sqrt{n})$-convergence to the optimal CVaR, where $n$ is the number of\\nsamples. For nonconvex and smooth loss functions, we show a generalization\\nbound on CVaR. By conducting numerical experiments on various machine learning\\ntasks, we demonstrate that our algorithms effectively minimize CVaR compared\\nwith other baseline algorithms. This paper is concerned with the study of constrained statistical learning\\nproblems, the unconstrained version of which are at the core of virtually all\\nof modern information processing. Accounting for constraints, however, is\\nparamount to incorporate prior knowledge and impose desired structural and\\nstatistical properties on the solutions. Still, solving constrained statistical\\nproblems remains challenging and guarantees scarce, leaving them to be tackled\\nusing regularized formulations. Though practical and effective, selecting\\nregularization parameters so as to satisfy requirements is challenging, if at\\nall possible, due to the lack of a straightforward relation between parameters\\nand constraints. In this work, we propose to directly tackle the constrained\\nstatistical problem overcoming its infinite dimensionality, unknown\\ndistributions, and constraints by leveraging finite dimensional\\nparameterizations, sample averages, and duality theory. Aside from making the\\nproblem tractable, these tools allow us to bound the empirical duality gap,\\ni.e., the difference between our approximate tractable solutions and the actual\\nsolutions of the original statistical problem. We demonstrate the effectiveness\\nand usefulness of this constrained formulation in a fair learning application. We consider statistical learning problems, when the distribution $P'$ of the\\ntraining observations $Z'_1,\\\\; \\\\ldots,\\\\; Z'_n$ differs from the distribution\\n$P$ involved in the risk one seeks to minimize (referred to as the test\\ndistribution) but is still defined on the same measurable space as $P$ and\\ndominates it. In the unrealistic case where the likelihood ratio\\n$\\\\Phi(z)=dP/dP'(z)$ is known, one may straightforwardly extends the Empirical\\nRisk Minimization (ERM) approach to this specific transfer learning setup using\\nthe same idea as that behind Importance Sampling, by minimizing a weighted\\nversion of the empirical risk functional computed from the 'biased' training\\ndata $Z'_i$ with weights $\\\\Phi(Z'_i)$. Although the importance function\\n$\\\\Phi(z)$ is generally unknown in practice, we show that, in various situations\\nfrequently encountered in practice, it takes a simple form and can be directly\\nestimated from the $Z'_i$'s and some auxiliary information on the statistical\\npopulation $P$. By means of linearization techniques, we then prove that the\\ngeneralization capacity of the approach aforementioned is preserved when\\nplugging the resulting estimates of the $\\\\Phi(Z'_i)$'s into the weighted\\nempirical risk. Beyond these theoretical guarantees, numerical results provide\\nstrong empirical evidence of the relevance of the approach promoted in this\\narticle. Within the framework of statistical learning theory it is possible to bound\\nthe minimum number of samples required by a learner to reach a target accuracy.\\nWe show that if the bound on the accuracy is taken into account, quantum\\nmachine learning algorithms for supervised learning---for which statistical\\nguarantees are available---cannot achieve polylogarithmic runtimes in the input\\ndimension. We conclude that, when no further assumptions on the problem are\\nmade, quantum machine learning algorithms for supervised learning can have at\\nmost polynomial speedups over efficient classical algorithms, even in cases\\nwhere quantum access to the data is naturally available. We propose a statistical learning-based traffic speed estimation method that\\nuses sparse vehicle trajectory information. Using a convolutional\\nencoder-decoder based architecture, we show that a well trained neural network\\ncan learn spatio-temporal traffic speed dynamics from time-space diagrams. We\\ndemonstrate this for a homogeneous road section using simulated vehicle\\ntrajectories and then validate it using real-world data from NGSIM. Our results\\nshow that with probe vehicle penetration levels as low as 5\\\\%, the proposed\\nestimation method can provide a sound reconstruction of macroscopic traffic\\nspeeds and reproduce realistic shockwave patterns, implying applicability in a\\nvariety of traffic conditions. We further discuss the model's reconstruction\\nmechanisms and confirm its ability to differentiate various traffic behaviors\\nsuch as congested and free-flow traffic states, transition dynamics, and\\nshockwave propagation. Understanding the pathways whereby an intervention has an effect on an\\noutcome is a common scientific goal. A rich body of literature provides various\\ndecompositions of the total intervention effect into pathway specific effects.\\nInterventional direct and indirect effects provide one such decomposition.\\nExisting estimators of these effects are based on parametric models with\\nconfidence interval estimation facilitated via the nonparametric bootstrap. We\\nprovide theory that allows for more flexible, possibly machine learning-based,\\nestimation techniques to be considered. In particular, we establish weak\\nconvergence results that facilitate the construction of closed-form confidence\\nintervals and hypothesis tests. Finally, we demonstrate multiple robustness\\nproperties of the proposed estimators. Simulations show that inference based on\\nlarge-sample theory has adequate small-sample performance. Our work thus\\nprovides a means of leveraging modern statistical learning techniques in\\nestimation of interventional mediation effects. Relation Extraction (RE), the task of detecting and characterizing semantic\\nrelations between entities in text, has gained much importance in the last two\\ndecades, mainly in the biomedical domain. Many papers have been published on\\nRelation Extraction using supervised machine learning techniques. Most of these\\ntechniques rely on statistical methods, such as feature-based and\\ntree-kernels-based methods. Such statistical learning techniques are usually\\nbased on a propositional hypothesis space for representing examples, i.e., they\\nemploy an attribute-value representation of features. This kind of\\nrepresentation has some drawbacks, particularly in the extraction of complex\\nrelations which demand more contextual information about the involving\\ninstances, i.e., it is not able to effectively capture structural information\\nfrom parse trees without loss of information. In this work, we present\\nOntoILPER, a logic-based relational learning approach to Relation Extraction\\nthat uses Inductive Logic Programming for generating extraction models in the\\nform of symbolic extraction rules. OntoILPER takes profit of a rich relational\\nrepresentation of examples, which can alleviate the aforementioned drawbacks.\\nThe proposed relational approach seems to be more suitable for Relation\\nExtraction than statistical ones for several reasons that we argue. Moreover,\\nOntoILPER uses a domain ontology that guides the background knowledge\\ngeneration process and is used for storing the extracted relation instances.\\nThe induced extraction rules were evaluated on three protein-protein\\ninteraction datasets from the biomedical domain. The performance of OntoILPER\\nextraction models was compared with other state-of-the-art RE systems. The\\nencouraging results seem to demonstrate the effectiveness of the proposed\\nsolution. In the context of supervised statistical learning, it is typically assumed\\nthat the training set comes from the same distribution that draws the test\\nsamples. When this is not the case, the behavior of the learned model is\\nunpredictable and becomes dependent upon the degree of similarity between the\\ndistribution of the training set and the distribution of the test set. One of\\nthe research topics that investigates this scenario is referred to as domain\\nadaptation. Deep neural networks brought dramatic advances in pattern\\nrecognition and that is why there have been many attempts to provide good\\ndomain adaptation algorithms for these models. Here we take a different avenue\\nand approach the problem from an incremental point of view, where the model is\\nadapted to the new domain iteratively. We make use of an existing unsupervised\\ndomain-adaptation algorithm to identify the target samples on which there is\\ngreater confidence about their true label. The output of the model is analyzed\\nin different ways to determine the candidate samples. The selected set is then\\nadded to the source training set by considering the labels provided by the\\nnetwork as ground truth, and the process is repeated until all target samples\\nare labelled. Our results report a clear improvement with respect to the\\nnon-incremental case in several datasets, also outperforming other\\nstate-of-the-art domain adaptation algorithms. We introduce a procedure for conditional density estimation under logarithmic\\nloss, which we call SMP (Sample Minmax Predictor). This estimator minimizes a\\nnew general excess risk bound for statistical learning. On standard examples,\\nthis bound scales as $d/n$ with $d$ the model dimension and $n$ the sample\\nsize, and critically remains valid under model misspecification. Being an\\nimproper (out-of-model) procedure, SMP improves over within-model estimators\\nsuch as the maximum likelihood estimator, whose excess risk degrades under\\nmisspecification. Compared to approaches reducing to the sequential problem,\\nour bounds remove suboptimal $\\\\log n$ factors and can handle unbounded classes.\\nFor the Gaussian linear model, the predictions and risk bound of SMP are\\ngoverned by leverage scores of covariates, nearly matching the optimal risk in\\nthe well-specified case without conditions on the noise variance or\\napproximation error of the linear model. For logistic regression, SMP provides\\na non-Bayesian approach to calibration of probabilistic predictions relying on\\nvirtual samples, and can be computed by solving two logistic regressions. It\\nachieves a non-asymptotic excess risk of $O((d + B^2R^2)/n)$, where $R$ bounds\\nthe norm of features and $B$ that of the comparison parameter; by contrast, no\\nwithin-model estimator can achieve better rate than $\\\\min({B R}/{\\\\sqrt{n}}, {d\\ne^{BR}}/{n} )$ in general. This provides a more practical alternative to\\nBayesian approaches, which require approximate posterior sampling, thereby\\npartly addressing a question raised by Foster et al. (2018). Natural language exhibits statistical dependencies at a wide range of scales.\\nFor instance, the mutual information between words in natural language decays\\nlike a power law with the temporal lag between them. However, many statistical\\nlearning models applied to language impose a sampling scale while extracting\\nstatistical structure. For instance, Word2Vec constructs a vector embedding\\nthat maximizes the prediction between a target word and the context words that\\nappear nearby in the corpus. The size of the context is chosen by the user and\\ndefines a strong scale; relationships over much larger temporal scales would be\\ninvisible to the algorithm. This paper examines the family of Word2Vec\\nembeddings generated while systematically manipulating the sampling scale used\\nto define the context around each word. The primary result is that different\\nlinguistic relationships are preferentially encoded at different scales.\\nDifferent scales emphasize different syntactic and semantic relations between\\nwords.Moreover, the neighborhoods of a given word in the embeddings change\\nsignificantly depending on the scale. These results suggest that any individual\\nscale can only identify a subset of the meaningful relationships a word might\\nhave, and point toward the importance of developing scale-free models of\\nsemantic meaning. Differential Privacy (DP) provides strong guarantees on the risk of\\ncompromising a user's data in statistical learning applications, though these\\nstrong protections make learning challenging and may be too stringent for some\\nuse cases. To address this, we propose element level differential privacy,\\nwhich extends differential privacy to provide protection against leaking\\ninformation about any particular \\\"element\\\" a user has, allowing better utility\\nand more robust results than classical DP. By carefully choosing these\\n\\\"elements,\\\" it is possible to provide privacy protections at a desired\\ngranularity. We provide definitions, associated privacy guarantees, and\\nanalysis to identify the tradeoffs with the new definition; we also develop\\nseveral private estimation and learning methodologies, providing careful\\nexamples for item frequency and M-estimation (empirical risk minimization) with\\nconcomitant privacy and utility analysis. We complement our theoretical and\\nmethodological advances with several real-world applications, estimating\\nhistograms and fitting several large-scale prediction models, including deep\\nnetworks. Statistical learning theory provides bounds of the generalization gap, using\\nin particular the Vapnik-Chervonenkis dimension and the Rademacher complexity.\\nAn alternative approach, mainly studied in the statistical physics literature,\\nis the study of generalization in simple synthetic-data models. Here we discuss\\nthe connections between these approaches and focus on the link between the\\nRademacher complexity in statistical learning and the theories of\\ngeneralization for typical-case synthetic models from statistical physics,\\ninvolving quantities known as Gardner capacity and ground state energy. We show\\nthat in these models the Rademacher complexity is closely related to the ground\\nstate energy computed by replica theories. Using this connection, one may\\nreinterpret many results of the literature as rigorous Rademacher bounds in a\\nvariety of models in the high-dimensional statistics limit. Somewhat\\nsurprisingly, we also show that statistical learning theory provides\\npredictions for the behavior of the ground-state energies in some full replica\\nsymmetry breaking models.\",\n          \"Planning has been part of the core pursuit for artificial intelligence since\\nits conception, but earlier AI agents mostly focused on constrained settings\\nbecause many of the cognitive substrates necessary for human-level planning\\nhave been lacking. Recently, language agents powered by large language models\\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\\nthese language agents capable of planning in more complex settings that are out\\nof the reach of prior AI agents? To advance this investigation, we propose\\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\\ncommon real-world planning scenario. It provides a rich sandbox environment,\\nvarious tools for accessing nearly four million data records, and 1,225\\nmeticulously curated planning intents and reference plans. Comprehensive\\nevaluations show that the current language agents are not yet capable of\\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\\n0.6%. Language agents struggle to stay on task, use the right tools to collect\\ninformation, or keep track of multiple constraints. However, we note that the\\nmere possibility for language agents to tackle such a complex problem is in\\nitself non-trivial progress. TravelPlanner provides a challenging yet\\nmeaningful testbed for future language agents. For analysing real-world networks, graph representation learning is a popular\\ntool. These methods, such as a graph autoencoder (GAE), typically rely on\\nlow-dimensional representations, also called embeddings, which are obtained\\nthrough minimising a loss function; these embeddings are used with a decoder\\nfor downstream tasks such as node classification and edge prediction. While\\nGAEs tend to be fairly accurate, they suffer from scalability issues. For\\nimproved speed, a Local2Global approach, which combines graph patch embeddings\\nbased on eigenvector synchronisation, was shown to be fast and achieve good\\naccuracy. Here we propose L2G2G, a Local2Global method which improves GAE\\naccuracy without sacrificing scalability. This improvement is achieved by\\ndynamically synchronising the latent node representations, while training the\\nGAEs. It also benefits from the decoder computing an only local patch loss.\\nHence, aligning the local embeddings in each epoch utilises more information\\nfrom the graph than a single post-training alignment does, while maintaining\\nscalability. We illustrate on synthetic benchmarks, as well as real-world\\nexamples, that L2G2G achieves higher accuracy than the standard Local2Global\\napproach and scales efficiently on the larger data sets. We find that for large\\nand dense networks, it even outperforms the slow, but assumed more accurate,\\nGAEs. This technical report describes the training of nomic-embed-text-v1, the\\nfirst fully reproducible, open-source, open-weights, open-data, 8192 context\\nlength English text embedding model that outperforms both OpenAI Ada-002 and\\nOpenAI text-embedding-3-small on short and long-context tasks. We release the\\ntraining code and model weights under an Apache 2 license. In contrast with\\nother open-source models, we release a training data loader with 235 million\\ncurated text pairs that allows for the full replication of nomic-embed-text-v1.\\nYou can find code and data to replicate the model at\\nhttps://github.com/nomic-ai/contrastors Counterfactual reasoning is pivotal in human cognition and especially\\nimportant for providing explanations and making decisions. While Judea Pearl's\\ninfluential approach is theoretically elegant, its generation of a\\ncounterfactual scenario often requires interventions that are too detached from\\nthe real scenarios to be feasible. In response, we propose a framework of\\nnatural counterfactuals and a method for generating counterfactuals that are\\nnatural with respect to the actual world's data distribution. Our methodology\\nrefines counterfactual reasoning, allowing changes in causally preceding\\nvariables to minimize deviations from realistic scenarios. To generate natural\\ncounterfactuals, we introduce an innovative optimization framework that permits\\nbut controls the extent of backtracking with a naturalness criterion. Empirical\\nexperiments indicate the effectiveness of our method. Foundation models (FMs) such as large language models have revolutionized the\\nfield of AI by showing remarkable performance in various tasks. However, they\\nexhibit numerous limitations that prevent their broader adoption in many\\nreal-world systems, which often require a higher bar for trustworthiness and\\nusability. Since FMs are trained using loss functions aimed at reconstructing\\nthe training corpus in a self-supervised manner, there is no guarantee that the\\nmodel's output aligns with users' preferences for a specific task at hand. In\\nthis survey paper, we propose a conceptual framework that encapsulates\\ndifferent modes by which agents could interact with FMs and guide them suitably\\nfor a set of tasks, particularly through knowledge augmentation and reasoning.\\nOur framework elucidates agent role categories such as updating the underlying\\nFM, assisting with prompting the FM, and evaluating the FM output. We also\\ncategorize several state-of-the-art approaches into agent interaction\\nprotocols, highlighting the nature and extent of involvement of the various\\nagent roles. The proposed framework provides guidance for future directions to\\nfurther realize the power of FMs in practical AI systems. Spatial sound reasoning is a fundamental human skill, enabling us to navigate\\nand interpret our surroundings based on sound. In this paper we present BAT,\\nwhich combines the spatial sound perception ability of a binaural acoustic\\nscene analysis model with the natural language reasoning capabilities of a\\nlarge language model (LLM) to replicate this innate ability. To address the\\nlack of existing datasets of in-the-wild spatial sounds, we synthesized a\\nbinaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed\\nSpatialSoundQA, a spatial sound-based question-answering dataset, offering a\\nrange of QA tasks that train BAT in various aspects of spatial sound perception\\nand reasoning. The acoustic front end encoder of BAT is a novel spatial audio\\nencoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by\\nitself achieves strong performance across sound event detection, spatial\\nlocalization, and distance estimation. By integrating Spatial-AST with LLaMA-2\\n7B model, BAT transcends standard Sound Event Localization and Detection (SELD)\\ntasks, enabling the model to reason about the relationships between the sounds\\nin its environment. Our experiments demonstrate BAT's superior performance on\\nboth spatial sound perception and reasoning, showcasing the immense potential\\nof LLMs in navigating and interpreting complex spatial audio environments. The emergence of LLM-based agents has garnered considerable attention, yet\\ntheir trustworthiness remains an under-explored area. As agents can directly\\ninteract with the physical environment, their reliability and safety is\\ncritical. This paper presents an Agent-Constitution-based agent framework,\\nTrustAgent, an initial investigation into improving the safety dimension of\\ntrustworthiness in LLM-based agents. This framework consists of threefold\\nstrategies: pre-planning strategy which injects safety knowledge to the model\\nprior to plan generation, in-planning strategy which bolsters safety during\\nplan generation, and post-planning strategy which ensures safety by\\npost-planning inspection. Through experimental analysis, we demonstrate how\\nthese approaches can effectively elevate an LLM agent's safety by identifying\\nand preventing potential dangers. Furthermore, we explore the intricate\\nrelationships between safety and helpfulness, and between the model's reasoning\\nability and its efficacy as a safe agent. This paper underscores the imperative\\nof integrating safety awareness and trustworthiness into the design and\\ndeployment of LLM-based agents, not only to enhance their performance but also\\nto ensure their responsible integration into human-centric environments. Data\\nand code are available at https://github.com/agiresearch/TrustAgent. This survey article has grown out of the GAIED (pronounced \\\"guide\\\") workshop\\norganized by the authors at the NeurIPS 2023 conference. We organized the GAIED\\nworkshop as part of a community-building effort to bring together researchers,\\neducators, and practitioners to explore the potential of generative AI for\\nenhancing education. This article aims to provide an overview of the workshop\\nactivities and highlight several future research directions in the area of\\nGAIED. Generating rich and controllable motion is a pivotal challenge in video\\nsynthesis. We propose Boximator, a new approach for fine-grained motion\\ncontrol. Boximator introduces two constraint types: hard box and soft box.\\nUsers select objects in the conditional frame using hard boxes and then use\\neither type of boxes to roughly or rigorously define the object's position,\\nshape, or motion path in future frames. Boximator functions as a plug-in for\\nexisting video diffusion models. Its training process preserves the base\\nmodel's knowledge by freezing the original weights and training only the\\ncontrol module. To address training challenges, we introduce a novel\\nself-tracking technique that greatly simplifies the learning of box-object\\ncorrelations. Empirically, Boximator achieves state-of-the-art video quality\\n(FVD) scores, improving on two base models, and further enhanced after\\nincorporating box constraints. Its robust motion controllability is validated\\nby drastic increases in the bounding box alignment metric. Human evaluation\\nalso shows that users favor Boximator generation results over the base model. In the realm of power systems, the increasing involvement of residential\\nusers in load forecasting applications has heightened concerns about data\\nprivacy. Specifically, the load data can inadvertently reveal the daily\\nroutines of residential users, thereby posing a risk to their property\\nsecurity. While federated learning (FL) has been employed to safeguard user\\nprivacy by enabling model training without the exchange of raw data, these FL\\nmodels have shown vulnerabilities to emerging attack techniques, such as Deep\\nLeakage from Gradients and poisoning attacks. To counteract these, we initially\\nemploy a Secure-Aggregation (SecAgg) algorithm that leverages multiparty\\ncomputation cryptographic techniques to mitigate the risk of gradient leakage.\\nHowever, the introduction of SecAgg necessitates the deployment of additional\\nsub-center servers for executing the multiparty computation protocol, thereby\\nescalating computational complexity and reducing system robustness, especially\\nin scenarios where one or more sub-centers are unavailable. To address these\\nchallenges, we introduce a Markovian Switching-based distributed training\\nframework, the convergence of which is substantiated through rigorous\\ntheoretical analysis. The Distributed Markovian Switching (DMS) topology shows\\nstrong robustness towards the poisoning attacks as well. Case studies employing\\nreal-world power system load data validate the efficacy of our proposed\\nalgorithm. It not only significantly minimizes communication complexity but\\nalso maintains accuracy levels comparable to traditional FL methods, thereby\\nenhancing the scalability of our load forecasting algorithm. In pervasive machine learning, especially in Human Behavior Analysis (HBA),\\nRGB has been the primary modality due to its accessibility and richness of\\ninformation. However, linked with its benefits are challenges, including\\nsensitivity to lighting conditions and privacy concerns. One possibility to\\novercome these vulnerabilities is to resort to different modalities. For\\ninstance, thermal is particularly adept at accentuating human forms, while\\ndepth adds crucial contextual layers. Despite their known benefits, only a few\\nHBA-specific datasets that integrate these modalities exist. To address this\\nshortage, our research introduces a novel generative technique for creating\\ntrimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique\\ncapitalizes on human segmentation masks derived from RGB images, combined with\\nthermal and depth backgrounds that are sourced automatically. With these two\\ningredients, we synthesize depth and thermal counterparts from existing RGB\\ndata utilizing conditional image-to-image translation. By employing this\\napproach, we generate trimodal data that can be leveraged to train models for\\nsettings with limited data, bad lightning conditions, or privacy-sensitive\\nareas. Large language models (LLMs) are now being used in a wide variety of\\ncontexts, including as creativity support tools (CSTs) intended to help their\\nusers come up with new ideas. But do LLMs actually support user creativity? We\\nhypothesized that the use of an LLM as a CST might make the LLM's users feel\\nmore creative, and even broaden the range of ideas suggested by each individual\\nuser, but also homogenize the ideas suggested by different users. We conducted\\na 36-participant comparative user study and found, in accordance with the\\nhomogenization hypothesis, that different users tended to produce less\\nsemantically distinct ideas with ChatGPT than with an alternative CST.\\nAdditionally, ChatGPT users generated a greater number of more detailed ideas,\\nbut felt less responsible for the ideas they generated. We discuss potential\\nimplications of these findings for users, designers, and developers of\\nLLM-based CSTs. Presenting high-level arguments is a crucial task for fostering participation\\nin online societal discussions. Current argument summarization approaches miss\\nan important facet of this task -- capturing diversity -- which is important\\nfor accommodating multiple perspectives. We introduce three aspects of\\ndiversity: those of opinions, annotators, and sources. We evaluate approaches\\nto a popular argument summarization task called Key Point Analysis, which shows\\nhow these approaches struggle to (1) represent arguments shared by few people,\\n(2) deal with data from various sources, and (3) align with subjectivity in\\nhuman-provided annotations. We find that both general-purpose LLMs and\\ndedicated KPA models exhibit this behavior, but have complementary strengths.\\nFurther, we observe that diversification of training data may ameliorate\\ngeneralization. Addressing diversity in argument summarization requires a mix\\nof strategies to deal with subjectivity. While Large Language Models (LLMs) have demonstrated their proficiency in\\ncomplex reasoning tasks, their performance in dynamic, interactive, and\\ncompetitive scenarios - such as business strategy and stock market analysis -\\nremains underexplored. To bridge this gap, we formally explore the dynamic\\nreasoning capabilities of LLMs for decision-making in rapidly evolving\\nenvironments. We introduce two game theory-based pilot challenges that mirror\\nthe complexities of real-world dynamic decision-making. These challenges are\\nwell-defined, enabling clear, controllable, and precise evaluation of LLMs'\\ndynamic reasoning abilities. Through extensive experiments, we find that\\nexisting reasoning methods tend to falter in dynamic settings that require\\nk-level thinking - a key concept not tackled by previous works. To address\\nthis, we propose a novel reasoning approach for LLMs, named \\\"K-Level\\nReasoning\\\". This approach adopts the perspective of rivals to recursively\\nemploy k-level thinking based on available historical information, which\\nsignificantly improves the prediction accuracy of rivals' subsequent moves and\\ninforms more strategic decision-making. This research not only sets a robust\\nquantitative benchmark for the assessment of dynamic reasoning but also\\nmarkedly enhances the proficiency of LLMs in dynamic contexts. Based on SGD, previous works have proposed many algorithms that have improved\\nconvergence speed and generalization in stochastic optimization, such as SGDm,\\nAdaGrad, Adam, etc. However, their convergence analysis under non-convex\\nconditions is challenging. In this work, we propose a unified framework to\\naddress this issue. For any first-order methods, we interpret the updated\\ndirection $g_t$ as the sum of the stochastic subgradient $\\\\nabla f_t(x_t)$ and\\nan additional acceleration term $\\\\frac{2|\\\\langle v_t, \\\\nabla f_t(x_t)\\n\\\\rangle|}{\\\\|v_t\\\\|_2^2} v_t$, thus we can discuss the convergence by analyzing\\n$\\\\langle v_t, \\\\nabla f_t(x_t) \\\\rangle$. Through our framework, we have\\ndiscovered two plug-and-play acceleration methods: \\\\textbf{Reject Accelerating}\\nand \\\\textbf{Random Vector Accelerating}, we theoretically demonstrate that\\nthese two methods can directly lead to an improvement in convergence rate. As artificial intelligence (AI) continues advancing, ensuring positive\\nsocietal impacts becomes critical, especially as AI systems become increasingly\\nubiquitous in various aspects of life. However, developing \\\"AI for good\\\" poses\\nsubstantial challenges around aligning systems with complex human values.\\nPresently, we lack mature methods for addressing these challenges. This article\\npresents and evaluates the Positive AI design method aimed at addressing this\\ngap. The method provides a human-centered process to translate wellbeing\\naspirations into concrete practices. First, we explain the method's four key\\nsteps: contextualizing, operationalizing, optimizing, and implementing\\nwellbeing supported by continuous measurement for feedback cycles. We then\\npresent a multiple case study where novice designers applied the method,\\nrevealing strengths and weaknesses related to efficacy and usability. Next, an\\nexpert evaluation study assessed the quality of the resulting concepts, rating\\nthem moderately high for feasibility, desirability, and plausibility of\\nachieving intended wellbeing benefits. Together, these studies provide\\npreliminary validation of the method's ability to improve AI design, while\\nsurfacing areas needing refinement like developing support for complex steps.\\nProposed adaptations such as examples and evaluation heuristics could address\\nweaknesses. Further research should examine sustained application over multiple\\nprojects. This human-centered approach shows promise for realizing the vision\\nof 'AI for Wellbeing' that does not just avoid harm, but actively benefits\\nhumanity. In recent years, there has been a surge in the development of 3D\\nstructure-based pre-trained protein models, representing a significant\\nadvancement over pre-trained protein language models in various downstream\\ntasks. However, most existing structure-based pre-trained models primarily\\nfocus on the residue level, i.e., alpha carbon atoms, while ignoring other\\natoms like side chain atoms. We argue that modeling proteins at both residue\\nand atom levels is important since the side chain atoms can also be crucial for\\nnumerous downstream tasks, for example, molecular docking. Nevertheless, we\\nfind that naively combining residue and atom information during pre-training\\ntypically fails. We identify a key reason is the information leakage caused by\\nthe inclusion of atom structure in the input, which renders residue-level\\npre-training tasks trivial and results in insufficiently expressive residue\\nrepresentations. To address this issue, we introduce a span mask pre-training\\nstrategy on 3D protein chains to learn meaningful representations of both\\nresidues and atoms. This leads to a simple yet effective approach to learning\\nprotein representation suitable for diverse downstream tasks. Extensive\\nexperimental results on binding site prediction and function prediction tasks\\ndemonstrate our proposed pre-training approach significantly outperforms other\\nmethods. Our code will be made public. While the great capability of Transformers significantly boosts prediction\\naccuracy, it could also yield overconfident predictions and require calibrated\\nuncertainty estimation, which can be commonly tackled by Gaussian processes\\n(GPs). Existing works apply GPs with symmetric kernels under variational\\ninference to the attention kernel; however, omitting the fact that attention\\nkernels are in essence asymmetric. Moreover, the complexity of deriving the GP\\nposteriors remains high for large-scale data. In this work, we propose\\nKernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building\\nuncertainty-aware self-attention where the asymmetry of attention kernels is\\ntackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through\\nKEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from\\nKSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using\\nonly a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP\\nposteriors can be based on the inversion of a diagonal matrix containing\\nsingular values, contributing to a reduction in time complexity; iii) an\\nevidence lower bound is derived so that variational parameters can be optimized\\ntowards this objective. Experiments verify our excellent performances and\\nefficiency on in-distribution, distribution-shift and out-of-distribution\\nbenchmarks. Can replay, as a widely observed neural activity pattern in brain regions,\\nparticularly in the hippocampus and neocortex, emerge in an artificial agent?\\nIf yes, does it contribute to the tasks? In this work, without heavy dependence\\non complex assumptions, we discover naturally emergent replay under\\ntask-optimized paradigm using a recurrent neural network-based reinforcement\\nlearning model, which mimics the hippocampus and prefrontal cortex, as well as\\ntheir intercommunication and the sensory cortex input. The emergent replay in\\nthe hippocampus, which results from the episodic memory and cognitive map as\\nwell as environment observations, well resembles animal experimental data and\\nserves as an effective indicator of high task performance. The model also\\nsuccessfully reproduces local and nonlocal replay, which matches the human\\nexperimental data. Our work provides a new avenue for understanding the\\nmechanisms behind replay. In practical statistical causal discovery (SCD), embedding domain expert\\nknowledge as constraints into the algorithm is widely accepted as significant\\nfor creating consistent meaningful causal models, despite the recognized\\nchallenges in systematic acquisition of the background knowledge. To overcome\\nthese challenges, this paper proposes a novel methodology for causal inference,\\nin which SCD methods and knowledge based causal inference (KBCI) with a large\\nlanguage model (LLM) are synthesized through \\\"statistical causal prompting\\n(SCP)\\\" for LLMs and prior knowledge augmentation for SCD. Experiments have\\nrevealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result\\nwith prior knowledge from LLM-KBCI to approach the ground truth, and that the\\nSCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has\\nbeen clarified that an LLM can improve SCD with its background knowledge, even\\nif the LLM does not contain information on the dataset. The proposed approach\\ncan thus address challenges such as dataset biases and limitations,\\nillustrating the potential of LLMs to improve data-driven causal inference\\nacross diverse scientific domains. We study how to use guidance to improve the throughput of lifelong\\nMulti-Agent Path Finding (MAPF). Previous studies have demonstrated that while\\nincorporating guidance, such as highways, can accelerate MAPF algorithms, this\\noften results in a trade-off with solution quality. In addition, how to\\ngenerate good guidance automatically remains largely unexplored, with current\\nmethods falling short of surpassing manually designed ones. In this work, we\\nintroduce the directed guidance graph as a versatile representation of guidance\\nfor lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of\\noptimizing its edge weights. We present two GGO algorithms to automatically\\ngenerate guidance for arbitrary lifelong MAPF algorithms and maps. The first\\nmethod directly solves GGO by employing CMA-ES, a black-box optimization\\nalgorithm. The second method, PIU, optimizes an update model capable of\\ngenerating guidance, demonstrating the ability to transfer optimized guidance\\ngraphs to larger maps with similar layouts. Empirically, we show that (1) our\\nguidance graphs improve the throughput of three representative lifelong MAPF\\nalgorithms in four benchmark maps, and (2) our update model can generate\\nguidance graphs for as large as $93 \\\\times 91$ maps and as many as 3000 agents. Satellite data has the potential to inspire a seismic shift for machine\\nlearning -- one in which we rethink existing practices designed for traditional\\ndata modalities. As machine learning for satellite data (SatML) gains traction\\nfor its real-world impact, our field is at a crossroads. We can either continue\\napplying ill-suited approaches, or we can initiate a new research agenda that\\ncenters around the unique characteristics and challenges of satellite data.\\nThis position paper argues that satellite data constitutes a distinct modality\\nfor machine learning research and that we must recognize it as such to advance\\nthe quality and impact of SatML research across theory, methods, and\\ndeployment. We outline critical discussion questions and actionable suggestions\\nto transform SatML from merely an intriguing application area to a dedicated\\nresearch discipline that helps move the needle on big challenges for machine\\nlearning and society. Graph representation learning, a critical step in graph-centric tasks, has\\nseen significant advancements. Earlier techniques often operate in an\\nend-to-end setting, where performance heavily relies on the availability of\\nample labeled data. This constraint has spurred the emergence of few-shot\\nlearning on graphs, where only a few task-specific labels are available for\\neach task. Given the extensive literature in this field, this survey endeavors\\nto synthesize recent developments, provide comparative insights, and identify\\nfuture directions. We systematically categorize existing studies into three\\nmajor families: meta-learning approaches, pre-training approaches, and hybrid\\napproaches, with a finer-grained classification in each family to aid readers\\nin their method selection process. Within each category, we analyze the\\nrelationships among these methods and compare their strengths and limitations.\\nFinally, we outline prospective future directions for few-shot learning on\\ngraphs to catalyze continued innovation in this field. In recent years, Large Language Models (LLMs) have achieved significant\\nsuccess in natural language processing (NLP) and various interdisciplinary\\nareas. However, applying LLMs to chemistry is a complex task that requires\\nspecialized domain knowledge. This paper provides a thorough exploration of the\\nnuanced methodologies employed in integrating LLMs into the field of chemistry,\\ndelving into the complexities and innovations at this interdisciplinary\\njuncture. Specifically, our analysis begins with examining how molecular\\ninformation is fed into LLMs through various representation and tokenization\\nmethods. We then categorize chemical LLMs into three distinct groups based on\\nthe domain and modality of their input data, and discuss approaches for\\nintegrating these inputs for LLMs. Furthermore, this paper delves into the\\npretraining objectives with adaptations to chemical LLMs. After that, we\\nexplore the diverse applications of LLMs in chemistry, including novel\\nparadigms for their application in chemistry tasks. Finally, we identify\\npromising research directions, including further integration with chemical\\nknowledge, advancements in continual learning, and improvements in model\\ninterpretability, paving the way for groundbreaking developments in the field. Context-aware Machine Translation aims to improve translations of sentences\\nby incorporating surrounding sentences as context. Towards this task, two main\\narchitectures have been applied, namely single-encoder (based on concatenation)\\nand multi-encoder models. In this study, we show that a special case of\\nmulti-encoder architecture, where the latent representation of the source\\nsentence is cached and reused as the context in the next step, achieves higher\\naccuracy on the contrastive datasets (where the models have to rank the correct\\ntranslation among the provided sentences) and comparable BLEU and COMET scores\\nas the single- and multi-encoder approaches. Furthermore, we investigate the\\napplication of Sequence Shortening to the cached representations. We test three\\npooling-based shortening techniques and introduce two novel methods - Latent\\nGrouping and Latent Selecting, where the network learns to group tokens or\\nselects the tokens to be cached as context. Our experiments show that the two\\nmethods achieve competitive BLEU and COMET scores and accuracies on the\\ncontrastive datasets to the other tested methods while potentially allowing for\\nhigher interpretability and reducing the growth of memory requirements with\\nincreased context size. Symbolic Machine Learning Prover (SMLP) is a tool and a library for system\\nexploration based on data samples obtained by simulating or executing the\\nsystem on a number of input vectors. SMLP aims at exploring the system based on\\nthis data by taking a grey-box approach: SMLP combines statistical methods of\\ndata exploration with building and exploring machine learning models in close\\nfeedback loop with the system's response, and exploring these models by\\ncombining probabilistic and formal methods. SMLP has been applied in industrial\\nsetting at Intel for analyzing and optimizing hardware designs at the analog\\nlevel. SMLP is a general purpose tool and can be applied to systems that can be\\nsampled and modeled by machine learning models. Skin cancer detection through dermoscopy image analysis is a critical task.\\nHowever, existing models used for this purpose often lack interpretability and\\nreliability, raising the concern of physicians due to their black-box nature.\\nIn this paper, we propose a novel approach for the diagnosis of melanoma using\\nan interpretable prototypical-part model. We introduce a guided supervision\\nbased on non-expert feedback through the incorporation of: 1) binary masks,\\nobtained automatically using a segmentation network; and 2) user-refined\\nprototypes. These two distinct information pathways aim to ensure that the\\nlearned prototypes correspond to relevant areas within the skin lesion,\\nexcluding confounding factors beyond its boundaries. Experimental results\\ndemonstrate that, even without expert supervision, our approach achieves\\nsuperior performance and generalization compared to non-interpretable models. Current deep learning models are not designed to simultaneously address three\\nfundamental questions: predict class labels to solve a given classification\\ntask (the \\\"What?\\\"), explain task predictions (the \\\"Why?\\\"), and imagine\\nalternative scenarios that could result in different predictions (the \\\"What\\nif?\\\"). The inability to answer these questions represents a crucial gap in\\ndeploying reliable AI agents, calibrating human trust, and deepening\\nhuman-machine interaction. To bridge this gap, we introduce CounterFactual\\nConcept Bottleneck Models (CF-CBMs), a class of models designed to efficiently\\naddress the above queries all at once without the need to run post-hoc\\nsearches. Our results show that CF-CBMs produce: accurate predictions (the\\n\\\"What?\\\"), simple explanations for task predictions (the \\\"Why?\\\"), and\\ninterpretable counterfactuals (the \\\"What if?\\\"). CF-CBMs can also sample or\\nestimate the most probable counterfactual to: (i) explain the effect of concept\\ninterventions on tasks, (ii) show users how to get a desired class label, and\\n(iii) propose concept interventions via \\\"task-driven\\\" interventions. To comply with AI and data regulations, the need to forget private or\\ncopyrighted information from trained machine learning models is increasingly\\nimportant. The key challenge in unlearning is forgetting the necessary data in\\na timely manner, while preserving model performance. In this work, we address\\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\\nto remove data given only a trained model and the data to be forgotten. Under\\nsuch a definition, existing state-of-the-art methods are insufficient. Building\\non the concepts of Lipschitz continuity, we present a method that induces\\nsmoothing of the forget sample's output, with respect to perturbations of that\\nsample. We show this smoothing successfully results in forgetting while\\npreserving general model performance. We perform extensive empirical evaluation\\nof our method over a range of contemporary benchmarks, verifying that our\\nmethod achieves state-of-the-art performance under the strict constraints of\\nzero-shot unlearning. Self-supervised learning (SSL) learns representations by leveraging an\\nauxiliary unsupervised task, such as classifying semantically related samples,\\ne.g. different data augmentations or modalities. Of the many approaches to SSL,\\ncontrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for\\nlearning representations that achieve downstream performance close to that of\\nsupervised learning. However, a theoretical understanding of the mechanism\\nbehind these methods eludes. We propose a generative latent variable model for\\nthe data and show that several families of discriminative self-supervised\\nalgorithms, including contrastive methods, approximately induce its latent\\nstructure over representations, providing a unifying theoretical framework. We\\nalso justify links to mutual information and the use of a projection head.\\nFitting our model generatively, as SimVE, improves performance over previous\\nVAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows\\nthe gap to discriminative methods on _content_ classification and, as our\\nanalysis predicts, outperforms them where _style_ information is required,\\ntaking a step toward task-agnostic representations. Evaluating natural language generation (NLG) is a vital but challenging\\nproblem in artificial intelligence. Traditional evaluation metrics mainly\\ncapturing content (e.g. n-gram) overlap between system outputs and references\\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\\nhave demonstrated great potential in NLG evaluation in recent years. Various\\nautomatic evaluation methods based on LLMs have been proposed, including\\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\\nevaluation methods, and discuss their pros and cons, respectively. We also\\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\\nopen problems in this area and point out future research directions. In this paper we generalize and extend an idea of low-rank adaptation (LoRA)\\nof large language models (LLMs) based on Transformer architecture. Widely used\\nLoRA-like methods of fine-tuning LLMs are based on matrix factorization of\\ngradient update. We introduce LoTR, a novel approach for parameter-efficient\\nfine-tuning of LLMs which represents a gradient update to parameters in a form\\nof tensor decomposition. Low-rank adapter for each layer is constructed as a\\nproduct of three matrices, and tensor structure arises from sharing left and\\nright multipliers of this product among layers. Simultaneous compression of a\\nsequence of layers with low-rank tensor representation allows LoTR to archive\\neven better parameter efficiency then LoRA especially for deep models.\\nMoreover, the core tensor does not depend on original weight dimension and can\\nbe made arbitrary small, which allows for extremely cheap and fast downstream\\nfine-tuning. We introduce FindingEmo, a new image dataset containing annotations for 25k\\nimages, specifically tailored to Emotion Recognition. Contrary to existing\\ndatasets, it focuses on complex scenes depicting multiple people in various\\nnaturalistic, social settings, with images being annotated as a whole, thereby\\ngoing beyond the traditional focus on faces or single individuals. Annotated\\ndimensions include Valence, Arousal and Emotion label, with annotations\\ngathered using Prolific. Together with the annotations, we release the list of\\nURLs pointing to the original images, as well as all associated source code. There is an intricate relation between the properties of an image and how\\nhumans behave while describing the image. This behavior shows ample variation,\\nas manifested in human signals such as eye movements and when humans start to\\ndescribe the image. Despite the value of such signals of visuo-linguistic\\nvariation, they are virtually disregarded in the training of current pretrained\\nmodels, which motivates further investigation. Using a corpus of Dutch image\\ndescriptions with concurrently collected eye-tracking data, we explore the\\nnature of the variation in visuo-linguistic signals, and find that they\\ncorrelate with each other. Given this result, we hypothesize that variation\\nstems partly from the properties of the images, and explore whether image\\nrepresentations encoded by pretrained vision encoders can capture such\\nvariation. Our results indicate that pretrained models do so to a\\nweak-to-moderate degree, suggesting that the models lack biases about what\\nmakes a stimulus complex for humans and what leads to variations in human\\noutputs. In the field of natural language processing (NLP), Large Language Models\\n(LLMs) have precipitated a paradigm shift, markedly enhancing performance in\\nnatural language generation tasks. Despite these advancements, the\\ncomprehensive evaluation of LLMs remains an inevitable challenge for the\\ncommunity. Recently, the utilization of Multiple Choice Question Answering\\n(MCQA) as a benchmark for LLMs has gained considerable traction. This study\\ninvestigates the rationality of MCQA as an evaluation method for LLMs. If LLMs\\ngenuinely understand the semantics of questions, their performance should\\nexhibit consistency across the varied configurations derived from the same\\nquestions. Contrary to this expectation, our empirical findings suggest a\\nnotable disparity in the consistency of LLM responses, which we define as\\nREsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current\\nMCQA-based benchmarks may not adequately capture the true capabilities of LLMs,\\nwhich underscores the need for more robust evaluation mechanisms in assessing\\nthe performance of LLMs. This paper introduces a novel perspective to significantly mitigate\\ncatastrophic forgetting in continuous learning (CL), which emphasizes models'\\ncapacity to preserve existing knowledge and assimilate new information. Current\\nreplay-based methods treat every task and data sample equally and thus can not\\nfully exploit the potential of the replay buffer. In response, we propose\\nCOgnitive REplay (CORE), which draws inspiration from human cognitive review\\nprocesses. CORE includes two key strategies: Adaptive Quantity Allocation and\\nQuality-Focused Data Selection. The former adaptively modulates the replay\\nbuffer allocation for each task based on its forgetting rate, while the latter\\nguarantees the inclusion of representative data that best encapsulates the\\ncharacteristics of each task within the buffer. Our approach achieves an\\naverage accuracy of 37.95% on split-CIFAR10, surpassing the best baseline\\nmethod by 6.52%. Additionally, it significantly enhances the accuracy of the\\npoorest-performing task by 6.30% compared to the top baseline. Recent advancements in large vision-language models (LVLMs) have demonstrated\\nimpressive capability in visual information understanding with human language.\\nDespite these advances, LVLMs still face challenges with multimodal\\nhallucination, such as generating text descriptions of objects that are not\\npresent in the visual information. However, the underlying fundamental reasons\\nof multimodal hallucinations remain poorly explored. In this paper, we propose\\na new perspective, suggesting that the inherent biases in LVLMs might be a key\\nfactor in hallucinations. Specifically, we systematically identify a semantic\\nshift bias related to paragraph breaks ('$\\\\textbackslash n\\\\textbackslash n$'),\\nwhere the content before and after '$\\\\textbackslash n\\\\textbackslash n$' in the\\ntraining data frequently exhibit significant semantic changes. This pattern\\nleads the model to infer that the contents following '$\\\\textbackslash\\nn\\\\textbackslash n$' should be obviously different from the preceding contents\\nwith less hallucinatory descriptions, thereby increasing the probability of\\nhallucinatory descriptions subsequent to the '$\\\\textbackslash n\\\\textbackslash\\nn$'. We have validated this hypothesis on multiple publicly available LVLMs.\\nBesides, we find that deliberately inserting '$\\\\textbackslash n\\\\textbackslash\\nn$' at the generated description can induce more hallucinations. A simple\\nmethod is proposed to effectively mitigate the hallucination of LVLMs by\\nskipping the output of `\\\\textbackslash n'. Domain randomization is an effective computer vision technique for improving\\ntransferability of vision models across visually distinct domains exhibiting\\nsimilar content. Existing approaches, however, rely extensively on tweaking\\ncomplex and specialized simulation engines that are difficult to construct,\\nsubsequently affecting their feasibility and scalability. This paper introduces\\nBehAVE, a video understanding framework that uniquely leverages the plethora of\\nexisting commercial video games for domain randomization, without requiring\\naccess to their simulation engines. Under BehAVE (1) the inherent rich visual\\ndiversity of video games acts as the source of randomization and (2) player\\nbehavior -- represented semantically via textual descriptions of actions --\\nguides the *alignment* of videos with similar content. We test BehAVE on 25\\ngames of the first-person shooter (FPS) genre across various video and text\\nfoundation models and we report its robustness for domain randomization. BehAVE\\nsuccessfully aligns player behavioral patterns and is able to zero-shot\\ntransfer them to multiple unseen FPS games when trained on just one FPS game.\\nIn a more challenging setting, BehAVE manages to improve the zero-shot\\ntransferability of foundation models to unseen FPS games (up to 22%) even when\\ntrained on a game of a different genre (Minecraft). Code and dataset can be\\nfound at https://github.com/nrasajski/BehAVE. Supervised fairness-aware machine learning under distribution shifts is an\\nemerging field that addresses the challenge of maintaining equitable and\\nunbiased predictions when faced with changes in data distributions from source\\nto target domains. In real-world applications, machine learning models are\\noften trained on a specific dataset but deployed in environments where the data\\ndistribution may shift over time due to various factors. This shift can lead to\\nunfair predictions, disproportionately affecting certain groups characterized\\nby sensitive attributes, such as race and gender. In this survey, we provide a\\nsummary of various types of distribution shifts and comprehensively investigate\\nexisting methods based on these shifts, highlighting six commonly used\\napproaches in the literature. Additionally, this survey lists publicly\\navailable datasets and evaluation metrics for empirical studies. We further\\nexplore the interconnection with related research fields, discuss the\\nsignificant challenges, and identify potential directions for future studies. Kahneman & Tversky's $\\\\textit{prospect theory}$ tells us that humans perceive\\nrandom variables in a biased but well-defined manner; for example, humans are\\nfamously loss-averse. We show that objectives for aligning LLMs with human\\nfeedback implicitly incorporate many of these biases -- the success of these\\nobjectives (e.g., DPO) over cross-entropy minimization can partly be ascribed\\nto them being $\\\\textit{human-aware loss functions}$ (HALOs). However, the\\nutility functions these methods attribute to humans still differ from those in\\nthe prospect theory literature. Using a Kahneman-Tversky model of human\\nutility, we propose a HALO that directly maximizes the utility of generations\\ninstead of maximizing the log-likelihood of preferences, as current methods do.\\nWe call this approach Kahneman-Tversky Optimization (KTO), and it matches or\\nexceeds the performance of preference-based methods at scales from 1B to 30B.\\nCrucially, KTO does not need preferences -- only a binary signal of whether an\\noutput is desirable or undesirable for a given input. This makes it far easier\\nto use in the real world, where preference data is scarce and expensive. We propose a framework to learn semantics from raw audio signals using two\\ntypes of representations, encoding contextual and phonetic information\\nrespectively. Specifically, we introduce a speech-to-unit processing pipeline\\nthat captures two types of representations with different time resolutions. For\\nthe language model, we adopt a dual-channel architecture to incorporate both\\ntypes of representation. We also present new training objectives, masked\\ncontext reconstruction and masked context prediction, that push models to learn\\nsemantics effectively. Experiments on the sSIMI metric of Zero Resource Speech\\nBenchmark 2021 and Fluent Speech Command dataset show our framework learns\\nsemantics better than models trained with only one type of representation. Data-driven weather forecast based on machine learning (ML) has experienced\\nrapid development and demonstrated superior performance in the global\\nmedium-range forecast compared to traditional physics-based dynamical models.\\nHowever, most of these ML models struggle with accurately predicting extreme\\nweather, which is closely related to the extreme value prediction. Through\\nmathematical analysis, we prove that the use of symmetric losses, such as the\\nMean Squared Error (MSE), leads to biased predictions and underestimation of\\nextreme values. To address this issue, we introduce Exloss, a novel loss\\nfunction that performs asymmetric optimization and highlights extreme values to\\nobtain accurate extreme weather forecast. Furthermore, we introduce a\\ntraining-free extreme value enhancement strategy named ExEnsemble, which\\nincreases the variance of pixel values and improves the forecast robustness.\\nCombined with an advanced global weather forecast model, extensive experiments\\nshow that our solution can achieve state-of-the-art performance in extreme\\nweather prediction, while maintaining the overall forecast accuracy comparable\\nto the top medium-range forecast models. Prior research on AI-assisted human decision-making has explored several\\ndifferent explainable AI (XAI) approaches. A recent paper has proposed a\\nparadigm shift calling for hypothesis-driven XAI through a conceptual framework\\ncalled evaluative AI that gives people evidence that supports or refutes\\nhypotheses without necessarily giving a decision-aid recommendation. In this\\npaper we describe and evaluate an approach for hypothesis-driven XAI based on\\nthe Weight of Evidence (WoE) framework, which generates both positive and\\nnegative evidence for a given hypothesis. Through human behavioural\\nexperiments, we show that our hypothesis-driven approach increases decision\\naccuracy, reduces reliance compared to a recommendation-driven approach and an\\nAI-explanation-only baseline, but with a small increase in under-reliance\\ncompared to the recommendation-driven approach. Further, we show that\\nparticipants used our hypothesis-driven approach in a materially different way\\nto the two baselines. This paper explores the multifaceted consequences of federated unlearning\\n(FU) with data heterogeneity. We introduce key metrics for FU assessment,\\nconcentrating on verification, global stability, and local fairness, and\\ninvestigate the inherent trade-offs. Furthermore, we formulate the unlearning\\nprocess with data heterogeneity through an optimization framework. Our key\\ncontribution lies in a comprehensive theoretical analysis of the trade-offs in\\nFU and provides insights into data heterogeneity's impacts on FU. Leveraging\\nthese insights, we propose FU mechanisms to manage the trade-offs, guiding\\nfurther development for FU mechanisms. We empirically validate that our FU\\nmechanisms effectively balance trade-offs, confirming insights derived from our\\ntheoretical analysis. The paper addresses the question of whether it is appropriate to talk about\\n`mechanical minds' at all, and whether ChatGPT models can indeed be thought of\\nas realizations of that. Our paper adds a semantic argument to the current\\ndebate. The act of human assertion requires the formation of a veridicality\\njudgment. Modification of assertions with modals (John must be at home) and the\\nuse of subjective elements (John is obviously at home) indicate that the\\nspeaker is manipulating her judgments and, in a cooperative context, intends\\nher epistemic state to be transparent to the addressee. Veridicality judgments\\nare formed on the basis of two components: (i) evidence that relates to reality\\n(exogenous evidence) and (ii) endogenous evidence, such as preferences and\\nprivate beliefs. `Mechanical minds' lack these two components: (i) they do not\\nrelate to reality and (ii) do not have endogenous evidence. Therefore they lack\\nthe ability to form a belief about the world and a veridicality judgments\\naltogether. They can only mimic that judgment, but the output is not ground in\\nthe very foundations for it. Since the pioneering work on the lottery ticket hypothesis for graph neural\\nnetworks (GNNs) was proposed in Chen et al. (2021), the study on finding graph\\nlottery tickets (GLT) has become one of the pivotal focus in the GNN community,\\ninspiring researchers to discover sparser GLT while achieving comparable\\nperformance to original dense networks. In parallel, the graph structure has\\ngained substantial attention as a crucial factor in GNN training dynamics, also\\nelucidated by several recent studies. Despite this, contemporary studies on\\nGLT, in general, have not fully exploited inherent pathways in the graph\\nstructure and identified tickets in an iterative manner, which is\\ntime-consuming and inefficient. To address these limitations, we introduce\\nTEDDY, a one-shot edge sparsification framework that leverages structural\\ninformation by incorporating edge-degree information. Following edge\\nsparsification, we encourage the parameter sparsity during training via simple\\nprojected gradient descent on the $\\\\ell_0$ ball. Given the target sparsity\\nlevels for both the graph structure and the model parameters, our TEDDY\\nfacilitates efficient and rapid realization of GLT within a single training.\\nRemarkably, our experimental results demonstrate that TEDDY significantly\\nsurpasses conventional iterative approaches in generalization, even when\\nconducting one-shot sparsification that solely utilizes graph structures,\\nwithout taking node features into account. Beamforming techniques are considered as essential parts to compensate the\\nsevere path loss in millimeter-wave (mmWave) communications by adopting large\\nantenna arrays and formulating narrow beams to obtain satisfactory received\\npowers. However, performing accurate beam alignment over such narrow beams for\\nefficient link configuration by traditional beam selection approaches, mainly\\nrelied on channel state information, typically impose significant latency and\\ncomputing overheads, which is often infeasible in vehicle-to-vehicle (V2V)\\ncommunications like highly dynamic scenarios. In contrast, utilizing\\nout-of-band contextual information, such as vehicular position information, is\\na potential alternative to reduce such overheads. In this context, this paper\\npresents a deep learning-based solution on utilizing the vehicular position\\ninformation for predicting the optimal beams having sufficient mmWave received\\npowers so that the best V2V line-of-sight links can be ensured proactively.\\nAfter experimental evaluation of the proposed solution on real-world measured\\nmmWave sensing and communications datasets, the results show that the solution\\ncan achieve up to 84.58% of received power of link status on average, which\\nconfirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V\\ncommunications. Recent advancements in deep generative models, particularly with the\\napplication of CLIP (Contrastive Language Image Pretraining) to Denoising\\nDiffusion Probabilistic Models (DDPMs), have demonstrated remarkable\\neffectiveness in text to image generation. The well structured embedding space\\nof CLIP has also been extended to image to shape generation with DDPMs,\\nyielding notable results. Despite these successes, some fundamental questions\\narise: Does CLIP ensure the best results in shape generation from images? Can\\nwe leverage conditioning to bring explicit 3D knowledge into the generative\\nprocess and obtain better quality? This study introduces CISP (Contrastive\\nImage Shape Pre training), designed to enhance 3D shape synthesis guided by 2D\\nimages. CISP aims to enrich the CLIP framework by aligning 2D images with 3D\\nshapes in a shared embedding space, specifically capturing 3D characteristics\\npotentially overlooked by CLIP's text image focus. Our comprehensive analysis\\nassesses CISP's guidance performance against CLIP guided models, focusing on\\ngeneration quality, diversity, and coherence of the produced shapes with the\\nconditioning image. We find that, while matching CLIP in generation quality and\\ndiversity, CISP substantially improves coherence with input images,\\nunderscoring the value of incorporating 3D knowledge into generative models.\\nThese findings suggest a promising direction for advancing the synthesis of 3D\\nvisual content by integrating multimodal systems with 3D representations. The World Wide Web's connectivity is greatly attributed to the HTTP protocol,\\nwith HTTP messages offering informative header fields that appeal to\\ndisciplines like web security and privacy, especially concerning web tracking.\\nDespite existing research employing HTTP/S request messages to identify web\\ntrackers, HTTP/S response headers are often overlooked. This study endeavors to\\ndesign effective machine learning classifiers for web tracker detection using\\nHTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers,\\nobtained through the traffic monitoring browser extension T.EX, serves as our\\ndata set. Eleven supervised models were trained on Chrome data and tested\\nacross all browsers. The results demonstrated high accuracy, F1-score,\\nprecision, recall, and minimal log-loss error for Chrome and Firefox, but\\nsubpar performance on Brave, potentially due to its distinct data distribution\\nand feature set. The research suggests that these classifiers are viable for\\ndetecting web trackers in Chrome and Firefox. However, real-world application\\ntesting remains pending, and the distinction between tracker types and broader\\nlabel sources could be explored in future studies. With the development of generative models, the quality of generated content\\nkeeps increasing. Recently, open-source models have made it surprisingly easy\\nto manipulate and edit photos and videos, with just a few simple prompts. While\\nthese cutting-edge technologies have gained popularity, they have also given\\nrise to concerns regarding the privacy and portrait rights of individuals.\\nMalicious users can exploit these tools for deceptive or illegal purposes.\\nAlthough some previous works focus on protecting photos against generative\\nmodels, we find there are still gaps between protecting videos and images in\\nthe aspects of efficiency and effectiveness. Therefore, we introduce our\\nprotection method, PRIME, to significantly reduce the time cost and improve the\\nprotection performance. Moreover, to evaluate our proposed protection method,\\nwe consider both objective metrics and human subjective metrics. Our evaluation\\nresults indicate that PRIME only costs 8.3% GPU hours of the cost of the\\nprevious state-of-the-art method and achieves better protection results on both\\nhuman evaluation and objective metrics. Code can be found in\\nhttps://github.com/GuanlinLee/prime. Information Bottleneck (IB) is a widely used framework that enables the\\nextraction of information related to a target random variable from a source\\nrandom variable. In the objective function, IB controls the trade-off between\\ndata compression and predictiveness through the Lagrange multiplier $\\\\beta$.\\nTraditionally, to find the trade-off to be learned, IB requires a search for\\n$\\\\beta$ through multiple training cycles, which is computationally expensive.\\nIn this study, we introduce Flexible Variational Information Bottleneck (FVIB),\\nan innovative framework for classification task that can obtain optimal models\\nfor all values of $\\\\beta$ with single, computationally efficient training. We\\ntheoretically demonstrate that across all values of reasonable $\\\\beta$, FVIB\\ncan simultaneously maximize an approximation of the objective function for\\nVariational Information Bottleneck (VIB), the conventional IB method. Then we\\nempirically show that FVIB can learn the VIB objective as effectively as VIB.\\nFurthermore, in terms of calibration performance, FVIB outperforms other IB and\\ncalibration methods by enabling continuous optimization of $\\\\beta$. Our codes\\nare available at https://github.com/sotakudo/fvib. Speech contains rich information on the emotions of humans, and Speech\\nEmotion Recognition (SER) has been an important topic in the area of\\nhuman-computer interaction. The robustness of SER models is crucial,\\nparticularly in privacy-sensitive and reliability-demanding domains like\\nprivate healthcare. Recently, the vulnerability of deep neural networks in the\\naudio domain to adversarial attacks has become a popular area of research.\\nHowever, prior works on adversarial attacks in the audio domain primarily rely\\non iterative gradient-based techniques, which are time-consuming and prone to\\noverfitting the specific threat model. Furthermore, the exploration of sparse\\nperturbations, which have the potential for better stealthiness, remains\\nlimited in the audio domain. To address these challenges, we propose a\\ngenerator-based attack method to generate sparse and transferable adversarial\\nexamples to deceive SER models in an end-to-end and efficient manner. We\\nevaluate our method on two widely-used SER datasets, Database of Elicited Mood\\nin Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP),\\nand demonstrate its ability to generate successful sparse adversarial examples\\nin an efficient manner. Moreover, our generated adversarial examples exhibit\\nmodel-agnostic transferability, enabling effective adversarial attacks on\\nadvanced victim models. Recent advances of artificial intelligence (AI) code generators are opening\\nnew opportunities in software security research, including misuse by malicious\\nactors. We review use cases for AI code generators for security and introduce\\nan evaluation benchmark. Rain precipitation prediction is a challenging task as it depends on weather\\nand meteorological features which vary from location to location. As a result,\\na prediction model that performs well at one location does not perform well at\\nother locations due to the distribution shifts. In addition, due to global\\nwarming, the weather patterns are changing very rapidly year by year which\\ncreates the possibility of ineffectiveness of those models even at the same\\nlocation as time passes. In our work, we have proposed an adaptive deep\\nlearning-based framework in order to provide a solution to the aforementioned\\nchallenges. Our method can generalize the model for the prediction of\\nprecipitation for any location where the methods without adaptation fail. Our\\nmethod has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a\\ndeep neural network for predicting the precipitation of Paris, Los Angeles, and\\nTokyo, respectively. We propose a novel framework that leverages LLMs for full causal graph\\ndiscovery. While previous LLM-based methods have used a pairwise query\\napproach, this requires a quadratic number of queries which quickly becomes\\nimpractical for larger causal graphs. In contrast, the proposed framework uses\\na breadth-first search (BFS) approach which allows it to use only a linear\\nnumber of queries. We also show that the proposed method can easily incorporate\\nobservational data when available, to improve performance. In addition to being\\nmore time and data-efficient, the proposed framework achieves state-of-the-art\\nresults on real-world causal graphs of varying sizes. The results demonstrate\\nthe effectiveness and efficiency of the proposed method in discovering causal\\nrelationships, showcasing its potential for broad applicability in causal graph\\ndiscovery tasks across different domains. Self-supervised learning (SSL) has been incorporated into many\\nstate-of-the-art models in various domains, where SSL defines pretext tasks\\nbased on unlabeled datasets to learn contextualized and robust representations.\\nRecently, SSL has been a new trend in exploring the representation learning\\ncapability in the realm of tabular data, which is more challenging due to not\\nhaving explicit relations for learning descriptive representations. This survey\\naims to systematically review and summarize the recent progress and challenges\\nof SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal\\ndefinition of NS-TD and clarify its correlation to related studies. Then, these\\napproaches are categorized into three groups -- predictive learning,\\ncontrastive learning, and hybrid learning, with their motivations and strengths\\nof representative methods within each direction. On top of this, application\\nissues of SSL4NS-TD are presented, including automatic data engineering,\\ncross-table transferability, and domain knowledge integration. In addition, we\\nelaborate on existing benchmarks and datasets for NS-TD applications to discuss\\nthe performance of existing tabular models. Finally, we discuss the challenges\\nof SSL4NS-TD and provide potential directions for future research. We expect\\nour work to be useful in terms of encouraging more research on lowering the\\nbarrier to entry SSL for the tabular domain and improving the foundations for\\nimplicit tabular data. To tackle the issues of catastrophic forgetting and overfitting in few-shot\\nclass-incremental learning (FSCIL), previous work has primarily concentrated on\\npreserving the memory of old knowledge during the incremental phase. The role\\nof pre-trained model in shaping the effectiveness of incremental learning is\\nfrequently underestimated in these studies. Therefore, to enhance the\\ngeneralization ability of the pre-trained model, we propose Learning with Prior\\nKnowledge (LwPK) by introducing nearly free prior knowledge from a few\\nunlabeled data of subsequent incremental classes. We cluster unlabeled\\nincremental class samples to produce pseudo-labels, then jointly train these\\nwith labeled base class samples, effectively allocating embedding space for\\nboth old and new class data. Experimental results indicate that LwPK\\neffectively enhances the model resilience against catastrophic forgetting, with\\ntheoretical analysis based on empirical risk minimization and class distance\\nmeasurement corroborating its operational principles. The source code of LwPK\\nis publicly available at: \\\\url{https://github.com/StevenJ308/LwPK}. Efficient sampling of the Boltzmann distribution of molecular systems is a\\nlong-standing challenge. Recently, instead of generating long molecular\\ndynamics simulations, generative machine learning methods such as normalizing\\nflows have been used to learn the Boltzmann distribution directly, without\\nsamples. However, this approach is susceptible to mode collapse and thus often\\ndoes not explore the full configurational space. In this work, we address this\\nchallenge by separating the problem into two levels, the fine-grained and\\ncoarse-grained degrees of freedom. A normalizing flow conditioned on the\\ncoarse-grained space yields a probabilistic connection between the two levels.\\nTo explore the configurational space, we employ coarse-grained simulations with\\nactive learning which allows us to update the flow and make all-atom potential\\nenergy evaluations only when necessary. Using alanine dipeptide as an example,\\nwe show that our methods obtain a speedup to molecular dynamics simulations of\\napproximately 15.9 to 216.2 compared to the speedup of 4.5 of the current\\nstate-of-the-art machine learning approach. SWIN transformer is a prominent vision transformer model that has\\nstate-of-the-art accuracy in image classification tasks. Despite this success,\\nits unique architecture causes slower inference compared with similar deep\\nneural networks. Integer quantization of the model is one of the methods used\\nto improve its inference latency. However, state-of-the-art has not been able\\nto fully quantize the model. In this work, we improve upon the inference\\nlatency of the state-of-the-art methods by removing the floating-point\\noperations, which are associated with the GELU activation in Swin Transformer.\\nWhile previous work proposed to replace the non-integer operations with linear\\napproximation functions, we propose to replace GELU with ReLU activation. The\\nadvantage of ReLU over previous methods is its low memory and computation\\ncomplexity. We use iterative knowledge distillation to compensate for the lost\\naccuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN\\ntransformer and show that on an RTX 4090 NVIDIA GPU we can improve the\\ninference latency of the quantized SWIN transformer by at least $11\\\\%$ while\\nmaintaining an accuracy drop of under $0.5\\\\%$ on the ImageNet evaluation\\ndataset. Recent years have witnessed remarkable advances in artificial intelligence\\ngenerated content(AIGC), with diverse input modalities, e.g., text, image,\\nvideo, audio and 3D. The 3D is the most close visual modality to real-world 3D\\nenvironment and carries enormous knowledge. The 3D content generation shows\\nboth academic and practical values while also presenting formidable technical\\nchallenges. This review aims to consolidate developments within the burgeoning\\ndomain of 3D content generation. Specifically, a new taxonomy is proposed that\\ncategorizes existing approaches into three types: 3D native generative methods,\\n2D prior-based 3D generative methods, and hybrid 3D generative methods. The\\nsurvey covers approximately 60 papers spanning the major techniques. Besides,\\nwe discuss limitations of current 3D content generation techniques, and point\\nout open challenges as well as promising directions for future work.\\nAccompanied with this survey, we have established a project website where the\\nresources on 3D content generation research are provided. The project page is\\navailable at https://github.com/hitcslj/Awesome-AIGC-3D. While abundant research has been conducted on improving high-level visual\\nunderstanding and reasoning capabilities of large multimodal models~(LMMs),\\ntheir visual quality assessment~(IQA) ability has been relatively\\nunder-explored. Here we take initial steps towards this goal by employing the\\ntwo-alternative forced choice~(2AFC) prompting, as 2AFC is widely regarded as\\nthe most reliable way of collecting human opinions of visual quality.\\nSubsequently, the global quality score of each image estimated by a particular\\nLMM can be efficiently aggregated using the maximum a posterior estimation.\\nMeanwhile, we introduce three evaluation criteria: consistency, accuracy, and\\ncorrelation, to provide comprehensive quantifications and deeper insights into\\nthe IQA capability of five LMMs. Extensive experiments show that existing LMMs\\nexhibit remarkable IQA ability on coarse-grained quality comparison, but there\\nis room for improvement on fine-grained quality discrimination. The proposed\\ndataset sheds light on the future development of IQA models based on LMMs. The\\ncodes will be made publicly available at https://github.com/h4nwei/2AFC-LMMs. The omnipresence of NP-hard combinatorial optimization problems (COPs)\\ncompels domain experts to engage in trial-and-error heuristic design process.\\nThe long-standing endeavor of design automation has gained new momentum with\\nthe rise of large language models (LLMs). This paper introduces Language\\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\\nLLMs for heuristic generation, featuring minimal manual intervention and\\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\\n(ReEvo), a generic searching framework that emulates the reflective design\\napproach of human experts while far surpassing human capabilities with its\\nscalable LLM inference, Internet-scale domain knowledge, and powerful\\nevolutionary search. Evaluations across 12 COP settings show that 1) verbal\\nreflections for evolution lead to smoother fitness landscapes, explicit\\ninference of black-box COP settings, and better search results; 2) heuristics\\ngenerated by ReEvo in minutes can outperform state-of-the-art human designs and\\nneural solvers; 3) LHHs enable efficient algorithm design automation even when\\nchallenged with black-box COPs, demonstrating its potential for complex and\\nnovel real-world applications. Our code is available:\\nhttps://github.com/ai4co/LLM-as-HH. The (variational) graph auto-encoder is extensively employed for learning\\nrepresentations of graph-structured data. However, the formation of real-world\\ngraphs is a complex and heterogeneous process influenced by latent factors.\\nExisting encoders are fundamentally holistic, neglecting the entanglement of\\nlatent factors. This not only makes graph analysis tasks less effective but\\nalso makes it harder to understand and explain the representations. Learning\\ndisentangled graph representations with (variational) graph auto-encoder poses\\nsignificant challenges, and remains largely unexplored in the existing\\nliterature. In this article, we introduce the Disentangled Graph Auto-Encoder\\n(DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that\\nleverage generative models to learn disentangled representations. Specifically,\\nwe first design a disentangled graph convolutional network with multi-channel\\nmessage-passing layers, as the encoder aggregating information related to each\\ndisentangled latent factor. Subsequently, a component-wise flow is applied to\\neach channel to enhance the expressive capabilities of disentangled variational\\ngraph auto-encoder. Additionally, we design a factor-wise decoder, considering\\nthe characteristics of disentangled representations. In order to further\\nenhance the independence among representations, we introduce independence\\nconstraints on mapping channels for different latent factors. Empirical\\nexperiments on both synthetic and real-world datasets show the superiority of\\nour proposed method compared to several state-of-the-art baselines. In recent years, microservices have gained widespread adoption in IT\\noperations due to their scalability, maintenance, and flexibility. However, it\\nbecomes challenging for site reliability engineers (SREs) to pinpoint the root\\ncause due to the complex relationships in microservices when facing system\\nmalfunctions. Previous research employed structured learning methods (e.g.,\\nPC-algorithm) to establish causal relationships and derive root causes from\\ncausal graphs. Nevertheless, they ignored the temporal order of time series\\ndata and failed to leverage the rich information inherent in the temporal\\nrelationships. For instance, in cases where there is a sudden spike in CPU\\nutilization, it can lead to an increase in latency for other microservices.\\nHowever, in this scenario, the anomaly in CPU utilization occurs before the\\nlatency increase, rather than simultaneously. As a result, the PC-algorithm\\nfails to capture such characteristics. To address these challenges, we propose\\nRUN, a novel approach for root cause analysis using neural Granger causal\\ndiscovery with contrastive learning. RUN enhances the backbone encoder by\\nintegrating contextual information from time series, and leverages a time\\nseries forecasting model to conduct neural Granger causal discovery. In\\naddition, RUN incorporates Pagerank with a personalization vector to\\nefficiently recommend the top-k root causes. Extensive experiments conducted on\\nthe synthetic and real-world microservice-based datasets demonstrate that RUN\\nnoticeably outperforms the state-of-the-art root cause analysis methods.\\nMoreover, we provide an analysis scenario for the sock-shop case to showcase\\nthe practicality and efficacy of RUN in microservice-based applications. Our\\ncode is publicly available at https://github.com/zmlin1998/RUN. Automated Aerial Triangulation (AAT), aiming to restore image pose and\\nreconstruct sparse points simultaneously, plays a pivotal role in earth\\nobservation. With its rich research heritage spanning several decades in\\nphotogrammetry, AAT has evolved into a fundamental process widely applied in\\nlarge-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its\\nadvancements, classic AAT methods still face challenges like low efficiency and\\nlimited robustness. This paper introduces DeepAAT, a deep learning network\\ndesigned specifically for AAT of UAV imagery. DeepAAT considers both spatial\\nand spectral characteristics of imagery, enhancing its capability to resolve\\nerroneous matching pairs and accurately predict image poses. DeepAAT marks a\\nsignificant leap in AAT's efficiency, ensuring thorough scene coverage and\\nprecision. Its processing speed outpaces incremental AAT methods by hundreds of\\ntimes and global AAT methods by tens of times while maintaining a comparable\\nlevel of reconstruction accuracy. Additionally, DeepAAT's scene clustering and\\nmerging strategy facilitate rapid localization and pose determination for\\nlarge-scale UAV images, even under constrained computing resources. The\\nexperimental results demonstrate DeepAAT's substantial improvements over\\nconventional AAT methods, highlighting its potential in the efficiency and\\naccuracy of UAV-based 3D reconstruction tasks. To benefit the photogrammetry\\nsociety, the code of DeepAAT will be released at:\\nhttps://github.com/WHU-USI3DV/DeepAAT. We introduce \\\\textsc{Pok\\\\'eLLMon}, the first LLM-embodied agent that achieves\\nhuman-parity performance in tactical battle games, as demonstrated in Pok\\\\'emon\\nbattles. The design of \\\\textsc{Pok\\\\'eLLMon} incorporates three key strategies:\\n(i) In-context reinforcement learning that instantly consumes text-based\\nfeedback derived from battles to iteratively refine the policy; (ii)\\nKnowledge-augmented generation that retrieves external knowledge to counteract\\nhallucination and enables the agent to act timely and properly; (iii)\\nConsistent action generation to mitigate the \\\\textit{panic switching}\\nphenomenon when the agent faces a powerful opponent and wants to elude the\\nbattle. We show that online battles against human demonstrates\\n\\\\textsc{Pok\\\\'eLLMon}'s human-like battle strategies and just-in-time decision\\nmaking, achieving 49\\\\% of win rate in the Ladder competitions and 56\\\\% of win\\nrate in the invited battles. Our implementation and playable battle logs are\\navailable at: \\\\url{https://github.com/git-disl/PokeLLMon}. Understanding the irregular electrical activity of atrial fibrillation (AFib)\\nhas been a key challenge in electrocardiography. For serious cases of AFib,\\ncatheter ablations are performed to collect intracardiac electrograms (EGMs).\\nEGMs offer intricately detailed and localized electrical activity of the heart\\nand are an ideal modality for interpretable cardiac studies. Recent\\nadvancements in artificial intelligence (AI) has allowed some works to utilize\\ndeep learning frameworks to interpret EGMs during AFib. Additionally, language\\nmodels (LMs) have shown exceptional performance in being able to generalize to\\nunseen domains, especially in healthcare. In this study, we are the first to\\nleverage pretrained LMs for finetuning of EGM interpolation and AFib\\nclassification via masked language modeling. We formulate the EGM as a textual\\nsequence and present competitive performances on AFib classification compared\\nagainst other representations. Lastly, we provide a comprehensive\\ninterpretability study to provide a multi-perspective intuition of the model's\\nbehavior, which could greatly benefit the clinical use. Transfer learning (TL) has been demonstrated to improve DNN model performance\\nwhen faced with a scarcity of training samples. However, the suitability of TL\\nas a solution to reduce vulnerability of overfitted DNNs to privacy attacks is\\nunexplored. A class of privacy attacks called membership inference attacks\\n(MIAs) aim to determine whether a given sample belongs to the training dataset\\n(member) or not (nonmember). We introduce Double-Dip, a systematic empirical\\nstudy investigating the use of TL (Stage-1) combined with randomization\\n(Stage-2) to thwart MIAs on overfitted DNNs without degrading classification\\naccuracy. Our study examines the roles of shared feature space and parameter\\nvalues between source and target models, number of frozen layers, and\\ncomplexity of pretrained models. We evaluate Double-Dip on three (Target,\\nSource) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii)\\n(CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a)\\nVGG-19, (b) ResNet-18, (c) Swin-T, and (d) FaceNet. Our experiments demonstrate\\nthat Stage-1 reduces adversary success while also significantly increasing\\nclassification accuracy of nonmembers against an adversary with either\\nwhite-box or black-box DNN model access, attempting to carry out SOTA\\nlabel-only MIAs. After Stage-2, success of an adversary carrying out a\\nlabel-only MIA is further reduced to near 50%, bringing it closer to a random\\nguess and showing the effectiveness of Double-Dip. Stage-2 of Double-Dip also\\nachieves lower ASR and higher classification accuracy than regularization and\\ndifferential privacy-based methods. We study the problem of multi-agent reinforcement learning (MARL) with\\nadaptivity constraints -- a new problem motivated by real-world applications\\nwhere deployments of new policies are costly and the number of policy updates\\nmust be minimized. For two-player zero-sum Markov Games, we design a (policy)\\nelimination based algorithm that achieves a regret of $\\\\widetilde{O}(\\\\sqrt{H^3\\nS^2 ABK})$, while the batch complexity is only $O(H+\\\\log\\\\log K)$. In the above,\\n$S$ denotes the number of states, $A,B$ are the number of actions for the two\\nplayers respectively, $H$ is the horizon and $K$ is the number of episodes.\\nFurthermore, we prove a batch complexity lower bound\\n$\\\\Omega(\\\\frac{H}{\\\\log_{A}K}+\\\\log\\\\log K)$ for all algorithms with\\n$\\\\widetilde{O}(\\\\sqrt{K})$ regret bound, which matches our upper bound up to\\nlogarithmic factors. As a byproduct, our techniques naturally extend to\\nlearning bandit games and reward-free MARL within near optimal batch\\ncomplexity. To the best of our knowledge, these are the first line of results\\ntowards understanding MARL with low adaptivity. The execution of graph algorithms using neural networks has recently\\nattracted significant interest due to promising empirical progress. This\\nmotivates further understanding of how neural networks can replicate reasoning\\nsteps with relational data. In this work, we study the ability of transformer\\nnetworks to simulate algorithms on graphs from a theoretical perspective. The\\narchitecture that we utilize is a looped transformer with extra attention heads\\nthat interact with the graph. We prove by construction that this architecture\\ncan simulate algorithms such as Dijkstra's shortest path algorithm, Breadth-\\nand Depth-First Search, and Kosaraju's strongly connected components algorithm.\\nThe width of the network does not increase with the size of the input graph,\\nwhich implies that the network can simulate the above algorithms for any graph.\\nDespite this property, we show that there is a limit to simulation in our\\nsolution due to finite precision. Finally, we show a Turing Completeness result\\nwith constant width when the extra attention heads are utilized. Large monolithic generative models trained on massive amounts of data have\\nbecome an increasingly dominant approach in AI research. In this paper, we\\nargue that we should instead construct large generative systems by composing\\nsmaller generative models together. We show how such a compositional generative\\napproach enables us to learn distributions in a more data-efficient manner,\\nenabling generalization to parts of the data distribution unseen at training\\ntime. We further show how this enables us to program and construct new\\ngenerative models for tasks completely unseen at training. Finally, we show\\nthat in many cases, we can discover separate compositional components from\\ndata. Emerging Distributed AI systems are revolutionizing big data computing and\\ndata processing capabilities with growing economic and societal impact.\\nHowever, recent studies have identified new attack surfaces and risks caused by\\nsecurity, privacy, and fairness issues in AI systems. In this paper, we review\\nrepresentative techniques, algorithms, and theoretical foundations for\\ntrustworthy distributed AI through robustness guarantee, privacy protection,\\nand fairness awareness in distributed learning. We first provide a brief\\noverview of alternative architectures for distributed learning, discuss\\ninherent vulnerabilities for security, privacy, and fairness of AI algorithms\\nin distributed learning, and analyze why these problems are present in\\ndistributed learning regardless of specific architectures. Then we provide a\\nunique taxonomy of countermeasures for trustworthy distributed AI, covering (1)\\nrobustness to evasion attacks and irregular queries at inference, and\\nrobustness to poisoning attacks, Byzantine attacks, and irregular data\\ndistribution during training; (2) privacy protection during distributed\\nlearning and model inference at deployment; and (3) AI fairness and governance\\nwith respect to both data and models. We conclude with a discussion on open\\nchallenges and future research directions toward trustworthy distributed AI,\\nsuch as the need for trustworthy AI policy guidelines, the AI\\nresponsibility-utility co-design, and incentives and compliance. The generalization ability of Deep Neural Networks (DNNs) is still not fully\\nunderstood, despite numerous theoretical and empirical analyses. Recently,\\nAllen-Zhu & Li (2023) introduced the concept of multi-views to explain the\\ngeneralization ability of DNNs, but their main target is ensemble or distilled\\nmodels, and no method for estimating multi-views used in a prediction of a\\nspecific input is discussed. In this paper, we propose Minimal Sufficient Views\\n(MSVs), which is similar to multi-views but can be efficiently computed for\\nreal images. MSVs is a set of minimal and distinct features in an input, each\\nof which preserves a model's prediction for the input. We empirically show that\\nthere is a clear relationship between the number of MSVs and prediction\\naccuracy across models, including convolutional and transformer models,\\nsuggesting that a multi-view like perspective is also important for\\nunderstanding the generalization ability of (non-ensemble or non-distilled)\\nDNNs. In the current era of vast data and transparent machine learning, it is\\nessential for techniques to operate at a large scale while providing a clear\\nmathematical comprehension of the internal workings of the method. Although\\nthere already exist interpretable semi-parametric regression methods for\\nlarge-scale applications that take into account non-linearity in the data, the\\ncomplexity of the models is still often limited. One of the main challenges is\\nthe absence of interactions in these models, which are left out for the sake of\\nbetter interpretability but also due to impractical computational costs. To\\novercome this limitation, we propose a new approach using a factorization\\nmethod to derive a highly scalable higher-order tensor product spline model.\\nOur method allows for the incorporation of all (higher-order) interactions of\\nnon-linear feature effects while having computational costs proportional to a\\nmodel without interactions. We further develop a meaningful penalization scheme\\nand examine the induced optimization problem. We conclude by evaluating the\\npredictive and estimation performance of our method. The development of electronic health records (EHR) systems has enabled the\\ncollection of a vast amount of digitized patient data. However, utilizing EHR\\ndata for predictive modeling presents several challenges due to its unique\\ncharacteristics. With the advancements in machine learning techniques, deep\\nlearning has demonstrated its superiority in various applications, including\\nhealthcare. This survey systematically reviews recent advances in deep\\nlearning-based predictive models using EHR data. Specifically, we begin by\\nintroducing the background of EHR data and providing a mathematical definition\\nof the predictive modeling task. We then categorize and summarize predictive\\ndeep models from multiple perspectives. Furthermore, we present benchmarks and\\ntoolkits relevant to predictive modeling in healthcare. Finally, we conclude\\nthis survey by discussing open challenges and suggesting promising directions\\nfor future research. With the widespread adoption of Large Language Models (LLMs), in this paper\\nwe investigate the multilingual capability of these models. Our preliminary\\nresults show that, translating the native language context, question and answer\\ninto a high resource language produced the best results. Training Large Language Models (LLMs) to follow user instructions has been\\nshown to supply the LLM with ample capacity to converse fluently while being\\naligned with humans. Yet, it is not completely clear how an LLM can lead a\\nplan-grounded conversation in mixed-initiative settings where instructions flow\\nin both directions of the conversation, i.e. both the LLM and the user provide\\ninstructions to one another. In this paper, we tackle a dual goal\\nmixed-initiative conversational setting where the LLM not only grounds the\\nconversation on an arbitrary plan but also seeks to satisfy both a procedural\\nplan and user instructions. The LLM is then responsible for guiding the user\\nthrough the plan and, at the same time, adapting to new circumstances,\\nanswering questions, and activating safety guardrails when needed. We propose a\\nnovel LLM that grounds the dialogue on a procedural plan, can take the dialogue\\ninitiative, and enforces guardrails on the system's behavior, while also\\nimproving the LLM's responses to unexpected user behavior. Experiments in\\ncontrolled settings and with real users show that the best-performing model,\\nwhich we call PlanLLM, achieves a 2.1x improvement over a strong baseline.\\nMoreover, experiments also show good generalization to unseen domains. Transformers are the dominant architecture for sequence modeling, but there\\nis growing interest in models that use a fixed-size latent state that does not\\ndepend on the sequence length, which we refer to as \\\"generalized state space\\nmodels\\\" (GSSMs). In this paper we show that while GSSMs are promising in terms\\nof inference-time efficiency, they are limited compared to transformer models\\non tasks that require copying from the input context. We start with a\\ntheoretical analysis of the simple task of string copying and prove that a two\\nlayer transformer can copy strings of exponential length while GSSMs are\\nfundamentally limited by their fixed-size latent state. Empirically, we find\\nthat transformers outperform GSSMs in terms of efficiency and generalization on\\nsynthetic tasks that require copying the context. Finally, we evaluate\\npretrained large language models and find that transformer models dramatically\\noutperform state space models at copying and retrieving information from\\ncontext. Taken together, these results suggest a fundamental gap between\\ntransformers and GSSMs on tasks of practical interest. Large Language Model (LLM) agents, capable of performing a broad range of\\nactions, such as invoking tools and controlling robots, show great potential in\\ntackling real-world challenges. LLM agents are typically prompted to produce\\nactions by generating JSON or text in a pre-defined format, which is usually\\nlimited by constrained action space (e.g., the scope of pre-defined tools) and\\nrestricted flexibility (e.g., inability to compose multiple tools). This work\\nproposes to use executable Python code to consolidate LLM agents' actions into\\na unified action space (CodeAct). Integrated with a Python interpreter, CodeAct\\ncan execute code actions and dynamically revise prior actions or emit new\\nactions upon new observations through multi-turn interactions. Our extensive\\nanalysis of 17 LLMs on API-Bank and a newly curated benchmark shows that\\nCodeAct outperforms widely used alternatives (up to 20% higher success rate).\\nThe encouraging performance of CodeAct motivates us to build an open-source LLM\\nagent that interacts with environments by executing interpretable code and\\ncollaborates with users using natural language. To this end, we collect an\\ninstruction-tuning dataset CodeActInstruct that consists of 7k multi-turn\\ninteractions using CodeAct. We show that it can be used with existing data to\\nimprove models in agent-oriented tasks without compromising their general\\ncapability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with\\nPython interpreter and uniquely tailored to perform sophisticated tasks (e.g.,\\nmodel training) using existing libraries and autonomously self-debug. We build on the theory of ontology logs (ologs) created by Spivak and Kent,\\nand define a notion of wiring diagrams. In this article, a wiring diagram is a\\nfinite directed labelled graph. The labels correspond to types in an olog; they\\ncan also be interpreted as readings of sensors in an autonomous system. As\\nsuch, wiring diagrams can be used as a framework for an autonomous system to\\nform abstract concepts. We show that the graphs underlying skeleton wiring\\ndiagrams form a category. This allows skeleton wiring diagrams to be compared\\nand manipulated using techniques from both graph theory and category theory. We\\nalso extend the usual definition of graph edit distance to the case of wiring\\ndiagrams by using operations only available to wiring diagrams, leading to a\\nmetric on the set of all skeleton wiring diagrams. In the end, we give an\\nextended example on calculating the distance between two concepts represented\\nby wiring diagrams, and explain how to apply our framework to any application\\ndomain. Recent advancements in Large Language Models (LLMs) have been reshaping\\nNatural Language Processing (NLP) task in several domains. Their use in the\\nfield of Human Resources (HR) has still room for expansions and could be\\nbeneficial for several time consuming tasks. Examples such as time-off\\nsubmissions, medical claims filing, and access requests are noteworthy, but\\nthey are by no means the sole instances. However, the aforementioned\\ndevelopments must grapple with the pivotal challenge of constructing a\\nhigh-quality training dataset. On one hand, most conversation datasets are\\nsolving problems for customers not employees. On the other hand, gathering\\nconversations with HR could raise privacy concerns. To solve it, we introduce\\nHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR\\ndomains to evaluate LLM Agent. Our work has the following contributions: (1) It\\nis the first labeled open-sourced conversation dataset in the HR domain for NLP\\nresearch. (2) It provides a detailed recipe for the data generation procedure\\nalong with data analysis and human evaluations. The data generation pipeline is\\ntransferable and can be easily adapted for labeled conversation data generation\\nin other domains. (3) The proposed data-collection pipeline is mostly based on\\nLLMs with minimal human involvement for annotation, which is time and\\ncost-efficient. Text-to-image generative AI models such as Stable Diffusion are used daily by\\nmillions worldwide. However, many have raised concerns regarding how these\\nmodels amplify racial and gender stereotypes. To study this phenomenon, we\\ndevelop a classifier to predict the race, gender, and age group of any given\\nface image, and show that it achieves state-of-the-art performance. Using this\\nclassifier, we quantify biases in Stable Diffusion across six races, two\\ngenders, five age groups, 32 professions, and eight attributes. We then propose\\nnovel debiasing solutions that outperform state-of-the-art alternatives.\\nAdditionally, we examine the degree to which Stable Diffusion depicts\\nindividuals of the same race as being similar to one another. This analysis\\nreveals a high degree of stereotyping, e.g., depicting most middle eastern\\nmales as being dark-skinned, bearded, and wearing a traditional headdress. We\\naddress these limitations by proposing yet another novel solution that\\nincreases facial diversity across genders and racial groups. Our solutions are\\nopen-sourced and made publicly available. Understanding the importance of the inputs on the output is useful across\\nmany tasks. This work provides an information-theoretic framework to analyse\\nthe influence of inputs for text classification tasks. Natural language\\nprocessing (NLP) tasks take either a single element input or multiple element\\ninputs to predict an output variable, where an element is a block of text. Each\\ntext element has two components: an associated semantic meaning and a\\nlinguistic realization. Multiple-choice reading comprehension (MCRC) and\\nsentiment classification (SC) are selected to showcase the framework. For MCRC,\\nit is found that the context influence on the output compared to the question\\ninfluence reduces on more challenging datasets. In particular, more challenging\\ncontexts allow a greater variation in complexity of questions. Hence, test\\ncreators need to carefully consider the choice of the context when designing\\nmultiple-choice questions for assessment. For SC, it is found the semantic\\nmeaning of the input text dominates (above 80\\\\% for all datasets considered)\\ncompared to its linguistic realisation when determining the sentiment. The\\nframework is made available at:\\nhttps://github.com/WangLuran/nlp-element-influence In this paper, we study the inductive biases of two major approaches to\\naugmenting Transformers with a recurrent mechanism - (1) the approach of\\nincorporating a depth-wise recurrence similar to Universal Transformers; and\\n(2) the approach of incorporating a chunk-wise temporal recurrence like\\nTemporal Latent Bottleneck. Furthermore, we propose and investigate novel ways\\nto extend and combine the above methods - for example, we propose a global\\nmean-based dynamic halting mechanism for Universal Transformer and an\\naugmentation of Temporal Latent Bottleneck with elements from Universal\\nTransformer. We compare the models and probe their inductive biases in several\\ndiagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling,\\nListOps, and Logical Inference. Knowledge Graphs popularity has been rapidly growing in last years. All that\\nknowledge is available for people to query it through the many online databases\\non the internet. Though, it would be a great achievement if non-programmer\\nusers could access whatever information they want to know. There has been a lot\\nof effort oriented to solve this task using natural language processing tools\\nand creativity encouragement by way of many challenges. Our approach focuses on\\nassuming a correct entity linking on the natural language questions and\\ntraining a GPT model to create SPARQL queries from them. We managed to isolate\\nwhich property of the task can be the most difficult to solve at few or\\nzero-shot and we proposed pre-training on all entities (under CWA) to improve\\nthe performance. We obtained a 62.703% accuracy of exact SPARQL matches on\\ntesting at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of\\n0.009 on the question answering challenge. Statistical learning theory is the foundation of machine learning, providing\\ntheoretical bounds for the risk of models learnt from a (single) training set,\\nassumed to issue from an unknown probability distribution. In actual\\ndeployment, however, the data distribution may (and often does) vary, causing\\ndomain adaptation/generalization issues. In this paper we lay the foundations\\nfor a `credal' theory of learning, using convex sets of probabilities (credal\\nsets) to model the variability in the data-generating distribution. Such credal\\nsets, we argue, may be inferred from a finite sample of training sets. Bounds\\nare derived for the case of finite hypotheses spaces (both assuming\\nrealizability or not) as well as infinite model spaces, which directly\\ngeneralize classical results. In this paper we present a novel approach to interpretable AI inspired by\\nQuantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified\\nautoencoder neural network whose latent layer is prescribed to be a subset of\\n$n$-point correlation functions. Regarding images as draws from a lattice field\\ntheory, this architecture mimics the task of perturbatively constructing the\\neffective action of the theory order by order in an expansion using Feynman\\ndiagrams. Alternatively, the NCoder may be regarded as simulating the procedure\\nof statistical inference whereby high dimensional data is first summarized in\\nterms of several lower dimensional summary statistics (here the $n$-point\\ncorrelation functions), and subsequent out-of-sample data is generated by\\ninferring the data generating distribution from these statistics. In this way\\nthe NCoder suggests a fascinating correspondence between perturbative\\nrenormalizability and the sufficiency of models. We demonstrate the efficacy of\\nthe NCoder by applying it to the generation of MNIST images, and find that\\ngenerated images can be correctly classified using only information from the\\nfirst three $n$-point functions of the image distribution. Existing methods for evaluating large language models face challenges such as\\ndata contamination, sensitivity to prompts, and the high cost of benchmark\\ncreation. To address this, we propose a lossless data compression based\\nevaluation approach that tests how models' predictive abilities generalize\\nafter their training cutoff. Specifically, we collect comprehensive test data\\nspanning 83 months from 2017 to 2023 and split the data into training and\\ntesting periods according to models' training data cutoff. We measure: 1) the\\ncompression performance on the testing period as a measure of generalization on\\nunseen data; and 2) the performance gap between the training and testing period\\nas a measure of robustness. Our experiments test 14 representative large\\nlanguage models with various sizes on sources including Wikipedia, news\\narticles, code, arXiv papers, and multi-modal data. We find that the\\ncompression rate of many models reduces significantly after their cutoff date,\\nbut models such as Mistral and Llama-2 demonstrate a good balance between\\nperformance and robustness. Results also suggest that models struggle to\\ngeneralize on news and code data, but work especially well on arXiv papers. We\\nalso find the context size and tokenization implementation have a big impact of\\non the overall compression performance. We introduce SymbolicAI, a versatile and modular framework employing a\\nlogic-based approach to concept learning and flow management in generative\\nprocesses. SymbolicAI enables the seamless integration of generative models\\nwith a diverse range of solvers by treating large language models (LLMs) as\\nsemantic parsers that execute tasks based on both natural and formal language\\ninstructions, thus bridging the gap between symbolic reasoning and generative\\nAI. We leverage probabilistic programming principles to tackle complex tasks,\\nand utilize differentiable and classical programming paradigms with their\\nrespective strengths. The framework introduces a set of polymorphic,\\ncompositional, and self-referential operations for data stream manipulation,\\naligning LLM outputs with user objectives. As a result, we can transition\\nbetween the capabilities of various foundation models endowed with zero- and\\nfew-shot learning capabilities and specialized, fine-tuned models or solvers\\nproficient in addressing specific problems. In turn, the framework facilitates\\nthe creation and evaluation of explainable computational graphs. We conclude by\\nintroducing a quality measure and its empirical score for evaluating these\\ncomputational graphs, and propose a benchmark that compares various\\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\\nempirical score as the \\\"Vector Embedding for Relational Trajectory Evaluation\\nthrough Cross-similarity\\\", or VERTEX score for short. The framework codebase\\nand benchmark are linked below. The effectiveness of Intrusion Detection Systems (IDS) is critical in an era\\nwhere cyber threats are becoming increasingly complex. Machine learning (ML)\\nand deep learning (DL) models provide an efficient and accurate solution for\\nidentifying attacks and anomalies in computer networks. However, using ML and\\nDL models in IDS has led to a trust deficit due to their non-transparent\\ndecision-making. This transparency gap in IDS research is significant,\\naffecting confidence and accountability. To address, this paper introduces a\\nnovel Explainable IDS approach, called X-CBA, that leverages the structural\\nadvantages of Graph Neural Networks (GNNs) to effectively process network\\ntraffic data, while also adapting a new Explainable AI (XAI) methodology.\\nUnlike most GNN-based IDS that depend on labeled network traffic and node\\nfeatures, thereby overlooking critical packet-level information, our approach\\nleverages a broader range of traffic data through network flows, including edge\\nattributes, to improve detection capabilities and adapt to novel threats.\\nThrough empirical testing, we establish that our approach not only achieves\\nhigh accuracy with 99.47% in threat detection but also advances the field by\\nproviding clear, actionable explanations of its analytical outcomes. This\\nresearch also aims to bridge the current gap and facilitate the broader\\nintegration of ML/DL technologies in cybersecurity defenses by offering a local\\nand global explainability solution that is both precise and interpretable. Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing\\ntasks of increasing importance in privacy research. Modern AA leverages an\\nauthor's consistent writing style to match a text to its author using an AA\\nclassifier. AO is the corresponding adversarial task, aiming to modify a text\\nin such a way that its semantics are preserved, yet an AA model cannot\\ncorrectly infer its authorship. To address privacy concerns raised by\\nstate-of-the-art (SOTA) AA methods, new AO methods have been proposed but\\nremain largely impractical to use due to their prohibitively slow training and\\nobfuscation speed, often taking hours. To this challenge, we propose a\\npractical AO method, ALISON, that (1) dramatically reduces training/obfuscation\\ntime, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2)\\nachieves better obfuscation success through attacking three transformer-based\\nAA methods on two benchmark datasets, typically performing 15% better than\\ncompeting methods, (3) does not require direct signals from a target AA\\nclassifier during obfuscation, and (4) utilizes unique stylometric features,\\nallowing sound model interpretation for explainable obfuscation. We also\\ndemonstrate that ALISON can effectively prevent four SOTA AA methods from\\naccurately determining the authorship of ChatGPT-generated texts, all while\\nminimally changing the original text semantics. To ensure the reproducibility\\nof our findings, our code and data are available at:\\nhttps://github.com/EricX003/ALISON. Despite the crucial importance of addressing Black Hole failures in Internet\\nbackbone networks, effective detection strategies in backbone networks are\\nlacking. This is largely because previous research has been centered on Mobile\\nAd-hoc Networks (MANETs), which operate under entirely different dynamics,\\nprotocols, and topologies, making their findings not directly transferable to\\nbackbone networks. Furthermore, detecting Black Hole failures in backbone\\nnetworks is particularly challenging. It requires a comprehensive range of\\nnetwork data due to the wide variety of conditions that need to be considered,\\nmaking data collection and analysis far from straightforward. Addressing this\\ngap, our study introduces a novel approach for Black Hole detection in backbone\\nnetworks using specialized Yet Another Next Generation (YANG) data models with\\nBlack Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our\\nmethod of selecting and analyzing four YANG models relevant to Black Hole\\ndetection in ISP networks, focusing on routing protocols and ISP-specific\\nconfigurations. Our BHMM approach derived from these models demonstrates a 10%\\nimprovement in detection accuracy and a 13% increase in packet delivery rate,\\nhighlighting the efficiency of our approach. Additionally, we evaluate the\\nMachine Learning approach leveraged with BHMM analysis in two different network\\nsettings, a commercial ISP network, and a scientific research-only network\\ntopology. This evaluation also demonstrates the practical applicability of our\\nmethod, yielding significantly improved prediction outcomes in both\\nenvironments. Mixture of Experts (MoE) architectures have recently started burgeoning due\\nto their ability to scale model's capacity while maintaining the computational\\ncost affordable. Furthermore, they can be applied to both Transformers and\\nState Space Models, the current state-of-the-art models in numerous fields.\\nWhile MoE has been mostly investigated for the pre-training stage, its use in\\nparameter-efficient transfer learning settings is under-explored. To narrow\\nthis gap, this paper attempts to demystify the use of MoE for\\nparameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and\\nspeech downstream tasks. Specifically, we propose Soft Mixture of Adapters\\n(Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft\\nMoE method, it relies on a soft assignment between the input tokens and experts\\nto keep the computational time limited. Extensive experiments across 4\\nbenchmarks demonstrate that Soft-MoA outperforms the single adapter method and\\nperforms on par with the dense MoA counterpart. We finally present ablation\\nstudies on key elements of Soft-MoA, showing for example that Soft-MoA achieves\\nbetter scaling with more experts, as well as ensuring that all experts\\ncontribute to the computation of the output tokens, thus dispensing with the\\nexpert imbalance issue. Self-supervised skill learning aims to acquire useful behaviors that leverage\\nthe underlying dynamics of the environment. Latent variable models, based on\\nmutual information maximization, have been particularly successful in this task\\nbut still struggle in the context of robotic manipulation. As it requires\\nimpacting a possibly large set of degrees of freedom composing the environment,\\nmutual information maximization fails alone in producing useful manipulation\\nbehaviors. To address this limitation, we introduce SLIM, a multi-critic\\nlearning approach for skill discovery with a particular focus on robotic\\nmanipulation. Our main insight is that utilizing multiple critics in an\\nactor-critic framework to gracefully combine multiple reward functions leads to\\na significant improvement in latent-variable skill discovery for robotic\\nmanipulation while overcoming possible interference occurring among rewards\\nwhich hinders convergence to useful skills. Furthermore, in the context of\\ntabletop manipulation, we demonstrate the applicability of our novel skill\\ndiscovery approach to acquire safe and efficient motor primitives in a\\nhierarchical reinforcement learning fashion and leverage them through planning,\\nsurpassing the state-of-the-art approaches for skill discovery by a large\\nmargin. Recent years have witnessed a growing interest in Wi-Fi-based gesture\\nrecognition. However, existing works have predominantly focused on closed-set\\nparadigms, where all testing gestures are predefined during training. This\\nposes a significant challenge in real-world applications, as unseen gestures\\nmight be misclassified as known classes during testing. To address this issue,\\nwe propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR)\\nframework. Implementing OSGR requires addressing challenges caused by the\\nunique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and\\ndomains, leads to widely scattered and irregular data distributions in\\ncollected Wi-Fi sensing data. Consequently, data ambiguity between classes and\\nchallenges in defining appropriate decision boundaries to identify unknowns\\narise. To tackle these challenges, WiOpen adopts a two-fold approach to\\neliminate uncertainty and define precise decision boundaries. Initially, it\\naddresses uncertainty induced by noise during data preprocessing by utilizing\\nthe CSI ratio. Next, it designs the OSGR network based on an uncertainty\\nquantification method. Throughout the learning process, this network\\neffectively mitigates uncertainty stemming from domains. Ultimately, the\\nnetwork leverages relationships among samples' neighbors to dynamically define\\nopen-set decision boundaries, successfully realizing OSGR. Comprehensive\\nexperiments on publicly accessible datasets confirm WiOpen's effectiveness.\\nNotably, WiOpen also demonstrates superiority in cross-domain tasks when\\ncompared to state-of-the-art approaches. Shielding is a popular technique for achieving safe reinforcement learning\\n(RL). However, classical shielding approaches come with quite restrictive\\nassumptions making them difficult to deploy in complex environments,\\nparticularly those with continuous state or action spaces. In this paper we\\nextend the more versatile approximate model-based shielding (AMBS) framework to\\nthe continuous setting. In particular we use Safety Gym as our test-bed,\\nallowing for a more direct comparison of AMBS with popular constrained RL\\nalgorithms. We also provide strong probabilistic safety guarantees for the\\ncontinuous setting. In addition, we propose two novel penalty techniques that\\ndirectly modify the policy gradient, which empirically provide more stable\\nconvergence in our experiments. In industrial scenarios, there is widespread use of collaborative robots\\n(cobots), and growing interest is directed at evaluating and measuring the\\nimpact of some characteristics of the cobot on the human factor. In the present\\npilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -\\nAdapted to the participant's pace) of a cobot has on the Experiential Locus of\\nControl (ELoC) and the emotional state of 31 participants has been examined.\\nThe operators' performance, the degree of basic internal Locus of Control, and\\nthe attitude towards the robots were also considered. No difference was found\\nregarding the emotional state and the ELoC in the three conditions, but\\nconsidering the other psychological variables, a more complex situation\\nemerges. Overall, results seem to indicate a need to consider the person's\\npsychological characteristics to offer a differentiated and optimal interaction\\nexperience. Recent advancements on Large Language Models (LLMs) enable AI Agents to\\nautomatically generate and execute multi-step plans to solve complex tasks.\\nHowever, since LLM's content generation process is hardly controllable, current\\nLLM-based agents frequently generate invalid or non-executable plans, which\\njeopardizes the performance of the generated plans and corrupts users' trust in\\nLLM-based agents. In response, this paper proposes a novel ``Formal-LLM''\\nframework for LLM-based agents by integrating the expressiveness of natural\\nlanguage and the precision of formal language. Specifically, the framework\\nallows human users to express their requirements or constraints for the\\nplanning process as an automaton. A stack-based LLM plan generation process is\\nthen conducted under the supervision of the automaton to ensure that the\\ngenerated plan satisfies the constraints, making the planning process\\ncontrollable. We conduct experiments on both benchmark tasks and practical\\nreal-life tasks, and our framework achieves over 50% overall performance\\nincrease, which validates the feasibility and effectiveness of employing\\nFormal-LLM to guide the plan generation of agents, preventing the agents from\\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\\nagents can facilitate the broader utilization of LLM in application scenarios\\nwhere high validity of planning is essential. The work is open-sourced at\\nhttps://github.com/agiresearch/Formal-LLM. Pretrained large language models (LLMs) are surprisingly effective at\\nperforming zero-shot tasks, including time-series forecasting. However,\\nunderstanding the mechanisms behind such capabilities remains highly\\nchallenging due to the complexity of the models. In this paper, we study LLMs'\\nability to extrapolate the behavior of dynamical systems whose evolution is\\ngoverned by principles of physical interest. Our results show that LLaMA 2, a\\nlanguage model trained primarily on texts, achieves accurate predictions of\\ndynamical system time series without fine-tuning or prompt engineering.\\nMoreover, the accuracy of the learned physical rules increases with the length\\nof the input context window, revealing an in-context version of neural scaling\\nlaw. Along the way, we present a flexible and efficient algorithm for\\nextracting probability density functions of multi-digit numbers directly from\\nLLMs. Feature attribution methods (FAs), such as gradients and attention, are\\nwidely employed approaches to derive the importance of all input features to\\nthe model predictions. Existing work in natural language processing has mostly\\nfocused on developing and testing FAs for encoder-only language models (LMs) in\\nclassification tasks. However, it is unknown if it is faithful to use these FAs\\nfor decoder-only models on text generation, due to the inherent differences\\nbetween model architectures and task settings respectively. Moreover, previous\\nwork has demonstrated that there is no `one-wins-all' FA across models and\\ntasks. This makes the selection of a FA computationally expensive for large LMs\\nsince input importance derivation often requires multiple forward and backward\\npasses including gradient computations that might be prohibitive even with\\naccess to large compute. To address these issues, we present a model-agnostic\\nFA for generative LMs called Recursive Attribution Generator (ReAGent). Our\\nmethod updates the token importance distribution in a recursive manner. For\\neach update, we compute the difference in the probability distribution over the\\nvocabulary for predicting the next token between using the original input and\\nusing a modified version where a part of the input is replaced with RoBERTa\\npredictions. Our intuition is that replacing an important token in the context\\nshould have resulted in a larger change in the model's confidence in predicting\\nthe token than replacing an unimportant token. Our method can be universally\\napplied to any generative LM without accessing internal model weights or\\nadditional training and fine-tuning, as most other FAs require. We extensively\\ncompare the faithfulness of ReAGent with seven popular FAs across six\\ndecoder-only LMs of various sizes. The results show that our method\\nconsistently provides more faithful token importance distributions. We introduce a novel framework for incorporating human expertise into\\nalgorithmic predictions. Our approach focuses on the use of human judgment to\\ndistinguish inputs which `look the same' to any feasible predictive algorithm.\\nWe argue that this framing clarifies the problem of human/AI collaboration in\\nprediction tasks, as experts often have access to information -- particularly\\nsubjective information -- which is not encoded in the algorithm's training\\ndata. We use this insight to develop a set of principled algorithms for\\nselectively incorporating human feedback only when it improves the performance\\nof any feasible predictor. We find empirically that although algorithms often\\noutperform their human counterparts on average, human judgment can\\nsignificantly improve algorithmic predictions on specific instances (which can\\nbe identified ex-ante). In an X-ray classification task, we find that this\\nsubset constitutes nearly 30% of the patient population. Our approach provides\\na natural way of uncovering this heterogeneity and thus enabling effective\\nhuman-AI collaboration. Attention mechanisms have been widely used to capture long-range dependencies\\namong nodes in Graph Transformers. Bottlenecked by the quadratic computational\\ncost, attention mechanisms fail to scale in large graphs. Recent improvements\\nin computational efficiency are mainly achieved by attention sparsification\\nwith random or heuristic-based graph subsampling, which falls short in\\ndata-dependent context reasoning. State space models (SSMs), such as Mamba,\\nhave gained prominence for their effectiveness and efficiency in modeling\\nlong-range dependencies in sequential data. However, adapting SSMs to\\nnon-sequential graph data presents a notable challenge. In this work, we\\nintroduce Graph-Mamba, the first attempt to enhance long-range context modeling\\nin graph networks by integrating a Mamba block with the input-dependent node\\nselection mechanism. Specifically, we formulate graph-centric node\\nprioritization and permutation strategies to enhance context-aware reasoning,\\nleading to a substantial improvement in predictive performance. Extensive\\nexperiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms\\nstate-of-the-art methods in long-range graph prediction tasks, with a fraction\\nof the computational cost in both FLOPs and GPU memory consumption. The code\\nand models are publicly available at https://github.com/bowang-lab/Graph-Mamba. Deep neural networks (DNNs), trained with gradient-based optimization and\\nbackpropagation, are currently the primary tool in modern artificial\\nintelligence, machine learning, and data science. In many applications, DNNs\\nare trained offline, through supervised learning or reinforcement learning, and\\ndeployed online for inference. However, training DNNs with standard\\nbackpropagation and gradient-based optimization gives no intrinsic performance\\nguarantees or bounds on the DNN, which is essential for applications such as\\ncontrols. Additionally, many offline-training and online-inference problems,\\nsuch as sim2real transfer of reinforcement learning policies, experience domain\\nshift from the training distribution to the real-world distribution. To address\\nthese stability and transfer learning issues, we propose using techniques from\\ncontrol theory to update DNN parameters online. We formulate the\\nfully-connected feedforward DNN as a continuous-time dynamical system, and we\\npropose novel last-layer update laws that guarantee desirable error convergence\\nunder various conditions on the time derivative of the DNN input vector. We\\nfurther show that training the DNN under spectral normalization controls the\\nupper bound of the error trajectories of the online DNN predictions, which is\\ndesirable when numerically differentiated quantities or noisy state\\nmeasurements are input to the DNN. The proposed online DNN adaptation laws are\\nvalidated in simulation to learn the dynamics of the Van der Pol system under\\ndomain shift, where parameters are varied in inference from the training\\ndataset. The simulations demonstrate the effectiveness of using\\ncontrol-theoretic techniques to derive performance improvements and guarantees\\nin DNN-based learning systems. We present a comprehensive survey of the advancements and techniques in the\\nfield of tractable probabilistic generative modeling, primarily focusing on\\nProbabilistic Circuits (PCs). We provide a unified perspective on the inherent\\ntrade-offs between expressivity and the tractability, highlighting the design\\nprinciples and algorithmic extensions that have enabled building expressive and\\nefficient PCs, and provide a taxonomy of the field. We also discuss recent\\nefforts to build deep and hybrid PCs by fusing notions from deep neural models,\\nand outline the challenges and open questions that can guide future research in\\nthis evolving field. Machine unlearning is a desirable operation as models get increasingly\\ndeployed on data with unknown provenance. However, achieving exact unlearning\\n-- obtaining a model that matches the model distribution when the data to be\\nforgotten was never used -- is challenging or inefficient, often requiring\\nsignificant retraining. In this paper, we focus on efficient unlearning methods\\nfor the task adaptation phase of a pretrained large language model (LLM). We\\nobserve that an LLM's ability to do in-context learning for task adaptation\\nallows for efficient exact unlearning of task adaptation training data. We\\nprovide an algorithm for selecting few-shot training examples to prepend to the\\nprompt given to an LLM (for task adaptation), ERASE, whose unlearning operation\\ncost is independent of model and dataset size, meaning it scales to large\\nmodels and datasets. We additionally compare our approach to fine-tuning\\napproaches and discuss the trade-offs between the two approaches. This leads us\\nto propose a new holistic measure of unlearning cost which accounts for varying\\ninference costs, and conclude that in-context learning can often be more\\nfavourable than fine-tuning for deployments involving unlearning requests. Artificial intelligence (AI) in healthcare has significantly advanced\\nintelligent medical treatment. However, traditional intelligent healthcare is\\nlimited by static data and unified standards, preventing full integration with\\nindividual situations and other challenges. Hence, a more professional and\\ndetailed intelligent healthcare method is needed for development. To this end,\\nwe propose an innovative framework named Heath-LLM, which combines large-scale\\nfeature extraction and medical knowledge trade-off scoring. Compared to\\ntraditional health management methods, our approach has three main advantages.\\nFirst, our method integrates health reports into a large model to provide\\ndetailed task information. Second, professional medical expertise is used to\\nadjust the weighted scores of health characteristics. Third, we use a\\nsemi-automated feature extraction framework to enhance the analytical power of\\nlanguage models and incorporate expert insights to improve the accuracy of\\ndisease prediction. We have conducted disease prediction experiments on a large\\nnumber of health reports to assess the effectiveness of Health-LLM. The results\\nof the experiments indicate that the proposed method surpasses traditional\\nmethods and has the potential to revolutionize disease prediction and\\npersonalized health management. The code is available at\\nhttps://github.com/jmyissb/HealthLLM. A common approach for aligning language models to human preferences is to\\nfirst learn a reward model from preference data, and then use this reward model\\nto update the language model. We study two closely related problems that arise\\nin this approach. First, any monotone transformation of the reward model\\npreserves preference ranking; is there a choice that is ``better'' than others?\\nSecond, we often wish to align language models to multiple properties: how\\nshould we combine multiple reward models? Using a probabilistic interpretation\\nof the alignment procedure, we identify a natural choice for transformation for\\n(the common case of) rewards learned from Bradley-Terry preference models. This\\nderived transformation has two important properties. First, it emphasizes\\nimproving poorly-performing outputs, rather than outputs that already score\\nwell. This mitigates both underfitting (where some prompts are not improved)\\nand reward hacking (where the model learns to exploit misspecification of the\\nreward model). Second, it enables principled aggregation of rewards by linking\\nsummation to logical conjunction: the sum of transformed rewards corresponds to\\nthe probability that the output is ``good'' in all measured properties, in a\\nsense we make precise. Experiments aligning language models to be both helpful\\nand harmless using RLHF show substantial improvements over the baseline\\n(non-transformed) approach. Many real-world applications involve some agents that fall into two teams,\\nwith payoffs that are equal within the same team but of opposite sign across\\nthe opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can\\nbe resolved with reinforcement learning in recent years. However, existing\\nmethods are thus inefficient in light of insufficient consideration of\\nintra-team credit assignment, data utilization and computational\\nintractability. In this paper, we propose the individual-global-minimax (IGMM)\\nprinciple to ensure the coherence between two-team minimax behaviors and the\\nindividual greedy behaviors through Q functions in 2t0sMGs. Based on it, we\\npresent a novel multi-agent reinforcement learning framework, Factorized\\nMulti-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q\\nfunction into individual ones and iteratively solve for the IGMM-satisfied\\nminimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with\\nneural networks is proposed to implement FM3Q and obtain the deterministic and\\ndecentralized minimax policies for two-team players. A theoretical analysis is\\nprovided to prove the convergence of FM3Q. Empirically, we use three\\nenvironments to evaluate the learning efficiency and final performance of FM3Q\\nand show its superiority on 2t0sMGs. Neural Style Transfer (NST) refers to a class of algorithms able to\\nmanipulate an element, most often images, to adopt the appearance or style of\\nanother one. Each element is defined as a combination of Content and Style: the\\nContent can be conceptually defined as the what and the Style as the how of\\nsaid element. In this context, we propose a custom NST framework for\\ntransferring a set of styles to the motion of a robotic manipulator, e.g., the\\nsame robotic task can be carried out in an angry, happy, calm, or sad way. An\\nautoencoder architecture extracts and defines the Content and the Style of the\\ntarget robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3)\\nnetwork generates the robot control policy using the loss defined by the\\nautoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the\\nrobot motion by introducing the trained style. Such an approach can be\\nimplemented either offline, for carrying out autonomous robot motions in\\ndynamic environments, or online, for adapting at runtime the style of a\\nteleoperated robot. The considered styles can be learned online from human\\ndemonstrations. We carried out an evaluation with human subjects enrolling 73\\nvolunteers, asking them to recognize the style behind some representative\\nrobotic motions. Results show a good recognition rate, proving that it is\\npossible to convey different styles to a robot using this approach. Intent-Based Networking (IBN) presents a paradigm shift for network\\nmanagement, by promising to align intents and business objectives with network\\noperations--in an automated manner. However, its practical realization is\\nchallenging: 1) processing intents, i.e., translate, decompose and identify the\\nlogic to fulfill the intent, and 2) intent conformance, that is, considering\\ndynamic networks, the logic should be adequately adapted to assure intents. To\\naddress the latter, intent assurance is tasked with continuous verification and\\nvalidation, including taking the necessary actions to align the operational and\\ntarget states. In this paper, we define an assurance framework that allows us\\nto detect and act when intent drift occurs. To do so, we leverage AI-driven\\npolicies, generated by Large Language Models (LLMs) which can quickly learn the\\nnecessary in-context requirements, and assist with the fulfillment and\\nassurance of intents. Quantifying uncertainty in automatically generated text is important for\\nletting humans check potential hallucinations and making systems more reliable.\\nConformal prediction is an attractive framework to provide predictions imbued\\nwith statistical guarantees, however, its application to text generation is\\nchallenging since any i.i.d. assumptions are not realistic. In this paper, we\\nbridge this gap by leveraging recent results on non-exchangeable conformal\\nprediction, which still ensures bounds on coverage. The result,\\nnon-exchangeable conformal nucleus sampling, is a novel extension of the\\nconformal prediction framework to generation based on nearest neighbors. Our\\nmethod can be used post-hoc for an arbitrary model without extra training and\\nsupplies token-level, calibrated prediction sets equipped with statistical\\nguarantees. Experiments in machine translation and language modeling show\\nencouraging results in generation quality. By also producing tighter prediction\\nsets with good coverage, we thus give a more theoretically principled way to\\nperform sampling with conformal guarantees. The development and training of deep learning models have become increasingly\\ncostly and complex. Consequently, software engineers are adopting pre-trained\\nmodels (PTMs) for their downstream applications. The dynamics of the PTM supply\\nchain remain largely unexplored, signaling a clear need for structured datasets\\nthat document not only the metadata but also the subsequent applications of\\nthese models. Without such data, the MSR community cannot comprehensively\\nunderstand the impact of PTM adoption and reuse. This paper presents the\\nPeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed\\nsnapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with\\n28,575 open-source software repositories from GitHub that utilize these models.\\nAdditionally, the dataset includes 44,337 mappings from 15,129 downstream\\nGitHub repositories to the 2,530 PTMs they use. To enhance the dataset's\\ncomprehensiveness, we developed prompts for a large language model to\\nautomatically extract model metadata, including the model's training datasets,\\nparameters, and evaluation metrics. Our analysis of this dataset provides the\\nfirst summary statistics for the PTM supply chain, showing the trend of PTM\\ndevelopment and common shortcomings of PTM package documentation. Our example\\napplication reveals inconsistencies in software licenses across PTMs and their\\ndependent projects. PeaTMOSS lays the foundation for future research, offering\\nrich opportunities to investigate the PTM supply chain. We outline mining\\nopportunities on PTMs, their downstream usage, and cross-cutting questions. $ $Large Language Models (LLMs) are being increasingly utilized in various\\napplications, with code generations being a notable example. While previous\\nresearch has shown that LLMs have the capability to generate both secure and\\ninsecure code, the literature does not take into account what factors help\\ngenerate secure and effective code. Therefore in this paper we focus on\\nidentifying and understanding the conditions and contexts in which LLMs can be\\neffectively and safely deployed in real-world scenarios to generate quality\\ncode. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and\\nGPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to\\nassess each model's code generation capabilities. We contextualized our study\\nto represent the typical use cases of a real-life developer employing LLMs for\\neveryday tasks as work. Additionally, we place an emphasis on security\\nawareness which is represented through the use of two distinct versions of our\\ndeveloper persona. In total, we collected 61 code outputs and analyzed them\\nacross several aspects: functionality, security, performance, complexity, and\\nreliability. These insights are crucial for understanding the models'\\ncapabilities and limitations, guiding future development and practical\\napplications in the field of automated code generation. One of the most important challenges of Smart City Applications is to adapt\\nthe system to interact with non-expert users. Robot imitation frameworks aim to\\nsimplify and reduce times of robot programming by allowing users to program\\ndirectly through demonstrations. In classical frameworks, actions are modeled\\nusing joint or Cartesian space trajectories. Other features, such as visual\\nones, are not always well represented with these pure geometrical approaches.\\nContinuous Goal-Directed Actions (CGDA) is an alternative to these methods, as\\nit encodes actions as changes of any feature that can be extracted from the\\nenvironment. As a consequence of this, the robot joint trajectories for\\nexecution must be fully computed to comply with this feature-agnostic encoding.\\nThis is achieved using Evolutionary Algorithms (EA), which usually requires too\\nmany evaluations to perform this evolution step in the actual robot. Current\\nstrategies involve performing evaluations in a simulation, transferring the\\nfinal joint trajectory to the actual robot. Smart City applications involve\\nworking in highly dynamic and complex environments, where having a precise\\nmodel is not always achievable. Our goal is to study the tractability of\\nperforming these evaluations directly in a real-world scenario. Two different\\napproaches to reduce the number of evaluations using EA, are proposed and\\ncompared. In the first approach, Particle Swarm Optimization (PSO)-based\\nmethods have been studied and compared within CGDA: naive PSO, Fitness\\nInheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO\\n(AFFG-PSO). The second approach studied the introduction of geometrical and\\nvelocity constraints within CGDA. The effects of both approaches were analyzed\\nand compared in the wax and paint actions, two CGDA commonly studied use cases.\\nResults from this paper depict an important reduction in the number of\\nevaluations. Style Transfer has been proposed in a number of fields: fine arts, natural\\nlanguage processing, and fixed trajectories. We scale this concept up to\\ncontrol policies within a Deep Reinforcement Learning infrastructure. Each\\nnetwork is trained to maximize the expected reward, which typically encodes the\\ngoal of an action, and can be described as the content. The expressive power of\\ndeep neural networks enables encoding a secondary task, which can be described\\nas the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to\\ntransfer the style of one policy to another, while maintaining the content of\\nthe latter. Different policies are defined via Deep Q-Network architectures.\\nThese models are trained using demonstrations through Inverse Reinforcement\\nLearning. Two different sets of user demonstrations are performed, one for\\ncontent and other for style. Different styles are encoded as defined by user\\ndemonstrations. The generated policy is the result of feeding a content policy\\nand a style policy to the NPST algorithm. Experiments are performed in a\\ncatch-ball game inspired by the Deep Reinforcement Learning classical Atari\\ngames; and a real-world painting scenario with a full-sized humanoid robot,\\nbased on previous works of the authors. The implementation of three different\\nQ-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode\\nthe policies within the NPST framework is proposed and the results obtained in\\nthe experiments with each of these architectures compared. The current success of Reinforcement Learning algorithms for its performance\\nin complex environments has inspired many recent theoretical approaches to\\ncognitive science. Artistic environments are studied within the cognitive\\nscience community as rich, natural, multi-sensory, multi-cultural environments.\\nIn this work, we propose the introduction of Reinforcement Learning for\\nimproving the control of artistic robot applications. Deep Q-learning Neural\\nNetworks (DQN) is one of the most successful algorithms for the implementation\\nof Reinforcement Learning in robotics. DQN methods generate complex control\\npolicies for the execution of complex robot applications in a wide set of\\nenvironments. Current art painting robot applications use simple control laws\\nthat limits the adaptability of the frameworks to a set of simple environments.\\nIn this work, the introduction of DQN within an art painting robot application\\nis proposed. The goal is to study how the introduction of a complex control\\npolicy impacts the performance of a basic art painting robot application. The\\nmain expected contribution of this work is to serve as a first baseline for\\nfuture works introducing DQN methods for complex art painting robot frameworks.\\nExperiments consist of real world executions of human drawn sketches using the\\nDQN generated policy and TEO, the humanoid robot. Results are compared in terms\\nof similarity and obtained reward with respect to the reference inputs Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to\\nretrieve pedestrian images of the same identity from different modalities\\nwithout annotations. While prior work focuses on establishing cross-modality\\npseudo-label associations to bridge the modality-gap, they ignore maintaining\\nthe instance-level homogeneous and heterogeneous consistency in pseudo-label\\nspace, resulting in coarse associations. In response, we introduce a\\nModality-Unified Label Transfer (MULT) module that simultaneously accounts for\\nboth homogeneous and heterogeneous fine-grained instance-level structures,\\nyielding high-quality cross-modality label associations. It models both\\nhomogeneous and heterogeneous affinities, leveraging them to define the\\ninconsistency for the pseudo-labels and then minimize it, leading to\\npseudo-labels that maintain alignment across modalities and consistency within\\nintra-modality structures. Additionally, a straightforward plug-and-play Online\\nCross-memory Label Refinement (OCLR) module is proposed to further mitigate the\\nimpact of noisy pseudo-labels while simultaneously aligning different\\nmodalities, coupled with a Modality-Invariant Representation Learning (MIRL)\\nframework. Experiments demonstrate that our proposed method outperforms\\nexisting USL-VI-ReID methods, highlighting the superiority of our MULT in\\ncomparison to other cross-modality association methods. The code will be\\navailable. Large Language Models (LLMs) have demonstrated significant potential in\\nhandling complex reasoning tasks through step-by-step rationale generation.\\nHowever, recent studies have raised concerns regarding the hallucination and\\nflaws in their reasoning process. Substantial efforts are being made to improve\\nthe reliability and faithfulness of the generated rationales. Some approaches\\nmodel reasoning as planning, while others focus on annotating for process\\nsupervision. Nevertheless, the planning-based search process often results in\\nhigh latency due to the frequent assessment of intermediate reasoning states\\nand the extensive exploration space. Additionally, supervising the reasoning\\nprocess with human annotation is costly and challenging to scale for LLM\\ntraining. To address these issues, in this paper, we propose a framework to\\nlearn planning-based reasoning through direct preference optimization (DPO) on\\ncollected trajectories, which are ranked according to synthesized process\\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\\nthe effectiveness of our learning framework, showing that our 7B model can\\nsurpass the strong counterparts like GPT-3.5-Turbo. We concentrate on a novel human-centric image synthesis task, that is, given\\nonly one reference facial photograph, it is expected to generate specific\\nindividual images with diverse head positions, poses, and facial expressions in\\ndifferent contexts. To accomplish this goal, we argue that our generative model\\nshould be capable of the following favorable characteristics: (1) a strong\\nvisual and semantic understanding of our world and human society for basic\\nobject and human image generation. (2) generalizable identity preservation\\nability. (3) flexible and fine-grained head control. Recently, large\\npre-trained text-to-image diffusion models have shown remarkable results,\\nserving as a powerful generative foundation. As a basis, we aim to unleash the\\nabove two capabilities of the pre-trained model. In this work, we present a new\\nframework named CapHuman. We embrace the ``encode then learn to align\\\"\\nparadigm, which enables generalizable identity preservation for new individuals\\nwithout cumbersome tuning at inference. CapHuman encodes identity features and\\nthen learns to align them into the latent space. Moreover, we introduce the 3D\\nfacial prior to equip our model with control over the human head in a flexible\\nand 3D-consistent manner. Extensive qualitative and quantitative analyses\\ndemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,\\nand high-fidelity portraits with content-rich representations and various head\\nrenditions, superior to established baselines. Code and checkpoint will be\\nreleased at https://github.com/VamosC/CapHuman. Time-series data presents limitations stemming from data quality issues, bias\\nand vulnerabilities, and generalization problem. Integrating universal data\\nsynthesis methods holds promise in improving generalization. However, current\\nmethods cannot guarantee that the generator's output covers all unseen real\\ndata. In this paper, we introduce InfoBoost -- a highly versatile cross-domain\\ndata synthesizing framework with time series representation learning\\ncapability. We have developed a method based on synthetic data that enables\\nmodel training without the need for real data, surpassing the performance of\\nmodels trained with real data. Additionally, we have trained a universal\\nfeature extractor based on our synthetic data that is applicable to all\\ntime-series data. Our approach overcomes interference from multiple sources\\nrhythmic signal, noise interference, and long-period features that exceed\\nsampling window capabilities. Through experiments, our non-deep-learning\\nsynthetic data enables models to achieve superior reconstruction performance\\nand universal explicit representation extraction without the need for real\\ndata. Video foreground segmentation (VFS) is an important computer vision task\\nwherein one aims to segment the objects under motion from the background. Most\\nof the current methods are image-based, i.e., rely only on spatial cues while\\nignoring motion cues. Therefore, they tend to overfit the training data and\\ndon't generalize well to out-of-domain (OOD) distribution. To solve the above\\nproblem, prior works exploited several cues such as optical flow, background\\nsubtraction mask, etc. However, having a video data with annotations like\\noptical flow is a challenging task. In this paper, we utilize the temporal\\ninformation and the spatial cues from the video data to improve OOD\\nperformance. However, the challenge lies in how we model the temporal\\ninformation given the video data in an interpretable way creates a very\\nnoticeable difference. We therefore devise a strategy that integrates the\\ntemporal context of the video in the development of VFS. Our approach give rise\\nto deep learning architectures, namely MUSTAN1 and MUSTAN2 and they are based\\non the idea of multi-scale temporal context as an attention, i.e., aids our\\nmodels to learn better representations that are beneficial for VFS. Further, we\\nintroduce a new video dataset, namely Indoor Surveillance Dataset (ISD) for\\nVFS. It has multiple annotations on a frame level such as foreground binary\\nmask, depth map, and instance semantic annotations. Therefore, ISD can benefit\\nother computer vision tasks. We validate the efficacy of our architectures and\\ncompare the performance with baselines. We demonstrate that proposed methods\\nsignificantly outperform the benchmark methods on OOD. In addition, the\\nperformance of MUSTAN2 is significantly improved on certain video categories on\\nOOD data due to ISD. This paper presents sandra, a neuro-symbolic reasoner combining vectorial\\nrepresentations with deductive reasoning. Sandra builds a vector space\\nconstrained by an ontology and performs reasoning over it. The geometric nature\\nof the reasoner allows its combination with neural networks, bridging the gap\\nwith symbolic knowledge representations. Sandra is based on the Description and\\nSituation (DnS) ontology design pattern, a formalization of frame semantics.\\nGiven a set of facts (a situation) it allows to infer all possible perspectives\\n(descriptions) that can provide a plausible interpretation for it, even in\\npresence of incomplete information. We prove that our method is correct with\\nrespect to the DnS model. We experiment with two different tasks and their\\nstandard benchmarks, demonstrating that, without increasing complexity, sandra\\n(i) outperforms all the baselines (ii) provides interpretability in the\\nclassification process, and (iii) allows control over the vector space, which\\nis designed a priori. Simultaneous localisation and mapping (SLAM) algorithms are commonly used in\\nrobotic systems for learning maps of novel environments. Brains also appear to\\nlearn maps, but the mechanisms are not known and it is unclear how to infer\\nthese maps from neural activity data. We present BrainSLAM; a method for\\nperforming SLAM using only population activity (local field potential, LFP)\\ndata simultaneously recorded from three brain regions in rats: hippocampus,\\nprefrontal cortex, and parietal cortex. This system uses a convolutional neural\\nnetwork (CNN) to decode velocity and familiarity information from wavelet\\nscalograms of neural local field potential data recorded from rats as they\\nnavigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture,\\npowering an attractor network which performs path integration plus a separate\\nsystem which performs `loop closure' (detecting previously visited locations\\nand correcting map aliasing errors). Together, these three components can\\nconstruct faithful representations of the environment while simultaneously\\ntracking the animal's location. This is the first demonstration of inference of\\na spatial map from brain recordings. Our findings expand SLAM to a new\\nmodality, enabling a new method of mapping environments and facilitating a\\nbetter understanding of the role of cognitive maps in navigation and decision\\nmaking. This work introduces EE-Tuning, a lightweight and economical solution to\\ntraining/tuning early-exit large language models (LLMs). In contrast to the\\ncommon approach of full-parameter pre-training, EE-Tuning augments any\\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\\nlayers that are tuned in a parameter-efficient manner, which requires\\nsignificantly less computational resources and training data. Our\\nimplementation of EE-Tuning achieves outstanding training efficiency via\\nextensive performance optimizations, as well as scalability due to its full\\ncompatibility with 3D parallelism. Results of systematic experiments validate\\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\\ncan be achieved with a limited training budget. In hope of making early-exit\\nLLMs accessible to the community, we release the source code of our\\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM. This paper introduces a user-friendly platform developed by the University of\\nKentucky Center for Applied AI, designed to make large, customized language\\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\\ndiverse range of users and projects. The paper outlines the system's\\narchitecture and key features, encompassing dataset curation, model training,\\nsecure inference, and text-based feature extraction.\\n  We illustrate the establishment of a tenant-aware computational network using\\nagent-based methods, securely utilizing islands of isolated resources as a\\nunified system. The platform strives to deliver secure LLM services,\\nemphasizing process and data isolation, end-to-end encryption, and role-based\\nresource authentication. This contribution aligns with the overarching goal of\\nenabling simplified access to cutting-edge AI models and technology in support\\nof scientific discovery. Explanations in interactive machine-learning systems facilitate debugging and\\nimproving prediction models. However, the effectiveness of various global\\nmodel-centric and data-centric explanations in aiding domain experts to detect\\nand resolve potential data issues for model improvement remains unexplored.\\nThis research investigates the influence of data-centric and model-centric\\nglobal explanations in systems that support healthcare experts in optimising\\nmodels through automated and manual data configurations. We conducted\\nquantitative (n=70) and qualitative (n=30) studies with healthcare experts to\\nexplore the impact of different explanations on trust, understandability and\\nmodel improvement. Our results reveal the insufficiency of global model-centric\\nexplanations for guiding users during data configuration. Although data-centric\\nexplanations enhanced understanding of post-configuration system changes, a\\nhybrid fusion of both explanation types demonstrated the highest effectiveness.\\nBased on our study results, we also present design implications for effective\\nexplanation-driven interactive machine-learning systems. In recent years, there has been an increasing recognition that when machine\\nlearning (ML) algorithms are used to automate decisions, they may mistreat\\nindividuals or groups, with legal, ethical, or economic implications.\\nRecommender systems are prominent examples of these machine learning (ML)\\nsystems that aid users in making decisions. The majority of past literature\\nresearch on RS fairness treats user and item fairness concerns independently,\\nignoring the fact that recommender systems function in a two-sided marketplace.\\nIn this paper, we propose CP-FairRank, an optimization-based re-ranking\\nalgorithm that seamlessly integrates fairness constraints from both the\\nconsumer and producer side in a joint objective framework. The framework is\\ngeneralizable and may take into account varied fairness settings based on group\\nsegmentation, recommendation model selection, and domain, which is one of its\\nkey characteristics. For instance, we demonstrate that the system may jointly\\nincrease consumer and producer fairness when (un)protected consumer groups are\\ndefined on the basis of their activity level and main-streamness, while\\nproducer groups are defined according to their popularity level. For empirical\\nvalidation, through large-scale on eight datasets and four mainstream\\ncollaborative filtering (CF) recommendation models, we demonstrate that our\\nproposed strategy is able to improve both consumer and producer fairness\\nwithout compromising or very little overall recommendation quality,\\ndemonstrating the role algorithms may play in avoiding data biases. Recent advances in large language models (LLMs) have demonstrated exceptional\\nperformance in various natural language processing (NLP) tasks. However, their\\neffective application in the medical domain is hampered by a lack of medical\\ndomain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable\\nframework that aims to inject medical knowledge into general-purpose LLMs\\nthrough instruction tuning, thereby enabling adaptability for various\\ndownstream tasks. SA-MDKIF consists of two stages: skill training and skill\\nadaptation. In the first stage, we define 12 basic medical skills and use\\nAdaLoRA to train these skills based on uniformly formatted instructional\\ndatasets that we have constructed. In the next stage, we train the skill router\\nusing task-specific downstream data and use this router to integrate the\\nacquired skills with LLMs during inference. Experimental results on 9 different\\nmedical tasks show that SA-MDKIF improves performance by 10-20% compared to the\\noriginal LLMs. Notably, this improvement is particularly pronounced for unseen\\nmedical tasks, showing an improvement of up to 30%. Concept Bottleneck Models (CBMs) are considered inherently interpretable\\nbecause they first predict a set of human-defined concepts before using these\\nconcepts to predict the output of a downstream task. For inherent\\ninterpretability to be fully realised, and ensure trust in a model's output, we\\nneed to guarantee concepts are predicted based on semantically mapped input\\nfeatures. For example, one might expect the pixels representing a broken bone\\nin an image to be used for the prediction of a fracture. However, current\\nliterature indicates this is not the case, as concept predictions are often\\nmapped to irrelevant input features. We hypothesise that this occurs when\\nconcept annotations are inaccurate or how input features should relate to\\nconcepts is unclear. In general, the effect of dataset labelling on concept\\nrepresentations in CBMs remains an understudied area. Therefore, in this paper,\\nwe examine how CBMs learn concepts from datasets with fine-grained concept\\nannotations. We demonstrate that CBMs can learn concept representations with\\nsemantic mapping to input features by removing problematic concept\\ncorrelations, such as two concepts always appearing together. To support our\\nevaluation, we introduce a new synthetic image dataset based on a playing cards\\ndomain, which we hope will serve as a benchmark for future CBM research. For\\nvalidation, we provide empirical evidence on a real-world dataset of chest\\nX-rays, to demonstrate semantically meaningful concepts can be learned in\\nreal-world applications. Recent advancements in deep reinforcement learning (DRL) techniques have\\nsparked its multifaceted applications in the automation sector. Managing\\ncomplex decision-making problems with DRL encourages its use in the nuclear\\nindustry for tasks such as optimizing radiation exposure to the personnel\\nduring normal operating conditions and potential accidental scenarios. However,\\nthe lack of efficient reward function and effective exploration strategy\\nthwarted its implementation in the development of radiation-aware autonomous\\nunmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here,\\nin this article, we address these intriguing issues and introduce a deep\\nQ-learning based architecture (RadDQN) that operates on a radiation-aware\\nreward function to provide time-efficient minimum radiation-exposure pathway in\\na radiation zone. We propose a set of unique exploration strategies that\\nfine-tune the extent of exploration and exploitation based on the state-wise\\nvariation in radiation exposure during training. Further, we benchmark the\\npredicted path with grid-based deterministic method. We demonstrate that the\\nformulated reward function in conjugation with adequate exploration strategy is\\neffective in handling several scenarios with drastically different radiation\\nfield distributions. When compared to vanilla DQN, our model achieves a\\nsuperior convergence rate and higher training stability. Resource constrained job scheduling is a hard combinatorial optimisation\\nproblem that originates in the mining industry. Off-the-shelf solvers cannot\\nsolve this problem satisfactorily in reasonable timeframes, while other\\nsolution methods such as many evolutionary computation methods and\\nmatheuristics cannot guarantee optimality and require low-level customisation\\nand specialised heuristics to be effective. This paper addresses this gap by\\nproposing a genetic programming algorithm to discover efficient search\\nstrategies of constraint programming for resource-constrained job scheduling.\\nIn the proposed algorithm, evolved programs represent variable selectors to be\\nused in the search process of constraint programming, and their fitness is\\ndetermined by the quality of solutions obtained for training instances. The\\nnovelties of this algorithm are (1) a new representation of variable selectors,\\n(2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests\\nwith a large set of random and benchmark instances, the evolved variable\\nselectors can significantly improve the efficiency of constraining programming.\\nCompared to highly customised metaheuristics and hybrid algorithms, evolved\\nvariable selectors can help constraint programming identify quality solutions\\nfaster and proving optimality is possible if sufficiently large run-times are\\nallowed. The evolved variable selectors are especially helpful when solving\\ninstances with large numbers of machines. Efficient parallel computing has become a pivotal element in advancing\\nartificial intelligence. Yet, the deployment of Spiking Neural Networks (SNNs)\\nin this domain is hampered by their inherent sequential computational\\ndependency. This constraint arises from the need for each time step's\\nprocessing to rely on the preceding step's outcomes, significantly impeding the\\nadaptability of SNN models to massively parallel computing environments.\\nAddressing this challenge, our paper introduces the innovative Parallel Spiking\\nUnit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware\\nPSU (RPSU). These variants skillfully decouple the leaky integration and firing\\nmechanisms in spiking neurons while probabilistically managing the reset\\nprocess. By preserving the fundamental computational attributes of the spiking\\nneuron model, our approach enables the concurrent computation of all membrane\\npotential instances within the SNN, facilitating parallel spike output\\ngeneration and substantially enhancing computational efficiency. Comprehensive\\ntesting across various datasets, including static and sequential images,\\nDynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the\\nPSU and its variants not only significantly boost performance and simulation\\nspeed but also augment the energy efficiency of SNNs through enhanced sparsity\\nin neural activity. These advancements underscore the potential of our method\\nin revolutionizing SNN deployment for high-performance parallel computing\\napplications. Due to the data imbalance and the diversity of defects, student-teacher\\nnetworks (S-T) are favored in unsupervised anomaly detection, which explores\\nthe discrepancy in feature representation derived from the knowledge\\ndistillation process to recognize anomalies. However, vanilla S-T network is\\nnot stable. Employing identical structures to construct the S-T network may\\nweaken the representative discrepancy on anomalies. But using different\\nstructures can increase the likelihood of divergent performance on normal data.\\nTo address this problem, we propose a novel dual-student knowledge distillation\\n(DSKD) architecture. Different from other S-T networks, we use two student\\nnetworks a single pre-trained teacher network, where the students have the same\\nscale but inverted structures. This framework can enhance the distillation\\neffect to improve the consistency in recognition of normal data, and\\nsimultaneously introduce diversity for anomaly representation. To explore\\nhigh-dimensional semantic information to capture anomaly clues, we employ two\\nstrategies. First, a pyramid matching mode is used to perform knowledge\\ndistillation on multi-scale feature maps in the intermediate layers of\\nnetworks. Second, an interaction is facilitated between the two student\\nnetworks through a deep feature embedding module, which is inspired by\\nreal-world group discussions. In terms of classification, we obtain pixel-wise\\nanomaly segmentation maps by measuring the discrepancy between the output\\nfeature maps of the teacher and student networks, from which an anomaly score\\nis computed for sample-wise determination. We evaluate DSKD on three benchmark\\ndatasets and probe the effects of internal modules through ablation\\nexperiments. The results demonstrate that DSKD can achieve exceptional\\nperformance on small models like ResNet18 and effectively improve vanilla S-T\\nnetworks. Graph-structured data, prevalent in domains ranging from social networks to\\nbiochemical analysis, serve as the foundation for diverse real-world systems.\\nWhile graph neural networks demonstrate proficiency in modeling this type of\\ndata, their success is often reliant on significant amounts of labeled data,\\nposing a challenge in practical scenarios with limited annotation resources. To\\ntackle this problem, tremendous efforts have been devoted to enhancing graph\\nmachine learning performance under low-resource settings by exploring various\\napproaches to minimal supervision. In this paper, we introduce a novel concept\\nof Data-Efficient Graph Learning (DEGL) as a research frontier, and present the\\nfirst survey that summarizes the current progress of DEGL. We initiate by\\nhighlighting the challenges inherent in training models with large labeled\\ndata, paving the way for our exploration into DEGL. Next, we systematically\\nreview recent advances on this topic from several key aspects, including\\nself-supervised graph learning, semi-supervised graph learning, and few-shot\\ngraph learning. Also, we state promising directions for future research,\\ncontributing to the evolution of graph machine learning. Addressing biases in AI models is crucial for ensuring fair and accurate\\npredictions. However, obtaining large, unbiased datasets for training can be\\nchallenging. This paper proposes a comprehensive approach using multiple\\nmethods to remove bias in AI models, with only a small dataset and a\\npotentially biased pretrained model. We train multiple models with the\\ncounter-bias of the pre-trained model through data splitting, local training,\\nand regularized fine-tuning, gaining potentially counter-biased models. Then,\\nwe employ ensemble learning for all models to reach unbiased predictions. To\\nfurther accelerate the inference time of our ensemble model, we conclude our\\nsolution with knowledge distillation that results in a single unbiased neural\\nnetwork. We demonstrate the effectiveness of our approach through experiments\\non the CIFAR10 and HAM10000 datasets, showcasing promising results. This work\\ncontributes to the ongoing effort to create more unbiased and reliable AI\\nmodels, even with limited data availability. In recent years, deep learning has gained increasing popularity in the fields\\nof Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM),\\nproviding domain practitioners with new powerful data-driven techniques such as\\nPhysics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator\\nNetworks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context,\\ndeep autoencoders based on Convolutional Neural Networks (CNNs) have proven\\nextremely effective, outperforming established techniques, such as the reduced\\nbasis method, when dealing with complex nonlinear problems. However, despite\\nthe empirical success of CNN-based autoencoders, there are only a few\\ntheoretical results supporting these architectures, usually stated in the form\\nof universal approximation theorems. In particular, although the existing\\nliterature provides users with guidelines for designing convolutional\\nautoencoders, the subsequent challenge of learning the latent features has been\\nbarely investigated. Furthermore, many practical questions remain unanswered,\\ne.g., the number of snapshots needed for convergence or the neural network\\ntraining strategy. In this work, using recent techniques from sparse\\nhigh-dimensional function approximation, we fill some of these gaps by\\nproviding a new practical existence theorem for CNN-based autoencoders when the\\nparameter-to-solution map is holomorphic. This regularity assumption arises in\\nmany relevant classes of parametric PDEs, such as the parametric diffusion\\nequation, for which we discuss an explicit application of our general theory. Augmenting large language models (LLMs) with user-specific knowledge is\\ncrucial for real-world applications, such as personal AI assistants. However,\\nLLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper\\ninvestigates utilizing the existing LLM capabilities to enable prompt-driven\\nknowledge capture, with a particular emphasis on knowledge graphs. We address\\nthis challenge by focusing on prompt-to-triple (P2T) generation. We explore\\nthree methods: zero-shot prompting, few-shot prompting, and fine-tuning, and\\nthen assess their performance via a specialized synthetic dataset. Our code and\\ndatasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC. Large language models (LLMs) have exhibited remarkable capabilities in text\\ngeneration tasks. However, the utilization of these models carries inherent\\nrisks, including but not limited to plagiarism, the dissemination of fake news,\\nand issues in educational exercises. Although several detectors have been\\nproposed to address these concerns, their effectiveness against adversarial\\nperturbations, specifically in the context of student essay writing, remains\\nlargely unexplored. This paper aims to bridge this gap by constructing\\nAIG-ASAP, an AI-generated student essay dataset, employing a range of text\\nperturbation methods that are expected to generate high-quality essays while\\nevading detection. Through empirical experiments, we assess the performance of\\ncurrent AIGC detectors on the AIG-ASAP dataset. The results reveal that the\\nexisting detectors can be easily circumvented using straightforward automatic\\nadversarial attacks. Specifically, we explore word substitution and sentence\\nsubstitution perturbation methods that effectively evade detection while\\nmaintaining the quality of the generated essays. This highlights the urgent\\nneed for more accurate and robust methods to detect AI-generated student essays\\nin the education domain. Compared to traditional Artificial Neural Network (ANN), Spiking Neural\\nNetwork (SNN) has garnered widespread academic interest for its intrinsic\\nability to transmit information in a more biological-inspired and\\nenergy-efficient manner. However, despite previous efforts to optimize the\\nlearning gradients and model structure of SNNs through various methods, SNNs\\nstill lag behind ANNs in terms of performance to some extent. The recently\\nproposed multi-threshold model provides more possibilities for further\\nenhancing the learning capability of SNNs. In this paper, we rigorously analyze\\nthe relationship among the multi-threshold model, vanilla spiking model and\\nquantized ANNs from a mathematical perspective, then propose a novel LM-HT\\nmodel, which is an equidistant multi-hierarchical model that can dynamically\\nregulate the global input current and membrane potential leakage on the time\\ndimension. In addition, we note that the direct training algorithm based on the\\nLM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion\\nframework. This novel hybrid learning framework can effectively improve the\\nrelatively poor performance of converted SNNs under low time latency. Extensive\\nexperimental results have demonstrated that our LM-HT model can significantly\\noutperform previous state-of-the-art works on various types of datasets, which\\npromote SNNs to achieve a brand-new level of performance comparable to\\nquantized ANNs. We address the challenge of societal bias in Large Language Models (LLMs),\\nfocusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into\\ndecision-making processes with substantial societal impact, it becomes\\nimperative to ensure these models do not reinforce existing biases. Our\\napproach employs activation steering to probe for and mitigate biases related\\nto gender, race, and religion. This method manipulates model activations to\\ndirect responses towards or away from biased outputs, utilizing steering\\nvectors derived from the StereoSet dataset and custom GPT4 generated gender\\nbias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,\\npersisting even after Reinforcement Learning from Human Feedback (RLHF). We\\nalso observe a predictable negative correlation between bias and the model's\\ntendency to refuse responses. Significantly, our study uncovers that RLHF tends\\nto increase the similarity in the model's representation of different forms of\\nsocietal biases, which raises questions about the model's nuanced understanding\\nof different forms of bias. This work also provides valuable insights into\\neffective red-teaming strategies for LLMs using activation steering,\\nparticularly emphasizing the importance of integrating a refusal vector. Traffic forecasting is crucial for intelligent transportation systems (ITS),\\naiding in efficient resource allocation and effective traffic control. However,\\nits effectiveness often relies heavily on abundant traffic data, while many\\ncities lack sufficient data due to limited device support, posing a significant\\nchallenge for traffic forecasting. Recognizing this challenge, we have made a\\nnoteworthy observation: traffic patterns exhibit similarities across diverse\\ncities. Building on this key insight, we propose a solution for the cross-city\\nfew-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank\\n(MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich\\nsource cities, effectively acquiring comprehensive traffic knowledge through a\\nspatial-temporal-aware pre-training process. Subsequently, the framework\\nemploys advanced clustering techniques to systematically generate a multi-scale\\ntraffic pattern bank derived from the learned knowledge. Next, the traffic data\\nof the data-scarce target city could query the traffic pattern bank,\\nfacilitating the aggregation of meta-knowledge. This meta-knowledge, in turn,\\nassumes a pivotal role as a robust guide in subsequent processes involving\\ngraph reconstruction and forecasting. Empirical assessments conducted on\\nreal-world traffic datasets affirm the superior performance of MTPB, surpassing\\nexisting methods across various categories and exhibiting numerous attributes\\nconducive to the advancement of cross-city few-shot forecasting methodologies.\\nThe code is available in https://github.com/zhyliu00/MTPB. We present evidence of substantial benefit from efficient exploration in\\ngathering human feedback to improve large language models. In our experiments,\\nan agent sequentially generates queries while fitting a reward model to the\\nfeedback received. Our best-performing agent generates queries using double\\nThompson sampling, with uncertainty represented by an epistemic neural network.\\nOur results demonstrate that efficient exploration enables high levels of\\nperformance with far fewer queries. Further, both uncertainty estimation and\\nthe choice of exploration scheme play critical roles. It is important to reveal the inverse dynamics of manipulators to improve\\ncontrol performance of model-based control. Neural networks (NNs) are promising\\ntechniques to represent complicated inverse dynamics while they require a large\\namount of motion data. However, motion data in dead zones of actuators is not\\nsuitable for training models decreasing the number of useful training data. In\\nthis study, based on the fact that the manipulator joint does not work\\nirrespective of input torque in dead zones, we propose a new loss function that\\nconsiders only errors of joints not in dead zones. The proposed method enables\\nto increase in the amount of motion data available for training and the\\naccuracy of the inverse dynamics computation. Experiments on actual equipment\\nusing a three-degree-of-freedom (DOF) manipulator showed higher accuracy than\\nconventional methods. We also confirmed and discussed the behavior of the model\\nof the proposed method in dead zones. In this age where data is abundant, the ability to distill meaningful\\ninsights from the sea of information is essential. Our research addresses the\\ncomputational and resource inefficiencies that current Sequential Recommender\\nSystems (SRSs) suffer from. especially those employing attention-based models\\nlike SASRec, These systems are designed for next-item recommendations in\\nvarious applications, from e-commerce to social networks. However, such systems\\nsuffer from substantial computational costs and resource consumption during the\\ninference stage. To tackle these issues, our research proposes a novel method\\nthat combines automatic pruning techniques with advanced model architectures.\\nWe also explore the potential of resource-constrained Neural Architecture\\nSearch (NAS), a technique prevalent in the realm of recommendation systems, to\\nfine-tune models for reduced FLOPs, latency, and energy usage while retaining\\nor even enhancing accuracy. The main contribution of our work is developing the\\nElastic Architecture Search for Efficient Long-term Sequential Recommender\\nSystems (EASRec). This approach aims to find optimal compact architectures for\\nattention-based SRSs, ensuring accuracy retention. EASRec introduces data-aware\\ngates that leverage historical information from input data batch to improve the\\nperformance of the recommendation network. Additionally, it utilizes a dynamic\\nresource constraint approach, which standardizes the search process and results\\nin more appropriate architectures. The effectiveness of our methodology is\\nvalidated through exhaustive experiments on three benchmark datasets, which\\ndemonstrates EASRec's superiority in SRSs. Our research set a new standard for\\nfuture exploration into efficient and accurate recommender systems, signifying\\na substantial advancement within this swiftly advancing field. Although adaptive gradient methods have been extensively used in deep\\nlearning, their convergence rates have not been thoroughly studied,\\nparticularly with respect to their dependence on the dimension. This paper\\nconsiders the classical RMSProp and its momentum extension and establishes the\\nconvergence rate of $\\\\frac{1}{T}\\\\sum_{k=1}^TE\\\\left[\\\\|\\\\nabla\\nf(x^k)\\\\|_1\\\\right]\\\\leq O(\\\\frac{\\\\sqrt{d}}{T^{1/4}})$ measured by $\\\\ell_1$ norm\\nwithout the bounded gradient assumption, where $d$ is the dimension of the\\noptimization variable and $T$ is the iteration number. Since\\n$\\\\|x\\\\|_2\\\\ll\\\\|x\\\\|_1\\\\leq\\\\sqrt{d}\\\\|x\\\\|_2$ for problems with extremely large $d$,\\nour convergence rate can be considered to be analogous to the\\n$\\\\frac{1}{T}\\\\sum_{k=1}^TE\\\\left[\\\\|\\\\nabla f(x^k)\\\\|_2\\\\right]\\\\leq\\nO(\\\\frac{1}{T^{1/4}})$ one of SGD measured by $\\\\ell_1$ norm. Temporal Point Processes (TPPs) hold a pivotal role in modeling event\\nsequences across diverse domains, including social networking and e-commerce,\\nand have significantly contributed to the advancement of recommendation systems\\nand information retrieval strategies. Through the analysis of events such as\\nuser interactions and transactions, TPPs offer valuable insights into\\nbehavioral patterns, facilitating the prediction of future trends. However,\\naccurately forecasting future events remains a formidable challenge due to the\\nintricate nature of these patterns. The integration of Neural Networks with\\nTPPs has ushered in the development of advanced deep TPP models. While these\\nmodels excel at processing complex and nonlinear temporal data, they encounter\\nlimitations in modeling intensity functions, grapple with computational\\ncomplexities in integral computations, and struggle to capture long-range\\ntemporal dependencies effectively. In this study, we introduce the CuFun model,\\nrepresenting a novel approach to TPPs that revolves around the Cumulative\\nDistribution Function (CDF). CuFun stands out by uniquely employing a monotonic\\nneural network for CDF representation, utilizing past events as a scaling\\nfactor. This innovation significantly bolsters the model's adaptability and\\nprecision across a wide range of data scenarios. Our approach addresses several\\ncritical issues inherent in traditional TPP modeling: it simplifies\\nlog-likelihood calculations, extends applicability beyond predefined density\\nfunction forms, and adeptly captures long-range temporal patterns. Our\\ncontributions encompass the introduction of a pioneering CDF-based TPP model,\\nthe development of a methodology for incorporating past event information into\\nfuture event prediction, and empirical validation of CuFun's effectiveness\\nthrough extensive experimentation on synthetic and real-world datasets. This paper introduces a novel proprioceptive state estimator for legged\\nrobots that combines model-based filters and deep neural networks. Recent\\nstudies have shown that neural networks such as multi-layer perceptron or\\nrecurrent neural networks can estimate the robot states, including contact\\nprobability and linear velocity. Inspired by this, we develop a state\\nestimation framework that integrates a neural measurement network (NMN) with an\\ninvariant extended Kalman filter. We show that our framework improves\\nestimation performance in various terrains. Existing studies that combine\\nmodel-based filters and learning-based approaches typically use real-world\\ndata. However, our approach relies solely on simulation data, as it allows us\\nto easily obtain extensive data. This difference leads to a gap between the\\nlearning and the inference domain, commonly referred to as a sim-to-real gap.\\nWe address this challenge by adapting existing learning techniques and\\nregularization. To validate our proposed method, we conduct experiments using a\\nquadruped robot on four types of terrain: \\\\textit{flat}, \\\\textit{debris},\\n\\\\textit{soft}, and \\\\textit{slippery}. We observe that our approach\\nsignificantly reduces position drift compared to the existing model-based state\\nestimator. Anthropogenic influences have been linked to tropical cyclone (TC) poleward\\nmigration, TC extreme precipitation, and an increased proportion of major\\nhurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is\\ncritical for projecting future TC impacts on human society considering the\\nchanging climate [5]. However, past trends of TC structure/energy remain\\nuncertain due to limited observations; subjective-analyzed and\\nspatiotemporal-heterogeneous \\\"best-track\\\" datasets lead to reduced confidence\\nin the assessed TC repose to climate change [6, 7]. Here, we use deep learning\\nto reconstruct past \\\"observations\\\" and yield an objective global TC wind\\nprofile dataset during 1981 to 2020, facilitating a comprehensive examination\\nof TC structure/energy. By training with uniquely labeled data integrating best\\ntracks and numerical model analysis of 2004 to 2018 TCs, our model converts\\nmultichannel satellite imagery to a 0-750-km wind profile of axisymmetric\\nsurface winds. The model performance is verified to be sufficient for climate\\nstudies by comparing it to independent satellite-radar surface winds. Based on\\nthe new homogenized dataset, the major TC proportion has increased by ~13% in\\nthe past four decades. Moreover, the proportion of extremely high-energy TCs\\nhas increased by ~25%, along with an increasing trend (> one standard deviation\\nof the 40-y variability) of the mean total energy of high-energy TCs. Although\\nthe warming ocean favors TC intensification, the TC track migration to higher\\nlatitudes and altered environments further affect TC structure/energy. This new\\ndeep learning method/dataset reveals novel trends regarding TC structure\\nextremes and may help verify simulations/studies regarding TCs in the changing\\nclimate. Primal-dual methods have a natural application in Safe Reinforcement Learning\\n(SRL), posed as a constrained policy optimization problem. In practice however,\\napplying primal-dual methods to SRL is challenging, due to the inter-dependency\\nof the learning rate (LR) and Lagrangian multipliers (dual variables) each time\\nan embedded unconstrained RL problem is solved. In this paper, we propose,\\nanalyze and evaluate adaptive primal-dual (APD) methods for SRL, where two\\nadaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the\\npolicy in each iteration. We theoretically establish the convergence,\\noptimality and feasibility of the APD algorithm. Finally, we conduct numerical\\nevaluation of the practical APD algorithm with four well-known environments in\\nBullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian\\nand DDPG-Lagrangian. All experiments show that the practical APD algorithm\\noutperforms (or achieves comparable performance) and attains more stable\\ntraining than the constant LR cases. Additionally, we substantiate the\\nrobustness of selecting the two adaptive LRs by empirical evidence. In the modern era where software plays a pivotal role, software security and\\nvulnerability analysis have become essential for software development. Fuzzing\\ntest, as an efficient software testing method, are widely used in various\\ndomains. Moreover, the rapid development of Large Language Models (LLMs) has\\nfacilitated their application in the field of software testing, demonstrating\\nremarkable performance. Considering that existing fuzzing test techniques are\\nnot entirely automated and software vulnerabilities continue to evolve, there\\nis a growing trend towards employing fuzzing test generated based on large\\nlanguage models. This survey provides a systematic overview of the approaches\\nthat fuse LLMs and fuzzing tests for software testing. In this paper, a\\nstatistical analysis and discussion of the literature in three areas, namely\\nLLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by\\nsummarising the state-of-the-art methods up until 2024. Our survey also\\ninvestigates the potential for widespread deployment and application of fuzzing\\ntest techniques generated by LLMs in the future. In this study, we investigate the DIstribution Correction Estimation (DICE)\\nmethods, an important line of work in offline reinforcement learning (RL) and\\nimitation learning (IL). DICE-based methods impose state-action-level behavior\\nconstraint, which is an ideal choice for offline learning. However, they\\ntypically perform much worse than current state-of-the-art (SOTA) methods that\\nsolely use action-level behavior constraint. After revisiting DICE-based\\nmethods, we find there exist two gradient terms when learning the value\\nfunction using true-gradient update: forward gradient (taken on the current\\nstate) and backward gradient (taken on the next state). Using forward gradient\\nbears a large similarity to many offline RL methods, and thus can be regarded\\nas applying action-level constraint. However, directly adding the backward\\ngradient may degenerate or cancel out its effect if these two gradients have\\nconflicting directions. To resolve this issue, we propose a simple yet\\neffective modification that projects the backward gradient onto the normal\\nplane of the forward gradient, resulting in an orthogonal-gradient update, a\\nnew learning rule for DICE-based methods. We conduct thorough theoretical\\nanalyses and find that the projected backward gradient brings state-level\\nbehavior regularization, which reveals the mystery of DICE-based methods: the\\nvalue learning objective does try to impose state-action-level constraint, but\\nneeds to be used in a corrected way. Through toy examples and extensive\\nexperiments on complex offline RL and IL tasks, we demonstrate that DICE-based\\nmethods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and\\ngreat robustness. Explanations of machine learning models are important, especially in\\nscientific areas such as chemistry, biology, and physics, where they guide\\nfuture laboratory experiments and resource requirements. These explanations can\\nbe derived from well-trained machine learning models (data-driven perspective)\\nor specific domain knowledge (domain-driven perspective). However, there exist\\ninconsistencies between these perspectives due to accurate yet misleading\\nmachine learning models and various stakeholders with specific needs, wants, or\\naims. This paper calls attention to these inconsistencies and suggests a way to\\nfind an accurate model with expected explanations that reinforce physical laws\\nand meet stakeholders' requirements from a set of equally-good models, also\\nknown as Rashomon sets. Our goal is to foster a comprehensive understanding of\\nthese inconsistencies and ultimately contribute to the integration of\\neXplainable Artificial Intelligence (XAI) into scientific domains. Anticipating possible future deployment of connected and automated vehicles\\n(CAVs), cooperative autonomous driving at intersections has been studied by\\nmany works in control theory and intelligent transportation across decades.\\nSimultaneously, recent parallel works in robotics have devised efficient\\nalgorithms for multi-agent path finding (MAPF), though often in environments\\nwith simplified kinematics. In this work, we hybridize insights and algorithms\\nfrom MAPF with the structure and heuristics of optimizing the crossing order of\\nCAVs at signal-free intersections. We devise an optimal and complete algorithm,\\nOrder-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which\\nsignificantly outperforms existing algorithms, fixed heuristics, and\\nprioritized planning with KATS. The performance is maintained under different\\nvehicle arrival rates, lane lengths, crossing speeds, and control horizon.\\nThrough ablations and dissections, we offer insight on the contributing factors\\nto OBS-KATS's performance. Our work is directly applicable to many similarly\\nscaled traffic and multi-robot scenarios with directed lanes. Visual storytelling aims to automatically generate a coherent story based on\\na given image sequence. Unlike tasks like image captioning, visual stories\\nshould contain factual descriptions, worldviews, and human social commonsense\\nto put disjointed elements together to form a coherent and engaging\\nhuman-writeable story. However, most models mainly focus on applying factual\\ninformation and using taxonomic/lexical external knowledge when attempting to\\ncreate stories. This paper introduces SCO-VIST, a framework representing the\\nimage sequence as a graph with objects and relations that includes human action\\nmotivation and its social interaction commonsense knowledge. SCO-VIST then\\ntakes this graph representing plot points and creates bridges between plot\\npoints with semantic and occurrence-based edge weights. This weighted story\\ngraph produces the storyline in a sequence of events using Floyd-Warshall's\\nalgorithm. Our proposed framework produces stories superior across multiple\\nmetrics in terms of visual grounding, coherence, diversity, and humanness, per\\nboth automatic and human evaluations. AI-enabled synthetic biology has tremendous potential but also significantly\\nincreases biorisks and brings about a new set of dual use concerns. The picture\\nis complicated given the vast innovations envisioned to emerge by combining\\nemerging technologies, as AI-enabled synthetic biology potentially scales up\\nbioengineering into industrial biomanufacturing. However, the literature review\\nindicates that goals such as maintaining a reasonable scope for innovation, or\\nmore ambitiously to foster a huge bioeconomy don't necessarily contrast with\\nbiosafety, but need to go hand in hand. This paper presents a literature review\\nof the issues and describes emerging frameworks for policy and practice that\\ntransverse the options of command-and control, stewardship, bottom-up, and\\nlaissez-faire governance. How to achieve early warning systems that enable\\nprevention and mitigation of future AI-enabled biohazards from the lab, from\\ndeliberate misuse, or from the public realm, will constantly need to evolve,\\nand adaptive, interactive approaches should emerge. Although biorisk is subject\\nto an established governance regime, and scientists generally adhere to\\nbiosafety protocols, even experimental, but legitimate use by scientists could\\nlead to unexpected developments. Recent advances in chatbots enabled by\\ngenerative AI have revived fears that advanced biological insight can more\\neasily get into the hands of malignant individuals or organizations. Given\\nthese sets of issues, society needs to rethink how AI-enabled synthetic biology\\nshould be governed. The suggested way to visualize the challenge at hand is\\nwhack-a-mole governance, although the emerging solutions are perhaps not so\\ndifferent either. We introduce AlphaRank, an artificial intelligence approach to address the\\nfixed-budget ranking and selection (R&S) problems. We formulate the sequential\\nsampling decision as a Markov decision process and propose a Monte Carlo\\nsimulation-based rollout policy that utilizes classic R&S procedures as base\\npolicies for efficiently learning the value function of stochastic dynamic\\nprogramming. We accelerate online sample-allocation by using deep reinforcement\\nlearning to pre-train a neural network model offline based on a given prior. We\\nalso propose a parallelizable computing framework for large-scale problems,\\neffectively combining \\\"divide and conquer\\\" and \\\"recursion\\\" for enhanced\\nscalability and efficiency. Numerical experiments demonstrate that the\\nperformance of AlphaRank is significantly improved over the base policies,\\nwhich could be attributed to AlphaRank's superior capability on the trade-off\\namong mean, variance, and induced correlation overlooked by many existing\\npolicies. Next location prediction is a discipline that involves predicting a users\\nnext location. Its applications include resource allocation, quality of\\nservice, energy efficiency, and traffic management. This paper proposes an\\nenergy-efficient, small, and low parameter machine learning (ML) architecture\\nfor accurate next location prediction, deployable on modest base stations and\\nedge devices. To accomplish this we ran a hundred hyperparameter experiments on\\nthe full human mobility patterns of an entire city, to determine an exact ML\\narchitecture that reached a plateau of accuracy with the least amount of model\\nparameters. We successfully achieved a reduction in the number of model\\nparameters within published ML architectures from 202 million down to 2\\nmillion. This reduced the total size of the model parameters from 791 MB down\\nto 8 MB. Additionally, this decreased the training time by a factor of four,\\nthe amount of graphics processing unit (GPU) memory needed for training by a\\nfactor of twenty, and the overall accuracy was increased from 80.16% to 82.54%.\\nThis improvement allows for modest base stations and edge devices which do not\\nhave a large amount of memory or storage, to deploy and utilize the proposed ML\\narchitecture for next location prediction. As graph representation learning often suffers from label scarcity problems\\nin real-world applications, researchers have proposed graph domain adaptation\\n(GDA) as an effective knowledge-transfer paradigm across graphs. In particular,\\nto enhance model performance on target graphs with specific tasks, GDA\\nintroduces a bunch of task-related graphs as source graphs and adapts the\\nknowledge learnt from source graphs to the target graphs. Since GDA combines\\nthe advantages of graph representation learning and domain adaptation, it has\\nbecome a promising direction of transfer learning on graphs and has attracted\\nan increasing amount of research interest in recent years. In this paper, we\\ncomprehensively overview the studies of GDA and present a detailed survey of\\nrecent advances. Specifically, we outline the research status and challenges,\\npropose a taxonomy, introduce the details of representative works, and discuss\\nthe prospects. To the best of our knowledge, this paper is the first survey for\\ngraph domain adaptation. A detailed paper list is available at\\nhttps://github.com/Skyorca/Awesome-Graph-Domain-Adaptation-Papers. Recently emerged prompt-based Recommendation Language Models (RLM) can solve\\nmultiple recommendation tasks uniformly. The RLMs make full use of the\\ninherited knowledge learned from the abundant pre-training data to solve the\\ndownstream recommendation tasks by prompts, without introducing additional\\nparameters or network training. However, handcrafted prompts require\\nsignificant expertise and human effort since slightly rewriting prompts may\\ncause massive performance changes. In this paper, we propose PAP-REC, a\\nframework to generate the Personalized Automatic Prompt for RECommendation\\nlanguage models to mitigate the inefficiency and ineffectiveness problems\\nderived from manually designed prompts. Specifically, personalized automatic\\nprompts allow different users to have different prompt tokens for the same\\ntask, automatically generated using a gradient-based method. One challenge for\\npersonalized automatic prompt generation for recommendation language models is\\nthe extremely large search space, leading to a long convergence time. To\\neffectively and efficiently address the problem, we develop surrogate metrics\\nand leverage an alternative updating schedule for prompting recommendation\\nlanguage models. Experimental results show that our PAP-REC framework manages\\nto generate personalized prompts, and the automatically generated prompts\\noutperform manually constructed prompts and also outperform various baseline\\nrecommendation models. The source code of the work is available at\\nhttps://github.com/rutgerswiselab/PAP-REC. Computational experiments have emerged as a valuable method for studying\\ncomplex systems, involving the algorithmization of counterfactuals. However,\\naccurately representing real social systems in Agent-based Modeling (ABM) is\\nchallenging due to the diverse and intricate characteristics of humans,\\nincluding bounded rationality and heterogeneity. To address this limitation,\\nthe integration of Large Language Models (LLMs) has been proposed, enabling\\nagents to possess anthropomorphic abilities such as complex reasoning and\\nautonomous learning. These agents, known as LLM-based Agent, offer the\\npotential to enhance the anthropomorphism lacking in ABM. Nonetheless, the\\nabsence of explicit explainability in LLMs significantly hinders their\\napplication in the social sciences. Conversely, computational experiments excel\\nin providing causal analysis of individual behaviors and complex phenomena.\\nThus, combining computational experiments with LLM-based Agent holds\\nsubstantial research potential. This paper aims to present a comprehensive\\nexploration of this fusion. Primarily, it outlines the historical development\\nof agent structures and their evolution into artificial societies, emphasizing\\ntheir importance in computational experiments. Then it elucidates the\\nadvantages that computational experiments and LLM-based Agents offer each\\nother, considering the perspectives of LLM-based Agent for computational\\nexperiments and vice versa. Finally, this paper addresses the challenges and\\nfuture trends in this research domain, offering guidance for subsequent related\\nstudies. In this paper, we propose a social robot capable of verbally interacting with\\nchildren with Autism Spectrum Disorder (ASD). This communication is meant to\\nteach perspective-taking using text generated using a Large Language Model\\n(LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a\\nsocial situation and asks a question), prompter (presents three options to\\nchoose from), and reinforcer (praises when the answer is correct). For the role\\nof the stimulator, the social situation, questions, and options are generated\\nusing our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 +\\nGPT-2, where the first GPT-2 common between the pipelines is used for\\nunsupervised social situation generation. We use the SOCIALIQA dataset to\\nfine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had\\na better BERTscore for generating the questions and the options by combining\\ntheir individual loss functions. This observation was also consistent with the\\nhuman evaluations. Lastly, the unsupervised generation of social situations was\\nvisualized using T-SNE plots, and the entire pipeline was evaluated for\\nappropriateness for children with ASD by human experts. Vertical Symbolic Regression (VSR) recently has been proposed to expedite the\\ndiscovery of symbolic equations with many independent variables from\\nexperimental data. VSR reduces the search spaces following the vertical\\ndiscovery path by building from reduced-form equations involving a subset of\\nindependent variables to full-fledged ones. Proved successful by many symbolic\\nregressors, deep neural networks are expected to further scale up VSR.\\nNevertheless, directly combining VSR with deep neural networks will result in\\ndifficulty in passing gradients and other engineering issues. We propose\\nVertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and\\ndemonstrate that VSR-DPG can recover ground-truth equations involving multiple\\ninput variables, significantly beyond both deep reinforcement learning-based\\napproaches and previous VSR variants. Our VSR-DPG models symbolic regression as\\na sequential decision-making process, in which equations are built from\\nrepeated applications of grammar rules. The integrated deep model is trained to\\nmaximize a policy gradient objective. Experimental results demonstrate that our\\nVSR-DPG significantly outperforms popular baselines in identifying both\\nalgebraic equations and ordinary differential equations on a series of\\nbenchmarks. Step-by-step decision planning with large language models (LLMs) is gaining\\nattention in AI agent development. This paper focuses on decision planning with\\nuncertainty estimation to address the hallucination problem in language models.\\nExisting approaches are either white-box or computationally demanding, limiting\\nuse of black-box proprietary LLMs within budgets. The paper's first\\ncontribution is a non-parametric uncertainty quantification method for LLMs,\\nefficiently estimating point-wise dependencies between input-decision on the\\nfly with a single inference, without access to token logits. This estimator\\ninforms the statistical interpretation of decision trustworthiness. The second\\ncontribution outlines a systematic design for a decision-making agent,\\ngenerating actions like ``turn on the bathroom light'' based on user prompts\\nsuch as ``take a bath''. Users will be asked to provide preferences when more\\nthan one action has high estimated point-wise dependencies. In conclusion, our\\nuncertainty estimation and decision-making agent design offer a cost-efficient\\napproach for AI agent development. Patients managing a complex illness such as cancer face a complex information\\nchallenge where they not only must learn about their illness but also how to\\nmanage it. Close interaction with healthcare experts (radiologists,\\noncologists) can improve patient learning and thereby, their disease outcome.\\nHowever, this approach is resource intensive and takes expert time away from\\nother critical tasks. Given the recent advancements in Generative AI models\\naimed at improving the healthcare system, our work investigates whether and how\\ngenerative visual question answering systems can responsibly support patient\\ninformation needs in the context of radiology imaging data. We conducted a\\nformative need-finding study in which participants discussed chest computed\\ntomography (CT) scans and associated radiology reports of a fictitious close\\nrelative with a cardiothoracic radiologist. Using thematic analysis of the\\nconversation between participants and medical experts, we identified commonly\\noccurring themes across interactions, including clarifying medical terminology,\\nlocating the problems mentioned in the report in the scanned image,\\nunderstanding disease prognosis, discussing the next diagnostic steps, and\\ncomparing treatment options. Based on these themes, we evaluated two\\nstate-of-the-art generative visual language models against the radiologist's\\nresponses. Our results reveal variability in the quality of responses generated\\nby the models across various themes. We highlight the importance of\\npatient-facing generative AI systems to accommodate a diverse range of\\nconversational themes, catering to the real-world informational needs of\\npatients. The present paper looks at one of the most thorough articles on the\\nintelligence of GPT, research conducted by engineers at Microsoft. Although\\nthere is a great deal of value in their work, I will argue that, for familiar\\nphilosophical reasons, their methodology, !Blackbox Interpretability\\\"#is\\nwrongheaded. But there is a better way. There is an exciting and emerging\\ndiscipline of !Inner Interpretability\\\"#(and specifically Mechanistic\\nInterpretability) that aims to uncover the internal activations and weights of\\nmodels in order to understand what they represent and the algorithms they\\nimplement. In my view, a crucial mistake in Black-box Interpretability is the\\nfailure to appreciate that how processes are carried out matters when it comes\\nto intelligence and understanding. I can#t pretend to have a full story that\\nprovides both necessary and sufficient conditions for being intelligent, but I\\ndo think that Inner Interpretability dovetails nicely with plausible\\nphilosophical views of what intelligence requires. So the conclusion is modest,\\nbut the important point in my view is seeing how to get the research on the\\nright track. Towards the end of the paper, I will show how some of the\\nphilosophical concepts can be used to further refine how Inner Interpretability\\nis approached, so the paper helps draw out a profitable, future two-way\\nexchange between Philosophers and Computer Scientists. Supervised contrastive learning (SCL) frameworks treat each class as\\nindependent and thus consider all classes to be equally important. This\\nneglects the common scenario in which label hierarchy exists, where\\nfine-grained classes under the same category show more similarity than very\\ndifferent ones. This paper introduces a family of Label-Aware SCL methods\\n(LASCL) that incorporates hierarchical information to SCL by leveraging\\nsimilarities between classes, resulting in creating a more well-structured and\\ndiscriminative feature space. This is achieved by first adjusting the distance\\nbetween instances based on measures of the proximity of their classes with the\\nscaled instance-instance-wise contrastive. An additional instance-center-wise\\ncontrastive is introduced to move within-class examples closer to their\\ncenters, which are represented by a set of learnable label parameters. The\\nlearned label parameters can be directly used as a nearest neighbor classifier\\nwithout further finetuning. In this way, a better feature representation is\\ngenerated with improvements of intra-cluster compactness and inter-cluster\\nseparation. Experiments on three datasets show that the proposed LASCL works\\nwell on text classification of distinguishing a single label among\\nmulti-labels, outperforming the baseline supervised approaches. Our code is\\npublicly available. Federated learning (FL) is a machine learning paradigm that allows multiple\\nclients to collaboratively train a shared model while keeping their data\\non-premise. However, the straggler issue, due to slow clients, often hinders\\nthe efficiency and scalability of FL. This paper presents FedCore, an algorithm\\nthat innovatively tackles the straggler problem via the decentralized selection\\nof coresets, representative subsets of a dataset. Contrary to existing\\ncentralized coreset methods, FedCore creates coresets directly on each client\\nin a distributed manner, ensuring privacy preservation in FL. FedCore\\ntranslates the coreset optimization problem into a more tractable k-medoids\\nclustering problem and operates distributedly on each client. Theoretical\\nanalysis confirms FedCore's convergence, and practical evaluations demonstrate\\nan 8x reduction in FL training time, without compromising model accuracy. Our\\nextensive evaluations also show that FedCore generalizes well to existing FL\\nframeworks. This study explores the potential of super-resolution techniques in enhancing\\nobject detection accuracy in football. Given the sport's fast-paced nature and\\nthe critical importance of precise object (e.g. ball, player) tracking for both\\nanalysis and broadcasting, super-resolution could offer significant\\nimprovements. We investigate how advanced image processing through\\nsuper-resolution impacts the accuracy and reliability of object detection\\nalgorithms in processing football match footage.\\n  Our methodology involved applying state-of-the-art super-resolution\\ntechniques to a diverse set of football match videos from SoccerNet, followed\\nby object detection using Faster R-CNN. The performance of these algorithms,\\nboth with and without super-resolution enhancement, was rigorously evaluated in\\nterms of detection accuracy.\\n  The results indicate a marked improvement in object detection accuracy when\\nsuper-resolution preprocessing is applied. The improvement of object detection\\nthrough the integration of super-resolution techniques yields significant\\nbenefits, especially for low-resolution scenarios, with a notable 12\\\\% increase\\nin mean Average Precision (mAP) at an IoU (Intersection over Union) range of\\n0.50:0.95 for 320x240 size images when increasing the resolution fourfold using\\nRLFN. As the dimensions increase, the magnitude of improvement becomes more\\nsubdued; however, a discernible improvement in the quality of detection is\\nconsistently evident. Additionally, we discuss the implications of these\\nfindings for real-time sports analytics, player tracking, and the overall\\nviewing experience. The study contributes to the growing field of sports\\ntechnology by demonstrating the practical benefits and limitations of\\nintegrating super-resolution techniques in football analytics and broadcasting. We present a new methodology for handling AI errors by introducing weakly\\nsupervised AI error correctors with a priori performance guarantees. These AI\\ncorrectors are auxiliary maps whose role is to moderate the decisions of some\\npreviously constructed underlying classifier by either approving or rejecting\\nits decisions. The rejection of a decision can be used as a signal to suggest\\nabstaining from making a decision. A key technical focus of the work is in\\nproviding performance guarantees for these new AI correctors through bounds on\\nthe probabilities of incorrect decisions. These bounds are distribution\\nagnostic and do not rely on assumptions on the data dimension. Our empirical\\nexample illustrates how the framework can be applied to improve the performance\\nof an image classifier in a challenging real-world task where training data are\\nscarce. Large language models and AI chatbots have been at the forefront of\\ndemocratizing artificial intelligence. However, the releases of ChatGPT and\\nother similar tools have been followed by growing concerns regarding the\\ndifficulty of controlling large language models and their outputs. Currently,\\nwe are witnessing a cat-and-mouse game where users attempt to misuse the models\\nwith a novel attack called prompt injections. In contrast, the developers\\nattempt to discover the vulnerabilities and block the attacks simultaneously.\\nIn this paper, we provide an overview of these emergent threats and present a\\ncategorization of prompt injections, which can guide future research on prompt\\ninjections and act as a checklist of vulnerabilities in the development of LLM\\ninterfaces. Moreover, based on previous literature and our own empirical\\nresearch, we discuss the implications of prompt injections to LLM end users,\\ndevelopers, and researchers. Submodular functions, as well as the sub-class of decomposable submodular\\nfunctions, and their optimization appear in a wide range of applications in\\nmachine learning, recommendation systems, and welfare maximization. However,\\noptimization of decomposable submodular functions with millions of component\\nfunctions is computationally prohibitive. Furthermore, the component functions\\nmay be private (they might represent user preference function, for example) and\\ncannot be widely shared. To address these issues, we propose a {\\\\em federated\\noptimization} setting for decomposable submodular optimization. In this\\nsetting, clients have their own preference functions, and a weighted sum of\\nthese preferences needs to be maximized. We implement the popular {\\\\em\\ncontinuous greedy} algorithm in this setting where clients take parallel small\\nlocal steps towards the local solution and then the local changes are\\naggregated at a central server. To address the large number of clients, the\\naggregation is performed only on a subsampled set. Further, the aggregation is\\nperformed only intermittently between stretches of parallel local steps, which\\nreduces communication cost significantly. We show that our federated algorithm\\nis guaranteed to provide a good approximate solution, even in the presence of\\nabove cost-cutting measures. Finally, we show how the federated setting can be\\nincorporated in solving fundamental discrete submodular optimization problems\\nsuch as Maximum Coverage and Facility Location. Metabolic energy consumption of a powered lower-limb exoskeleton user mainly\\ncomes from the upper body effort since the lower body is considered to be\\npassive. However, the upper body effort of the users is largely ignored in the\\nliterature when designing motion controllers. In this work, we use deep\\nreinforcement learning to develop a locomotion controller that minimizes ground\\nreaction forces (GRF) on crutches. The rationale for minimizing GRF is to\\nreduce the upper body effort of the user. Accordingly, we design a model and a\\nlearning framework for a human-exoskeleton system with crutches. We formulate a\\nreward function to encourage the forward displacement of a human-exoskeleton\\nsystem while satisfying the predetermined constraints of a physical robot. We\\nevaluate our new framework using Proximal Policy Optimization, a\\nstate-of-the-art deep reinforcement learning (RL) method, on the MuJoCo physics\\nsimulator with different hyperparameters and network architectures over\\nmultiple trials. We empirically show that our learning model can generate joint\\ntorques based on the joint angle, velocities, and the GRF on the feet and\\ncrutch tips. The resulting exoskeleton model can directly generate joint\\ntorques from states in line with the RL framework. Finally, we empirically show\\nthat policy trained using our method can generate a gait with a 35% reduction\\nin GRF with respect to the baseline. There is increasing interest in employing large language models (LLMs) as\\ncognitive models. For such purposes, it is central to understand which\\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\\nwe study the biases of LLMs in relation to those known in children when solving\\narithmetic word problems. Surveying the learning science literature, we posit\\nthat the problem-solving process can be split into three distinct steps: text\\ncomprehension, solution planning and solution execution. We construct tests for\\neach one in order to understand which parts of this process can be faithfully\\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\\nproblems for each of these tests, using a neuro-symbolic method that enables\\nfine-grained control over the problem features. We find evidence that LLMs,\\nwith and without instruction-tuning, exhibit human-like biases in both the\\ntext-comprehension and the solution-planning steps of the solving process, but\\nnot during the final step which relies on the problem's arithmetic expressions\\n(solution execution). Recent advancements in language models have significantly enhanced\\nperformance in multiple speech-related tasks. Existing speech language models\\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\\na single model. However, this design omits the intrinsic connections between\\ndifferent speech tasks, which can potentially boost the performance of each\\ntask. In this work, we propose a novel decoder-only speech language model,\\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\\nrecognition, speech language modeling, and text language modeling --\\nSpeechComposer can easily extend to more speech tasks via compositions of\\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\\nunification of prompt tokens also makes it possible for knowledge sharing among\\ndifferent speech tasks in a more structured manner. Experimental results\\ndemonstrate that our proposed SpeechComposer can improve the performance of\\nboth primary tasks and composite tasks, showing the effectiveness of the shared\\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\\nand even better performance than the baselines which are expert models designed\\nfor single tasks. End-to-end multi-task dialogue systems are usually designed with separate\\nmodules for the dialogue pipeline. Among these, the policy module is essential\\nfor deciding what to do in response to user input. This policy is trained by\\nreinforcement learning algorithms by taking advantage of an environment in\\nwhich an agent receives feedback in the form of a reward signal. The current\\ndialogue systems, however, only provide meagre and simplistic rewards.\\nInvestigating intrinsic motivation reinforcement learning algorithms is the\\ngoal of this study. Through this, the agent can quickly accelerate training and\\nimprove its capacity to judge the quality of its actions by teaching it an\\ninternal incentive system. In particular, we adapt techniques for random\\nnetwork distillation and curiosity-driven reinforcement learning to measure the\\nfrequency of state visits and encourage exploration by using semantic\\nsimilarity between utterances. Experimental results on MultiWOZ, a\\nheterogeneous dataset, show that intrinsic motivation-based debate systems\\noutperform policies that depend on extrinsic incentives. By adopting random\\nnetwork distillation, for example, which is trained using semantic similarity\\nbetween user-system dialogues, an astounding average success rate of 73% is\\nachieved. This is a significant improvement over the baseline Proximal Policy\\nOptimization (PPO), which has an average success rate of 60%. In addition,\\nperformance indicators such as booking rates and completion rates show a 10%\\nrise over the baseline. Furthermore, these intrinsic incentive models help\\nimprove the system's policy's resilience in an increasing amount of domains.\\nThis implies that they could be useful in scaling up to settings that cover a\\nwider range of domains. We present Gyan AI Paramanu (\\\"atom\\\"), a family of novel language models for\\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\\nand multilingual Indic language models pretrained from scratch on a single GPU\\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\\npretrained with a context size of 1024 on a single GPU. The models are very\\nefficient, small, fast, and powerful. We have also developed an efficient most\\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\\navoid the \\\"curse of multi-linguality\\\" in our multilingual mParamanu model, we\\npretrained on comparable corpora by typological grouping using the same script.\\nWe performed human evaluation of our pretrained models for open end text\\ngeneration on grammar, coherence, creativity, and factuality metrics for\\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\\nmodels on 23k instructions in respective languages. Our pretrained and\\ninstruction-tuned models which are first of its kind, most powerful efficient\\nsmall generative language models ever developed for Indic languages, and the\\nvarious results lead to the conclusion that high quality generative language\\nmodels are possible without high amount of compute power and humongous number\\nof parameters. We plan to release our models at https://www.bharatgpts.com. Anticipating the negative impacts of emerging AI technologies is a challenge,\\nespecially in the early stages of development. An understudied approach to such\\nanticipation is the use of LLMs to enhance and guide this process. Despite\\nadvancements in LLMs and evaluation metrics to account for biases in generated\\ntext, it is unclear how well these models perform in anticipatory tasks.\\nSpecifically, the use of LLMs to anticipate AI impacts raises questions about\\nthe quality and range of categories of negative impacts these models are\\ncapable of generating. In this paper we leverage news media, a diverse data\\nsource that is rich with normative assessments of emerging technologies, to\\nformulate a taxonomy of impacts to act as a baseline for comparing against. By\\ncomputationally analyzing thousands of news articles published by hundreds of\\nonline news domains around the world, we develop a taxonomy consisting of ten\\ncategories of AI impacts. We then evaluate both instruction-based (GPT-4 and\\nMistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)\\nusing a sample from this baseline. We find that the generated impacts using\\nMistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively\\non par with impacts generated using a larger scale model such as GPT-4.\\nMoreover, we find that these LLMs generate impacts that largely reflect the\\ntaxonomy of negative impacts identified in the news media, however the impacts\\nproduced by instruction-based models had gaps in the production of certain\\ncategories of impacts in comparison to fine-tuned models. This research\\nhighlights a potential bias in state-of-the-art LLMs when used for anticipating\\nimpacts and demonstrates the advantages of aligning smaller LLMs with a diverse\\nrange of impacts, such as those reflected in the news media, to better reflect\\nsuch impacts during anticipatory exercises. Prepending model inputs with safety prompts is a common practice of\\nsafeguarding large language models (LLMs) from complying with queries that\\ncontain harmful intents. However, the working mechanisms of safety prompts have\\nnot yet been fully understood, which hinders the potential for automatically\\noptimizing them for improved LLM safety. Motivated by this problem, we\\ninvestigate the impact of safety prompts from the perspective of model\\nrepresentations. We find that in models' representation space, harmful and\\nharmless queries can be largely distinguished, but this is not noticeably\\nenhanced by safety prompts. Instead, the queries' representations are moved by\\ndifferent safety prompts in similar directions, where models become more prone\\nto refusal (i.e., refusing to provide assistance) even when the queries are\\nharmless. Inspired by these findings, we propose a method called DRO (Directed\\nRepresentation Optimization) for automatic safety prompt optimization. DRO\\ntreats safety prompts as continuous, trainable embeddings and learns to move\\nthe representations of harmful/harmless queries along/opposite the direction in\\nwhich the model's refusal probability increases. We demonstrate that DRO\\nremarkably improves the safeguarding performance of human-crafted safety\\nprompts and outperforms strong baselines, as evaluated on out-of-domain\\nbenchmarks, without compromising the general model capability. Monitoring the distribution and size structure of long-living shrubs, such as\\nJuniperus communis, can be used to estimate the long-term effects of climate\\nchange on high-mountain and high latitude ecosystems. Historical aerial\\nvery-high resolution imagery offers a retrospective tool to monitor shrub\\ngrowth and distribution at high precision. Currently, deep learning models\\nprovide impressive results for detecting and delineating the contour of objects\\nwith defined shapes. However, adapting these models to detect natural objects\\nthat express complex growth patterns, such as junipers, is still a challenging\\ntask.\\n  This research presents a novel approach that leverages remotely sensed RGB\\nimagery in conjunction with Mask R-CNN-based instance segmentation models to\\nindividually delineate Juniperus shrubs above the treeline in Sierra Nevada\\n(Spain). In this study, we propose a new data construction design that consists\\nin using photo interpreted (PI) and field work (FW) data to respectively\\ndevelop and externally validate the model. We also propose a new shrub-tailored\\nevaluation algorithm based on a new metric called Multiple Intersections over\\nGround Truth Area (MIoGTA) to assess and optimize the model shrub delineation\\nperformance. Finally, we deploy the developed model for the first time to\\ngenerate a wall-to-wall map of Juniperus individuals.\\n  The experimental results demonstrate the efficiency of our dual data\\nconstruction approach in overcoming the limitations associated with traditional\\nfield survey methods. They also highlight the robustness of MIoGTA metric in\\nevaluating instance segmentation models on species with complex growth patterns\\nshowing more resilience against data annotation uncertainty. Furthermore, they\\nshow the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in\\ndelineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%,\\nrespectively. Despite the impressive capabilities of Multimodal Large Language Models\\n(MLLMs) in integrating text and image modalities, challenges remain in\\naccurately interpreting detailed visual elements. This paper presents an\\nempirical study on enhancing MLLMs with state-of-the-art (SOTA) object\\ndetection and Optical Character Recognition models to improve fine-grained\\nimage understanding and reduce hallucination in responses. Our research\\ninvestigates the embedding-based infusion of detection information, the impact\\nof such infusion on the MLLMs' original abilities, and the interchangeability\\nof detection models. We conduct systematic experiments with models such as\\nLLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines\\nMLLMs' performance in specific visual tasks but also maintains their original\\nstrengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10\\nbenchmarks, achieving an improvement of up to 12.99% on the normalized average\\nscore, marking a notable advancement in multimodal understanding. We release\\nour codes to facilitate further exploration into the fine-grained multimodal\\ndialogue capabilities of MLLMs. Quantum computing holds immense potential for solving classically intractable\\nproblems by leveraging the unique properties of quantum mechanics. The\\nscalability of quantum architectures remains a significant challenge.\\nMulti-core quantum architectures are proposed to solve the scalability problem,\\narising a new set of challenges in hardware, communications and compilation,\\namong others. One of these challenges is to adapt a quantum algorithm to fit\\nwithin the different cores of the quantum computer. This paper presents a novel\\napproach for circuit partitioning using Deep Reinforcement Learning,\\ncontributing to the advancement of both quantum computing and graph\\npartitioning. This work is the first step in integrating Deep Reinforcement\\nLearning techniques into Quantum Circuit Mapping, opening the door to a new\\nparadigm of solutions to such problems. Despite substantial efforts, neural network interpretability remains an\\nelusive goal, with previous research failing to provide succinct explanations\\nof most single neurons' impact on the network output. This limitation is due to\\nthe polysemantic nature of most neurons, whereby a given neuron is involved in\\nmultiple unrelated network states, complicating the interpretation of that\\nneuron. In this paper, we apply tools developed in neuroscience and information\\ntheory to propose both a novel practical approach to network interpretability\\nand theoretical insights into polysemanticity and the density of codes. We\\ninfer levels of redundancy in the network's code by inspecting the\\neigenspectrum of the activation's covariance matrix. Furthermore, we show how\\nrandom projections can reveal whether a network exhibits a smooth or\\nnon-differentiable code and hence how interpretable the code is. This same\\nframework explains the advantages of polysemantic neurons to learning\\nperformance and explains trends found in recent results by Elhage et\\nal.~(2022). Our approach advances the pursuit of interpretability in neural\\nnetworks, providing insights into their underlying structure and suggesting new\\navenues for circuit-level interpretability. In this study, a novel deep learning algorithm for object detection, named\\nMelNet, was introduced. MelNet underwent training utilizing the KITTI dataset\\nfor object detection. Following 300 training epochs, MelNet attained an mAP\\n(mean average precision) score of 0.732. Additionally, three alternative models\\n-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI\\ndataset and juxtaposed with MelNet for object detection.\\n  The outcomes underscore the efficacy of employing transfer learning in\\ncertain instances. Notably, preexisting models trained on prominent datasets\\n(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding\\nunderscores the viability of creating a new model tailored to a specific\\nscenario and training it on a specific dataset. This investigation demonstrates\\nthat training MelNet exclusively on the KITTI dataset also surpasses\\nEfficientDet after 150 epochs. Consequently, post-training, MelNet's\\nperformance closely aligns with that of other pre-trained models. Learning robot navigation strategies among pedestrian is crucial for domain\\nbased applications. Combining perception, planning and prediction allows us to\\nmodel the interactions between robots and pedestrians, resulting in impressive\\noutcomes especially with recent approaches based on deep reinforcement learning\\n(RL). However, these works do not consider multi-robot scenarios. In this\\npaper, we present MultiSoc, a new method for learning multi-agent socially\\naware navigation strategies using RL. Inspired by recent works on multi-agent\\ndeep RL, our method leverages graph-based representation of agent interactions,\\ncombining the positions and fields of view of entities (pedestrians and\\nagents). Each agent uses a model based on two Graph Neural Network combined\\nwith attention mechanisms. First an edge-selector produces a sparse graph, then\\na crowd coordinator applies node attention to produce a graph representing the\\ninfluence of each entity on the others. This is incorporated into a model-free\\nRL framework to learn multi-agent policies. We evaluate our approach on\\nsimulation and provide a series of experiments in a set of various conditions\\n(number of agents / pedestrians). Empirical results show that our method learns\\nfaster than social navigation deep RL mono-agent techniques, and enables\\nefficient multi-agent implicit coordination in challenging crowd navigation\\nwith multiple heterogeneous humans. Furthermore, by incorporating customizable\\nmeta-parameters, we can adjust the neighborhood density to take into account in\\nour navigation strategy. We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene\\nediting method that enables the replacement of specific objects within a scene.\\nGiven multi-view images of a scene, a text prompt describing the object to\\nreplace, and a text prompt describing the new object, our Erase-and-Replace\\napproach can effectively swap objects in the scene with newly generated content\\nwhile maintaining 3D consistency across multiple viewpoints. We demonstrate the\\nversatility of ReplaceAnything3D by applying it to various realistic 3D scenes,\\nshowcasing results of modified foreground objects that are well-integrated with\\nthe rest of the scene without affecting its overall integrity. We introduce a new class of deep neural networks (DNNs) with multilayered\\ntree-like architectures. The architectures are codified using numbers from the\\nring of integers of non-Archimdean local fields. These rings have a natural\\nhierarchical organization as infinite rooted trees. Natural morphisms on these\\nrings allow us to construct finite multilayered architectures. The new DNNs are\\nrobust universal approximators of real-valued functions defined on the\\nmentioned rings. We also show that the DNNs are robust universal approximators\\nof real-valued square-integrable functions defined in the unit interval. Subseasonal forecasting, which is pivotal for agriculture, water resource\\nmanagement, and early warning of disasters, faces challenges due to the chaotic\\nnature of the atmosphere. Recent advances in machine learning (ML) have\\nrevolutionized weather forecasting by achieving competitive predictive skills\\nto numerical models. However, training such foundation models requires\\nthousands of GPU days, which causes substantial carbon emissions and limits\\ntheir broader applicability. Moreover, ML models tend to fool the pixel-wise\\nerror scores by producing smoothed results which lack physical consistency and\\nmeteorological meaning. To deal with the aforementioned problems, we propose a\\nteleconnection-informed transformer. Our architecture leverages the pretrained\\nPangu model to achieve good initial weights and integrates a\\nteleconnection-informed temporal module to improve predictability in an\\nextended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's\\nparameters, our method enhances predictability on four surface and five\\nupper-level atmospheric variables at a two-week lead time. Furthermore, the\\nteleconnection-filtered features improve the spatial granularity of outputs\\nsignificantly, indicating their potential physical consistency. Our research\\nunderscores the importance of atmospheric and oceanic teleconnections in\\ndriving future weather conditions. Besides, it presents a resource-efficient\\npathway for researchers to leverage existing foundation models on versatile\\ndownstream tasks. Sensemaking is a constant and ongoing process by which people associate\\nmeaning to experiences. It can be an individual process, known as abduction, or\\na group process by which people give meaning to collective experiences. The\\nsensemaking of a group is influenced by the abduction process of each person\\nabout the experience. Every collaborative process needs some level of\\nsensemaking to show results. For a knowledge intensive process, sensemaking is\\ncentral and related to most of its tasks. We present findings from a fieldwork\\nexecuted in knowledge intensive process from the Oil and Gas industry. Our\\nfindings indicated that different types of knowledge can be combined to compose\\nthe result of a sensemaking process (e.g. decision, the need for more\\ndiscussion, etc.). This paper presents an initial set of knowledge types that\\ncan be combined to compose the result of the sensemaking of a collaborative\\ndecision making process. We also discuss ideas for using systems powered by\\nArtificial Intelligence to support sensemaking processes. Machine teaching often involves the creation of an optimal (typically\\nminimal) dataset to help a model (referred to as the `student') achieve\\nspecific goals given by a teacher. While abundant in the continuous domain, the\\nstudies on the effectiveness of machine teaching in the discrete domain are\\nrelatively limited. This paper focuses on machine teaching in the discrete\\ndomain, specifically on manipulating student models' predictions based on the\\ngoals of teachers via changing the training data efficiently. We formulate this\\ntask as a combinatorial optimization problem and solve it by proposing an\\niterative searching algorithm. Our algorithm demonstrates significant numerical\\nmerit in the scenarios where a teacher attempts at correcting erroneous\\npredictions to improve the student's models, or maliciously manipulating the\\nmodel to misclassify some specific samples to the target class aligned with his\\npersonal profits. Experimental results show that our proposed algorithm can\\nhave superior performance in effectively and efficiently manipulating the\\npredictions of the model, surpassing conventional baselines. Benchmarking heuristic algorithms is vital to understand under which\\nconditions and on what kind of problems certain algorithms perform well. In\\nmost current research into heuristic optimization algorithms, only a very\\nlimited number of scenarios, algorithm configurations and hyper-parameter\\nsettings are explored, leading to incomplete and often biased insights and\\nresults. This paper presents a novel approach we call explainable benchmarking.\\nIntroducing the IOH-Xplainer software framework, for analyzing and\\nunderstanding the performance of various optimization algorithms and the impact\\nof their different components and hyper-parameters. We showcase the framework\\nin the context of two modular optimization frameworks. Through this framework,\\nwe examine the impact of different algorithmic components and configurations,\\noffering insights into their performance across diverse scenarios. We provide a\\nsystematic method for evaluating and interpreting the behaviour and efficiency\\nof iterative optimization heuristics in a more transparent and comprehensible\\nmanner, allowing for better benchmarking and algorithm design. The increasing reliance on AI-driven solutions, particularly Large Language\\nModels (LLMs) like the GPT series, for information retrieval highlights the\\ncritical need for their factuality and fairness, especially amidst the rampant\\nspread of misinformation and disinformation online. Our study evaluates the\\nfactual accuracy, stability, and biases in widely adopted GPT models, including\\nGPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated\\ninformation dissemination.\\n  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of\\ngeographic and temporal representation, facilitating a more nuanced evaluation\\nof LLM biases. Our analysis reveals that newer iterations of GPT models do not\\nalways equate to improved performance. Notably, the GPT-4 version from March\\ndemonstrates higher factual accuracy than its subsequent June release.\\nFurthermore, a concerning bias is observed, privileging statements from the\\nGlobal North over the Global South, thus potentially exacerbating existing\\ninformational inequities. Regions such as Africa and the Middle East are at a\\ndisadvantage, with much lower factual accuracy. The performance fluctuations\\nover time suggest that model updates may not consistently benefit all regions\\nequally.\\n  Our study also offers insights into the impact of various LLM configuration\\nsettings, such as binary decision forcing, model re-runs and temperature, on\\nmodel's factuality. Models constrained to binary (true/false) choices exhibit\\nreduced factuality compared to those allowing an 'unclear' option. Single\\ninference at a low temperature setting matches the reliability of majority\\nvoting across various configurations. The insights gained highlight the need\\nfor culturally diverse and geographically inclusive model training and\\nevaluation. This approach is key to achieving global equity in technology,\\ndistributing AI benefits fairly worldwide. The rapidly changing landscape of technology and industries leads to dynamic\\nskill requirements, making it crucial for employees and employers to anticipate\\nsuch shifts to maintain a competitive edge in the labor market. Existing\\nefforts in this area either rely on domain-expert knowledge or regarding skill\\nevolution as a simplified time series forecasting problem. However, both\\napproaches overlook the sophisticated relationships among different skills and\\nthe inner-connection between skill demand and supply variations. In this paper,\\nwe propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH)\\nframework for joint skill demand-supply prediction. Specifically, CHGH is an\\nencoder-decoder network consisting of i) a cross-view graph encoder to capture\\nthe interconnection between skill demand and supply, ii) a hierarchical graph\\nencoder to model the co-evolution of skills from a cluster-wise perspective,\\nand iii) a conditional hyper-decoder to jointly predict demand and supply\\nvariations by incorporating historical demand-supply gaps. Extensive\\nexperiments on three real-world datasets demonstrate the superiority of the\\nproposed framework compared to seven baselines and the effectiveness of the\\nthree modules. In recent years, weakly supervised semantic segmentation using image-level\\nlabels as supervision has received significant attention in the field of\\ncomputer vision. Most existing methods have addressed the challenges arising\\nfrom the lack of spatial information in these labels by focusing on\\nfacilitating supervised learning through the generation of pseudo-labels from\\nclass activation maps (CAMs). Due to the localized pattern detection of\\nConvolutional Neural Networks (CNNs), CAMs often emphasize only the most\\ndiscriminative parts of an object, making it challenging to accurately\\ndistinguish foreground objects from each other and the background. Recent\\nstudies have shown that Vision Transformer (ViT) features, due to their global\\nview, are more effective in capturing the scene layout than CNNs. However, the\\nuse of hierarchical ViTs has not been extensively explored in this field. This\\nwork explores the use of Swin Transformer by proposing \\\"SWTformer\\\" to enhance\\nthe accuracy of the initial seed CAMs by bringing local and global views\\ntogether. SWTformer-V1 generates class probabilities and CAMs using only the\\npatch tokens as features. SWTformer-V2 incorporates a multi-scale feature\\nfusion mechanism to extract additional information and utilizes a\\nbackground-aware mechanism to generate more accurate localization maps with\\nimproved cross-object discrimination. Based on experiments on the PascalVOC\\n2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy,\\noutperforming state-of-the-art models. It also yields comparable performance by\\n0.82% mIoU on average higher than other methods in generating initial\\nlocalization maps, depending only on the classification network. SWTformer-V2\\nfurther improves the accuracy of the generated seed CAMs by 5.32% mIoU, further\\nproving the effectiveness of the local-to-global view provided by the Swin\\ntransformer. This study explores four methods of generating paraphrases in Malayalam,\\nutilizing resources available for English paraphrasing and pre-trained Neural\\nMachine Translation (NMT) models. We evaluate the resulting paraphrases using\\nboth automated metrics, such as BLEU, METEOR, and cosine similarity, as well as\\nhuman annotation. Our findings suggest that automated evaluation measures may\\nnot be fully appropriate for Malayalam, as they do not consistently align with\\nhuman judgment. This discrepancy underscores the need for more nuanced\\nparaphrase evaluation approaches especially for highly agglutinative languages. This paper details the privacy and security landscape in today's cloud\\necosystem and identifies that there is a gap in addressing the risks introduced\\nby machine learning models. As machine learning algorithms continue to evolve\\nand find applications across diverse domains, the need to categorize and\\nquantify privacy and security risks becomes increasingly critical. With the\\nemerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML\\nmodels) are deployed on the cloud by model providers and used by model\\nconsumers. We first survey the AIaaS landscape to document the various kinds of\\nliabilities that ML models, especially Deep Neural Networks pose and then\\nintroduce a taxonomy to bridge this gap by holistically examining the risks\\nthat creators and consumers of ML models are exposed to and their known\\ndefences till date. Such a structured approach will be beneficial for ML model\\nproviders to create robust solutions. Likewise, ML model consumers will find it\\nvaluable to evaluate such solutions and understand the implications of their\\nengagement with such services. The proposed taxonomies provide a foundational\\nbasis for solutions in private, secure and robust ML, paving the way for more\\ntransparent and resilient AI systems. With the development of new Internet services such as computation-intensive\\nand delay-sensitive tasks, the traditional \\\"Best Effort\\\" network transmission\\nmode has been greatly challenged. The network system is urgently required to\\nprovide end-to-end transmission determinacy and computing determinacy for new\\napplications to ensure the safe and efficient operation of services. Based on\\nthe research of the convergence of computing and networking, a new network\\nparadigm named deterministic computing power networking (Det-CPN) is proposed.\\nIn this article, we firstly introduce the research advance of computing power\\nnetworking. And then the motivations and scenarios of Det-CPN are analyzed.\\nFollowing that, we present the system architecture, technological capabilities,\\nworkflow as well as key technologies for Det-CPN. Finally, the challenges and\\nfuture trends of Det-CPN are analyzed and discussed. Model editing has recently gained widespread attention. Current model editing\\nmethods primarily involve modifying model parameters or adding additional\\nmodules to the existing model. However, the former causes irreversible damage\\nto LLMs, while the latter incurs additional inference overhead and fuzzy vector\\nmatching is not always reliable. To address these issues, we propose an\\nexpandable Subject Word Embedding Altering (SWEA) framework, which modifies the\\nrepresentation of subjects and achieve the goal of editing knowledge during the\\ninference stage. SWEA uses precise key matching outside the model and performs\\nreliable subject word embedding altering, thus protecting the original weights\\nof the model without increasing inference overhead. We then propose optimizing\\nthen suppressing fusion method, which first optimizes the embedding vector for\\nthe editing target and then suppresses the Knowledge Embedding Dimension (KED)\\nto obtain the final fused embedding. We thus propose SWEAOS method for editing\\nfactual knowledge in LLMs. We demonstrate the state-of-the-art performance of\\nSWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoning\\nability of SWEAOS in editing knowledge, we evaluate it on the more complex\\nRIPPLEEDITS benchmark. The results on two subdatasets demonstrate that our\\nSWEAOS possesses state-of-the-art reasoning ability. The dominant paradigm in AI ethics and value alignment is highly\\nanthropocentric. The focus of these disciplines is strictly on human values\\nwhich limits the depth and breadth of their insights. Recently, attempts to\\nexpand to a sentientist perspective have been initiated. We argue that neither\\nof these outlooks is sufficient to capture the actual complexity of the\\nbiosphere and ensure that AI does not damage it. Thus, we propose a new\\nparadigm -- Biospheric AI that assumes an ecocentric perspective. We discuss\\nhypothetical ways in which such an AI might be designed. Moreover, we give\\ndirections for research and application of the modern AI models that would be\\nconsistent with the biospheric interests. All in all, this work attempts to\\ntake first steps towards a comprehensive program of research that focuses on\\nthe interactions between AI and the biosphere. Contrastive representation learning is crucial in time series analysis as it\\nalleviates the issue of data noise and incompleteness as well as sparsity of\\nsupervision signal. However, existing constrastive learning frameworks usually\\nfocus on intral-temporal features, which fails to fully exploit the intricate\\nnature of time series data. To address this issue, we propose DE-TSMCL, an\\ninnovative distillation enhanced framework for long sequence time series\\nforecasting. Specifically, we design a learnable data augmentation mechanism\\nwhich adaptively learns whether to mask a timestamp to obtain optimized\\nsub-sequences. Then, we propose a contrastive learning task with momentum\\nupdate to explore inter-sample and intra-temporal correlations of time series\\nto learn the underlying structure feature on the unlabeled time series.\\nMeanwhile, we design a supervised task to learn more robust representations and\\nfacilitate the contrastive learning process. Finally, we jointly optimize the\\nabove two tasks. By developing model loss from multiple tasks, we can learn\\neffective representations for downstream forecasting task. Extensive\\nexperiments, in comparison with state-of-the-arts, well demonstrate the\\neffectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%. Recently, Transformers for graph representation learning have become\\nincreasingly popular, achieving state-of-the-art performance on a wide-variety\\nof datasets, either alone or in combination with message-passing graph neural\\nnetworks (MP-GNNs). Infusing graph inductive-biases in the innately\\nstructure-agnostic transformer architecture in the form of structural or\\npositional encodings (PEs) is key to achieving these impressive results.\\nHowever, designing such encodings is tricky and disparate attempts have been\\nmade to engineer such encodings including Laplacian eigenvectors, relative\\nrandom-walk probabilities (RRWP), spatial encodings, centrality encodings, edge\\nencodings etc. In this work, we argue that such encodings may not be required\\nat all, provided the attention mechanism itself incorporates information about\\nthe graph structure. We introduce Eigenformer, which uses a novel\\nspectrum-aware attention mechanism cognizant of the Laplacian spectrum of the\\ngraph, and empirically show that it achieves performance comparable to SOTA\\nMP-GNN architectures and Graph Transformers on a number of standard GNN\\nbenchmark datasets, even surpassing the SOTA on some datasets. We also find\\nthat our architecture is much faster to train in terms of number of epochs,\\npresumably due to the innate graph inductive biases. SDRDPy is a desktop application that allows experts an intuitive graphic and\\ntabular representation of the knowledge extracted by any supervised descriptive\\nrule discovery algorithm. The application is able to provide an analysis of the\\ndata showing the relevant information of the data set and the relationship\\nbetween the rules, data and the quality measures associated for each rule\\nregardless of the tool where algorithm has been executed. All of the\\ninformation is presented in a user-friendly application in order to facilitate\\nexpert analysis and also the exportation of reports in different formats. Contrastive Analysis (CA) deals with the discovery of what is common and what\\nis distinctive of a target domain compared to a background one. This is of\\ngreat interest in many applications, such as medical imaging. Current\\nstate-of-the-art (SOTA) methods are latent variable models based on VAE\\n(CA-VAEs). However, they all either ignore important constraints or they don't\\nenforce fundamental assumptions. This may lead to sub-optimal solutions where\\ndistinctive factors are mistaken for common ones (or viceversa). Furthermore,\\nthe generated images have a rather poor quality, typical of VAEs, decreasing\\ntheir interpretability and usefulness. Here, we propose Double InfoGAN, the\\nfirst GAN based method for CA that leverages the high-quality synthesis of GAN\\nand the separation power of InfoGAN. Experimental results on four visual\\ndatasets, from simple synthetic examples to complex medical images, show that\\nthe proposed method outperforms SOTA CA-VAEs in terms of latent separation and\\nimage quality. Datasets and code are available online. Message passing Graph Neural Networks (GNNs) are known to be limited in\\nexpressive power by the 1-WL color-refinement test for graph isomorphism. Other\\nmore expressive models either are computationally expensive or need\\npreprocessing to extract structural features from the graph. In this work, we\\npropose to make GNNs universal by guiding the learning process with exact\\nisomorphism solver techniques which operate on the paradigm of\\nIndividualization and Refinement (IR), a method to artificially introduce\\nasymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers\\ngenerate a search tree of colorings whose leaves uniquely identify the graph.\\nHowever, the tree grows exponentially large and needs hand-crafted pruning\\ntechniques which are not desirable from a learning perspective. We take a\\nprobabilistic view and approximate the search tree of colorings (i.e.\\nembeddings) by sampling multiple paths from root to leaves of the search tree.\\nTo learn more discriminative representations, we guide the sampling process\\nwith particle filter updates, a principled approach for sequential state\\nestimation. Our algorithm is end-to-end differentiable, can be applied with any\\nGNN as backbone and learns richer graph representations with only linear\\nincrease in runtime. Experimental evaluation shows that our approach\\nconsistently outperforms leading GNN models on both synthetic benchmarks for\\nisomorphism detection as well as real-world datasets. Large language models (LLMs) have recently garnered significant\\naccomplishments in various exploratory tasks, even surpassing the performance\\nof traditional reinforcement learning-based methods that have historically\\ndominated the agent-based field. The purpose of this paper is to investigate\\nthe efficacy of LLMs in executing real-time strategy war tasks within the\\nStarCraft II gaming environment. In this paper, we introduce SwarmBrain, an\\nembodied agent leveraging LLM for real-time strategy implementation in the\\nStarCraft II game environment. The SwarmBrain comprises two key components: 1)\\na Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed\\nto orchestrate macro-level strategies from a high-level perspective. This\\nmatrix emulates the overarching consciousness of the Zerg intelligence brain,\\nsynthesizing strategic foresight with the aim of allocating resources,\\ndirecting expansion, and coordinating multi-pronged assaults. 2) a Swarm\\nReflexNet, which is agile counterpart to the calculated deliberation of the\\nOvermind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the\\nSwarm ReflexNet employs a condition-response state machine framework, enabling\\nexpedited tactical responses for fundamental Zerg unit maneuvers. In the\\nexperimental setup, SwarmBrain is in control of the Zerg race in confrontation\\nwith an Computer-controlled Terran adversary. Experimental results show the\\ncapacity of SwarmBrain to conduct economic augmentation, territorial expansion,\\nand tactical formulation, and it shows the SwarmBrain is capable of achieving\\nvictory against Computer players set at different difficulty levels. This paper presents Haris, an advanced autonomous mobile robot system for\\ntracking the location of vehicles in crowded car parks using license plate\\nrecognition. The system employs simultaneous localization and mapping (SLAM)\\nfor autonomous navigation and precise mapping of the parking area, eliminating\\nthe need for GPS dependency. In addition, the system utilizes a sophisticated\\nframework using computer vision techniques for object detection and automatic\\nlicense plate recognition (ALPR) for reading and associating license plate\\nnumbers with location data. This information is subsequently synchronized with\\na back-end service and made accessible to users via a user-friendly mobile app,\\noffering effortless vehicle location and alleviating congestion within the\\nparking facility. The proposed system has the potential to improve the\\nmanagement of short-term large outdoor parking areas in crowded places such as\\nsports stadiums. The demo of the robot can be found on\\nhttps://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx. There is a mystery at the heart of operator learning: how can one recover a\\nnon-self-adjoint operator from data without probing the adjoint? Current\\npractical approaches suggest that one can accurately recover an operator while\\nonly using data generated by the forward action of the operator without access\\nto the adjoint. However, naively, it seems essential to sample the action of\\nthe adjoint. In this paper, we partially explain this mystery by proving that\\nwithout querying the adjoint, one can approximate a family of non-self-adjoint\\ninfinite-dimensional compact operators via projection onto a Fourier basis. We\\nthen apply the result to recovering Green's functions of elliptic partial\\ndifferential operators and derive an adjoint-free sample complexity bound.\\nWhile existing theory justifies low sample complexity in operator learning,\\nours is the first adjoint-free analysis that attempts to close the gap between\\ntheory and practice. The increasing usage of Artificial Intelligence (AI) models, especially Deep\\nNeural Networks (DNNs), is increasing the power consumption during training and\\ninference, posing environmental concerns and driving the need for more\\nenergy-efficient algorithms and hardware solutions. This work addresses the\\ngrowing energy consumption problem in Machine Learning (ML), particularly\\nduring the inference phase. Even a slight reduction in power usage can lead to\\nsignificant energy savings, benefiting users, companies, and the environment.\\nOur approach focuses on maximizing the accuracy of Artificial Neural Network\\n(ANN) models using a neuroevolutionary framework whilst minimizing their power\\nconsumption. To do so, power consumption is considered in the fitness function.\\nWe introduce a new mutation strategy that stochastically reintroduces modules\\nof layers, with power-efficient modules having a higher chance of being chosen.\\nWe introduce a novel technique that allows training two separate models in a\\nsingle training step whilst promoting one of them to be more power efficient\\nthan the other while maintaining similar accuracy. The results demonstrate a\\nreduction in power consumption of ANN models by up to 29.2% without a\\nsignificant decrease in predictive performance. Episodic training is a mainstream training strategy for few-shot learning. In\\nfew-shot scenarios, however, this strategy is often inferior to some\\nnon-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA),\\nwhich challenges the principle that training conditions must match testing\\nconditions. Thus, a question is naturally asked: How to search for\\nepisodic-free tasks for better few-shot learning? In this work, we propose a\\nnovel meta-training framework beyond episodic training. In this framework,\\nepisodic tasks are not used directly for training, but for evaluating the\\neffectiveness of some selected episodic-free tasks from a task set that are\\nperformed for training the meta-learners. The selection criterion is designed\\nwith the affinity, which measures the degree to which loss decreases when\\nexecuting the target tasks after training with the selected tasks. In\\nexperiments, the training task set contains some promising types, e. g.,\\ncontrastive learning and classification, and the target few-shot tasks are\\nachieved with the nearest centroid classifiers on the miniImageNet,\\ntiered-ImageNet and CIFAR-FS datasets. The experimental results demonstrate the\\neffectiveness of our approach. Conceptual architecture involves a highly creative exploration of novel\\nideas, often taken from other disciplines as architects consider radical new\\nforms, materials, textures and colors for buildings. While today's generative\\nAI systems can produce remarkable results, they lack the creativity\\ndemonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool,\\ncombines evolutionary search with generative AI, enabling users to explore\\ncreative and good quality designs inspired by their initial input through a\\nsimple point and click interface. SCAPE injects randomness into generative AI,\\nand enables memory, making use of the built-in language skills of GPT-4 to vary\\nprompts via text-based mutation and crossover. We demonstrate that compared to\\nDALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements\\nin quality and effectiveness of use; we show that in just 3 iterations SCAPE\\nhas a 24% image novelty increase enabling effective exploration, plus\\noptimization of images by users. We use more than 20 independent architects to\\nassess SCAPE, who provide markedly positive feedback. Predicting and understanding the changes in cognitive performance, especially\\nafter a longitudinal intervention, is a fundamental goal in neuroscience.\\nLongitudinal brain stimulation-based interventions like transcranial direct\\ncurrent stimulation (tDCS) induce short-term changes in the resting membrane\\npotential and influence cognitive processes. However, very little research has\\nbeen conducted on predicting these changes in cognitive performance\\npost-intervention. In this research, we intend to address this gap in the\\nliterature by employing different EEG-based functional connectivity analyses\\nand machine learning algorithms to predict changes in cognitive performance in\\na complex multitasking task. Forty subjects were divided into experimental and\\nactive-control conditions. On Day 1, all subjects executed a multitasking task\\nwith simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjects\\nin the experimental condition undertook 15 minutes of 2mA anodal tDCS\\nstimulation during task training. Subjects in the active-control condition\\nundertook 15 minutes of sham stimulation during task training. On Day 10, all\\nsubjects again executed the multitasking task with EEG acquisition.\\nSource-level functional connectivity metrics, namely phase lag index and\\ndirected transfer function, were extracted from the EEG data on Day 1 and Day\\n10. Various machine learning models were employed to predict changes in\\ncognitive performance. Results revealed that the multi-layer perceptron and\\ndirected transfer function recorded a cross-validation training RMSE of 5.11%\\nand a test RMSE of 4.97%. We discuss the implications of our results in\\ndeveloping real-time cognitive state assessors for accurately predicting\\ncognitive performance in dynamic and complex tasks post-tDCS intervention Interior design is all about creating spaces that look and feel good.\\nHowever, the subjective nature of aesthetic preferences presents a significant\\nchallenge in defining and quantifying what makes an interior design visually\\nappealing. The current paper addresses this gap by introducing a novel\\nmethodology for quantifying and predicting aesthetic preferences in interior\\ndesign. Our study combines fuzzy logic with image processing techniques. We\\ncollected a dataset of interior design images from social media platforms,\\nfocusing on essential visual attributes such as color harmony, lightness, and\\ncomplexity. We integrate these features using weighted average to compute a\\ngeneral aesthetic score. Our approach considers individual color preferences in\\ncalculating the overall aesthetic preference. We initially gather user ratings\\nfor primary colors like red, brown, and others to understand their preferences.\\nThen, we use the pixel count of the top five dominant colors in the image to\\nget the color scheme preference. The color scheme preference and the aesthetic\\nscore are then passed as inputs to the fuzzy inference system to calculate an\\noverall preference score. This score represents a comprehensive measure of the\\nuser's preference for a particular interior design, considering their color\\nchoices and general aesthetic appeal. We used the 2AFC (Two-Alternative Forced\\nChoice) method to validate our methodology, achieving a notable hit rate of\\n0.7. This study can help designers and professionals better understand and meet\\npeople's interior design preferences, especially in a world that relies heavily\\non digital media. Executive functioning is a cognitive process that enables humans to plan,\\norganize, and regulate their behavior in a goal-directed manner. Understanding\\nand classifying the changes in executive functioning after longitudinal\\ninterventions (like transcranial direct current stimulation (tDCS)) has not\\nbeen explored in the literature. This study employs functional connectivity and\\nmachine learning algorithms to classify executive functioning performance\\npost-tDCS. Fifty subjects were divided into experimental and placebo control\\ngroups. EEG data was collected while subjects performed an executive\\nfunctioning task on Day 1. The experimental group received tDCS during task\\ntraining from Day 2 to Day 8, while the control group received sham tDCS. On\\nDay 10, subjects repeated the tasks specified on Day 1. Different functional\\nconnectivity metrics were extracted from EEG data and eventually used for\\nclassifying executive functioning performance using different machine learning\\nalgorithms. Results revealed that a novel combination of partial directed\\ncoherence and multi-layer perceptron (along with recursive feature elimination)\\nresulted in a high classification accuracy of 95.44%. We discuss the\\nimplications of our results in developing real-time neurofeedback systems for\\nassessing and enhancing executive functioning performance post-tDCS\\nadministration. We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP\\nemploys two acoustic representation models, EnCodec and CLAP, along with a\\npretrained language model, BART. We also introduce a new training objective\\ncalled masked codec modeling that improves acoustic awareness of the pretrained\\nlanguage model. Experimental results on AudioCaps and Clotho demonstrate that\\nour model surpasses the performance of baseline models. Source code will be\\navailable at https://github.com/jaeyeonkim99/EnCLAP . An online demo is\\navailable at https://huggingface.co/spaces/enclap-team/enclap . Recent advancements in artificial intelligence have sparked interest in the\\nparallels between large language models (LLMs) and human neural processing,\\nparticularly in language comprehension. While prior research has established\\nsimilarities in the representation of LLMs and the brain, the underlying\\ncomputational principles that cause this convergence, especially in the context\\nof evolving LLMs, remain elusive. Here, we examined a diverse selection of\\nhigh-performance LLMs with similar parameter sizes to investigate the factors\\ncontributing to their alignment with the brain's language processing\\nmechanisms. We find that as LLMs achieve higher performance on benchmark tasks,\\nthey not only become more brain-like as measured by higher performance when\\npredicting neural responses from LLM embeddings, but also their hierarchical\\nfeature extraction pathways map more closely onto the brain's while using fewer\\nlayers to do the same encoding. We also compare the feature extraction pathways\\nof the LLMs to each other and identify new ways in which high-performing models\\nhave converged toward similar hierarchical processing mechanisms. Finally, we\\nshow the importance of contextual information in improving model performance\\nand brain similarity. Our findings reveal the converging aspects of language\\nprocessing in the brain and LLMs and offer new directions for developing models\\nthat align more closely with human cognitive processing. Many different worldwide initiatives are promoting the transformation from\\nmachine dominant manufacturing to digital manufacturing. Thus, to achieve a\\nsuccessful transformation to Industry 4.0 standard, manufacturing enterprises\\nare required to implement a clear roadmap. However, Small and Medium\\nManufacturing Enterprises (SMEs) encounter many barriers and difficulties\\n(economical, technical, cultural, etc.) in the implementation of Industry 4.0.\\nAlthough several works deal with the incorporation of Industry 4.0 technologies\\nin the area of the product and supply chain life cycles, which SMEs could use\\nas reference, this is not the case for the customer life cycle. Thus, we\\npresent two contributions that can help the software engineers of those SMEs to\\nincorporate Industry 4.0 technologies in the context of the customer life\\ncycle. The first contribution is a methodology that can help those software\\nengineers in the task of creating new software services, aligned with Industry\\n4.0, that allow to change how customers interact with enterprises and the\\nexperiences they have while interacting with them. The methodology details a\\nset of stages that are divided into phases which in turn are made up of\\nactivities. It places special emphasis on the incorporation of semantics\\ndescriptions and 3D visualization in the implementation of those new services.\\nThe second contribution is a system developed for a real manufacturing\\nscenario, using the proposed methodology, which allows to observe the\\npossibilities that this kind of systems can offer to SMEs in two phases of the\\ncustomer life cycle: Discover & Shop, and Use & Service. Use energy-based model for bridge-type innovation. The loss function is\\nexplained by the game theory, the logic is clear and the formula is simple and\\nclear. Thus avoid the use of maximum likelihood estimation to explain the loss\\nfunction and eliminate the need for Monte Carlo methods to solve the normalized\\ndenominator. Assuming that the bridge-type population follows a Boltzmann\\ndistribution, a neural network is constructed to represent the energy function.\\nUse Langevin dynamics technology to generate a new sample with low energy\\nvalue, thus a generative model of bridge-type based on energy is established.\\nTrain energy function on symmetric structured image dataset of three span beam\\nbridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately\\ncalculate the energy values of real and fake samples. Sampling from latent\\nspace, using gradient descent algorithm, the energy function transforms the\\nsampling points into low energy score samples, thereby generating new bridge\\ntypes different from the dataset. Due to unstable and slow training in this\\nattempt, the possibility of generating new bridge types is rare and the image\\ndefinition of generated images is low. Federated search, which involves integrating results from multiple\\nindependent search engines, will become increasingly pivotal in the context of\\nRetrieval-Augmented Generation pipelines empowering LLM-based applications such\\nas chatbots. These systems often distribute queries among various search\\nengines, ranging from specialized (e.g., PubMed) to general (e.g., Google),\\nbased on the nature of user utterances. A critical aspect of federated search\\nis resource selection - the selection of appropriate resources prior to issuing\\nthe query to ensure high-quality and rapid responses, and contain costs\\nassociated with calling the external search engines. However, current SOTA\\nresource selection methodologies primarily rely on feature-based learning\\napproaches. These methods often involve the labour intensive and expensive\\ncreation of training labels for each resource. In contrast, LLMs have exhibited\\nstrong effectiveness as zero-shot methods across NLP and IR tasks. We\\nhypothesise that in the context of federated search LLMs can assess the\\nrelevance of resources without the need for extensive predefined labels or\\nfeatures. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to\\ndrive the selection of resources in federated search in a zero-shot setting. In\\naddition, we devise an unsupervised fine tuning protocol, the Synthetic Label\\nAugmentation Tuning (SLAT), where the relevance of previously logged queries\\nand snippets from resources is predicted using an off-the-shelf LLM and then in\\nturn used to fine-tune ReSLLM with respect to resource selection. Our empirical\\nevaluation and analysis details the factors influencing the effectiveness of\\nLLMs in this context. The results showcase the merits of ReSLLM for resource\\nselection: not only competitive effectiveness in the zero-shot setting, but\\nalso obtaining large when fine-tuned using SLAT-protocol. Recent advances in machine learning (ML) have expedited retrosynthesis\\nresearch by assisting chemists to design experiments more efficiently. However,\\nall ML-based methods consume substantial amounts of paired training data (i.e.,\\nchemical reaction: product-reactant(s) pair), which is costly to obtain.\\nMoreover, companies view reaction data as a valuable asset and restrict the\\naccessibility to researchers. These issues prevent the creation of more\\npowerful retrosynthesis models due to their data-driven nature. As a response,\\nwe exploit easy-to-access unpaired data (i.e., one component of\\nproduct-reactant(s) pair) for generating in-silico paired data to facilitate\\nmodel training. Specifically, we present RetroWISE, a self-boosting framework\\nthat employs a base model inferred from real paired data to perform in-silico\\nreaction generation and augmentation using unpaired data, ultimately leading to\\na superior model. On three benchmark datasets, RetroWISE achieves the best\\noverall performance against state-of-the-art models (e.g., +8.6% top-1 accuracy\\non the USPTO-50K test dataset). Moreover, it consistently improves the\\nprediction accuracy of rare transformations. These results show that Retro-\\nWISE overcomes the training bottleneck by in-silico reactions, thereby paving\\nthe way toward more effective ML-based retrosynthesis models. Large language models are meticulously aligned to be both helpful and\\nharmless. However, recent research points to a potential overkill which means\\nmodels may refuse to answer benign queries. In this paper, we investigate the\\nfactors for overkill by exploring how models handle and determine the safety of\\nqueries. Our findings reveal the presence of shortcuts within models, leading\\nto an over-attention of harmful words like 'kill' and prompts emphasizing\\nsafety will exacerbate overkill. Based on these insights, we introduce\\nSelf-Contrastive Decoding (Self-CD), a training-free and model-agnostic\\nstrategy, to alleviate this phenomenon. We first extract such over-attention by\\namplifying the difference in the model's output distributions when responding\\nto system prompts that either include or omit an emphasis on safety. Then we\\ndetermine the final next-token predictions by downplaying the over-attention\\nfrom the model via contrastive decoding. Empirical results indicate that our\\nmethod has achieved an average reduction of the refusal rate by 20\\\\% while\\nhaving almost no impact on safety. Generating fake data is an essential dimension of modern software testing, as\\ndemonstrated by the number and significance of data faking libraries. Yet,\\ndevelopers of faking libraries cannot keep up with the wide range of data to be\\ngenerated for different natural languages and domains. In this paper, we assess\\nthe ability of generative AI for generating test data in different domains. We\\ndesign three types of prompts for Large Language Models (LLMs), which perform\\ntest data generation tasks at different levels of integrability: 1) raw test\\ndata generation, 2) synthesizing programs in a specific language that generate\\nuseful test data, and 3) producing programs that use state-of-the-art faker\\nlibraries. We evaluate our approach by prompting LLMs to generate test data for\\n11 domains. The results show that LLMs can successfully generate realistic test\\ndata generators in a wide range of domains at all three levels of\\nintegrability. Training task-oriented dialog agents based on reinforcement learning is\\ntime-consuming and requires a large number of interactions with real users. How\\nto grasp dialog policy within limited dialog experiences remains an obstacle\\nthat makes the agent training process less efficient. In addition, most\\nprevious frameworks start training by randomly choosing training samples, which\\ndiffers from the human learning method and hurts the efficiency and stability\\nof training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a\\ncuriosity-driven curriculum learning framework based on a state-of-the-art\\nmodel-based reinforcement learning dialog model, Deep Dyna-Q (DDQ).\\nFurthermore, we designed learning schedules for SC-DDQ and DDQ, respectively,\\nfollowing two opposite training strategies: classic curriculum learning and its\\nreverse version. Our results show that by introducing scheduled learning and\\ncuriosity, the new framework leads to a significant improvement over the DDQ\\nand Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum\\nlearning was not always effective. Specifically, according to the experimental\\nresults, the easy-first and difficult-first strategies are more suitable for\\nSC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled\\nactions to depict action exploration and found that training strategies with\\nhigh entropy in the first stage and low entropy in the last stage lead to\\nbetter performance. Multi-view multi-human association and tracking (MvMHAT), is a new but\\nimportant problem for multi-person scene video surveillance, aiming to track a\\ngroup of people over time in each view, as well as to identify the same person\\nacross different views at the same time, which is different from previous MOT\\nand multi-camera MOT tasks only considering the over-time human tracking. This\\nway, the videos for MvMHAT require more complex annotations while containing\\nmore information for self learning. In this work, we tackle this problem with a\\nself-supervised learning aware end-to-end network. Specifically, we propose to\\ntake advantage of the spatial-temporal self-consistency rationale by\\nconsidering three properties of reflexivity, symmetry and transitivity. Besides\\nthe reflexivity property that naturally holds, we design the self-supervised\\nlearning losses based on the properties of symmetry and transitivity, for both\\nappearance feature learning and assignment matrix optimization, to associate\\nthe multiple humans over time and across views. Furthermore, to promote the\\nresearch on MvMHAT, we build two new large-scale benchmarks for the network\\ntraining and testing of different algorithms. Extensive experiments on the\\nproposed benchmarks verify the effectiveness of our method. We have released\\nthe benchmark and code to the public. Neural network compression techniques, such as knowledge distillation (KD)\\nand network pruning, have received increasing attention. Recent work `Prune,\\nthen Distill' reveals that a pruned student-friendly teacher network can\\nbenefit the performance of KD. However, the conventional teacher-student\\npipeline, which entails cumbersome pre-training of the teacher and complicated\\ncompression steps, makes pruning with KD less efficient. In addition to\\ncompressing models, recent compression techniques also emphasize the aspect of\\nefficiency. Early pruning demands significantly less computational cost in\\ncomparison to the conventional pruning methods as it does not require a large\\npre-trained model. Likewise, a special case of KD, known as self-distillation\\n(SD), is more efficient since it requires no pre-training or student-teacher\\npair selection. This inspires us to collaborate early pruning with SD for\\nefficient model compression. In this work, we propose the framework named Early\\nPruning with Self-Distillation (EPSD), which identifies and preserves\\ndistillable weights in early pruning for a given SD task. EPSD efficiently\\ncombines early pruning and self-distillation in a two-step process, maintaining\\nthe pruned network's trainability for compression. Instead of a simple\\ncombination of pruning and SD, EPSD enables the pruned network to favor SD by\\nkeeping more distillable weights before training to ensure better distillation\\nof the pruned network. We demonstrated that EPSD improves the training of\\npruned networks, supported by visual and quantitative analyses. Our evaluation\\ncovered diverse benchmarks (CIFAR-10/100, Tiny-ImageNet, full ImageNet,\\nCUB-200-2011, and Pascal VOC), with EPSD outperforming advanced pruning and SD\\ntechniques. Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are\\nsimultaneously more impacted in terms of health and more disadvantaged in terms\\nof access to the vaccine, we formalize and study the problem of resource\\nallocation when there are inherent access differences that correlate with\\nadvantage and disadvantage. We identify reducing resource disparity as a key\\ngoal in this context and show its role as a proxy to more nuanced downstream\\nimpacts. We develop a concrete access model that helps quantify how a given\\nallocation translates to resource flow for the advantaged vs. the\\ndisadvantaged, based on the access gap between them. We then provide a\\nmethodology for access-aware allocation. Intuitively, the resulting allocation\\nleverages more vaccines in locations with higher vulnerable populations to\\nmitigate the access gap and reduce overall disparity. Surprisingly, knowledge\\nof the access gap is often not needed to perform access-aware allocation. To\\nsupport this formalism, we provide empirical evidence for our access model and\\nshow that access-aware allocation can significantly reduce resource disparity\\nand thus improve downstream outcomes. We demonstrate this at various scales,\\nincluding at county, state, national, and global levels. Large Vision-Language Models (VLMs) have demonstrated impressive performance\\non complex tasks involving visual input with natural language instructions.\\nHowever, it remains unclear to what extent capabilities on natural images\\ntransfer to Earth observation (EO) data, which are predominantly satellite and\\naerial images less common in VLM training data. In this work, we propose a\\ncomprehensive benchmark to gauge the progress of VLMs toward being useful tools\\nfor EO data by assessing their abilities on scene understanding, localization\\nand counting, and change detection tasks. Motivated by real-world applications,\\nour benchmark includes scenarios like urban monitoring, disaster relief, land\\nuse, and conservation. We discover that, although state-of-the-art VLMs like\\nGPT-4V possess extensive world knowledge that leads to strong performance on\\nopen-ended tasks like location understanding and image captioning, their poor\\nspatial reasoning limits usefulness on object localization and counting tasks.\\nOur benchmark will be made publicly available at https://vleo.danielz.ch/ and\\non Hugging Face at\\nhttps://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70\\nfor easy model evaluation. Local feature matching enjoys wide-ranging applications in the realm of\\ncomputer vision, encompassing domains such as image retrieval, 3D\\nreconstruction, and object recognition. However, challenges persist in\\nimproving the accuracy and robustness of matching due to factors like viewpoint\\nand lighting variations. In recent years, the introduction of deep learning\\nmodels has sparked widespread exploration into local feature matching\\ntechniques. The objective of this endeavor is to furnish a comprehensive\\noverview of local feature matching methods. These methods are categorized into\\ntwo key segments based on the presence of detectors. The Detector-based\\ncategory encompasses models inclusive of Detect-then-Describe, Joint Detection\\nand Description, Describe-then-Detect, as well as Graph Based techniques. In\\ncontrast, the Detector-free category comprises CNN Based, Transformer Based,\\nand Patch Based methods. Our study extends beyond methodological analysis,\\nincorporating evaluations of prevalent datasets and metrics to facilitate a\\nquantitative comparison of state-of-the-art techniques. The paper also explores\\nthe practical application of local feature matching in diverse domains such as\\nStructure from Motion, Remote Sensing Image Registration, and Medical Image\\nRegistration, underscoring its versatility and significance across various\\nfields. Ultimately, we endeavor to outline the current challenges faced in this\\ndomain and furnish future research directions, thereby serving as a reference\\nfor researchers involved in local feature matching and its interconnected\\ndomains. Current approaches of knowledge editing struggle to effectively propagate\\nupdates to interconnected facts. In this work, we delve into the barriers that\\nhinder the appropriate propagation of updated knowledge within these models for\\naccurate reasoning. To support our analysis, we introduce a novel\\nreasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing\\ndataset) -- which covers six common reasoning schemes in real world. We conduct\\na thorough analysis of existing knowledge editing techniques, including input\\naugmentation, finetuning, and locate-and-edit. We found that all model editing\\nmethods show notably low performance on this dataset, especially in certain\\nreasoning schemes. Our analysis over the chain-of-thought generation of edited\\nmodels further uncover key reasons behind the inadequacy of existing knowledge\\nediting methods from a reasoning standpoint, involving aspects on fact-wise\\nediting, fact recall ability, and coherence in generation. We will make our\\nbenchmark publicly available. Legged robots navigating cluttered environments must be jointly agile for\\nefficient task execution and safe to avoid collisions with obstacles or humans.\\nExisting studies either develop conservative controllers (< 1.0 m/s) to ensure\\nsafety, or focus on agility without considering potentially fatal collisions.\\nThis paper introduces Agile But Safe (ABS), a learning-based control framework\\nthat enables agile and collision-free locomotion for quadrupedal robots. ABS\\ninvolves an agile policy to execute agile motor skills amidst obstacles and a\\nrecovery policy to prevent failures, collaboratively achieving high-speed and\\ncollision-free navigation. The policy switch in ABS is governed by a learned\\ncontrol-theoretic reach-avoid value network, which also guides the recovery\\npolicy as an objective function, thereby safeguarding the robot in a closed\\nloop. The training process involves the learning of the agile policy, the\\nreach-avoid value network, the recovery policy, and an exteroception\\nrepresentation network, all in simulation. These trained modules can be\\ndirectly deployed in the real world with onboard sensing and computation,\\nleading to high-speed and collision-free navigation in confined indoor and\\noutdoor spaces with both static and dynamic obstacles. The application of mixture-of-experts (MoE) is gaining popularity due to its\\nability to improve model's performance. In an MoE structure, the gate layer\\nplays a significant role in distinguishing and routing input features to\\ndifferent experts. This enables each expert to specialize in processing their\\ncorresponding sub-tasks. However, the gate's routing mechanism also gives rise\\nto narrow vision: the individual MoE's expert fails to use more samples in\\nlearning the allocated sub-task, which in turn limits the MoE to further\\nimprove its generalization ability. To effectively address this, we propose a\\nmethod called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual\\ndistillation among experts to enable each expert to pick up more features\\nlearned by other experts and gain more accurate perceptions on their original\\nallocated sub-tasks. We conduct plenty experiments including tabular, NLP and\\nCV datasets, which shows MoDE's effectiveness, universality and robustness.\\nFurthermore, we develop a parallel study through innovatively constructing\\n\\\"expert probing\\\", to experimentally prove why MoDE works: moderate distilling\\nknowledge can improve each individual expert's test performances on their\\nassigned tasks, leading to MoE's overall performance improvement. The advent of Large Models marks a new era in machine learning, significantly\\noutperforming smaller models by leveraging vast datasets to capture and\\nsynthesize complex patterns. Despite these advancements, the exploration into\\nscaling, especially in the audio generation domain, remains limited, with\\nprevious efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and\\nsuffering from both spectral discontinuities and blurriness in the\\nhigh-frequency domain, alongside a lack of robustness against out-of-domain\\ndata. These limitations restrict the applicability of models to diverse use\\ncases, including music and singing generation. Our work introduces Enhanced\\nVarious Audio Generation via Scalable Generative Adversarial Networks\\n(EVA-GAN), yields significant improvements over previous state-of-the-art in\\nspectral and high-frequency reconstruction and robustness in out-of-domain data\\nperformance, enabling the generation of HiFi audios by employing an extensive\\ndataset of 36,000 hours of 44.1kHz audio, a context-aware module, a\\nHuman-In-The-Loop artifact measurement toolkit, and expands the model to\\napproximately 200 million parameters. Demonstrations of our work are available\\nat https://double-blind-eva-gan.cc. Recently, channel-independent methods have achieved state-of-the-art\\nperformance in multivariate time series (MTS) forecasting. Despite reducing\\noverfitting risks, these methods miss potential opportunities in utilizing\\nchannel dependence for accurate predictions. We argue that there exist locally\\nstationary lead-lag relationships between variates, i.e., some lagged variates\\nmay follow the leading indicators within a short time period. Exploiting such\\nchannel dependence is beneficial since leading indicators offer advance\\ninformation that can be used to reduce the forecasting difficulty of the lagged\\nvariates. In this paper, we propose a new method named LIFT that first\\nefficiently estimates leading indicators and their leading steps at each time\\nstep and then judiciously allows the lagged variates to utilize the advance\\ninformation from leading indicators. LIFT plays as a plugin that can be\\nseamlessly collaborated with arbitrary time series forecasting methods.\\nExtensive experiments on six real-world datasets demonstrate that LIFT improves\\nthe state-of-the-art methods by 5.5% in average forecasting performance. Data-effective learning aims to use data in the most impactful way to train\\nAI models, which involves strategies that focus on data quality rather than\\nquantity, ensuring the data used for training has high informational value.\\nData-effective learning plays a profound role in accelerating AI training,\\nreducing computational costs, and saving data storage, which is very important\\nas the volume of medical data in recent years has grown beyond many people's\\nexpectations. However, due to the lack of standards and comprehensive\\nbenchmark, research on medical data-effective learning is poorly studied. To\\naddress this gap, our paper introduces a comprehensive benchmark specifically\\nfor evaluating data-effective learning in the medical field. This benchmark\\nincludes a dataset with millions of data samples from 31 medical centers\\n(DataDEL), a baseline method for comparison (MedDEL), and a new evaluation\\nmetric (NormDEL) to objectively measure data-effective learning performance.\\nOur extensive experimental results show the baseline MedDEL can achieve\\nperformance comparable to the original large dataset with only 5% of the data.\\nEstablishing such an open data-effective learning benchmark is crucial for the\\nmedical AI research community because it facilitates efficient data use,\\npromotes collaborative breakthroughs, and fosters the development of\\ncost-effective, scalable, and impactful healthcare solutions. The project can\\nbe accessed at\\nhttps://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git. Cutting planes (cuts) play an important role in solving mixed-integer linear\\nprograms (MILPs), as they significantly tighten the dual bounds and improve the\\nsolving performance. A key problem for cuts is when to stop cuts generation,\\nwhich is important for the efficiency of solving MILPs. However, many modern\\nMILP solvers employ hard-coded heuristics to tackle this problem, which tends\\nto neglect underlying patterns among MILPs from certain applications. To\\naddress this challenge, we formulate the cuts generation stopping problem as a\\nreinforcement learning problem and propose a novel hybrid graph representation\\nmodel (HYGRO) to learn effective stopping strategies. An appealing feature of\\nHYGRO is that it can effectively capture both the dynamic and static features\\nof MILPs, enabling dynamic decision-making for the stopping strategies. To the\\nbest of our knowledge, HYGRO is the first data-driven method to tackle the cuts\\ngeneration stopping problem. By integrating our approach with modern solvers,\\nexperiments demonstrate that HYGRO significantly improves the efficiency of\\nsolving MILPs compared to competitive baselines, achieving up to 31%\\nimprovement. In this paper, a deep learning method for solving an improved one-dimensional\\nPoisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning\\nsolver, is presented. In particular, it combines a novel local neural network\\nscheme with an effective PNPic finite element solver. Since the input data of\\nthe neural network scheme only involves a small local patch of coarse grid\\nsolutions, which the finite element solver can quickly produce, the PNPic deep\\nlearning solver can be trained much faster than any corresponding conventional\\nglobal neural network solvers. After properly trained, it can output a\\npredicted PNPic solution in a much higher degree of accuracy than the low cost\\ncoarse grid solutions and can reflect different perturbation cases on the\\nparameters, ion channel subregions, and interface and boundary values, etc.\\nConsequently, the PNPic deep learning solver can generate a numerical solution\\nwith high accuracy for a family of PNPic models. As an initial study, two types\\nof numerical tests were done by perturbing one and two parameters of the PNPic\\nmodel, respectively, as well as the tests done by using a few perturbed\\ninterface positions of the model as training samples. These tests demonstrate\\nthat the PNPic deep learning solver can generate highly accurate PNPic\\nnumerical solutions. This paper addresses the unique challenges associated with uncertainty\\nquantification in AI models when applied to patient-facing contexts within\\nhealthcare. Unlike traditional eXplainable Artificial Intelligence (XAI)\\nmethods tailored for model developers or domain experts, additional\\nconsiderations of communicating in natural language, its presentation and\\nevaluating understandability are necessary. We identify the challenges in\\ncommunication model performance, confidence, reasoning and unknown knowns using\\nnatural language in the context of risk prediction. We propose a design aimed\\nat addressing these challenges, focusing on the specific application of\\nin-vitro fertilisation outcome prediction. We study the probabilistic modeling performed by Autoregressive Large\\nLanguage Models through the angle of time directionality. We empirically find a\\ntime asymmetry exhibited by such models in their ability to model natural\\nlanguage: a difference in the average log-perplexity when trying to predict the\\nnext token versus when trying to predict the previous one. This difference is\\nat the same time subtle and very consistent across various modalities\\n(language, model size, training time, ...). Theoretically, this is surprising:\\nfrom an information-theoretic point of view, there should be no such\\ndifference. We provide a theoretical framework to explain how such an asymmetry\\ncan appear from sparsity and computational complexity considerations, and\\noutline a number of perspectives opened by our results. This paper introduces LeTO, a method for learning constrained visuomotor\\npolicy via differentiable trajectory optimization. Our approach uniquely\\nintegrates a differentiable optimization layer into the neural network. By\\nformulating the optimization layer as a trajectory optimization problem, we\\nenable the model to end-to-end generate actions in a safe and controlled\\nfashion without extra modules. Our method allows for the introduction of\\nconstraints information during the training process, thereby balancing the\\ntraining objectives of satisfying constraints, smoothing the trajectories, and\\nminimizing errors with demonstrations. This \\\"gray box\\\" method marries the\\noptimization-based safety and interpretability with the powerful\\nrepresentational abilities of neural networks. We quantitatively evaluate LeTO\\nin simulation and on the real robot. In simulation, LeTO achieves a success\\nrate comparable to state-of-the-art imitation learning methods, but the\\ngenerated trajectories are of less uncertainty, higher quality, and smoother.\\nIn real-world experiments, we deployed LeTO to handle constraints-critical\\ntasks. The results show the effectiveness of LeTO comparing with\\nstate-of-the-art imitation learning approaches. We release our code at\\nhttps://github.com/ZhengtongXu/LeTO. In the digital era, the prevalence of depressive symptoms expressed on social\\nmedia has raised serious concerns, necessitating advanced methodologies for\\ntimely detection. This paper addresses the challenge of interpretable\\ndepression detection by proposing a novel methodology that effectively combines\\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\\nconversational agents like ChatGPT. In our methodology, explanations are\\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\\nclassification and explanations via masked attention. The interpretability is\\nfurther enhanced using ChatGPT to transform technical explanations into\\nhuman-readable commentaries. By introducing an effective and modular approach\\nfor interpretable depression detection, our methodology can contribute to the\\ndevelopment of socially responsible digital platforms, fostering early\\nintervention and support for mental health challenges under the guidance of\\nqualified healthcare professionals. The Conditional Markov Chain Search (CMCS) is a framework for automated\\ndesign of metaheuristics for discrete combinatorial optimisation problems.\\nGiven a set of algorithmic components such as hill climbers and mutations, CMCS\\ndecides in which order to apply those components. The decisions are dictated by\\nthe CMCS configuration that can be learnt offline. CMCS does not have an\\nacceptance criterion; any moves are accepted by the framework. As a result, it\\nis particularly good in exploration but is not as good at exploitation. In this\\nstudy, we explore several extensions of the framework to improve its\\nexploitation abilities. To perform a computational study, we applied the\\nframework to the three-index assignment problem. The results of our experiments\\nshowed that a two-stage CMCS is indeed superior to a single-stage CMCS. Linear programming (LP) problems are pervasive in real-life applications.\\nHowever, despite their apparent simplicity, an untrained user may find it\\ndifficult to determine the linear model of their specific problem. We envisage\\nthe creation of a goal-oriented conversational agent that will engage in\\nconversation with the user to elicit all information required so that a\\nsubsequent agent can generate the linear model. In this paper, we present an\\napproach for the generation of sample dialogues that can be used to develop and\\ntrain such a conversational agent. Using prompt engineering, we develop two\\nagents that \\\"talk\\\" to each other, one acting as the conversational agent, and\\nthe other acting as the user. Using a set of text descriptions of linear\\nproblems from NL4Opt available to the user only, the agent and the user engage\\nin conversation until the agent has retrieved all key information from the\\noriginal problem description. We also propose an extrinsic evaluation of the\\ndialogues by assessing how well the summaries generated by the dialogues match\\nthe original problem descriptions. We conduct human and automatic evaluations,\\nincluding an evaluation approach that uses GPT-4 to mimic the human evaluation\\nmetrics. The evaluation results show an overall good quality of the dialogues,\\nthough research is still needed to improve the quality of the GPT-4 evaluation\\nmetrics. The resulting dialogues, including the human annotations of a subset,\\nare available to the research community. The conversational agent used for the\\ngeneration of the dialogues can be used as a baseline. Large language models (LLM) are perceived to offer promising potentials for\\nautomating security tasks, such as those found in security operation centers\\n(SOCs). As a first step towards evaluating this perceived potential, we\\ninvestigate the use of LLMs in software pentesting, where the main task is to\\nautomatically identify software security vulnerabilities in source code. We\\nhypothesize that an LLM-based AI agent can be improved over time for a specific\\nsecurity task as human operators interact with it. Such improvement can be\\nmade, as a first step, by engineering prompts fed to the LLM based on the\\nresponses produced, to include relevant contexts and structures so that the\\nmodel provides more accurate results. Such engineering efforts become\\nsustainable if the prompts that are engineered to produce better results on\\ncurrent tasks, also produce better results on future unknown tasks. To examine\\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\\n2,740 hand-crafted source code test cases containing various types of\\nvulnerabilities. We divide the test cases into training and testing data, where\\nwe engineer the prompts based on the training data (only), and evaluate the\\nfinal system on the testing data. We compare the AI agent's performance on the\\ntesting data against the performance of the agent without the prompt\\nengineering. We also compare the AI agent's results against those from\\nSonarQube, a widely used static code analyzer for security testing. We built\\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\\nboth chat completion and assistant APIs). The results show that using LLMs is a\\nviable approach to build an AI agent for software pentesting that can improve\\nthrough repeated use and prompt engineering. When auditing a redistricting plan, a persuasive method is to compare the\\nplan with an ensemble of neutrally drawn redistricting plans. Ensembles are\\ngenerated via algorithms that sample distributions on balanced graph\\npartitions. To audit the partisan difference between the ensemble and a given\\nplan, one must ensure that the non-partisan criteria are matched so that we may\\nconclude that partisan differences come from bias rather than, for example,\\nlevels of compactness or differences in community preservation. Certain\\nsampling algorithms allow one to explicitly state the policy-based probability\\ndistribution on plans, however, these algorithms have shown poor mixing times\\nfor large graphs (i.e. redistricting spaces) for all but a few specialized\\nmeasures. In this work, we generate a multiscale parallel tempering approach\\nthat makes local moves at each scale. The local moves allow us to adopt a wide\\nvariety of policy-based measures. We examine our method in the state of\\nConnecticut and succeed at achieving fast mixing on a policy-based distribution\\nthat has never before been sampled at this scale. Our algorithm shows promise\\nto expand to a significantly wider class of measures that will (i) allow for\\nmore principled and situation-based comparisons and (ii) probe for the typical\\npartisan impact that policy can have on redistricting. We argue that there is a strong connection between ensemble learning and a\\ndelegative voting paradigm -- liquid democracy -- that can be leveraged to\\nreduce ensemble training costs. We present an incremental training procedure\\nthat identifies and removes redundant classifiers from an ensemble via\\ndelegation mechanisms inspired by liquid democracy. Through both analysis and\\nextensive experiments we show that this process greatly reduces the\\ncomputational cost of training compared to training a full ensemble. By\\ncarefully selecting the underlying delegation mechanism, weight centralization\\nin the classifier population is avoided, leading to higher accuracy than some\\nboosting methods. Furthermore, this work serves as an exemplar of how\\nframeworks from computational social choice literature can be applied to\\nproblems in nontraditional domains. Explainable AI has brought transparency into complex ML blackboxes, enabling,\\nin particular, to identify which features these models use for their\\npredictions. So far, the question of explaining predictive uncertainty, i.e.\\nwhy a model 'doubts', has been scarcely studied. Our investigation reveals that\\npredictive uncertainty is dominated by second-order effects, involving single\\nfeatures or product interactions between them. We contribute a new method for\\nexplaining predictive uncertainty based on these second-order effects.\\nComputationally, our method reduces to a simple covariance computation over a\\ncollection of first-order explanations. Our method is generally applicable,\\nallowing for turning common attribution techniques (LRP, Gradient x Input,\\netc.) into powerful second-order uncertainty explainers, which we call CovLRP,\\nCovGI, etc. The accuracy of the explanations our method produces is\\ndemonstrated through systematic quantitative evaluations, and the overall\\nusefulness of our method is demonstrated via two practical showcases. Difficulty is one of the key drivers of player engagement and it is often one\\nof the aspects that designers tweak most to optimise the player experience;\\noperationalising it is, therefore, a crucial task for game development studios.\\nA common practice consists of creating metrics out of data collected by player\\ninteractions with the content; however, this allows for estimation only after\\nthe content is released and does not consider the characteristics of potential\\nfuture players.\\n  In this article, we present a number of potential solutions for the\\nestimation of difficulty under such conditions, and we showcase the results of\\na comparative study intended to understand which method and which types of data\\nperform better in different scenarios.\\n  The results reveal that models trained on a combination of cohort statistics\\nand simulated data produce the most accurate estimations of difficulty in all\\nscenarios. Furthermore, among these models, artificial neural networks show the\\nmost consistent results. Economic choice prediction is an essential challenging task, often\\nconstrained by the difficulties in acquiring human choice data. Indeed,\\nexperimental economics studies had focused mostly on simple choice settings.\\nThe AI community has recently contributed to that effort in two ways:\\nconsidering whether LLMs can substitute for humans in the above-mentioned\\nsimple choice prediction settings, and the study through ML lens of more\\nelaborated but still rigorous experimental economics settings, employing\\nincomplete information, repetitive play, and natural language communication,\\nnotably language-based persuasion games. This leaves us with a major\\ninspiration: can LLMs be used to fully simulate the economic environment and\\ngenerate data for efficient human choice prediction, substituting for the\\nelaborated economic lab studies? We pioneer the study of this subject,\\ndemonstrating its feasibility. In particular, we show that a model trained\\nsolely on LLM-generated data can effectively predict human behavior in a\\nlanguage-based persuasion game, and can even outperform models trained on\\nactual human data. Hackathons and software competitions, increasingly pivotal in the software\\nindustry, serve as vital catalysts for innovation and skill development for\\nboth organizations and students. These platforms enable companies to prototype\\nideas swiftly, while students gain enriched learning experiences, enhancing\\ntheir practical skills. Over the years, hackathons have transitioned from mere\\ncompetitive events to significant educational tools, fusing theoretical\\nknowledge with real-world problem-solving. The integration of hackathons into\\ncomputer science and software engineering curricula aims to align educational\\nproficiencies within a collaborative context, promoting peer connectivity and\\nenriched learning via industry-academia collaborations. However, the infusion\\nof advanced technologies, notably artificial intelligence (AI), and machine\\nlearning, into hackathons is revolutionizing their structure and outcomes. This\\nevolution brings forth both opportunities, like enhanced learning experiences,\\nand challenges, such as ethical concerns. This study delves into the impact of\\ngenerative AI, examining its influence on student's technological choices based\\non a case study on the University of Iowa 2023 event. The exploration provides\\ninsights into AI's role in hackathons, and its educational implications, and\\noffers a roadmap for the integration of such technologies in future events,\\nensuring innovation is balanced with ethical and educational considerations. We present a theoretical analysis of the performance of transformer with\\nsoftmax attention in in-context learning with linear regression tasks. While\\nthe existing literature predominantly focuses on the convergence of\\ntransformers with single-/multi-head attention, our research centers on\\ncomparing their performance. We conduct an exact theoretical analysis to\\ndemonstrate that multi-head attention with a substantial embedding dimension\\nperforms better than single-head attention. When the number of in-context\\nexamples D increases, the prediction loss using single-/multi-head attention is\\nin O(1/D), and the one for multi-head attention has a smaller multiplicative\\nconstant. In addition to the simplest data distribution setting, we consider\\nmore scenarios, e.g., noisy labels, local examples, correlated features, and\\nprior knowledge. We observe that, in general, multi-head attention is preferred\\nover single-head attention. Our results verify the effectiveness of the design\\nof multi-head attention in the transformer architecture. Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense\\nastrophysical environments such as core-collapse supernovae (CCSNe) and neutron\\nstar mergers (NSMs). In this study, we explore FFCs in a \\\\emph{multi-energy}\\nneutrino gas, revealing that when the FFC growth rate significantly exceeds\\nthat of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a\\ncommon survival probability dictated by the energy-integrated neutrino\\nspectrum. We then employ physics-informed neural networks (PINNs) to predict\\nthe asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These\\npredictions are based on the first two moments of neutrino angular\\ndistributions for each energy bin, typically available in state-of-the-art CCSN\\nand NSM simulations. Our PINNs achieve errors as low as $\\\\lesssim6\\\\%$ and\\n$\\\\lesssim 18\\\\%$ for predicting the number of neutrinos in the electron channel\\nand the relative absolute error in the neutrino moments, respectively. This work presents a seminal approach for synthesizing images from WiFi\\nChannel State Information (CSI) in through-wall scenarios. Leveraging the\\nstrengths of WiFi, such as cost-effectiveness, illumination invariance, and\\nwall-penetrating capabilities, our approach enables visual monitoring of indoor\\nenvironments beyond room boundaries and without the need for cameras. More\\ngenerally, it improves the interpretability of WiFi CSI by unlocking the option\\nto perform image-based downstream tasks, e.g., visual activity recognition. In\\norder to achieve this crossmodal translation from WiFi CSI to images, we rely\\non a multimodal Variational Autoencoder (VAE) adapted to our problem specifics.\\nWe extensively evaluate our proposed methodology through an ablation study on\\narchitecture configuration and a quantitative/qualitative assessment of\\nreconstructed images. Our results demonstrate the viability of our method and\\nhighlight its potential for practical applications. Extensive fine-tuning on Large Language Models does not always yield better\\nresults. Oftentimes, models tend to get better at imitating one form of data\\nwithout gaining greater reasoning ability and may even end up losing some\\nintelligence. Here I introduce EvoMerge, a systematic approach to large\\nlanguage model training and merging. Leveraging model merging for weight\\ncrossover and fine-tuning for weight mutation, EvoMerge establishes an\\nevolutionary process aimed at pushing models beyond the limits of conventional\\nfine-tuning. In continual learning, a learner has to keep learning from the data over its\\nwhole life time. A key issue is to decide what knowledge to keep and what\\nknowledge to let go. In a neural network, this can be implemented by using a\\nstep-size vector to scale how much gradient samples change network weights.\\nCommon algorithms, like RMSProp and Adam, use heuristics, specifically\\nnormalization, to adapt this step-size vector. In this paper, we show that\\nthose heuristics ignore the effect of their adaptation on the overall objective\\nfunction, for example by moving the step-size vector away from better step-size\\nvectors. On the other hand, stochastic meta-gradient descent algorithms, like\\nIDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to\\nthe overall objective function. On simple problems, we show that IDBD is able\\nto consistently improve step-size vectors, where RMSProp and Adam do not. We\\nexplain the differences between the two approaches and their respective\\nlimitations. We conclude by suggesting that combining both approaches could be\\na promising future direction to improve the performance of neural networks in\\ncontinual learning. Artificial Intelligence (AI) has witnessed remarkable growth, particularly\\nthrough the proliferation of Deep Neural Networks (DNNs). These powerful models\\ndrive technological advancements across various domains. However, to harness\\ntheir potential in real-world applications, specialized hardware accelerators\\nare essential. This demand has sparked a market for parameterizable AI hardware\\naccelerators offered by different vendors.\\n  Manufacturers of AI-integrated products face a critical challenge: selecting\\nan accelerator that aligns with their product's performance requirements. The\\ndecision involves choosing the right hardware and configuring a suitable set of\\nparameters. However, comparing different accelerator design alternatives\\nremains a complex task. Often, engineers rely on data sheets, spreadsheet\\ncalculations, or slow black-box simulators, which only offer a coarse\\nunderstanding of the performance characteristics.\\n  The Abstract Computer Architecture Description Language (ACADL) is a concise\\nformalization of computer architecture block diagrams, which helps to\\ncommunicate computer architecture on different abstraction levels and allows\\nfor inferring performance characteristics. In this paper, we demonstrate how to\\nuse the ACADL to model AI hardware accelerators, use their ACADL description to\\nmap DNNs onto them, and explain the timing simulation semantics to gather\\nperformance results. Deep learning-based and lately Transformer-based language models have been\\ndominating the studies of natural language processing in the last years. Thanks\\nto their accurate and fast fine-tuning characteristics, they have outperformed\\ntraditional machine learning-based approaches and achieved state-of-the-art\\nresults for many challenging natural language understanding (NLU) problems.\\nRecent studies showed that the Transformer-based models such as BERT, which is\\nBidirectional Encoder Representations from Transformers, have reached\\nimpressive achievements on many tasks. Moreover, thanks to their transfer\\nlearning capacity, these architectures allow us to transfer pre-built models\\nand fine-tune them to specific NLU tasks such as question answering. In this\\nstudy, we provide a Transformer-based model and a baseline benchmark for the\\nTurkish Language. We successfully fine-tuned a Turkish BERT model, namely\\nBERTurk that is trained with base settings, to many downstream tasks and\\nevaluated with a the Turkish Benchmark dataset. We showed that our studies\\nsignificantly outperformed other existing baseline approaches for Named-Entity\\nRecognition, Sentiment Analysis, Question Answering and Text Classification in\\nTurkish Language. We publicly released these four fine-tuned models and\\nresources in reproducibility and with the view of supporting other Turkish\\nresearchers and applications. Large language models (LLMs) are becoming increasingly important for machine\\nlearning applications. However, it can be challenging to align LLMs with our\\nintent, particularly when we want to generate content that is preferable over\\nothers or when we want the LLM to respond in a certain style or tone that is\\nhard to describe. To address this challenge, we propose an approach that uses\\ncontrastive examples to better describe our intent. This involves providing\\npositive examples that illustrate the true intent, along with negative examples\\nthat show what characteristics we want LLMs to avoid. The negative examples can\\nbe retrieved from labeled data, written by a human, or generated by the LLM\\nitself. Before generating an answer, we ask the model to analyze the examples\\nto teach itself what to avoid. This reasoning step provides the model with the\\nappropriate articulation of the user's need and guides it towards generting a\\nbetter answer. We tested our approach on both synthesized and real-world\\ndatasets, including StackExchange and Reddit, and found that it significantly\\nimproves performance compared to standard few-shot prompting Are n-gram language models still relevant in this era of neural large\\nlanguage models (LLMs)? Our answer is yes, and we show their values in both\\ntext analysis and improving neural LLMs. Yet this necessitates modernizing\\nn-gram models in two aspects. First, we train them at the same data scale as\\nneural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever\\nbuilt. Second, existing n-gram models use small n which hinders their\\nperformance; we instead allow n to be arbitrarily large, by introducing a new\\n$\\\\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables\\n(which would be very expensive), we develop an engine named infini-gram --\\npowered by suffix arrays -- that can compute $\\\\infty$-gram (as well as n-gram\\nwith arbitrary n) probabilities with millisecond-level latency. The\\n$\\\\infty$-gram framework and infini-gram engine enable us to conduct many novel\\nand interesting analyses of human-written and machine-generated text: we find\\nthat the $\\\\infty$-gram LM has fairly high accuracy for next-token prediction\\n(47%), and can complement neural LLMs to greatly reduce their language modeling\\nperplexities. When analyzing machine-generated text, we also observe\\nirregularities in the machine--$\\\\infty$-gram agreement level with respect to\\nthe suffix length, which indicates deficiencies in neural LLM pretraining and\\nthe positional embeddings of Transformers. We open-source our infini-gram\\nengine in the hopes of enabling more study on how to best use verbatim\\ninformation retrieved from large text corpora. Speech acts are a speakers actions when performing an utterance within a\\nconversation, such as asking, recommending, greeting, or thanking someone,\\nexpressing a thought, or making a suggestion. Understanding speech acts helps\\ninterpret the intended meaning and actions behind a speakers or writers words.\\nThis paper proposes a Twitter dialectal Arabic speech act classification\\napproach based on a transformer deep learning neural network. Twitter and\\nsocial media, are becoming more and more integrated into daily life. As a\\nresult, they have evolved into a vital source of information that represents\\nthe views and attitudes of their users. We proposed a BERT based weighted\\nensemble learning approach to integrate the advantages of various BERT models\\nin dialectal Arabic speech acts classification. We compared the proposed model\\nagainst several variants of Arabic BERT models and sequence-based models. We\\ndeveloped a dialectal Arabic tweet act dataset by annotating a subset of a\\nlarge existing Arabic sentiment analysis dataset (ASAD) based on six speech act\\ncategories. We also evaluated the models on a previously developed Arabic Tweet\\nAct dataset (ArSAS). To overcome the class imbalance issue commonly observed in\\nspeech act problems, a transformer-based data augmentation model was\\nimplemented to generate an equal proportion of speech act categories. The\\nresults show that the best BERT model is araBERTv2-Twitter models with a\\nmacro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The\\nperformance improved using a BERT-based ensemble method with a 0.74 and 0.85\\naveraged F1 score and accuracy on our dataset, respectively. This work introduces Weaver, our first family of large language models (LLMs)\\ndedicated to content creation. Weaver is pre-trained on a carefully selected\\ncorpus that focuses on improving the writing capabilities of large language\\nmodels. We then fine-tune Weaver for creative and professional writing purposes\\nand align it to the preference of professional writers using a suit of novel\\nmethods for instruction data synthesis and LLM alignment, making it able to\\nproduce more human-like texts and follow more diverse instructions for content\\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\\ndifferent applications and can be dynamically dispatched by a routing agent\\naccording to query complexity to balance response quality and computation cost.\\nEvaluation on a carefully curated benchmark for assessing the writing\\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\\nscenarios, demonstrating the advantage of training specialized LLMs for writing\\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\\n(RAG) and function calling (tool usage). We present various use cases of these\\nabilities for improving AI-assisted writing systems, including integration of\\nexternal knowledge bases, tools, or APIs, and providing personalized writing\\nassistance. Furthermore, we discuss and summarize a guideline and best\\npractices for pre-training and fine-tuning domain-specific LLMs. In the rapidly evolving field of speech generative models, there is a\\npressing need to ensure audio authenticity against the risks of voice cloning.\\nWe present AudioSeal, the first audio watermarking technique designed\\nspecifically for localized detection of AI-generated speech. AudioSeal employs\\na generator/detector architecture trained jointly with a localization loss to\\nenable localized watermark detection up to the sample level, and a novel\\nperceptual loss inspired by auditory masking, that enables AudioSeal to achieve\\nbetter imperceptibility. AudioSeal achieves state-of-the-art performance in\\nterms of robustness to real life audio manipulations and imperceptibility based\\non automatic and human evaluation metrics. Additionally, AudioSeal is designed\\nwith a fast, single-pass detector, that significantly surpasses existing models\\nin speed - achieving detection up to two orders of magnitude faster, making it\\nideal for large-scale and real-time applications. Despite advances in AI alignment, language models (LM) remain vulnerable to\\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\\nto induce harmful behavior. While some defenses have been proposed, they focus\\non narrow threat models and fall short of a strong defense, which we posit\\nshould be effective, universal, and practical. To achieve this, we propose the\\nfirst adversarial objective for defending LMs against jailbreaking attacks and\\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\\noptimization to enforce harmless outputs. This results in an easily accessible\\nsuffix that significantly improves robustness to both jailbreaks seen during\\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\\nthat RPO has a minor effect on normal LM use, is successful under adaptive\\nattacks, and can transfer to black-box models, reducing the success rate of the\\nstrongest attack on GPT-4 from 92% to 6%. Reducing hallucination of Large Language Models (LLMs) is imperative for use\\nin the sciences where reproducibility is crucial. However, LLMs inherently lack\\nlong-term memory, making it a nontrivial, ad hoc, and inevitably biased task to\\nfine-tune them on domain-specific literature and data. Here we introduce LLaMP,\\na multimodal retrieval-augmented generation (RAG) framework of multiple\\ndata-aware reasoning-and-acting (ReAct) agents that dynamically interact with\\ncomputational and experimental data on Materials Project (MP). Without\\nfine-tuning, LLaMP demonstrates an ability to comprehend and integrate various\\nmodalities of materials science concepts, fetch relevant data stores on the\\nfly, process higher-order data (such as crystal structures and elastic\\ntensors), and summarize multi-step procedures for solid-state synthesis. We\\nshow that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge,\\nreducing a 5.21% MAPE on frequently-documented bandgaps and a significant\\n1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from\\nmixed data sources. Additionally, LLaMP substantially reduces the hallucinated\\nvolumetric strain in a diamond cubic silicon structure from 66.3% to 0. The\\nproposed framework offers an intuitive and nearly hallucination-free approach\\nto exploring materials informatics and establishes a pathway for knowledge\\ndistillation and fine-tuning other language models. We envision the framework\\nas a valuable component for scientific hypotheses and a foundation for future\\nautonomous laboratories where multiple LLM agents communicate and cooperate\\nwith robotics to drive material synthesis and chemical reactions without\\nhard-coded human logic and intervention. Despite the remarkable strides made in artificial intelligence, current\\nobject recognition models still lag behind in emulating the mechanism of visual\\ninformation processing in human brains. Recent studies have highlighted the\\npotential of using neural data to mimic brain processing; however, these often\\nreply on invasive neural recordings from non-human subjects, leaving a critical\\ngap in our understanding of human visual perception and the development of more\\nhuman brain-like vision models. Addressing this gap, we present, for the first\\ntime, \\\"Re(presentational)Al(ignment)net\\\", a vision model aligned with human\\nbrain activity based on non-invasive EEG recordings, demonstrating a\\nsignificantly higher similarity to human brain representations. Our innovative\\nimage-to-brain multi-layer encoding alignment framework not only optimizes\\nmultiple layers of the model, marking a substantial leap in neural alignment,\\nbut also enables the model to efficiently learn and mimic human brain's visual\\nrepresentational patterns across object categories and different neural data\\nmodalities. Furthermore, we discover that alignment with human brain\\nrepresentations improves the model's adversarial robustness. Our findings\\nsuggest that ReAlnet sets a new precedent in the field, bridging the gap\\nbetween artificial and human vision, and paving the way for more brain-like\\nartificial intelligence systems. This paper introduces ESPnet-SPK, a toolkit designed with several objectives\\nfor training speaker embedding extractors. First, we provide an open-source\\nplatform for researchers in the speaker recognition community to effortlessly\\nbuild models. We provide several models, ranging from x-vector to recent\\nSKA-TDNN. Through the modularized architecture design, variants can be\\ndeveloped easily. We also aspire to bridge developed models with other domains,\\nfacilitating the broad research community to effortlessly incorporate\\nstate-of-the-art embedding extractors. Pre-trained embedding extractors can be\\naccessed in an off-the-shelf manner and we demonstrate the toolkit's\\nversatility by showcasing its integration with two tasks. Another goal is to\\nintegrate with diverse self-supervised learning features. We release a\\nreproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O\\nevaluation protocol using WavLM-Large with ECAPA-TDNN. Current large vision-language models (VLMs) often encounter challenges such\\nas insufficient capabilities of a single visual component and excessively long\\nvisual tokens. These issues can limit the model's effectiveness in accurately\\ninterpreting complex visual information and over-lengthy contextual\\ninformation. Addressing these challenges is crucial for enhancing the\\nperformance and applicability of VLMs. This paper proposes the use of ensemble\\nexperts technique to synergizes the capabilities of individual visual encoders,\\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\\nThis technique introduces a fusion network to unify the processing of outputs\\nfrom different visual experts, while bridging the gap between image encoders\\nand pre-trained LLMs. In addition, we explore different positional encoding\\nschemes to alleviate the waste of positional encoding caused by lengthy image\\nfeature sequences, effectively addressing the issue of position overflow and\\nlength limitations. For instance, in our implementation, this technique\\nsignificantly reduces the positional occupancy in models like SAM, from a\\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\\nExperimental results demonstrate that VLMs with multiple experts exhibit\\nconsistently superior performance over isolated visual encoders and mark a\\nsignificant performance boost as more experts are integrated. We have\\nopen-sourced the training code used in this report. All of these resources can\\nbe found on our project website. We introduce the BL model and the Perspective Matrix to optimize supplier\\nselection and order allocation, focusing on both temporal and spatial dynamics.\\nOur development of a Supplier Relationship Network, using a Spatio-Temporal\\nGraph Neural Network, enhances the understanding of complex supplier\\ninterdependencies. Additionally, we address credibility issues in zero-order\\nscenarios with a Masked Ranking Mechanism, improving supplier ranking\\nefficiency. Our model demonstrates superior results on two datasets compared to\\nthe traditional models. Our evaluations using real-world datasets highlight\\nDBLM's superiority in providing accurate predictions and precise confidence\\nintervals, particularly in high-resolution scenarios. Clinical trials are typically run in order to understand the effects of a new\\ntreatment on a given population of patients. However, patients in large\\npopulations rarely respond the same way to the same treatment. This\\nheterogeneity in patient responses necessitates trials that investigate effects\\non multiple subpopulations - especially when a treatment has marginal or no\\nbenefit for the overall population but might have significant benefit for a\\nparticular subpopulation. Motivated by this need, we propose Syntax, an\\nexploratory trial design that identifies subpopulations with positive treatment\\neffect among many subpopulations. Syntax is sample efficient as it (i) recruits\\nand allocates patients adaptively and (ii) estimates treatment effects by\\nforming synthetic controls for each subpopulation that combines control samples\\nfrom other subpopulations. We validate the performance of Syntax and provide\\ninsights into when it might have an advantage over conventional trial designs\\nthrough experiments. This paper presents a comprehensive comparative analysis of explainable\\nartificial intelligence (XAI) ensembling methods. Our research brings three\\nsignificant contributions. Firstly, we introduce a novel ensembling method,\\nNormEnsembleXAI, that leverages minimum, maximum, and average functions in\\nconjunction with normalization techniques to enhance interpretability.\\nSecondly, we offer insights into the strengths and weaknesses of XAI ensemble\\nmethods. Lastly, we provide a library, facilitating the practical\\nimplementation of XAI ensembling, thus promoting the adoption of transparent\\nand interpretable deep learning models. Tailoring polar code construction for decoding algorithms beyond successive\\ncancellation has remained a topic of significant interest in the field.\\nHowever, despite the inherent nested structure of polar codes, the use of\\nsequence models in polar code construction is understudied. In this work, we\\npropose using a sequence modeling framework to iteratively construct a polar\\ncode for any given length and rate under various channel conditions.\\nSimulations show that polar codes designed via sequential modeling using\\ntransformers outperform both 5G-NR sequence and Density Evolution based\\napproaches for both AWGN and Rayleigh fading channels. While vision-language pre-trained models (VL-PTMs) have advanced multimodal\\nresearch in recent years, their mastery in a few languages like English\\nrestricts their applicability in broader communities. To this end, there is an\\nincreasing interest in developing multilingual VL models via a joint-learning\\nsetup, which, however, could be unrealistic due to expensive costs and data\\navailability. In this work, we propose to extend VL-PTMs' language capacity by\\ncontinual language learning (CLL), where a model needs to update its linguistic\\nknowledge incrementally without suffering from catastrophic forgetting (CF). We\\nbegin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP,\\na prevailing VL-PTM that has acquired image-English text alignment.\\nSpecifically, CLL-CLIP contains an expandable token embedding layer to handle\\nlinguistic differences. It solely trains token embeddings to improve memory\\nstability and is optimized under cross-modal and cross-lingual objectives to\\nlearn the alignment between images and multilingual texts. To alleviate CF\\nraised by covariate shift and lexical overlap, we further propose a novel\\napproach that ensures the identical distribution of all token embeddings during\\ninitialization and regularizes token embedding learning during training. We\\nconstruct a CLL benchmark covering 36 languages based on MSCOCO and XM3600\\ndatasets and then evaluate multilingual image-text retrieval performance.\\nExtensive experiments verify the effectiveness of CLL-CLIP and show that our\\napproach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on\\nXM3600, and improve various state-of-the-art methods consistently. Our code and\\ndata are available at \\\\url{https://github.com/yangbang18/CLFM}. GNNs are widely used to solve various tasks including node classification and\\nlink prediction. Most of the GNN architectures assume the initial embedding to\\nbe random or generated from popular distributions. These initial embeddings\\nrequire multiple layers of transformation to converge into a meaningful latent\\nrepresentation. While number of layers allow accumulation of larger\\nneighbourhood of a node it also introduce the problem of over-smoothing. In\\naddition, GNNs are inept at representing structural information. For example,\\nthe output embedding of a node does not capture its triangles participation. In\\nthis paper, we presented a novel feature extraction methodology GraphViz2Vec\\nthat can capture the structural information of a node's local neighbourhood to\\ncreate meaningful initial embeddings for a GNN model. These initial embeddings\\nhelps existing models achieve state-of-the-art results in various\\nclassification tasks. Further, these initial embeddings help the model to\\nproduce desired results with only two layers which in turn reduce the problem\\nof over-smoothing. The initial encoding of a node is obtained from an image\\nclassification model trained on multiple energy diagrams of its local\\nneighbourhood. These energy diagrams are generated with the induced sub-graph\\nof the nodes traversed by multiple random walks. The generated encodings\\nincrease the performance of existing models on classification tasks (with a\\nmean increase of $4.65\\\\%$ and $2.58\\\\%$ for the node and link classification\\ntasks, respectively), with some models achieving state-of-the-art results. Although reinforcement learning (RL) can solve many challenging sequential\\ndecision making problems, achieving zero-shot transfer across related tasks\\nremains a challenge. The difficulty lies in finding a good representation for\\nthe current task so that the agent understands how it relates to previously\\nseen tasks. To achieve zero-shot transfer, we introduce the function encoder, a\\nrepresentation learning algorithm which represents a function as a weighted\\ncombination of learned, non-linear basis functions. By using a function encoder\\nto represent the reward function or the transition function, the agent has\\ninformation on how the current task relates to previously seen tasks via a\\ncoherent vector representation. Thus, the agent is able to achieve transfer\\nbetween related tasks at run time with no additional training. We demonstrate\\nstate-of-the-art data efficiency, asymptotic performance, and training\\nstability in three RL fields by augmenting basic RL algorithms with a function\\nencoder task representation. The reasoning abilities of large language models (LLMs) are the topic of a\\ngrowing body of research in artificial intelligence and cognitive science. In\\nthis paper, we probe the extent to which a dozen LLMs are able to distinguish\\nlogically correct inferences from logically fallacious ones. We focus on\\ninference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob\\nhas a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must\\nhave a king'). These inference patterns have been of special interest to\\nlogicians, philosophers, and linguists, since they plausibly play a central\\nrole in human reasoning. Assessing LLMs on these inference patterns is thus\\nhighly relevant to the question of how much the reasoning abilities of LLMs\\nmatch those of humans. Among the LLMs we tested, all but GPT-4 often make basic\\nmistakes with conditionals. Moreover, even GPT-4 displays logically\\ninconsistent judgments across inference patterns involving epistemic modals. The rise of Large Language Models (LLMs) has revolutionized our comprehension\\nof intelligence bringing us closer to Artificial Intelligence. Since their\\nintroduction, researchers have actively explored the applications of LLMs\\nacross diverse fields, significantly elevating capabilities. Cybersecurity,\\ntraditionally resistant to data-driven solutions and slow to embrace machine\\nlearning, stands out as a domain. This study examines the existing literature,\\nproviding a thorough characterization of both defensive and adversarial\\napplications of LLMs within the realm of cybersecurity. Our review not only\\nsurveys and categorizes the current landscape but also identifies critical\\nresearch gaps. By evaluating both offensive and defensive applications, we aim\\nto provide a holistic understanding of the potential risks and opportunities\\nassociated with LLM-driven cybersecurity. Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling\\nusers to tailor them for their unique set of instances, thus dramatically\\nenhancing solver performance for their use case. However, this approach of\\nstrategy customization presents a significant challenge: handcrafting an\\noptimized strategy for a class of SMT instances remains a complex and demanding\\ntask for both solver developers and users alike.\\n  In this paper, we address this problem of automatic SMT strategy synthesis\\nvia a novel Monte Carlo Tree Search (MCTS) based method. Our method treats\\nstrategy synthesis as a sequential decision-making process, whose search tree\\ncorresponds to the strategy space, and employs MCTS to navigate this vast\\nsearch space. The key innovations that enable our method to identify effective\\nstrategies, while keeping costs low, are the ideas of layered and staged MCTS\\nsearch. These novel approaches allow for a deeper and more efficient\\nexploration of the strategy space, enabling us to synthesize more effective\\nstrategies than the default ones in state-of-the-art (SOTA) SMT solvers. We\\nimplement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Through\\nextensive evaluations across 6 important SMT logics, Z3alpha demonstrates\\nsuperior performance compared to the SOTA synthesis tool FastSMT, the default\\nZ3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging\\nQF_BV benchmark set, Z3alpha solves 42.7% more instances than the default\\nstrategy in the Z3 SMT solver. Large language models (LLMs) have revolutionized the field of natural\\nlanguage processing, extending their strong capabilities into multi-modal\\ndomains. Thus, it is vital to define proper and diversified metrics for the\\nevaluation of LLMs.\\n  In this paper, we introduce matrix entropy, a novel metric rooted in\\ninformation theory and geometry principles to quantify the data compression\\nproficiency in LLMs. It reflects the model's ability to extract relevant\\ninformation and eliminate unnecessary elements, thereby providing insight into\\nthe language model's intrinsic capability. Specifically, we demonstrate its\\napplicability in both single-modal (language) and multi-modal settings. For\\nlanguage models, our findings reveal that the matrix entropy of representations\\nfollows a scaling law type reduction when the model scales up, serving as a\\ncomplement to the traditional loss scaling law. For the multi-modal setting, we\\nalso propose an evaluation method based on matrix entropy for assessing\\nalignment quality and we find that modern large multi-modal models exhibit\\ngreat alignment performance. Singing voice conversion (SVC) automates song covers by converting one\\nsinger's singing voice into another target singer's singing voice with the\\noriginal lyrics and melody. However, it raises serious concerns about copyright\\nand civil right infringements to multiple entities. This work proposes\\nSongBsAb, the first proactive approach to mitigate unauthorized SVC-based\\nillegal song covers. SongBsAb introduces human-imperceptible perturbations to\\nsinging voices before releasing them, so that when they are used, the\\ngeneration process of SVC will be interfered, resulting in unexpected singing\\nvoices. SongBsAb features a dual prevention effect by causing both (singer)\\nidentity disruption and lyric disruption, namely, the SVC-covered singing voice\\nneither imitates the target singer nor preserves the original lyrics. To\\nimprove the imperceptibility of perturbations, we refine a psychoacoustic\\nmodel-based loss with the backing track as an additional masker, a unique\\naccompanying element for singing voices compared to ordinary speech voices. To\\nenhance the transferability, we propose to utilize a frame-level interaction\\nreduction-based loss. We demonstrate the prevention effectiveness, utility, and\\nrobustness of SongBsAb on three SVC models and two datasets using both\\nobjective and human study-based subjective metrics. Our work fosters an\\nemerging research direction for mitigating illegal automated song covers. This paper leverages macroscopic models and multi-source spatiotemporal data\\ncollected from automatic traffic counters and probe vehicles to accurately\\nestimate traffic flow and travel time in links where these measurements are\\nunavailable. This problem is critical in transportation planning applications\\nwhere the sensor coverage is low and the planned interventions have\\nnetwork-wide impacts. The proposed model, named the Macroscopic Traffic\\nEstimator (MaTE), can perform network-wide estimations of traffic flow and\\ntravel time only using the set of observed measurements of these quantities.\\nBecause MaTE is grounded in macroscopic flow theory, all parameters and\\nvariables are interpretable. The estimated traffic flow satisfies fundamental\\nflow conservation constraints and exhibits an increasing monotonic relationship\\nwith the estimated travel time. Using logit-based stochastic traffic assignment\\nas the principle for routing flow behavior makes the model fully differentiable\\nwith respect to the model parameters. This property facilitates the application\\nof computational graphs to learn parameters from vast amounts of spatiotemporal\\ndata. We also integrate neural networks and polynomial kernel functions to\\ncapture link flow interactions and enrich the mapping of traffic flows into\\ntravel times. MaTE also adds a destination choice model and a trip generation\\nmodel that uses historical data on the number of trips generated by location.\\nExperiments on synthetic data show that the model can accurately estimate\\ntravel time and traffic flow in out-of-sample links. Results obtained using\\nreal-world multi-source data from a large-scale transportation network suggest\\nthat MaTE outperforms data-driven benchmarks, especially in travel time\\nestimation. The estimated parameters of MaTE are also informative about the\\nhourly change in travel demand and supply characteristics of the transportation\\nnetwork. State of health (SOH) is a crucial indicator for assessing the degradation\\nlevel of batteries that cannot be measured directly but requires estimation.\\nAccurate SOH estimation enhances detection, control, and feedback for Li-ion\\nbatteries, allowing for safe and efficient energy management and guiding the\\ndevelopment of new-generation batteries. Despite the significant progress in\\ndata-driven SOH estimation, the time and resource-consuming degradation\\nexperiments for generating lifelong training data pose a challenge in\\nestablishing one large model capable of handling diverse types of Li-ion\\nbatteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity.\\nHence, this paper utilizes the strong generalization capability of large\\nlanguage model (LLM) to proposes a novel framework for adaptable SOH estimation\\nacross diverse batteries. To match the real scenario where unlabeled data\\nsequentially arrives in use with distribution shifts, the proposed model is\\nmodified by a test-time training technique to ensure estimation accuracy even\\nat the battery's end of life. The validation results demonstrate that the\\nproposed framework achieves state-of-the-art accuracy on four widely recognized\\ndatasets collected from 62 batteries. Furthermore, we analyze the theoretical\\nchallenges of cross-battery estimation and provide a quantitative explanation\\nof the effectiveness of our method. We present BlockFusion, a diffusion-based model that generates 3D scenes as\\nunit blocks and seamlessly incorporates new blocks to extend the scene.\\nBlockFusion is trained using datasets of 3D blocks that are randomly cropped\\nfrom complete 3D scene meshes. Through per-block fitting, all training blocks\\nare converted into the hybrid neural fields: with a tri-plane containing the\\ngeometry features, followed by a Multi-layer Perceptron (MLP) for decoding the\\nsigned distance values. A variational auto-encoder is employed to compress the\\ntri-planes into the latent tri-plane space, on which the denoising diffusion\\nprocess is performed. Diffusion applied to the latent representations allows\\nfor high-quality and diverse 3D scene generation. To expand a scene during\\ngeneration, one needs only to append empty blocks to overlap with the current\\nscene and extrapolate existing latent tri-planes to populate new blocks. The\\nextrapolation is done by conditioning the generation process with the feature\\nsamples from the overlapping tri-planes during the denoising iterations. Latent\\ntri-plane extrapolation produces semantically and geometrically meaningful\\ntransitions that harmoniously blend with the existing scene. A 2D layout\\nconditioning mechanism is used to control the placement and arrangement of\\nscene elements. Experimental results indicate that BlockFusion is capable of\\ngenerating diverse, geometrically consistent and unbounded large 3D scenes with\\nunprecedented high-quality shapes in both indoor and outdoor scenarios. As computer vision continues to advance and finds widespread applications\\nacross various domains, the need for interpretability in deep learning models\\nbecomes paramount. Existing methods often resort to post-hoc techniques or\\nprototypes to explain the decision-making process, which can be indirect and\\nlack intrinsic illustration. In this research, we introduce ViTree, a novel\\napproach for fine-grained visual categorization that combines the popular\\nvision transformer as a feature extraction backbone with neural decision trees.\\nBy traversing the tree paths, ViTree effectively selects patches from\\ntransformer-processed features to highlight informative local regions, thereby\\nrefining representations in a step-wise manner. Unlike previous tree-based\\nmodels that rely on soft distributions or ensembles of paths, ViTree selects a\\nsingle tree path, offering a clearer and simpler decision-making process. This\\npatch and path selectivity enhances model interpretability of ViTree, enabling\\nbetter insights into the model's inner workings. Remarkably, extensive\\nexperimentation validates that this streamlined approach surpasses various\\nstrong competitors and achieves state-of-the-art performance while maintaining\\nexceptional interpretability which is proved by multi-perspective methods. Code\\ncan be found at https://github.com/SJTU-DeepVisionLab/ViTree. The emergence of tools based on artificial intelligence has also led to the\\nneed of producing explanations which are understandable by a human being. In\\nsome approaches, the system is not transparent (often referred to as a \\\"black\\nbox\\\"), making it difficult to generate appropriate explanations. In this work,\\nthough, we consider probabilistic logic programming, a combination of logic\\nprogramming (for knowledge representation) and probability (to model\\nuncertainty). In this setting, one can say that models are interpretable, which\\neases its understanding. However, given a particular query, the usual notion of\\n\\\"explanation\\\" is associated with a set of choices, one for each random variable\\nof the model. Unfortunately, this set does not have a causal structure and, in\\nfact, some of the choices are actually irrelevant to the considered query. In\\norder to overcome these shortcomings, we present an approach to explaining\\nexplanations which is based on the definition of a query-driven inference\\nmechanism for probabilistic logic programs. Multi-Agent Path Finding (MAPF) involves determining paths for multiple\\nagents to travel simultaneously through a shared area toward particular goal\\nlocations. This problem is computationally complex, especially when dealing\\nwith large numbers of agents, as is common in realistic applications like\\nautonomous vehicle coordination. Finding an optimal solution is often\\ncomputationally infeasible, making the use of approximate algorithms essential.\\nAdding to the complexity, agents might act in a self-interested and strategic\\nway, possibly misrepresenting their goals to the MAPF algorithm if it benefits\\nthem. Although the field of mechanism design offers tools to align incentives,\\nusing these tools without careful consideration can fail when only having\\naccess to approximately optimal outcomes. Since approximations are crucial for\\nscalable MAPF algorithms, this poses a significant challenge. In this work, we\\nintroduce the problem of scalable mechanism design for MAPF and propose three\\nstrategyproof mechanisms, two of which even use approximate MAPF algorithms. We\\ntest our mechanisms on realistic MAPF domains with problem sizes ranging from\\ndozens to hundreds of agents. Our findings indicate that they improve welfare\\nbeyond a simple baseline. Instructional videos provide detailed how-to guides for various tasks, with\\nviewers often posing questions regarding the content. Addressing these\\nquestions is vital for comprehending the content, yet receiving immediate\\nanswers is difficult. While numerous computational models have been developed\\nfor Video Question Answering (Video QA) tasks, they are primarily trained on\\nquestions generated based on video content, aiming to produce answers from\\nwithin the content. However, in real-world situations, users may pose questions\\nthat go beyond the video's informational boundaries, highlighting the necessity\\nto determine if a video can provide the answer. Discerning whether a question\\ncan be answered by video content is challenging due to the multi-modal nature\\nof videos, where visual and verbal information are intertwined. To bridge this\\ngap, we present the YTCommentQA dataset, which contains naturally-generated\\nquestions from YouTube, categorized by their answerability and required\\nmodality to answer -- visual, script, or both. Experiments with answerability\\nclassification tasks demonstrate the complexity of YTCommentQA and emphasize\\nthe need to comprehend the combined role of visual and script information in\\nvideo reasoning. The dataset is available at\\nhttps://github.com/lgresearch/YTCommentQA. Safety measures need to be systemically investigated to what extent they\\nevaluate the intended performance of Deep Neural Networks (DNNs) for critical\\napplications. Due to a lack of verification methods for high-dimensional DNNs,\\na trade-off is needed between accepted performance and handling of\\nout-of-distribution (OOD) samples.\\n  This work evaluates rejecting outputs from semantic segmentation DNNs by\\napplying a Mahalanobis distance (MD) based on the most probable\\nclass-conditional Gaussian distribution for the predicted class as an OOD\\nscore. The evaluation follows three DNNs trained on the Cityscapes dataset and\\ntested on four automotive datasets and finds that classification risk can\\ndrastically be reduced at the cost of pixel coverage, even when applied on\\nunseen datasets. The applicability of our findings will support legitimizing\\nsafety measures and motivate their usage when arguing for safe usage of DNNs in\\nautomotive perception. This paper presents the results of finetuning large language models (LLMs)\\nfor the task of detecting vulnerabilities in source code. We leverage\\nWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and\\nadapt it for vulnerability detection through further finetuning. To accelerate\\ntraining, we modify WizardCoder's training procedure, also we investigate\\noptimal training regimes. For the imbalanced dataset with many more negative\\nexamples than positive, we also explore different techniques to improve\\nclassification performance. The finetuned WizardCoder model achieves\\nimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerability\\ndatasets over CodeBERT-like model, demonstrating the effectiveness of adapting\\npretrained LLMs for vulnerability detection in source code. The key\\ncontributions are finetuning the state-of-the-art code LLM, WizardCoder,\\nincreasing its training speed without the performance harm, optimizing the\\ntraining procedure and regimes, handling class imbalance, and improving\\nperformance on difficult vulnerability detection datasets. This demonstrates\\nthe potential for transfer learning by finetuning large pretrained language\\nmodels for specialized source code analysis tasks. This study presents a new approach for estimating confidence in machine\\nlearning model predictions, specifically in regression tasks utilizing Earth\\nObservation (EO) data, with a particular focus on mosquito abundance (MA)\\nestimation. We take advantage of a Variational AutoEncoder architecture, to\\nderive a confidence metric by the latent space representations of EO datasets.\\nThis methodology is pivotal in establishing a correlation between the Euclidean\\ndistance in latent representations and the Absolute Error (AE) in individual MA\\npredictions. Our research focuses on EO datasets from the Veneto region in\\nItaly and the Upper Rhine Valley in Germany, targeting areas significantly\\naffected by mosquito populations. A key finding is a notable correlation of\\n0.46 between the AE of MA predictions and the proposed confidence metric. This\\ncorrelation signifies a robust, new metric for quantifying the reliability and\\nenhancing the trustworthiness of the AI model's predictions in the context of\\nboth EO data analysis and mosquito abundance studies. The growing popularity of Android requires malware detection systems that can\\nkeep up with the pace of new software being released. According to a recent\\nstudy, a new piece of malware appears online every 12 seconds. To address this,\\nwe treat Android malware detection as a streaming data problem and explore the\\nuse of active online learning as a means of mitigating the problem of labelling\\napplications in a timely and cost-effective manner. Our resulting framework\\nachieves accuracies of up to 96\\\\%, requires as little of 24\\\\% of the training\\ndata to be labelled, and compensates for concept drift that occurs between the\\nrelease and labelling of an application. We also consider the broader\\npracticalities of online learning within Android malware detection, and\\nsystematically explore the trade-offs between using different static, dynamic\\nand hybrid feature sets to classify malware. Causal discovery is the challenging task of inferring causal structure from\\ndata. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive\\nobservations alone are not enough to distinguish correlation from causation,\\nthere has been a recent push to incorporate interventions into machine learning\\nresearch. Reinforcement learning provides a convenient framework for such an\\nactive approach to learning. This paper presents CORE, a deep reinforcement\\nlearning-based approach for causal discovery and intervention planning. CORE\\nlearns to sequentially reconstruct causal graphs from data while learning to\\nperform informative interventions. Our results demonstrate that CORE\\ngeneralizes to unseen graphs and efficiently uncovers causal structures.\\nFurthermore, CORE scales to larger graphs with up to 10 variables and\\noutperforms existing approaches in structure estimation accuracy and sample\\nefficiency. All relevant code and supplementary material can be found at\\nhttps://github.com/sa-and/CORE Entity alignment, which is a prerequisite for creating a more comprehensive\\nKnowledge Graph (KG), involves pinpointing equivalent entities across disparate\\nKGs. Contemporary methods for entity alignment have predominantly utilized\\nknowledge embedding models to procure entity embeddings that encapsulate\\nvarious similarities-structural, relational, and attributive. These embeddings\\nare then integrated through attention-based information fusion mechanisms.\\nDespite this progress, effectively harnessing multifaceted information remains\\nchallenging due to inherent heterogeneity. Moreover, while Large Language\\nModels (LLMs) have exhibited exceptional performance across diverse downstream\\ntasks by implicitly capturing entity semantics, this implicit knowledge has yet\\nto be exploited for entity alignment. In this study, we propose a Large\\nLanguage Model-enhanced Entity Alignment framework (LLMEA), integrating\\nstructural knowledge from KGs with semantic knowledge from LLMs to enhance\\nentity alignment. Specifically, LLMEA identifies candidate alignments for a\\ngiven entity by considering both embedding similarities between entities across\\nKGs and edit distances to a virtual equivalent entity. It then engages an LLM\\niteratively, posing multiple multi-choice questions to draw upon the LLM's\\ninference capability. The final prediction of the equivalent entity is derived\\nfrom the LLM's output. Experiments conducted on three public datasets reveal\\nthat LLMEA surpasses leading baseline models. Additional ablation studies\\nunderscore the efficacy of our proposed framework. This paper presents a comprehensive study on using deep reinforcement\\nlearning (RL) to create dynamic locomotion controllers for bipedal robots.\\nGoing beyond focusing on a single locomotion skill, we develop a general\\ncontrol solution that can be used for a range of dynamic bipedal skills, from\\nperiodic walking and running to aperiodic jumping and standing. Our RL-based\\ncontroller incorporates a novel dual-history architecture, utilizing both a\\nlong-term and short-term input/output (I/O) history of the robot. This control\\narchitecture, when trained through the proposed end-to-end RL approach,\\nconsistently outperforms other methods across a diverse range of skills in both\\nsimulation and the real world.The study also delves into the adaptivity and\\nrobustness introduced by the proposed RL system in developing locomotion\\ncontrollers. We demonstrate that the proposed architecture can adapt to both\\ntime-invariant dynamics shifts and time-variant changes, such as contact\\nevents, by effectively using the robot's I/O history. Additionally, we identify\\ntask randomization as another key source of robustness, fostering better task\\ngeneralization and compliance to disturbances. The resulting control policies\\ncan be successfully deployed on Cassie, a torque-controlled human-sized bipedal\\nrobot. This work pushes the limits of agility for bipedal robots through\\nextensive real-world experiments. We demonstrate a diverse range of locomotion\\nskills, including: robust standing, versatile walking, fast running with a\\ndemonstration of a 400-meter dash, and a diverse set of jumping skills, such as\\nstanding long jumps and high jumps. The transformation model is an essential component of any deformable image\\nregistration approach. It provides a representation of physical deformations\\nbetween images, thereby defining the range and realism of registrations that\\ncan be found. Two types of transformation models have emerged as popular\\nchoices: B-spline models and mesh models. Although both models have been\\ninvestigated in detail, a direct comparison has not yet been made, since the\\nmodels are optimized using very different optimization methods in practice.\\nB-spline models are predominantly optimized using gradient-descent methods,\\nwhile mesh models are typically optimized using finite-element method solvers\\nor evolutionary algorithms. Multi-objective optimization methods, which aim to\\nfind a diverse set of high-quality trade-off registrations, are increasingly\\nacknowledged to be important in deformable image registration. Since these\\nmethods search for a diverse set of registrations, they can provide a more\\ncomplete picture of the capabilities of different transformation models, making\\nthem suitable for a comparison of models. In this work, we conduct the first\\ndirect comparison between B-spline and mesh transformation models, by\\noptimizing both models with the same state-of-the-art multi-objective\\noptimization method, the Multi-Objective Real-Valued Gene-pool Optimal Mixing\\nEvolutionary Algorithm (MO-RV-GOMEA). The combination with B-spline\\ntransformation models, moreover, is novel. We experimentally compare both\\nmodels on two different registration problems that are both based on pelvic CT\\nscans of cervical cancer patients, featuring large deformations. Our results,\\non three cervical cancer patients, indicate that the choice of transformation\\nmodel can have a profound impact on the diversity and quality of achieved\\nregistration outcomes. Modeling time series data remains a pervasive issue as the temporal dimension\\nis inherent to numerous domains. Despite significant strides in time series\\nforecasting, high noise-to-signal ratio, non-normality, non-stationarity, and\\nlack of data continue challenging practitioners. In response, we leverage a\\nsimple representation augmentation technique to overcome these challenges. Our\\naugmented representation acts as a statistical-space prior encoded at each time\\nstep. In response, we name our method Statistical-space Augmented\\nRepresentation (SSAR). The underlying high-dimensional data-generating process\\ninspires our representation augmentation. We rigorously examine the empirical\\ngeneralization performance on two data sets with two downstream temporal\\nlearning algorithms. Our approach significantly beats all five up-to-date\\nbaselines. Moreover, the highly modular nature of our approach can easily be\\napplied to various settings. Lastly, fully-fledged theoretical perspectives are\\navailable throughout the writing for a clear and rigorous understanding. Large Language Models (LLMs), exemplified by ChatGPT, have significantly\\nreshaped text generation, particularly in the realm of writing assistance.\\nWhile ethical considerations underscore the importance of transparently\\nacknowledging LLM use, especially in scientific communication, genuine\\nacknowledgment remains infrequent. A potential avenue to encourage accurate\\nacknowledging of LLM-assisted writing involves employing automated detectors.\\nOur evaluation of four cutting-edge LLM-generated text detectors reveals their\\nsuboptimal performance compared to a simple ad-hoc detector designed to\\nidentify abrupt writing style changes around the time of LLM proliferation. We\\ncontend that the development of specialized detectors exclusively dedicated to\\nLLM-assisted writing detection is necessary. Such detectors could play a\\ncrucial role in fostering more authentic recognition of LLM involvement in\\nscientific communication, addressing the current challenges in acknowledgment\\npractices. We developed an artificial intelligence approach to predict the transfer fee\\nof a football player. This model can help clubs make better decisions about\\nwhich players to buy and sell, which can lead to improved performance and\\nincreased club budgets. Having collected data on player performance, transfer\\nfees, and other factors that might affect a player's value, we then used this\\ndata to train a machine learning model that can accurately predict a player's\\nimpact on the game. We further passed the obtained results as one of the\\nfeatures to the predictor of transfer fees. The model can help clubs identify\\nplayers who are undervalued and who could be sold for a profit. It can also\\nhelp clubs avoid overpaying for players. We believe that our model can be a\\nvaluable tool for football clubs. It can help them make better decisions about\\nplayer recruitment and transfers. Training an effective Machine learning (ML) model is an iterative process\\nthat requires effort in multiple dimensions. Vertically, a single pipeline\\ntypically includes an initial ETL (Extract, Transform, Load) of raw datasets, a\\nmodel training stage, and an evaluation stage where the practitioners obtain\\nstatistics of the model performance. Horizontally, many such pipelines may be\\nrequired to find the best model within a search space of model configurations.\\nMany practitioners resort to maintaining logs manually and writing simple glue\\ncode to automate the workflow. However, carrying out this process on the cloud\\nis not a trivial task in terms of resource provisioning, data management, and\\nbookkeeping of job histories to make sure the results are reproducible. We\\npropose an end-to-end cloud-based machine learning platform, Accelerated Cloud\\nfor AI (ACAI), to help improve the productivity of ML practitioners. ACAI\\nachieves this goal by enabling cloud-based storage of indexed, labeled, and\\nsearchable data, as well as automatic resource provisioning, job scheduling,\\nand experiment tracking. Specifically, ACAI provides practitioners (1) a data\\nlake for storing versioned datasets and their corresponding metadata, and (2)\\nan execution engine for executing ML jobs on the cloud with automatic resource\\nprovisioning (auto-provision), logging and provenance tracking. To evaluate\\nACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten\\ndigit classification task, and we study the usability of our system using\\nexperiments and interviews. We show that our auto-provisioner produces a 1.7x\\nspeed-up and 39% cost reduction, and our system reduces experiment time for ML\\nscientists by 20% on typical ML use cases. Despite the utility of Large Language Models (LLMs) across a wide range of\\ntasks and scenarios, developing a method for reliably evaluating LLMs across\\nvaried contexts continues to be challenging. Modern evaluation approaches often\\nuse LLMs to assess responses generated by LLMs. However, the meta-evaluation\\nconducted to assess the effectiveness of these LLMs as evaluators is typically\\nconstrained by the coverage of existing benchmarks or requires extensive human\\nannotation. This underscores the urgency of methods for scalable\\nmeta-evaluation that can effectively, reliably, and efficiently evaluate the\\nperformance of LLMs as evaluators across diverse tasks and scenarios,\\nparticularly in potentially new, user-defined scenarios. To fill this gap, we\\npropose ScaleEval, an agent-debate-assisted meta-evaluation framework that\\nleverages the capabilities of multiple communicative LLM agents. This framework\\nsupports multi-round discussions to assist human annotators in discerning the\\nmost capable LLMs as evaluators, which significantly eases their workload in\\ncases that used to require large-scale annotations during meta-evaluation. We\\nrelease the code for our framework, which is publicly available at:\\n\\\\url{https://github.com/GAIR-NLP/scaleeval}. Graph neural networks (GNNs) have achieved remarkable performance on\\ngraph-structured data. However, GNNs may inherit prejudice from the training\\ndata and make discriminatory predictions based on sensitive attributes, such as\\ngender and race. Recently, there has been an increasing interest in ensuring\\nfairness on GNNs, but all of them are under the assumption that the training\\nand testing data are under the same distribution, i.e., training data and\\ntesting data are from the same graph. Will graph fairness performance decrease\\nunder distribution shifts? How does distribution shifts affect graph fairness\\nlearning? All these open questions are largely unexplored from a theoretical\\nperspective. To answer these questions, we first theoretically identify the\\nfactors that determine bias on a graph. Subsequently, we explore the factors\\ninfluencing fairness on testing graphs, with a noteworthy factor being the\\nrepresentation distances of certain groups between the training and testing\\ngraph. Motivated by our theoretical analysis, we propose our framework\\nFatraGNN. Specifically, to guarantee fairness performance on unknown testing\\ngraphs, we propose a graph generator to produce numerous graphs with\\nsignificant bias and under different distributions. Then we minimize the\\nrepresentation distances for each certain group between the training graph and\\ngenerated graphs. This empowers our model to achieve high classification and\\nfairness performance even on generated graphs with significant bias, thereby\\neffectively handling unknown testing graphs. Experiments on real-world and\\nsemi-synthetic datasets demonstrate the effectiveness of our model in terms of\\nboth accuracy and fairness. Imitation learning is often used in addition to reinforcement learning in\\nenvironments where reward design is difficult or where the reward is sparse,\\nbut it is difficult to be able to imitate well in unknown states from a small\\namount of expert data and sampling data. Supervised learning methods such as\\nBehavioral Cloning do not require sampling data, but usually suffer from\\ndistribution shift. The methods based on reinforcement learning, such as\\ninverse reinforcement learning and Generative Adversarial imitation learning\\n(GAIL), can learn from only a few expert data. However, they often need to\\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\\nproblems, and it was shown that it could learn efficiently by combining\\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\\nthis algorithm more robust to distribution shift, we propose more efficient and\\nrobust algorithm by adding to this method a reward function based on\\nadversarial inverse reinforcement learning that rewards the agent for\\nperforming actions in status similar to the demo. We call this algorithm\\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\\nenvironments. Deep Neural Network (DNN) models when implemented on executing devices as the\\ninference engines are susceptible to Fault Injection Attacks (FIAs) that\\nmanipulate model parameters to disrupt inference execution with disastrous\\nperformance. This work introduces Contrastive Learning (CL) of visual\\nrepresentations i.e., a self-supervised learning approach into the deep\\nlearning training and inference pipeline to implement DNN inference engines\\nwith self-resilience under FIAs. Our proposed CL based FIA Detection and\\nRecovery (CFDR) framework features (i) real-time detection with only a single\\nbatch of testing data and (ii) fast recovery effective even with only a small\\namount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on\\nmultiple types of FIAs, our CFDR shows promising detection and recovery\\neffectiveness. Large Language Models (LLMs) have become increasingly popular for their\\nadvanced text generation capabilities across various domains. However, like any\\nsoftware, they face security challenges, including the risk of 'jailbreak'\\nattacks that manipulate LLMs to produce prohibited content. A particularly\\nunderexplored area is the Multilingual Jailbreak attack, where malicious\\nquestions are translated into various languages to evade safety filters.\\nCurrently, there is a lack of comprehensive empirical studies addressing this\\nspecific threat.\\n  To address this research gap, we conducted an extensive empirical study on\\nMultilingual Jailbreak attacks. We developed a novel semantic-preserving\\nalgorithm to create a multilingual jailbreak dataset and conducted an\\nexhaustive evaluation on both widely-used open-source and commercial LLMs,\\nincluding GPT-4 and LLaMa. Additionally, we performed interpretability analysis\\nto uncover patterns in Multilingual Jailbreak attacks and implemented a\\nfine-tuning mitigation method. Our findings reveal that our mitigation strategy\\nsignificantly enhances model defense, reducing the attack success rate by\\n96.2%. This study provides valuable insights into understanding and mitigating\\nMultilingual Jailbreak attacks. Executing deep neural networks (DNNs) on edge artificial intelligence (AI)\\ndevices enables various autonomous mobile computing applications. However, the\\nmemory budget of edge AI devices restricts the number and complexity of DNNs\\nallowed in such applications. Existing solutions, such as model compression or\\ncloud offloading, reduce the memory footprint of DNN inference at the cost of\\ndecreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN\\ninto blocks and swap them in and out in order, such that large DNNs can execute\\nwithin a small memory budget. Nevertheless, naive swapping on edge AI devices\\ninduces significant delays due to the redundant memory operations in the DNN\\ndevelopment ecosystem for edge AI devices. To this end, we develop SwapNet, an\\nefficient DNN block swapping middleware for edge AI devices. We systematically\\neliminate the unnecessary memory operations during block swapping while\\nretaining compatible with the deep learning frameworks, GPU backends, and\\nhardware architectures of edge AI devices. We further showcase the utility of\\nSwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference\\ntasks in three applications demonstrate that SwapNet achieves almost the same\\nlatency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x\\nmemory beyond the available budget. The design of SwapNet also provides novel\\nand feasible insights for deploying large language models (LLMs) on edge AI\\ndevices in the future. Dynamical behaviors of complex interacting systems, including brain\\nactivities, financial price movements, and physical collective phenomena, are\\nassociated with underlying interactions between the system's components. The\\nissue of uncovering interaction relations in such systems using observable\\ndynamics is called relational inference. In this study, we propose a Diffusion\\nmodel for Relational Inference (DiffRI), inspired by a self-supervised method\\nfor probabilistic time series imputation. DiffRI learns to infer the\\nprobability of the presence of connections between components through\\nconditional diffusion modeling. Experiments on both simulated and quasi-real\\ndatasets show that DiffRI is highly competent compared with other\\nstate-of-the-art models in discovering ground truth interactions in an\\nunsupervised manner. Our code will be made public soon. Powered by the increasing predictive capabilities of machine learning\\nalgorithms, artificial intelligence (AI) systems have begun to be used to\\noverrule human mistakes in many settings. We provide the first field evidence\\nthis AI oversight carries psychological costs that can impact human\\ndecision-making. We investigate one of the highest visibility settings in which\\nAI oversight has occurred: the Hawk-Eye review of umpires in top tennis\\ntournaments. We find that umpires lowered their overall mistake rate after the\\nintroduction of Hawk-Eye review, in line with rational inattention given\\npsychological costs of being overruled by AI. We also find that umpires\\nincreased the rate at which they called balls in, which produced a shift from\\nmaking Type II errors (calling a ball out when in) to Type I errors (calling a\\nball in when out). We structurally estimate the psychological costs of being\\noverruled by AI using a model of rational inattentive umpires, and our results\\nsuggest that because of these costs, umpires cared twice as much about Type II\\nerrors under AI oversight. Algorithmic decisions in critical domains such as hiring, college admissions,\\nand lending are often based on rankings. Because of the impact these decisions\\nhave on individuals, organizations, and population groups, there is a need to\\nunderstand them: to know whether the decisions are abiding by the law, to help\\nindividuals improve their rankings, and to design better ranking procedures.\\n  In this paper, we present ShaRP (Shapley for Rankings and Preferences), a\\nframework that explains the contributions of features to different aspects of a\\nranked outcome, and is based on Shapley values. Using ShaRP, we show that even\\nwhen the scoring function used by an algorithmic ranker is known and linear,\\nthe weight of each feature does not correspond to its Shapley value\\ncontribution. The contributions instead depend on the feature distributions,\\nand on the subtle local interactions between the scoring features. ShaRP builds\\non the Quantitative Input Influence framework, and can compute the\\ncontributions of features for multiple Quantities of Interest, including score,\\nrank, pair-wise preference, and top-k. Because it relies on black-box access to\\nthe ranker, ShaRP can be used to explain both score-based and learned ranking\\nmodels. We show results of an extensive experimental validation of ShaRP using\\nreal and synthetic datasets, showcasing its usefulness for qualitative\\nanalysis. While generative AI is now widespread and useful in society, there are\\npotential risks of misuse, e.g., unconsciously influencing cognitive processes\\nor decision-making. Although this causes a security problem in the cognitive\\ndomain, there has been no research about neural and computational mechanisms\\ncounteracting the impact of malicious generative AI in humans. We propose\\nDecNefGAN, a novel framework that combines a generative adversarial system and\\na neural reinforcement model. More specifically, DecNefGAN bridges human and\\ngenerative AI in a closed-loop system, with the AI creating stimuli that induce\\nspecific mental states, thus exerting external control over neural activity.\\nThe objective of the human is the opposite, to compete and reach an orthogonal\\nmental state. This framework can contribute to elucidating how the human brain\\nresponds to and counteracts the potential influence of generative AI. Recent developments in transformer-based language models have allowed them to\\ncapture a wide variety of world knowledge that can be adapted to downstream\\ntasks with limited resources. However, what pieces of information are\\nunderstood in these models is unclear, and neuron-level contributions in\\nidentifying them are largely unknown. Conventional approaches in neuron\\nexplainability either depend on a finite set of pre-defined descriptors or\\nrequire manual annotations for training a secondary model that can then explain\\nthe neurons of the primary model. In this paper, we take BERT as an example and\\nwe try to remove these constraints and propose a novel and scalable framework\\nthat ties textual descriptions to neurons. We leverage the potential of\\ngenerative language models to discover human-interpretable descriptors present\\nin a dataset and use an unsupervised approach to explain neurons with these\\ndescriptors. Through various qualitative and quantitative analyses, we\\ndemonstrate the effectiveness of this framework in generating useful\\ndata-specific descriptors with little human involvement in identifying the\\nneurons that encode these descriptors. In particular, our experiment shows that\\nthe proposed approach achieves 75% precision@2, and 50% recall@2 Large Language Models (LLMs) have demonstrated extraordinary capabilities and\\ncontributed to multiple fields, such as generating and summarizing text,\\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\\npopular tool in computerized language processing tasks, with the capability to\\nanalyze complicated linguistic patterns and provide relevant and appropriate\\nresponses depending on the context. While offering significant advantages,\\nthese models are also vulnerable to security and privacy attacks, such as\\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\\nInformation (PII) leakage attacks. This survey provides a thorough review of\\nthe security and privacy challenges of LLMs for both training data and users,\\nalong with the application-based risks in various domains, such as\\ntransportation, education, and healthcare. We assess the extent of LLM\\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\\nand review the potential defense mechanisms. Additionally, the survey outlines\\nexisting research gaps in this domain and highlights future research\\ndirections. There has been a proliferation of artificial intelligence applications, where\\nmodel training is key to promising high-quality services for these\\napplications. However, the model training process is both time-intensive and\\nenergy-intensive, inevitably affecting the user's demand for application\\nefficiency. Layer freezing, an efficient model training technique, has been\\nproposed to improve training efficiency. Although existing layer freezing\\nmethods demonstrate the great potential to reduce model training costs, they\\nstill remain shortcomings such as lacking generalizability and compromised\\naccuracy. For instance, existing layer freezing methods either require the\\nfreeze configurations to be manually defined before training, which does not\\napply to different networks, or use heuristic freezing criteria that is hard to\\nguarantee decent accuracy in different scenarios. Therefore, there lacks a\\ngeneric and smart layer freezing method that can automatically perform\\n``in-situation'' layer freezing for different networks during training\\nprocesses. To this end, we propose a generic and efficient training framework\\n(SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer\\nfreezing, which can automatically select the appropriate layers to freeze\\nwithout compromising accuracy. Experimental results show that SmartFRZ\\neffectively reduces the amount of computation in training and achieves\\nsignificant training acceleration, and outperforms the state-of-the-art layer\\nfreezing approaches. This paper introduces the multivariate beta mixture model (MBMM), a new\\nprobabilistic model for soft clustering. MBMM adapts to diverse cluster shapes\\nbecause of the flexible probability density function of the multivariate beta\\ndistribution. We introduce the properties of MBMM, describe the parameter\\nlearning procedure, and present the experimental results, showing that MBMM\\nfits diverse cluster shapes on synthetic and real datasets. The code is\\nreleased anonymously at \\\\url{https://github.com/hhchen1105/mbmm/}. In the rapidly evolving field of scientific research, efficiently extracting\\nkey information from the burgeoning volume of scientific papers remains a\\nformidable challenge. This paper introduces an innovative framework designed to\\nautomate the extraction of vital data from scientific PDF documents, enabling\\nresearchers to discern future research trajectories more readily. AutoIE\\nuniquely integrates four novel components: (1) A multi-semantic feature\\nfusion-based approach for PDF document layout analysis; (2) Advanced functional\\nblock recognition in scientific texts; (3) A synergistic technique for\\nextracting and correlating information on molecular sieve synthesis; (4) An\\nonline learning paradigm tailored for molecular sieve literature. Our SBERT\\nmodel achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE\\ndatasets. In addition, a practical application of AutoIE in the petrochemical\\nmolecular sieve synthesis domain demonstrates its efficacy, evidenced by an\\nimpressive 78\\\\% accuracy rate. This research paves the way for enhanced data\\nmanagement and interpretation in molecular sieve synthesis. It is a valuable\\nasset for seasoned experts and newcomers in this specialized field. The rapid advancement of artificial intelligence technologies, particularly\\nin recent years, has led to the emergence of several large parameter artificial\\nintelligence weather forecast models. These models represent a significant\\nbreakthrough, overcoming the limitations of traditional numerical weather\\nprediction models and indicating a potential second revolution for weather\\nforecast. This study explores the evolution of these advanced artificial\\nintelligence forecast models, and based on the identified commonalities,\\nproposes the \\\"Three Large Rules\\\" for their development. We discuss the\\npotential of artificial intelligence in revolutionizing numerical weather\\nprediction, briefly outlining the underlying reasons for this potential.\\nAdditionally, we explore key areas for future development prospects for large\\nartificial intelligence weather forecast models, integrating the entire\\nnumerical prediction process. Through an example that combines a large\\nartificial intelligence model with ocean wave forecasting, we illustrate how\\nforecasters can adapt and leverage the advanced artificial intelligence model.\\nWhile acknowledging the high accuracy, computational efficiency, and ease of\\ndeployment of large artificial intelligence forecast models, we emphasize the\\nirreplaceable values of traditional numerical forecasts. We believe that the\\noptimal future of weather forecasting lies in achieving a seamless integration\\nof artificial intelligence and traditional numerical models. Such a synthesis\\nis anticipated to offer a more comprehensive and reliable approach for future\\nweather forecasting. Simulating sampling algorithms with people has proven a useful method for\\nefficiently probing and understanding their mental representations. We propose\\nthat the same methods can be used to study the representations of Large\\nLanguage Models (LLMs). While one can always directly prompt either humans or\\nLLMs to disclose their mental representations introspectively, we show that\\nincreased efficiency can be achieved by using LLMs as elements of a sampling\\nalgorithm. We explore the extent to which we recover human-like representations\\nwhen LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo\\n(MCMC). We found a significant increase in efficiency and performance using\\nadaptive sampling algorithms based on MCMC. We also highlight the potential of\\nour method to yield a more general method of conducting Bayesian inference\\n\\\\textit{with} LLMs. In continual RL, the environment of a reinforcement learning (RL) agent\\nundergoes change. A successful system should appropriately balance the\\nconflicting requirements of retaining agent performance on already learned\\ntasks, stability, whilst learning new tasks, plasticity. The first-in-first-out\\nbuffer is commonly used to enhance learning in such settings but requires\\nsignificant memory. We explore the application of an augmentation to this\\nbuffer which alleviates the memory constraints, and use it with a world model\\nmodel-based reinforcement learning algorithm, to evaluate its effectiveness in\\nfacilitating continual learning. We evaluate the effectiveness of our method in\\nProcgen and Atari RL benchmarks and show that the distribution matching\\naugmentation to the replay-buffer used in the context of latent world models\\ncan successfully prevent catastrophic forgetting with significantly reduced\\ncomputational overhead. Yet, we also find such a solution to not be entirely\\ninfallible, and other failure modes such as the opposite -- lacking plasticity\\nand being unable to learn a new task -- to be a potential limitation in\\ncontinual learning systems. Autoregressive Large Language Models (LLMs) trained for next-word prediction\\nhave demonstrated remarkable proficiency at producing coherent text. But are\\nthey equally adept at forming coherent probability judgments? We use\\nprobabilistic identities and repeated judgments to assess the coherence of\\nprobability judgments made by LLMs. Our results show that the judgments\\nproduced by these models are often incoherent, displaying human-like systematic\\ndeviations from the rules of probability theory. Moreover, when prompted to\\njudge the same event, the mean-variance relationship of probability judgments\\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\\npropose that these deviations from rationality can be explained by linking\\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\\nthe Bayesian Sampler model of human probability judgments. Fine-tuning large pre-trained language models (LLMs) on particular datasets\\nis a commonly employed strategy in Natural Language Processing (NLP)\\nclassification tasks. However, this approach usually results in a loss of\\nmodels generalizability. In this paper, we present a framework that allows for\\nmaintaining generalizability, and enhances the performance on the downstream\\ntask by utilizing task-specific context attribution. We show that a linear\\ntransformation of the text representation from any transformer model using the\\ntask-specific concept operator results in a projection onto the latent concept\\nspace, referred to as context attribution in this paper. The specific concept\\noperator is optimized during the supervised learning stage via novel loss\\nfunctions. The proposed framework demonstrates that context attribution of the\\ntext representation for each task objective can improve the capacity of the\\ndiscriminator function and thus achieve better performance for the\\nclassification task. Experimental results on three datasets, namely HateXplain,\\nIMDB reviews, and Social Media Attributions, illustrate that the proposed model\\nattains superior accuracy and generalizability. Specifically, for the\\nnon-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in\\naccuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,\\nfine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and\\nF1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT\\nfine-tuned on the IMDB dataset in conjunction with the proposed model improves\\nthe F1-score on the HateXplain dataset by 7%. For the Social Media Attributions\\ndataset of YouTube comments, we observe 5.2% increase in F1-metric. The\\nproposed framework is implemented with PyTorch and provided open-source on\\nGitHub. Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\\napproach for aligning large language models with human values. However, RLHF\\nrelies on a reward model that is trained with a limited amount of human\\npreference data, which could lead to inaccurate predictions. As a result, RLHF\\nmay produce outputs that are misaligned with human values. To mitigate this\\nissue, we contribute a reward ensemble method that allows the reward model to\\nmake more accurate predictions. As using an ensemble of large language\\nmodel-based reward models can be computationally and resource-expensive, we\\nexplore efficient ensemble methods including linear-layer ensemble and\\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\\nOptimization with our ensembled reward models, and verify that our ensemble\\nmethods help improve the alignment performance of RLHF outputs. The execution failure of cyber-physical systems (e.g., autonomous driving\\nsystems, unmanned aerial systems, and robotic systems) could result in the loss\\nof life, severe injuries, large-scale environmental damage, property\\ndestruction, and major economic loss. Hence, such systems usually require a\\nstrong justification that they will effectively support critical requirements\\n(e.g., safety, security, and reliability) for which they were designed. Thus,\\nit is often mandatory to develop compelling assurance cases to support that\\njustification and allow regulatory bodies to certify such systems. In such\\ncontexts, detecting assurance deficits, relying on patterns to improve the\\nstructure of assurance cases, improving existing assurance case notations, and\\n(semi-)automating the generation of assurance cases are key to develop\\ncompelling assurance cases and foster consumer acceptance. We therefore explore\\nchallenges related to such assurance enablers and outline some potential\\ndirections that could be explored to tackle them. In this paper, we present an exploration and assessment of employing a\\ncentralized deep Q-network (DQN) controller as a substitute for the prevalent\\nuse of PID controllers in the context of 6DOF swimming robots. Our primary\\nfocus centers on illustrating this transition with the specific case of\\nunderwater object tracking. DQN offers advantages such as data efficiency and\\noff-policy learning, while remaining simpler to implement than other\\nreinforcement learning methods. Given the absence of a dynamic model for our\\nrobot, we propose an RL agent to control this multi-input-multi-output (MIMO)\\nsystem, where a centralized controller may offer more robust control than\\ndistinct PIDs. Our approach involves initially using classical controllers for\\nsafe exploration, then gradually shifting to DQN to take full control of the\\nrobot.\\n  We divide the underwater tracking task into vision and control modules. We\\nuse established methods for vision-based tracking and introduce a centralized\\nDQN controller. By transmitting bounding box data from the vision module to the\\ncontrol module, we enable adaptation to various objects and effortless vision\\nsystem replacement. Furthermore, dealing with low-dimensional data facilitates\\ncost-effective online learning for the controller. Our experiments, conducted\\nwithin a Unity-based simulator, validate the effectiveness of a centralized RL\\nagent over separated PID controllers, showcasing the applicability of our\\nframework for training the underwater RL agent and improved performance\\ncompared to traditional control methods. The code for both real and simulation\\nimplementations is at https://github.com/FARAZLOTFI/underwater-object-tracking. This study explores linguistic differences between human and LLM-generated\\ndialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the\\nEmpathicDialogues dataset. The research employs Linguistic Inquiry and Word\\nCount (LIWC) analysis, comparing ChatGPT-generated conversations with human\\nconversations across 118 linguistic categories. Results show greater\\nvariability and authenticity in human dialogues, but ChatGPT excels in\\ncategories such as social processes, analytical style, cognition, attentional\\nfocus, and positive emotional tone, reinforcing recent findings of LLMs being\\n\\\"more human than human.\\\" However, no significant difference was found in\\npositive or negative affect between ChatGPT and human dialogues. Classifier\\nanalysis of dialogue embeddings indicates implicit coding of the valence of\\naffect despite no explicit mention of affect in the conversations. The research\\nalso contributes a novel, companion ChatGPT-generated dataset of conversations\\nbetween two independent chatbots, which were designed to replicate a corpus of\\nhuman conversations available for open access and used widely in AI research on\\nlanguage modeling. Our findings increase understanding of ChatGPT's linguistic\\ncapabilities and inform ongoing efforts to distinguish between human and\\nLLM-generated text, which is critical in detecting AI-generated fakes,\\nmisinformation, and disinformation. Job shop scheduling problems are one of the most important and challenging\\ncombinatorial optimization problems that have been tackled mainly by exact or\\napproximate solution approaches. However, finding an exact solution can be\\ninfeasible for real-world problems, and even with an approximate solution\\napproach, it can require a prohibitive amount of time to find a near-optimal\\nsolution, and the found solutions are not applicable to new problems in\\ngeneral. To address these challenges, we propose an attention-based\\nreinforcement learning method for the class of job shop scheduling problems by\\nintegrating policy gradient reinforcement learning with a modified transformer\\narchitecture. An important result is that our trained learners in the proposed\\nmethod can be reused to solve large-scale problems not used in training and\\ndemonstrate that our approach outperforms the results of recent studies and\\nwidely adopted heuristic rules. In radiology, Artificial Intelligence (AI) has significantly advanced report\\ngeneration, but automatic evaluation of these AI-produced reports remains\\nchallenging. Current metrics, such as Conventional Natural Language Generation\\n(NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic\\nintricacies of clinical contexts or overemphasize clinical details, undermining\\nreport clarity. To overcome these issues, our proposed method synergizes the\\nexpertise of professional radiologists with Large Language Models (LLMs), like\\nGPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain\\nof Thought (CoT) reasoning, our approach aligns LLM evaluations with\\nradiologist standards, enabling detailed comparisons between human and AI\\ngenerated reports. This is further enhanced by a Regression model that\\naggregates sentence evaluation scores. Experimental results show that our\\n\\\"Detailed GPT-4 (5-shot)\\\" model achieves a 0.48 score, outperforming the METEOR\\nmetric by 0.19, while our \\\"Regressed GPT-4\\\" model shows even greater alignment\\nwith expert evaluations, exceeding the best existing metric by a 0.35 margin.\\nMoreover, the robustness of our explanations has been validated through a\\nthorough iterative strategy. We plan to publicly release annotations from\\nradiology experts, setting a new standard for accuracy in future assessments.\\nThis underscores the potential of our approach in enhancing the quality\\nassessment of AI-driven medical reports. Large Language Models (LLMs) have demonstrated remarkable language\\nunderstanding and generation capabilities. However, training, deploying, and\\naccessing these models pose notable challenges, including resource-intensive\\ndemands, extended training durations, and scalability issues. To address these\\nissues, we introduce a concept of hierarchical, distributed LLM architecture\\nthat aims at enhancing the accessibility and deployability of LLMs across\\nheterogeneous computing platforms, including general-purpose computers (e.g.,\\nlaptops) and IoT-style devices (e.g., embedded systems). By introducing a\\n\\\"layered\\\" approach, the proposed architecture enables on-demand accessibility\\nto LLMs as a customizable service. This approach also ensures optimal\\ntrade-offs between the available computational resources and the user's\\napplication needs. We envision that the concept of hierarchical LLM will\\nempower extensive, crowd-sourced user bases to harness the capabilities of\\nLLMs, thereby fostering advancements in AI technology in general. Communication with the goal of accurately conveying meaning, rather than\\naccurately transmitting symbols, has become an area of growing interest. This\\nparadigm, termed semantic communication, typically leverages modern\\ndevelopments in artificial intelligence and machine learning to improve the\\nefficiency and robustness of communication systems. However, a standard model\\nfor capturing and quantifying the details of \\\"meaning\\\" is lacking, with many\\nleading approaches to semantic communication adopting a black-box framework\\nwith little understanding of what exactly the model is learning. One solution\\nis to utilize the conceptual spaces framework, which models meaning explicitly\\nin a geometric manner. Though prior work studying semantic communication with\\nconceptual spaces has shown promising results, these previous attempts involve\\nhand-crafting a conceptual space model, severely limiting the scalability and\\npracticality of the approach. In this work, we develop a framework for learning\\na domain of a conceptual space model using only the raw data with high-level\\nproperty labels. In experiments using the MNIST and CelebA datasets, we show\\nthat the domains learned using the framework maintain semantic similarity\\nrelations and possess interpretable dimensions. The number of Hindi speakers on social media has increased dramatically in\\nrecent years. Regret is a common emotional experience in our everyday life.\\nMany speakers on social media, share their regretful experiences and opinions\\nregularly. It might cause a re-evaluation of one's choices and a desire to make\\na different option if given the chance. As a result, knowing the source of\\nregret is critical for investigating its impact on behavior and\\ndecision-making. This study focuses on regret and how it is expressed,\\nspecifically in Hindi, on various social media platforms. In our study, we\\npresent a novel dataset from three different sources, where each sentence has\\nbeen manually classified into one of three classes \\\"Regret by action\\\", \\\"Regret\\nby inaction\\\", and \\\"No regret\\\". Next, we use this dataset to investigate the\\nlinguistic expressions of regret in Hindi text and also identify the textual\\ndomains that are most frequently associated with regret. Our findings indicate\\nthat individuals on social media platforms frequently express regret for both\\npast inactions and actions, particularly within the domain of interpersonal\\nrelationships. We use a pre-trained BERT model to generate word embeddings for\\nthe Hindi dataset and also compare deep learning models with conventional\\nmachine learning models in order to demonstrate accuracy. Our results show that\\nBERT embedding with CNN consistently surpassed other models. This described the\\neffectiveness of BERT for conveying the context and meaning of words in the\\nregret domain. Training large language models (LLMs) with a large and diverse instruction\\ndataset aligns the models to comprehend and follow human instructions. Recent\\nworks have shown that using a small set of high-quality instructions can\\noutperform using large yet more noisy ones. Because instructions are unlabeled\\nand their responses are natural text, traditional active learning schemes with\\nthe model's confidence cannot be directly applied to the selection of unlabeled\\ninstructions. In this work, we propose a novel method for instruction\\nselection, called SelectLLM, that leverages LLMs for the selection of\\nhigh-quality instructions. Our high-level idea is to use LLMs to estimate the\\nusefulness and impactfulness of each instruction without the corresponding\\nlabels (i.e., responses), via prompting. SelectLLM involves two steps: dividing\\nthe unlabelled instructions using a clustering algorithm (e.g., CoreSet) to\\nmultiple clusters, and then prompting LLMs to choose high-quality instructions\\nwithin each cluster. SelectLLM showed comparable or slightly better performance\\non the popular instruction benchmarks, compared to the recent state-of-the-art\\nselection methods. All code and data are publicly available\\n(https://github.com/minnesotanlp/select-llm). The intricate relationship between human decision-making and emotions,\\nparticularly guilt and regret, has significant implications on behavior and\\nwell-being. Yet, these emotions subtle distinctions and interplay are often\\noverlooked in computational models. This paper introduces a dataset tailored to\\ndissect the relationship between guilt and regret and their unique textual\\nmarkers, filling a notable gap in affective computing research. Our approach\\ntreats guilt and regret recognition as a binary classification task and employs\\nthree machine learning and six transformer-based deep learning techniques to\\nbenchmark the newly created dataset. The study further implements innovative\\nreasoning methods like chain-of-thought and tree-of-thought to assess the\\nmodels interpretive logic. The results indicate a clear performance edge for\\ntransformer-based models, achieving a 90.4% macro F1 score compared to the\\n85.3% scored by the best machine learning classifier, demonstrating their\\nsuperior capability in distinguishing complex emotional states. The forecasting of entity trajectories at future points in time is a critical\\ncapability gap in applications across both Commercial and Defense sectors.\\nTransformers, and specifically Generative Pre-trained Transformer (GPT)\\nnetworks have recently revolutionized several fields of Artificial\\nIntelligence, most notably Natural Language Processing (NLP) with the advent of\\nLarge Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we\\nintroduce TrackGPT, a GPT-based model for entity trajectory forecasting that\\nhas shown utility across both maritime and air domains, and we expect to\\nperform well in others. TrackGPT stands as a pioneering GPT model capable of\\nproducing accurate predictions across diverse entity time series datasets,\\ndemonstrating proficiency in generating both long-term forecasts with sustained\\naccuracy and short-term forecasts with high precision. We present benchmarks\\nagainst state-of-the-art deep learning techniques, showing that TrackGPT's\\nforecasting capability excels in terms of accuracy, reliability, and\\nmodularity. Importantly, TrackGPT achieves these results while remaining\\ndomain-agnostic and requiring minimal data features (only location and time)\\ncompared to models achieving similar performance. In conclusion, our findings\\nunderscore the immense potential of applying GPT architectures to the task of\\nentity trajectory forecasting, exemplified by the innovative TrackGPT model. This work undertakes studies to evaluate Interpretability Methods for\\nTime-Series Deep Learning. Sensitivity analysis assesses how input changes\\naffect the output, constituting a key component of interpretation. Among the\\npost-hoc interpretation methods such as back-propagation, perturbation, and\\napproximation, my work will investigate perturbation-based sensitivity Analysis\\nmethods on modern Transformer models to benchmark their performances.\\nSpecifically, my work answers three research questions: 1) Do different\\nsensitivity analysis (SA) methods yield comparable outputs and attribute\\nimportance rankings? 2) Using the same sensitivity analysis method, do\\ndifferent Deep Learning (DL) models impact the output of the sensitivity\\nanalysis? 3) How well do the results from sensitivity analysis methods align\\nwith the ground truth? This paper presents a modeling effort to explore the underlying physics of\\ntemperature evolution during additive friction stir deposition (AFSD) by a\\nhuman-AI teaming approach. AFSD is an emerging solid-state additive\\nmanufacturing technology that deposits materials without melting. However, both\\nprocess modeling and modeling of the AFSD tool are at an early stage. In this\\npaper, a human-AI teaming approach is proposed to combine models based on first\\nprinciples with AI. The resulting human-informed machine learning method,\\ndenoted as AFSD-Physics, can effectively learn the governing equations of\\ntemperature evolution at the tool and the build from in-process measurements.\\nExperiments are designed and conducted to collect in-process measurements for\\nthe deposition of aluminum 7075 with a total of 30 layers. The acquired\\ngoverning equations are physically interpretable models with low computational\\ncost and high accuracy. Model predictions show good agreement with the\\nmeasurements. Experimental validation with new process parameters demonstrates\\nthe model's generalizability and potential for use in tool temperature control\\nand process optimization. In this work, we leverage the intrinsic segmentation of language sequences\\nand design a new positional encoding method called Bilevel Positional Encoding\\n(BiPE). For each position, our BiPE blends an intra-segment encoding and an\\ninter-segment encoding. The intra-segment encoding identifies the locations\\nwithin a segment and helps the model capture the semantic information therein\\nvia absolute positional encoding. The inter-segment encoding specifies the\\nsegment index, models the relationships between segments, and aims to improve\\nextrapolation capabilities via relative positional encoding. Theoretical\\nanalysis shows this disentanglement of positional information makes learning\\nmore effective. The empirical results also show that our BiPE has superior\\nlength extrapolation capabilities across a wide range of tasks in diverse text\\nmodalities. Deep generative models (DGMs) have been widely developed for graph data.\\nHowever, much less investigation has been carried out on understanding the\\nlatent space of such pretrained graph DGMs. These understandings possess the\\npotential to provide constructive guidelines for crucial tasks, such as graph\\ncontrollable generation. Thus in this work, we are interested in studying this\\nproblem and propose GraphCG, a method for the unsupervised discovery of\\nsteerable factors in the latent space of pretrained graph DGMs. We first\\nexamine the representation space of three pretrained graph DGMs with six\\ndisentanglement metrics, and we observe that the pretrained representation\\nspace is entangled. Motivated by this observation, GraphCG learns the steerable\\nfactors via maximizing the mutual information between semantic-rich directions,\\nwhere the controlled graph moving along the same direction will share the same\\nsteerable factors. We quantitatively verify that GraphCG outperforms four\\ncompetitive baselines on two graph DGMs pretrained on two molecule datasets.\\nAdditionally, we qualitatively illustrate seven steerable factors learned by\\nGraphCG on five pretrained DGMs over five graph datasets, including two for\\nmolecules and three for point clouds. By classic results in social choice theory, any reasonable preferential\\nvoting method sometimes gives individuals an incentive to report an insincere\\npreference. The extent to which different voting methods are more or less\\nresistant to such strategic manipulation has become a key consideration for\\ncomparing voting methods. Here we measure resistance to manipulation by whether\\nneural networks of varying sizes can learn to profitably manipulate a given\\nvoting method in expectation, given different types of limited information\\nabout how other voters will vote. We trained nearly 40,000 neural networks of\\n26 sizes to manipulate against 8 different voting methods, under 6 types of\\nlimited information, in committee-sized elections with 5-21 voters and 3-6\\ncandidates. We find that some voting methods, such as Borda, are highly\\nmanipulable by networks with limited information, while others, such as Instant\\nRunoff, are not, despite being quite profitably manipulated by an ideal\\nmanipulator with full information. While large language models (LLMs) are increasingly being used for program\\nsynthesis, they lack the global view needed to develop useful abstractions;\\nthey generally predict programs one at a time, often repeating the same\\nfunctionality. Generating redundant code from scratch is both inefficient and\\nerror-prone. To address this, we propose Refactoring for Generalizable\\nAbstraction Learning (ReGAL), a gradient-free method for learning a library of\\nreusable functions via code refactorization, i.e. restructuring code without\\nchanging its execution output. ReGAL learns from a small set of existing\\nprograms, iteratively verifying and refining its abstractions via execution. We\\nfind that the shared function libraries discovered by ReGAL make programs\\neasier to predict across diverse domains. On three datasets (LOGO graphics\\ngeneration, Date reasoning, and TextCraft, a Minecraft-based text game), both\\nopen-source and proprietary LLMs improve in accuracy when predicting programs\\nwith ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy\\nincreases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on\\nTextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals\\nReGAL's abstractions encapsulate frequently-used subroutines as well as\\nenvironment dynamics. Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with\\ninstructions or human feedback) due to their sheer number of parameters. A\\nfamily of parameter-efficient sparse fine-tuning methods have proven promising\\nin terms of performance but their memory requirements increase proportionally\\nto the size of the LLMs. In this work, we scale sparse fine-tuning to\\nstate-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse\\nfine-tuning method which, for a desired density level, maintains an array of\\nparameter indices and the deltas of these parameters relative to their\\npretrained values. It iterates over: (a) updating the active deltas, (b)\\npruning indices (based on the change of magnitude of their deltas) and (c)\\nregrowth of indices. For regrowth, we explore two criteria based on either the\\naccumulated gradients of a few candidate parameters or their approximate\\nmomenta estimated using the efficient SM3 optimizer. We experiment with\\ninstruction-tuning of LLMs on standard dataset mixtures, finding that SpIEL is\\noften superior to popular parameter-efficient fine-tuning methods like LoRA\\n(low-rank adaptation) in terms of performance and comparable in terms of run\\ntime. We additionally show that SpIEL is compatible with both quantization and\\nefficient optimizers, to facilitate scaling to ever-larger model sizes. We\\nrelease the code for SpIEL at https://github.com/AlanAnsell/peft and for the\\ninstruction-tuning experiments at https://github.com/ducdauge/sft-llm. Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the\\nconcept of normality in visual data, widely applied across diverse domains,\\ne.g., industrial defect inspection, and medical lesion detection. This survey\\ncomprehensively examines recent advancements in VAD by identifying three\\nprimary challenges: 1) scarcity of training data, 2) diversity of visual\\nmodalities, and 3) complexity of hierarchical anomalies. Starting with a brief\\noverview of the VAD background and its generic concept definitions, we\\nprogressively categorize, emphasize, and discuss the latest VAD progress from\\nthe perspective of sample number, data modality, and anomaly hierarchy. Through\\nan in-depth analysis of the VAD field, we finally summarize future developments\\nfor VAD and conclude the key findings and contributions of this survey. Behavioral cloning uses a dataset of demonstrations to learn a policy. To\\novercome computationally expensive training procedures and address the policy\\nadaptation problem, we propose to use latent spaces of pre-trained foundation\\nmodels to index a demonstration dataset, instantly access similar relevant\\nexperiences, and copy behavior from these situations. Actions from a selected\\nsimilar situation can be performed by the agent until representations of the\\nagent's current situation and the selected experience diverge in the latent\\nspace. Thus, we formulate our control problem as a dynamic search problem over\\na dataset of experts' demonstrations. We test our approach on BASALT\\nMineRL-dataset in the latent representation of a Video Pre-Training model. We\\ncompare our model to state-of-the-art, Imitation Learning-based Minecraft\\nagents. Our approach can effectively recover meaningful demonstrations and show\\nhuman-like behavior of an agent in the Minecraft environment in a wide variety\\nof scenarios. Experimental results reveal that performance of our search-based\\napproach clearly wins in terms of accuracy and perceptual evaluation over\\nlearning-based models. We introduce TQCompressor, a novel method for neural network model\\ncompression with improved tensor decompositions. We explore the challenges\\nposed by the computational and storage demands of pre-trained language models\\nin NLP tasks and propose a permutation-based enhancement to Kronecker\\ndecomposition. This enhancement makes it possible to reduce loss in model\\nexpressivity which is usually associated with factorization. We demonstrate\\nthis method applied to the GPT-2$_{small}$. The result of the compression is\\nTQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in\\nthe GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further\\nenhance the performance of the TQCompressedGPT-2 through a training strategy\\ninvolving multi-step knowledge distillation, using only a 3.1% of the\\nOpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative\\nevaluations, marking an advancement in the efficient and effective deployment\\nof models in resource-constrained environments. The deep neural networks are known to be vulnerable to well-designed\\nadversarial attacks. The most successful defense technique based on adversarial\\ntraining (AT) can achieve optimal robustness against particular attacks but\\ncannot generalize well to unseen attacks. Another effective defense technique\\nbased on adversarial purification (AP) can enhance generalization but cannot\\nachieve optimal robustness. Meanwhile, both methods share one common limitation\\non the degraded standard accuracy. To mitigate these issues, we propose a novel\\nframework called Adversarial Training on Purification (AToP), which comprises\\ntwo components: perturbation destruction by random transforms (RT) and purifier\\nmodel fine-tuned (FT) by adversarial loss. RT is essential to avoid\\noverlearning to known attacks resulting in the robustness generalization to\\nunseen attacks and FT is essential for the improvement of robustness. To\\nevaluate our method in an efficient and scalable way, we conduct extensive\\nexperiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our\\nmethod achieves state-of-the-art results and exhibits generalization ability\\nagainst unseen attacks. Federated Learning (FL) is an emerging paradigm in machine learning without\\nexposing clients' raw data. In practical scenarios with numerous clients,\\nencouraging fair and efficient client participation in federated learning is of\\nutmost importance, which is also challenging given the heterogeneity in data\\ndistribution and device properties. Existing works have proposed different\\nclient-selection methods that consider fairness; however, they fail to select\\nclients with high utilities while simultaneously achieving fair accuracy\\nlevels. In this paper, we propose a fair client-selection approach that unlocks\\nthreefold fairness in federated learning. In addition to having a fair\\nclient-selection strategy, we enforce an equitable number of rounds for client\\nparticipation and ensure a fair accuracy distribution over the clients. The\\nexperimental results demonstrate that FedFair^3, in comparison to the\\nstate-of-the-art baselines, achieves 18.15% less accuracy variance on the IID\\ndata and 54.78% on the non-IID data, without decreasing the global accuracy.\\nFurthermore, it shows 24.36% less wall-clock training time on average. Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that\\naligns language models closely with human-centric values. The initial phase of\\nRLHF involves learning human values using a reward model from ranking data. It\\nis observed that the performance of the reward model degrades after one epoch\\nof training, and optimizing too much against the learned reward model\\neventually hinders the true objective. This paper delves into these issues,\\nleveraging the theoretical insights to design improved reward learning\\nalgorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during\\neach training epoch, we not only update the model with the data, but also\\nupdate the date using the model, replacing hard labels with soft labels. Our\\nempirical findings highlight the superior performance of this approach over the\\ntraditional methods. Language model alignment has become an important component of AI safety,\\nallowing safe interactions between humans and language models, by enhancing\\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\\nmodel or inserting preset aligning prompts. Recently, representation\\nengineering, a method which alters the model's behavior via changing its\\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\\net al., 2023a). Representation engineering yields gains in alignment oriented\\ntasks such as resistance to adversarial attacks and reduction of social biases,\\nbut was also shown to cause a decrease in the ability of the model to perform\\nbasic tasks. In this paper we study the tradeoff between the increase in\\nalignment and decrease in helpfulness of the model. We propose a theoretical\\nframework which provides bounds for these two quantities, and demonstrate their\\nrelevance empirically. Interestingly, we find that while the helpfulness\\ngenerally decreases, it does so quadratically with the norm of the\\nrepresentation engineering vector, while the alignment increases linearly with\\nit, indicating a regime in which it is efficient to use representation\\nengineering. We validate our findings empirically, and chart the boundaries to\\nthe usefulness of representation engineering for alignment. Faithfully summarizing the knowledge encoded by a deep neural network (DNN)\\ninto a few symbolic primitive patterns without losing much information\\nrepresents a core challenge in explainable AI. To this end, Ren et al. (2023c)\\nhave derived a series of theorems to prove that the inference score of a DNN\\ncan be explained as a small set of interactions between input variables.\\nHowever, the lack of generalization power makes it still hard to consider such\\ninteractions as faithful primitive patterns encoded by the DNN. Therefore,\\ngiven different DNNs trained for the same task, we develop a new method to\\nextract interactions that are shared by these DNNs. Experiments show that the\\nextracted interactions can better reflect common knowledge shared by different\\nDNNs. Security code review aims to combine automated tools and manual efforts to\\ndetect security defects during development. The rapid development of Large\\nLanguage Models (LLMs) has shown promising potential in software development,\\nas well as opening up new possibilities in automated security code review. To\\nexplore the challenges of applying LLMs in practical code review for security\\ndefect detection, this study compared the detection performance of three\\nstate-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on\\n549 code files that contain security defects from real-world code reviews.\\nThrough analyzing 82 responses generated by the best-performing LLM-prompt\\ncombination based on 100 randomly selected code files, we extracted and\\ncategorized quality problems present in these responses into 5 themes and 16\\ncategories. Our results indicate that the responses produced by LLMs often\\nsuffer from verbosity, vagueness, and incompleteness, highlighting the\\nnecessity to enhance their conciseness, understandability, and compliance to\\nsecurity defect detection. This work reveals the deficiencies of LLM-generated\\nresponses in security code review and paves the way for future optimization of\\nLLMs towards this task. Pretrained Graph Neural Networks have been widely adopted for various\\nmolecular property prediction tasks. Despite their ability to encode structural\\nand relational features of molecules, traditional fine-tuning of such\\npretrained GNNs on the target task can lead to poor generalization. To address\\nthis, we explore the adaptation of pretrained GNNs to the target task by\\njointly training them with multiple auxiliary tasks. This could enable the GNNs\\nto learn both general and task-specific features, which may benefit the target\\ntask. However, a major challenge is to determine the relatedness of auxiliary\\ntasks with the target task. To address this, we investigate multiple strategies\\nto measure the relevance of auxiliary tasks and integrate such tasks by\\nadaptively combining task gradients or by learning task weights via bi-level\\noptimization. Additionally, we propose a novel gradient surgery-based approach,\\nRotation of Conflicting Gradients ($\\\\mathtt{RCGrad}$), that learns to align\\nconflicting auxiliary task gradients through rotation. Our experiments with\\nstate-of-the-art pretrained GNNs demonstrate the efficacy of our proposed\\nmethods, with improvements of up to 7.7% over fine-tuning. This suggests that\\nincorporating auxiliary tasks along with target task fine-tuning can be an\\neffective way to improve the generalizability of pretrained GNNs for molecular\\nproperty prediction. Active learning (AL) has found wide applications in medical image\\nsegmentation, aiming to alleviate the annotation workload and enhance\\nperformance. Conventional uncertainty-based AL methods, such as entropy and\\nBayesian, often rely on an aggregate of all pixel-level metrics. However, in\\nimbalanced settings, these methods tend to neglect the significance of target\\nregions, eg., lesions, and tumors. Moreover, uncertainty-based selection\\nintroduces redundancy. These factors lead to unsatisfactory performance, and in\\nmany cases, even underperform random sampling. To solve this problem, we\\nintroduce a novel approach called the Selective Uncertainty-based AL, avoiding\\nthe conventional practice of summing up the metrics of all pixels. Through a\\nfiltering process, our strategy prioritizes pixels within target areas and\\nthose near decision boundaries. This resolves the aforementioned disregard for\\ntarget areas and redundancy. Our method showed substantial improvements across\\nfive different uncertainty-based methods and two distinct datasets, utilizing\\nfewer labeled data to reach the supervised baseline and consistently achieving\\nthe highest overall performance. Our code is available at\\nhttps://github.com/HelenMa9998/Selective\\\\_Uncertainty\\\\_AL. A new approach to the local and global explanation is proposed. It is based\\non selecting a convex hull constructed for the finite number of points around\\nan explained instance. The convex hull allows us to consider a dual\\nrepresentation of instances in the form of convex combinations of extreme\\npoints of a produced polytope. Instead of perturbing new instances in the\\nEuclidean feature space, vectors of convex combination coefficients are\\nuniformly generated from the unit simplex, and they form a new dual dataset. A\\ndual linear surrogate model is trained on the dual dataset. The explanation\\nfeature importance values are computed by means of simple matrix calculations.\\nThe approach can be regarded as a modification of the well-known model LIME.\\nThe dual representation inherently allows us to get the example-based\\nexplanation. The neural additive model is also considered as a tool for\\nimplementing the example-based explanation approach. Many numerical experiments\\nwith real datasets are performed for studying the approach. The code of\\nproposed algorithms is available. Knowledge base population seeks to expand knowledge graphs with facts that\\nare typically extracted from a text corpus. Recently, language models\\npretrained on large corpora have been shown to contain factual knowledge that\\ncan be retrieved using cloze-style strategies. Such approach enables zero-shot\\nrecall of facts, showing competitive results in object prediction compared to\\nsupervised baselines. However, prompt-based fact retrieval can be brittle and\\nheavily depend on the prompts and context used, which may produce results that\\nare unintended or hallucinatory.We propose to use textual entailment to\\nvalidate facts extracted from language models through cloze statements. Our\\nresults show that triple validation based on textual entailment improves\\nlanguage model predictions in different training regimes. Furthermore, we show\\nthat entailment-based triple validation is also effective to validate candidate\\nfacts extracted from other sources including existing knowledge graphs and text\\npassages where named entities are recognized. Geometry problem solving presents a formidable challenge within the NLP\\ncommunity. Existing approaches often rely on models designed for solving math\\nword problems, neglecting the unique characteristics of geometry math problems.\\nAdditionally, the current research predominantly focuses on geometry\\ncalculation problems, while overlooking other essential aspects like proving.\\nIn this study, we address these limitations by proposing the Geometry-Aware\\nProblem Solver (GAPS) model. GAPS is specifically designed to generate solution\\nprograms for geometry math problems of various types with the help of its\\nunique problem-type classifier. To achieve this, GAPS treats the solution\\nprogram as a composition of operators and operands, segregating their\\ngeneration processes. Furthermore, we introduce the geometry elements\\nenhancement method, which enhances the ability of GAPS to recognize geometry\\nelements accurately. By leveraging these improvements, GAPS showcases\\nremarkable performance in resolving geometry math problems. Our experiments\\nconducted on the UniGeo dataset demonstrate the superiority of GAPS over the\\nstate-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracy\\nimprovement of more than 5.3% for calculation tasks and an impressive 41.1% for\\nproving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% on\\nproving problems, representing a significant advancement in solving geometry\\nproving tasks. Preventing the spread of misinformation is challenging. The detection of\\nmisleading content presents a significant hurdle due to its extreme linguistic\\nand domain variability. Content-based models have managed to identify deceptive\\nlanguage by learning representations from textual data such as social media\\nposts and web articles. However, aggregating representative samples of this\\nheterogeneous phenomenon and implementing effective real-world applications is\\nstill elusive. Based on analytical work on the language of misinformation, this\\npaper analyzes the linguistic attributes that characterize this phenomenon and\\nhow representative of such features some of the most popular misinformation\\ndatasets are. We demonstrate that the appropriate use of pertinent symbolic\\nknowledge in combination with neural language models is helpful in detecting\\nmisleading content. Our results achieve state-of-the-art performance in\\nmisinformation datasets across the board, showing that our approach offers a\\nvalid and robust alternative to multi-task transfer learning without requiring\\nany additional training data. Furthermore, our results show evidence that\\nstructured knowledge can provide the extra boost required to address a complex\\nand unpredictable real-world problem like misinformation detection, not only in\\nterms of accuracy but also time efficiency and resource utilization. Claim verification is an essential step in the automated fact-checking\\npipeline which assesses the veracity of a claim against a piece of evidence. In\\nthis work, we explore the potential of few-shot claim verification, where only\\nvery limited data is available for supervision. We propose MAPLE (Micro\\nAnalysis of Pairwise Language Evolution), a pioneering approach that explores\\nthe alignment between a claim and its evidence with a small seq2seq model and a\\nnovel semantic measure. Its innovative utilization of micro language evolution\\npath leverages unlabelled pairwise data to facilitate claim verification while\\nimposing low demand on data annotations and computing resources. MAPLE\\ndemonstrates significant performance improvements over SOTA baselines SEED, PET\\nand LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and\\nSciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE This paper addresses the resolution of the 3-SAT problem using a QAOA-like\\napproach. The chosen principle involves modeling the solution ranks of the\\n3-SAT problem, which, in this particular case, directly represent a solution.\\nThis results in a highly compact circuit with few gates, enabling the modeling\\nof large-sized 3-SAT problems. Numerical experimentation demonstrates that the\\napproach can solve instances composed of 91 clauses and 20 variables with an\\nimplementation based on Qiskit. Region based knowledge graph embeddings represent relations as geometric\\nregions. This has the advantage that the rules which are captured by the model\\nare made explicit, making it straightforward to incorporate prior knowledge and\\nto inspect learned models. Unfortunately, existing approaches are severely\\nrestricted in their ability to model relational composition, and hence also\\ntheir ability to model rules, thus failing to deliver on the main promise of\\nregion based models. With the aim of addressing these limitations, we\\ninvestigate regions which are composed of axis-aligned octagons. Such octagons\\nare particularly easy to work with, as intersections and compositions can be\\nstraightforwardly computed, while they are still sufficiently expressive to\\nmodel arbitrary knowledge graphs. Among others, we also show that our octagon\\nembeddings can properly capture a non-trivial class of rule bases. Finally, we\\nshow that our model achieves competitive experimental results. Millions of people around the world are infected with mosquito-borne diseases\\neach year. One of the most dangerous species is Aedes aegypti, the main vector\\nof viruses such as dengue, yellow fever, chikungunya, and Zika, among others.\\nMosquito prevention and eradication campaigns are essential to avoid major\\npublic health consequences. In this respect, entomological surveillance is an\\nimportant tool. At present, this traditional monitoring tool is executed\\nmanually and requires digital transformation to help authorities make better\\ndecisions, improve their planning efforts, speed up execution, and better\\nmanage available resources. Therefore, new technological tools based on proven\\ntechniques need to be designed and developed. However, such tools should also\\nbe cost-effective, autonomous, reliable, and easy to implement, and should be\\nenabled by connectivity and multi-platform software applications. This paper\\npresents the design, development, and testing of an innovative system named\\nMosquIoT. It is based on traditional ovitraps with embedded Internet of Things\\n(IoT) and Tiny Machine Learning (TinyML) technologies, which enable the\\ndetection and quantification of Ae. aegypti eggs. This innovative and promising\\nsolution may help dynamically understand the behavior of Ae. aegypti\\npopulations in cities, shifting from the current reactive entomological\\nmonitoring model to a proactive and predictive digital one. Federated learning enhanced by differential privacy has emerged as a popular\\napproach to better safeguard the privacy of client-side data by protecting\\nclients' contributions during the training process. Existing solutions\\ntypically assume a uniform privacy budget for all records and provide\\none-size-fits-all solutions that may not be adequate to meet each record's\\nprivacy requirement. In this paper, we explore the uncharted territory of\\ncross-silo FL with record-level personalized differential privacy. We devise a\\nnovel framework named rPDP-FL, employing a two-stage hybrid sampling scheme\\nwith both client-level sampling and non-uniform record-level sampling to\\naccommodate varying privacy requirements. A critical and non-trivial problem is\\nto select the ideal per-record sampling probability q given the personalized\\nprivacy budget {\\\\epsilon}. We introduce a versatile solution named\\nSimulation-CurveFitting, allowing us to uncover a significant insight into the\\nnonlinear correlation between q and {\\\\epsilon} and derive an elegant\\nmathematical model to tackle the problem. Our evaluation demonstrates that our\\nsolution can provide significant performance gains over the baselines that do\\nnot consider personalized privacy preservation. We introduce the new task of clinically meaningful summarisation of social\\nmedia user timelines, appropriate for mental health monitoring. We develop a\\nnovel approach for unsupervised abstractive summarisation that produces a\\ntwo-layer summary consisting of both high-level information, covering aspects\\nuseful to clinical experts, as well as accompanying time sensitive evidence\\nfrom a user's social media timeline. A key methodological novelty comes from\\nthe timeline summarisation component based on a version of hierarchical\\nvariational autoencoder (VAE) adapted to represent long texts and guided by\\nLLM-annotated key phrases. The resulting timeline summary is input into a LLM\\n(LLaMA-2) to produce the final summary containing both the high level\\ninformation, obtained through instruction prompting, as well as corresponding\\nevidence from the user's timeline. We assess the summaries generated by our\\nnovel architecture via automatic evaluation against expert written summaries\\nand via human evaluation with clinical experts, showing that timeline\\nsummarisation by TH-VAE results in logically coherent summaries rich in\\nclinical utility and superior to LLM-only approaches in capturing changes over\\ntime. A major challenge in inductive logic programming is learning big rules. To\\naddress this challenge, we introduce an approach where we join small rules to\\nlearn big rules. We implement our approach in a constraint-driven system and\\nuse constraint solvers to efficiently join rules. Our experiments on many\\ndomains, including game playing and drug design, show that our approach can (i)\\nlearn rules with more than 100 literals, and (ii) drastically outperform\\nexisting approaches in terms of predictive accuracies. We introduce MultiMUC, the first multilingual parallel corpus for template\\nfilling, comprising translations of the classic MUC-4 template filling\\nbenchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. We\\nobtain automatic translations from a strong multilingual machine translation\\nsystem and manually project the original English annotations into each target\\nlanguage. For all languages, we also provide human translations for sentences\\nin the dev and test splits that contain annotated template arguments. Finally,\\nwe present baselines on MultiMUC both with state-of-the-art template filling\\nmodels and with ChatGPT. Many real-life contractual relations differ completely from the clean, static\\nmodel at the heart of principal-agent theory. Typically, they involve repeated\\nstrategic interactions of the principal and agent, taking place under\\nuncertainty and over time. While appealing in theory, players seldom use\\ncomplex dynamic strategies in practice, often preferring to circumvent\\ncomplexity and approach uncertainty through learning. We initiate the study of\\nrepeated contracts with a learning agent, focusing on agents who achieve\\nno-regret outcomes.\\n  Optimizing against a no-regret agent is a known open problem in general\\ngames; we achieve an optimal solution to this problem for a canonical contract\\nsetting, in which the agent's choice among multiple actions leads to\\nsuccess/failure. The solution has a surprisingly simple structure: for some\\n$\\\\alpha > 0$, initially offer the agent a linear contract with scalar $\\\\alpha$,\\nthen switch to offering a linear contract with scalar $0$. This switch causes\\nthe agent to ``free-fall'' through their action space and during this time\\nprovides the principal with non-zero reward at zero cost. Despite apparent\\nexploitation of the agent, this dynamic contract can leave \\\\emph{both} players\\nbetter off compared to the best static contract. Our results generalize beyond\\nsuccess/failure, to arbitrary non-linear contracts which the principal rescales\\ndynamically.\\n  Finally, we quantify the dependence of our results on knowledge of the time\\nhorizon, and are the first to address this consideration in the study of\\nstrategizing against learning agents. Background: Recent studies have used basic epicardial adipose tissue (EAT)\\nassessments (e.g., volume and mean HU) to predict risk of\\natherosclerosis-related, major adverse cardiovascular events (MACE).\\nObjectives: Create novel, hand-crafted EAT features, 'fat-omics', to capture\\nthe pathophysiology of EAT and improve MACE prediction. Methods: We segmented\\nEAT using a previously-validated deep learning method with optional manual\\ncorrection. We extracted 148 radiomic features (morphological, spatial, and\\nintensity) and used Cox elastic-net for feature reduction and prediction of\\nMACE. Results: Traditional fat features gave marginal prediction\\n(EAT-volume/EAT-mean-HU/ BMI gave C-index 0.53/0.55/0.57, respectively).\\nSignificant improvement was obtained with 15 fat-omics features (C-index=0.69,\\ntest set). High-risk features included\\nvolume-of-voxels-having-elevated-HU-[-50, -30-HU] and HU-negative-skewness,\\nboth of which assess high HU, which as been implicated in fat inflammation.\\nOther high-risk features include kurtosis-of-EAT-thickness, reflecting the\\nheterogeneity of thicknesses, and EAT-volume-in-the-top-25%-of-the-heart,\\nemphasizing adipose near the proximal coronary arteries. Kaplan-Meyer plots of\\nCox-identified, high- and low-risk patients were well separated with the median\\nof the fat-omics risk, while high-risk group having HR 2.4 times that of the\\nlow-risk group (P<0.001). Conclusion: Preliminary findings indicate an\\nopportunity to use more finely tuned, explainable assessments on EAT for\\nimproved cardiovascular risk prediction. The problem of the Remaining Useful Life (RUL) prediction, aiming at\\nproviding an accurate estimate of the remaining time from the current\\npredicting moment to the complete failure of the device, has gained significant\\nattention from researchers in recent years. In this paper, to overcome the\\nshortcomings of rigid combination for temporal and spatial features in most\\nexisting RUL prediction approaches, a spatial-temporal homogeneous feature\\nextractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise\\nprogressive feature fusion is employed to ensure the homogeneity of\\nspatial-temporal features and enhance the prediction accuracy. Secondly, the\\nFeature Space Global Relationship Invariance (FSGRI) training method is\\nintroduced based on supervised contrastive learning. This method maintains the\\nconsistency of relationships among sample features with their degradation\\npatterns during model training, simplifying the subsequently regression task in\\nthe output layer and improving the model's performance in RUL prediction.\\nFinally, the effectiveness of the proposed method is validated through\\ncomparisons with other latest research works on the C-MAPSS dataset. The\\nDual-Mixer model demonstrates superiority across most metrics, while the FSGRI\\ntraining method shows an average improvement of 7.00% and 2.41% in RMSE and\\nMAPE, respectively, for all baseline models. Our experiments and model code are\\npublicly available at https://github.com/fuen1590/PhmDeepLearningProjects. Large Language Models (LLMs) represent a leap in artificial intelligence,\\nexcelling in tasks using human language(s). Although the main focus of\\ngeneral-purpose LLMs is not code generation, they have shown promising results\\nin the domain. However, the usefulness of LLMs in an academic software\\nengineering project has not been fully explored yet. In this study, we explore\\nthe usefulness of LLMs for 214 students working in teams consisting of up to\\nsix members. Notably, in the academic course through which this study is\\nconducted, students were encouraged to integrate LLMs into their development\\ntool-chain, in contrast to most other academic courses that explicitly prohibit\\nthe use of LLMs.\\n  In this paper, we analyze the AI-generated code, prompts used for code\\ngeneration, and the human intervention levels to integrate the code into the\\ncode base. We also conduct a perception study to gain insights into the\\nperceived usefulness, influencing factors, and future outlook of LLM from a\\ncomputer science student's perspective. Our findings suggest that LLMs can play\\na crucial role in the early stages of software development, especially in\\ngenerating foundational code structures, and helping with syntax and error\\ndebugging. These insights provide us with a framework on how to effectively\\nutilize LLMs as a tool to enhance the productivity of software engineering\\nstudents, and highlight the necessity of shifting the educational focus toward\\npreparing students for successful human-AI collaboration. Large language models (LLMs) have demonstrated significant potential for many\\ndownstream tasks, including those requiring human-level intelligence, such as\\nvulnerability detection. However, recent attempts to use LLMs for vulnerability\\ndetection are still preliminary, as they lack an in-depth understanding of a\\nsubject LLM's vulnerability reasoning capability -- whether it originates from\\nthe model itself or from external assistance, such as invoking tool support and\\nretrieving vulnerability knowledge. In this paper, we aim to decouple LLMs'\\nvulnerability reasoning capability from their other capabilities, including the\\nability to actively seek additional information (e.g., via function calling in\\nSOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based\\nmatching and retrieval), and follow instructions to output structured results.\\nTo this end, we propose a unified evaluation framework named LLM4Vuln, which\\nseparates LLMs' vulnerability reasoning from their other capabilities and\\nevaluates how LLMs' vulnerability reasoning could be enhanced when combined\\nwith the enhancement of other capabilities. To demonstrate the effectiveness of\\nLLM4Vuln, we have designed controlled experiments using 75 ground-truth smart\\ncontract vulnerabilities, which were extensively audited as high-risk on\\nCode4rena from August to November 2023, and tested them in 4,950 different\\nscenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama).\\nOur results not only reveal ten findings regarding the varying effects of\\nknowledge enhancement, context supplementation, prompt schemes, and models but\\nalso enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty\\nprograms with over 1,000 USD being awarded. This report introduces LLaMandement, a state-of-the-art Large Language Model,\\nfine-tuned by the French government and designed to enhance the efficiency and\\nefficacy of processing parliamentary sessions (including the production of\\nbench memoranda and documents required for interministerial meetings) by\\ngenerating neutral summaries of legislative proposals. Addressing the\\nadministrative challenges of manually processing a growing volume of\\nlegislative amendments, LLaMandement stands as a significant legal\\ntechnological milestone, providing a solution that exceeds the scalability of\\ntraditional human efforts while matching the robustness of a specialized legal\\ndrafter. We release all our fine-tuned models and training data to the\\ncommunity. The transformer architecture has shown remarkable success in various domains,\\nsuch as natural language processing and computer vision. When it comes to graph\\nlearning, transformers are required not only to capture the interactions\\nbetween pairs of nodes but also to preserve graph structures connoting the\\nunderlying relations and proximity between them, showing the expressive power\\nto capture different graph structures. Accordingly, various\\nstructure-preserving graph transformers have been proposed and widely used for\\nvarious tasks, such as graph-level tasks in bioinformatics and\\nchemoinformatics. However, strategies related to graph structure preservation\\nhave not been well organized and systematized in the literature. In this paper,\\nwe provide a comprehensive overview of structure-preserving graph transformers\\nand generalize these methods from the perspective of their design objective.\\nFirst, we divide strategies into four main groups: node feature modulation,\\ncontext node sampling, graph rewriting, and transformer architecture\\nimprovements. We then further divide the strategies according to the coverage\\nand goals of graph structure preservation. Furthermore, we also discuss\\nchallenges and future directions for graph transformer models to preserve the\\ngraph structure and understand the nature of graphs. Recently, text-to-image diffusion models have demonstrated impressive ability\\nto generate high-quality images conditioned on the textual input. However,\\nthese models struggle to accurately adhere to textual instructions regarding\\nspatial layout information. While previous research has primarily focused on\\naligning cross-attention maps with layout conditions, they overlook the impact\\nof the initialization noise on the layout guidance. To achieve better layout\\ncontrol, we propose leveraging a spatial-aware initialization noise during the\\ndenoising process. Specifically, we find that the inverted reference image with\\nfinite inversion steps contains valuable spatial awareness regarding the\\nobject's position, resulting in similar layouts in the generated images. Based\\non this observation, we develop an open-vocabulary framework to customize a\\nspatial-aware initialization noise for each layout condition. Without modifying\\nother modules except the initialization noise, our approach can be seamlessly\\nintegrated as a plug-and-play module within other training-free layout guidance\\nframeworks. We evaluate our approach quantitatively and qualitatively on the\\navailable Stable Diffusion model and COCO dataset. Equipped with the\\nspatial-aware latent initialization, our method significantly improves the\\neffectiveness of layout guidance while preserving high-quality content. Neural radiance fields (NeRFs) have exhibited potential in synthesizing\\nhigh-fidelity views of 3D scenes but the standard training paradigm of NeRF\\npresupposes an equal importance for each image in the training set. This\\nassumption poses a significant challenge for rendering specific views\\npresenting intricate geometries, thereby resulting in suboptimal performance.\\nIn this paper, we take a closer look at the implications of the current\\ntraining paradigm and redesign this for more superior rendering quality by\\nNeRFs. Dividing input views into multiple groups based on their visual\\nsimilarities and training individual models on each of these groups enables\\neach model to specialize on specific regions without sacrificing speed or\\nefficiency. Subsequently, the knowledge of these specialized models is\\naggregated into a single entity via a teacher-student distillation paradigm,\\nenabling spatial efficiency for online render-ing. Empirically, we evaluate our\\nnovel training framework on two publicly available datasets, namely NeRF\\nsynthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training\\npipeline enhances the rendering quality of a state-of-the-art baseline model\\nwhile exhibiting convergence to a superior minimum. Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning,\\naim to fine-tune a pre-trained language model (PLM) using a minimal number of\\nparameters for a specific task or profile. Although adapter tuning provides\\nincreased parameter efficiency compared to full-model fine-tuning, it\\nintroduces a small set of additional parameters attached to a PLM for each\\nprofile. This can become problematic in practical applications with multiple\\nprofiles, particularly when a significant increase in the number of profiles\\nlinearly boosts the total number of additional parameters. To mitigate this\\nissue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of\\ngiven adapters by fine-tuning an extremely small set of compact tensors for a\\nnew profile, which serve as binary masks to adaptively select the given\\nadapters. To efficiently validate our proposed method, we implement it using a\\nlarge number of trained or untrained (random) adapters. We evaluate the\\nperformance of X-PEFT through LaMP and GLUE tasks and demonstrate that it\\neither matches or surpasses the effectiveness of conventional adapter tuning,\\ndespite reducing the memory requirements per profile by a factor of 10,000\\ncompared to it. We present an approach to outsourcing of training neural networks while\\npreserving data confidentiality from malicious parties. We use fully\\nhomomorphic encryption to build a unified training approach that works on\\nencrypted data and learns quantized neural network models. The data can be\\nhorizontally or vertically split between multiple parties, enabling\\ncollaboration on confidential data. We train logistic regression and\\nmulti-layer perceptrons on several datasets. The representation of a dynamic problem in ASP usually boils down to using\\ncopies of variables and constraints, one for each time stamp, no matter whether\\nit is directly encoded or via an action or temporal language. The\\nmultiplication of variables and constraints is commonly done during grounding\\nand the solver is completely ignorant about the temporal relationship among the\\ndifferent instances. On the other hand, a key factor in the performance of\\ntoday's ASP solvers is conflict-driven constraint learning. Our question is now\\nwhether a constraint learned for particular time steps can be generalized and\\nreused at other time stamps, and ultimately whether this enhances the overall\\nsolver performance on temporal problems. Knowing full well the domain of time,\\nwe study conditions under which learned dynamic constraints can be generalized.\\nWe propose a simple translation of the original logic program such that, for\\nthe translated programs, the learned constraints can be generalized to other\\ntime points. Additionally, we identify a property of temporal problems that\\nallows us to generalize all learned constraints to all time steps. It turns out\\nthat this property is satisfied by many planning problems. Finally, we\\nempirically evaluate the impact of adding the generalized constraints to an ASP\\nsolver The rapid advancement of the automotive industry towards automated and\\nsemi-automated vehicles has rendered traditional methods of vehicle\\ninteraction, such as touch-based and voice command systems, inadequate for a\\nwidening range of non-driving related tasks, such as referencing objects\\noutside of the vehicle. Consequently, research has shifted toward gestural\\ninput (e.g., hand, gaze, and head pose gestures) as a more suitable mode of\\ninteraction during driving. However, due to the dynamic nature of driving and\\nindividual variation, there are significant differences in drivers' gestural\\ninput performance. While, in theory, this inherent variability could be\\nmoderated by substantial data-driven machine learning models, prevalent\\nmethodologies lean towards constrained, single-instance trained models for\\nobject referencing. These models show a limited capacity to continuously adapt\\nto the divergent behaviors of individual drivers and the variety of driving\\nscenarios. To address this, we propose \\\\textit{IcRegress}, a novel\\nregression-based incremental learning approach that adapts to changing behavior\\nand the unique characteristics of drivers engaged in the dual task of driving\\nand referencing objects. We suggest a more personalized and adaptable solution\\nfor multimodal gestural interfaces, employing continuous lifelong learning to\\nenhance driver experience, safety, and convenience. Our approach was evaluated\\nusing an outside-the-vehicle object referencing use case, highlighting the\\nsuperiority of the incremental learning models adapted over a single trained\\nmodel across various driver traits such as handedness, driving experience, and\\nnumerous driving conditions. Finally, to facilitate reproducibility, ease\\ndeployment, and promote further research, we offer our approach as an\\nopen-source framework at \\\\url{https://github.com/amrgomaaelhady/IcRegress}. Multimodal learning has exhibited a significant advantage in affective\\nanalysis tasks owing to the comprehensive information of various modalities,\\nparticularly the complementary information. Thus, many emerging studies focus\\non disentangling the modality-invariant and modality-specific representations\\nfrom input data and then fusing them for prediction. However, our study shows\\nthat modality-specific representations may contain information that is\\nirrelevant or conflicting with the tasks, which downgrades the effectiveness of\\nlearned multimodal representations. We revisit the disentanglement issue, and\\npropose a novel triple disentanglement approach, TriDiRA, which disentangles\\nthe modality-invariant, effective modality-specific and ineffective\\nmodality-specific representations from input data. By fusing only the\\nmodality-invariant and effective modality-specific representations, TriDiRA can\\nsignificantly alleviate the impact of irrelevant and conflicting information\\nacross modalities during model training. Extensive experiments conducted on\\nfour benchmark datasets demonstrate the effectiveness and generalization of our\\ntriple disentanglement, which outperforms SOTA methods. Automatic diagnosis is a significant application of AI in healthcare, where\\ndiagnoses are generated based on the symptom description of patients. Previous\\nworks have approached this task directly by modeling the relationship between\\nthe normalized symptoms and all possible diseases. However, in the clinical\\ndiagnostic process, patients are initially consulted by a general practitioner\\nand, if necessary, referred to specialists in specific domains for a more\\ncomprehensive evaluation. The final diagnosis often emerges from a\\ncollaborative consultation among medical specialist groups. Recently, large\\nlanguage models have shown impressive capabilities in natural language\\nunderstanding. In this study, we adopt tuning-free LLM-based agents as medical\\npractitioners and propose the Agent-derived Multi-Specialist Consultation\\n(AMSC) framework to model the diagnosis process in the real world by adaptively\\nfusing probability distributions of agents over potential diseases.\\nExperimental results demonstrate the superiority of our approach compared with\\nbaselines. Notably, our approach requires significantly less parameter updating\\nand training time, enhancing efficiency and practical utility. Furthermore, we\\ndelve into a novel perspective on the role of implicit symptoms within the\\ncontext of automatic diagnosis. The early prediction of battery life (EPBL) is vital for enhancing the\\nefficiency and extending the lifespan of lithium batteries. Traditional models\\nwith fixed architectures often encounter underfitting or overfitting issues due\\nto the diverse data distributions in different EPBL tasks. An interpretable\\ndeep learning model of flexible parallel neural network (FPNN) is proposed,\\nwhich includes an InceptionBlock, a 3D convolutional neural network (CNN), a 2D\\nCNN, and a dual-stream network. The proposed model effectively extracts\\nelectrochemical features from video-like formatted data using the 3D CNN and\\nachieves advanced multi-scale feature abstraction through the InceptionBlock.\\nThe FPNN can adaptively adjust the number of InceptionBlocks to flexibly handle\\ntasks of varying complexity in EPBL. The test on the MIT dataset shows that the\\nFPNN model achieves outstanding predictive accuracy in EPBL tasks, with MAPEs\\nof 2.47%, 1.29%, 1.08%, and 0.88% when the input cyclic data volumes are 10,\\n20, 30, and 40, respectively. The interpretability of the FPNN is mainly\\nreflected in its flexible unit structure and parameter selection: its diverse\\nbranching structure enables the model to capture features at different scales,\\nthus allowing the machine to learn informative features. The approach presented\\nherein provides an accurate, adaptable, and comprehensible solution for early\\nlife prediction of lithium batteries, opening new possibilities in the field of\\nbattery health monitoring. In the realm of precision medicine, effective patient stratification and\\ndisease subtyping demand innovative methodologies tailored for multi-omics\\ndata. Clustering techniques applied to multi-omics data have become\\ninstrumental in identifying distinct subgroups of patients, enabling a\\nfiner-grained understanding of disease variability. This work establishes a\\npowerful framework for advancing precision medicine through unsupervised\\nrandom-forest-based clustering and federated computing. We introduce a novel\\nmulti-omics clustering approach utilizing unsupervised random-forests. The\\nunsupervised nature of the random forest enables the determination of\\ncluster-specific feature importance, unraveling key molecular contributors to\\ndistinct patient groups. Moreover, our methodology is designed for federated\\nexecution, a crucial aspect in the medical domain where privacy concerns are\\nparamount. We have validated our approach on machine learning benchmark data\\nsets as well as on cancer data from The Cancer Genome Atlas (TCGA). Our method\\nis competitive with the state-of-the-art in terms of disease subtyping, but at\\nthe same time substantially improves the cluster interpretability. Experiments\\nindicate that local clustering performance can be improved through federated\\ncomputing. Algorithmic recourse -- providing recommendations to those affected\\nnegatively by the outcome of an algorithmic system on how they can take action\\nand change that outcome -- has gained attention as a means of giving persons\\nagency in their interactions with artificial intelligence (AI) systems. Recent\\nwork has shown that even if an AI decision-making classifier is ``fair''\\n(according to some reasonable criteria), recourse itself may be unfair due to\\ndifferences in the initial circumstances of individuals, compounding\\ndisparities for marginalized populations and requiring them to exert more\\neffort than others. There is a need to define more methods and metrics for\\nevaluating fairness in recourse that span a range of normative views of the\\nworld, and specifically those that take into account time. Time is a critical\\nelement in recourse because the longer it takes an individual to act, the more\\nthe setting may change due to model or data drift.\\n  This paper seeks to close this research gap by proposing two notions of\\nfairness in recourse that are in normative alignment with substantive equality\\nof opportunity, and that consider time. The first considers the (often\\nrepeated) effort individuals exert per successful recourse event, and the\\nsecond considers time per successful recourse event. Building upon an\\nagent-based framework for simulating recourse, this paper demonstrates how much\\neffort is needed to overcome disparities in initial circumstances. We then\\nproposes an intervention to improve the fairness of recourse by rewarding\\neffort, and compare it to existing strategies. Managing transition plans is one of the major problems of people with\\ncognitive disabilities. Therefore, finding an automated way to generate such\\nplans would be a helpful tool for this community. In this paper we have\\nspecifically proposed and compared different alternative ways to merge plans\\nformed by sequences of actions of unknown similarities between goals and\\nactions executed by several operator agents which cooperate between them\\napplying such actions over some passive elements (node agents) that require\\nadditional executions of another plan after some time of use. Such ignorance of\\nthe similarities between plan actions and goals would justify the use of a\\ndistributed recommendation system that would provide an useful plan to be\\napplied for a certain goal to a given operator agent, generated from the known\\nresults of previous executions of different plans by other operator agents.\\nHere we provide the general framework of execution (agent system), and the\\ndifferent merging algorithms applied to this problem. The proposed agent system\\nwould act as an useful cognitive assistant for people with intelectual\\ndisabilities such as autism. A multiagent system can be viewed as a society of autonomous agents, whose\\ninteractions can be effectively regulated via social norms. In general, the\\nnorms of a society are not hardcoded but emerge from the agents' interactions.\\nSpecifically, how the agents in a society react to each other's behavior and\\nrespond to the reactions of others determines which norms emerge in the\\nsociety. We think of these reactions by an agent to the satisfactory or\\nunsatisfactory behaviors of another agent as communications from the first\\nagent to the second agent. Understanding these communications is a kind of\\nsocial intelligence: these communications provide natural drivers for norm\\nemergence by pushing agents toward certain behaviors, which can become\\nestablished as norms. Whereas it is well-known that sanctioning can lead to the\\nemergence of norms, we posit that a broader kind of social intelligence can\\nprove more effective in promoting cooperation in a multiagent system.\\n  Accordingly, we develop Nest, a framework that models social intelligence in\\nthe form of a wider variety of communications and understanding of them than in\\nprevious work. To evaluate Nest, we develop a simulated pandemic environment\\nand conduct simulation experiments to compare Nest with baselines considering a\\ncombination of three kinds of social communication: sanction, tell, and hint.\\n  We find that societies formed of Nest agents achieve norms faster; moreover,\\nNest agents effectively avoid undesirable consequences, which are negative\\nsanctions and deviation from goals, and yield higher satisfaction for\\nthemselves than baseline agents despite requiring only an equivalent amount of\\ninformation. Answering complex logical queries on incomplete knowledge graphs (KGs) is a\\nfundamental and challenging task in multi-hop reasoning. Recent work defines\\nthis task as an end-to-end optimization problem, which significantly reduces\\nthe training cost and enhances the generalization of the model by a pretrained\\nlink predictors for query answering. However, most existing proposals ignore\\nthe critical semantic knowledge inherently available in KGs, such as type\\ninformation, which could help answer complex logical queries. To this end, we\\npropose TypE-based Neural Link Prediction Adapter (TENLPA), a novel model that\\nconstructs type-based entity-relation graphs to discover the latent\\nrelationships between entities and relations by leveraging type information in\\nKGs. Meanwhile, in order to effectively combine type information with complex\\nlogical queries, an adaptive learning mechanism is introduced, which is trained\\nby back-propagating during the complex query answering process to achieve\\nadaptive adjustment of neural link predictors. Experiments on 3 standard\\ndatasets show that TENLPA model achieves state-of-the-art performance on\\ncomplex query answering with good generalization and robustness. Abstract reasoning is a cornerstone of human intelligence, and replicating it\\nwith artificial intelligence (AI) presents an ongoing challenge. This study\\nfocuses on efficiently solving Raven's progressive matrices (RPM), a visual\\ntest for assessing abstract reasoning abilities, by using distributed\\ncomputation and operators provided by vector-symbolic architectures (VSA).\\nInstead of hard-coding the rule formulations associated with RPMs, our approach\\ncan learn the VSA rule formulations (hence the name Learn-VRF) with just one\\npass through the training data. Yet, our approach, with compact parameters,\\nremains transparent and interpretable. Learn-VRF yields accurate predictions on\\nI-RAVEN's in-distribution data, and exhibits strong out-of-distribution\\ncapabilities concerning unseen attribute-rule pairs, significantly\\noutperforming pure connectionist baselines including large language models. Our\\ncode is available at\\nhttps://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations. Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\\nlinking borrowers with lenders through online platforms. However, P2P lending\\nfaces the challenge of information asymmetry, as lenders often lack sufficient\\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\\napproach to address this issue by leveraging the textual descriptions provided\\nby borrowers during the loan application process. Our methodology involves\\nprocessing these textual descriptions using a Large Language Model (LLM), a\\npowerful tool capable of discerning patterns and semantics within the text.\\nTransfer learning is applied to adapt the LLM to the specific task at hand.\\n  Our results derived from the analysis of the Lending Club dataset show that\\nthe risk score generated by BERT, a widely used LLM, significantly improves the\\nperformance of credit risk classifiers. However, the inherent opacity of\\nLLM-based systems, coupled with uncertainties about potential biases,\\nunderscores critical considerations for regulatory frameworks and engenders\\ntrust-related concerns among end-users, opening new avenues for future research\\nin the dynamic landscape of P2P lending and artificial intelligence. In recent years, significant progress has been made in the field of robotic\\nreinforcement learning (RL), enabling methods that handle complex image\\nobservations, train in the real world, and incorporate auxiliary data, such as\\ndemonstrations and prior experience. However, despite these advances, robotic\\nRL remains hard to use. It is acknowledged among practitioners that the\\nparticular implementation details of these algorithms are often just as\\nimportant (if not more so) for performance as the choice of algorithm. We posit\\nthat a significant challenge to widespread adoption of robotic RL, as well as\\nfurther development of robotic RL methods, is the comparative inaccessibility\\nof such methods. To address this challenge, we developed a carefully\\nimplemented library containing a sample efficient off-policy deep RL method,\\ntogether with methods for computing rewards and resetting the environment, a\\nhigh-quality controller for a widely-adopted robot, and a number of challenging\\nexample tasks. We provide this library as a resource for the community,\\ndescribe its design choices, and present experimental results. Perhaps\\nsurprisingly, we find that our implementation can achieve very efficient\\nlearning, acquiring policies for PCB board assembly, cable routing, and object\\nrelocation between 25 to 50 minutes of training per policy on average,\\nimproving over state-of-the-art results reported for similar tasks in the\\nliterature. These policies achieve perfect or near-perfect success rates,\\nextreme robustness even under perturbations, and exhibit emergent recovery and\\ncorrection behaviors. We hope that these promising results and our high-quality\\nopen-source implementation will provide a tool for the robotics community to\\nfacilitate further developments in robotic RL. Our code, documentation, and\\nvideos can be found at https://serl-robot.github.io/ Self-supervised graph representation learning has recently shown considerable\\npromise in a range of fields, including bioinformatics and social networks. A\\nlarge number of graph contrastive learning approaches have shown promising\\nperformance for representation learning on graphs, which train models by\\nmaximizing agreement between original graphs and their augmented views (i.e.,\\npositive views). Unfortunately, these methods usually involve pre-defined\\naugmentation strategies based on the knowledge of human experts. Moreover,\\nthese strategies may fail to generate challenging positive views to provide\\nsufficient supervision signals. In this paper, we present a novel approach\\nnamed Graph Pooling ContraSt (GPS) to address these issues. Motivated by the\\nfact that graph pooling can adaptively coarsen the graph with the removal of\\nredundancy, we rethink graph pooling and leverage it to automatically generate\\nmulti-scale positive views with varying emphasis on providing challenging\\npositives and preserving semantics, i.e., strongly-augmented view and\\nweakly-augmented view. Then, we incorporate both views into a joint contrastive\\nlearning framework with similarity learning and consistency learning, where our\\npooling module is adversarially trained with respect to the encoder for\\nadversarial robustness. Experiments on twelve datasets on both graph\\nclassification and transfer learning tasks verify the superiority of the\\nproposed method over its counterparts. Bias mitigation of Language Models has been the topic of many studies with a\\nrecent focus on learning separate modules like adapters for on-demand\\ndebiasing. Besides optimizing for a modularized debiased model, it is often\\ncritical in practice to control the degree of bias reduction at inference time,\\ne.g., in order to tune for a desired performance-fairness trade-off in search\\nresults or to control the strength of debiasing in classification tasks. In\\nthis paper, we introduce Controllable Gate Adapter (ConGater), a novel modular\\ngating mechanism with adjustable sensitivity parameters, which allows for a\\ngradual transition from the biased state of the model to the fully debiased\\nversion at inference time. We demonstrate ConGater performance by (1)\\nconducting adversarial debiasing experiments with three different models on\\nthree classification tasks with four protected attributes, and (2) reducing the\\nbias of search results through fairness list-wise regularization to enable\\nadjusting a trade-off between performance and fairness metrics. Our experiments\\non the classification tasks show that compared to baselines of the same\\ncaliber, ConGater can maintain higher task performance while containing less\\ninformation regarding the attributes. Our results on the retrieval task show\\nthat the fully debiased ConGater can achieve the same fairness performance\\nwhile maintaining more than twice as high task performance than recent strong\\nbaselines. Overall, besides strong performance ConGater enables the continuous\\ntransitioning between biased and debiased states of models, enhancing\\npersonalization of use and interpretability through controllability. Secure two-party computation with homomorphic encryption (HE) protects data\\nprivacy with a formal security guarantee but suffers from high communication\\noverhead. While previous works, e.g., Cheetah, Iron, etc, have proposed\\nefficient HE-based protocols for different neural network (NN) operations, they\\nstill assume high precision, e.g., fixed point 37 bit, for the NN operations\\nand ignore NNs' native robustness against quantization error. In this paper, we\\npropose HEQuant, which features low-precision-quantization-aware optimization\\nfor the HE-based protocols. We observe the benefit of a naive combination of\\nquantization and HE quickly saturates as bit precision goes down. Hence, to\\nfurther improve communication efficiency, we propose a series of optimizations,\\nincluding an intra-coefficient packing algorithm and a quantization-aware\\ntiling algorithm, to simultaneously reduce the number and precision of the\\ntransferred data. Compared with prior-art HE-based protocols, e.g., CrypTFlow2,\\nCheetah, Iron, etc, HEQuant achieves $3.5\\\\sim 23.4\\\\times$ communication\\nreduction and $3.0\\\\sim 9.3\\\\times$ latency reduction. Meanwhile, when compared\\nwith prior-art network optimization frameworks, e.g., SENet, SNL, etc, HEQuant\\nalso achieves $3.1\\\\sim 3.6\\\\times$ communication reduction. Mixture-of-Experts (MoE) models are a promising way to scale up model\\ncapacity without significantly increasing computational cost. A key component\\nof MoEs is the router, which decides which subset of parameters (experts)\\nprocess which feature embeddings (tokens). In this paper, we present a\\ncomprehensive study of routers in MoEs for computer vision tasks. We introduce\\na unified MoE formulation that subsumes different MoEs with two parametric\\nrouting tensors. This formulation covers both sparse MoE, which uses a binary\\nor hard assignment between experts and tokens, and soft MoE, which uses a soft\\nassignment between experts and weighted combinations of tokens. Routers for\\nsparse MoEs can be further grouped into two variants: Token Choice, which\\nmatches experts to each token, and Expert Choice, which matches tokens to each\\nexpert. We conduct head-to-head experiments with 6 different routers, including\\nexisting routers from prior work and new ones we introduce. We show that (i)\\nmany routers originally developed for language modeling can be adapted to\\nperform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers\\ngenerally outperform Token Choice routers, and (iii) soft MoEs generally\\noutperform sparse MoEs with a fixed compute budget. These results provide new\\ninsights regarding the crucial role of routers in vision MoE models. Dialogue systems controlled by predefined or rule-based scenarios derived\\nfrom counseling techniques, such as cognitive behavioral therapy (CBT), play an\\nimportant role in mental health apps. Despite the need for responsible\\nresponses, it is conceivable that using the newly emerging LLMs to generate\\ncontextually relevant utterances will enhance these apps. In this study, we\\nconstruct dialogue modules based on a CBT scenario focused on conventional\\nSocratic questioning using two kinds of LLMs: a Transformer-based dialogue\\nmodel further trained with a social media empathetic counseling dataset,\\nprovided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLM\\ncreated by OpenAI. By comparing systems that use LLM-generated responses with\\nthose that do not, we investigate the impact of generated responses on\\nsubjective evaluations such as mood change, cognitive change, and dialogue\\nquality (e.g., empathy). As a result, no notable improvements are observed when\\nusing the OsakaED model. When using GPT-4, the amount of mood change, empathy,\\nand other dialogue qualities improve significantly. Results suggest that GPT-4\\npossesses a high counseling ability. However, they also indicate that even when\\nusing a dialogue model trained with a human counseling dataset, it does not\\nnecessarily yield better outcomes compared to scenario-based dialogues. While\\npresenting LLM-generated responses, including GPT-4, and having them interact\\ndirectly with users in real-life mental health care services may raise ethical\\nissues, it is still possible for human professionals to produce example\\nresponses or response templates using LLMs in advance in systems that use\\nrules, scenarios, or example responses. Existing evaluation benchmarks of language models of code (code LMs) focus\\nalmost exclusively on whether the LMs can generate functionally-correct code.\\nIn real-world software engineering, developers think beyond functional\\ncorrectness. They have requirements on \\\"how\\\" a functionality should be\\nimplemented to meet overall system design objectives like efficiency, security,\\nand maintainability. They would also trust the code LMs more if the LMs\\ndemonstrate robust understanding of requirements and code semantics.\\n  We propose a new benchmark NoFunEval to evaluate code LMs on non-functional\\nrequirements and simple classification instances for both functional and\\nnon-functional requirements. We propose a prompting method, Coding Concepts\\n(CoCo), as a way for a developer to communicate the domain knowledge to the\\nLMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is\\nthat they generally falter when tested on our benchmark, hinting at fundamental\\nblindspots in their training setups. Surprisingly, even the classification\\naccuracy on functional-correctness instances derived from the popular HumanEval\\nbenchmark is low, calling in question the depth of their comprehension and the\\nsource of their success in generating functionally-correct code in the first\\nplace. We will release our benchmark and evaluation scripts publicly at\\nhttps://aka.ms/NoFunEval. Federated unlearning has emerged as a promising paradigm to erase the\\nclient-level data effect without affecting the performance of collaborative\\nlearning models. However, the federated unlearning process often introduces\\nextensive storage overhead and consumes substantial computational resources,\\nthus hindering its implementation in practice. To address this issue, this\\npaper proposes a scalable federated unlearning framework based on isolated\\nsharding and coded computing. We first divide distributed clients into multiple\\nisolated shards across stages to reduce the number of clients being affected.\\nThen, to reduce the storage overhead of the central server, we develop a coded\\ncomputing mechanism by compressing the model parameters across different\\nshards. In addition, we provide the theoretical analysis of time efficiency and\\nstorage effectiveness for the isolated and coded sharding. Finally, extensive\\nexperiments on two typical learning tasks, i.e., classification and generation,\\ndemonstrate that our proposed framework can achieve better performance than\\nthree state-of-the-art frameworks in terms of accuracy, retraining time,\\nstorage overhead, and F1 scores for resisting membership inference attacks. Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\\nlabeled source domain to an unlabeled target domain. In this paper, we\\nintroduce a novel approach called class-aware optimal transport (OT), which\\nmeasures the OT distance between a distribution over the source\\nclass-conditional distributions and a mixture of source and target data\\ndistribution. Our class-aware OT leverages a cost function that determines the\\nmatching extent between a given data example and a source class-conditional\\ndistribution. By optimizing this cost function, we find the optimal matching\\nbetween target examples and source class-conditional distributions, effectively\\naddressing the data and label shifts that occur between the two domains. To\\nhandle the class-aware OT efficiently, we propose an amortization solution that\\nemploys deep neural networks to formulate the transportation probabilities and\\nthe cost function. Additionally, we propose minimizing class-aware Higher-order\\nMoment Matching (HMM) to align the corresponding class regions on the source\\nand target domains. The class-aware HMM component offers an economical\\ncomputational approach for accurately evaluating the HMM distance between the\\ntwo distributions. Extensive experiments on benchmark datasets demonstrate that\\nour proposed method significantly outperforms existing state-of-the-art\\nbaselines. Recently, reference-based image super-resolution (RefSR) has shown excellent\\nperformance in image super-resolution (SR) tasks. The main idea of RefSR is to\\nutilize additional information from the reference (Ref) image to recover the\\nhigh-frequency components in low-resolution (LR) images. By transferring\\nrelevant textures through feature matching, RefSR models outperform existing\\nsingle image super-resolution (SISR) models. However, their performance\\nsignificantly declines when a domain gap between Ref and LR images exists,\\nwhich often occurs in real-world scenarios, such as satellite imaging. In this\\nletter, we introduce a Domain Matching (DM) module that can be seamlessly\\nintegrated with existing RefSR models to enhance their performance in a\\nplug-and-play manner. To the best of our knowledge, we are the first to explore\\nDomain Matching-based RefSR in remote sensing image processing. Our analysis\\nreveals that their domain gaps often occur in different satellites, and our\\nmodel effectively addresses these challenges, whereas existing models struggle.\\nOur experiments demonstrate that the proposed DM module improves SR performance\\nboth qualitatively and quantitatively for remote sensing super-resolution\\ntasks. This study investigates self-supervised learning techniques to obtain\\nrepresentations of Event Sequences. It is a key modality in various\\napplications, including but not limited to banking, e-commerce, and healthcare.\\n  We perform a comprehensive study of generative and contrastive approaches in\\nself-supervised learning, applying them both independently. We find that there\\nis no single supreme method. Consequently, we explore the potential benefits of\\ncombining these approaches. To achieve this goal, we introduce a novel method\\nthat aligns generative and contrastive embeddings as distinct modalities,\\ndrawing inspiration from contemporary multimodal research.\\n  Generative and contrastive approaches are often treated as mutually\\nexclusive, leaving a gap for their combined exploration. Our results\\ndemonstrate that this aligned model performs at least on par with, and mostly\\nsurpasses, existing methods and is more universal across a variety of tasks.\\nFurthermore, we demonstrate that self-supervised methods consistently\\noutperform the supervised approach on our datasets. Evolutionary algorithms have been successful in solving multi-objective\\noptimization problems (MOPs). However, as a class of population-based search\\nmethodology, evolutionary algorithms require a large number of evaluations of\\nthe objective functions, preventing them from being applied to a wide range of\\nexpensive MOPs. To tackle the above challenge, this work proposes for the first\\ntime a diffusion model that can learn to perform evolutionary multi-objective\\nsearch, called EmoDM. This is achieved by treating the reversed convergence\\nprocess of evolutionary search as the forward diffusion and learn the noise\\ndistributions from previously solved evolutionary optimization tasks. The\\npre-trained EmoDM can then generate a set of non-dominated solutions for a new\\nMOP by means of its reverse diffusion without further evolutionary search,\\nthereby significantly reducing the required function evaluations. To enhance\\nthe scalability of EmoDM, a mutual entropy-based attention mechanism is\\nintroduced to capture the decision variables that are most important for the\\nobjectives. Experimental results demonstrate the competitiveness of EmoDM in\\nterms of both the search performance and computational efficiency compared with\\nstate-of-the-art evolutionary algorithms in solving MOPs having up to 5000\\ndecision variables. The pre-trained EmoDM is shown to generalize well to unseen\\nproblems, revealing its strong potential as a general and efficient MOP solver. Existing vision-language models exhibit strong generalization on a variety of\\nvisual domains and tasks. However, such models mainly perform zero-shot\\nrecognition in a closed-set manner, and thus struggle to handle open-domain\\nvisual concepts by design. There are recent finetuning methods, such as prompt\\nlearning, that not only study the discrimination between in-distribution (ID)\\nand out-of-distribution (OOD) samples, but also show some improvements in both\\nID and OOD accuracies. In this paper, we first demonstrate that vision-language\\nmodels, after long enough finetuning but without proper regularization, tend to\\noverfit the known classes in the given dataset, with degraded performance on\\nunknown classes. Then we propose a novel approach OGEN to address this pitfall,\\nwith the main focus on improving the OOD GENeralization of finetuned models.\\nSpecifically, a class-conditional feature generator is introduced to synthesize\\nOOD features using just the class name of any unknown class. Such synthesized\\nfeatures will provide useful knowledge about unknowns and help regularize the\\ndecision boundary between ID and OOD data when optimized jointly. Equally\\nimportant is our adaptive self-distillation mechanism to regularize our feature\\ngeneration model during joint optimization, i.e., adaptively transferring\\nknowledge between model states to further prevent overfitting. Experiments\\nvalidate that our method yields convincing gains in OOD generalization\\nperformance in different settings. An effective multi-turn instruction-following assistant can be developed by\\ncreating a simulator that can generate useful interaction data. Apart from\\nrelying on its intrinsic weights, an ideal user simulator should also be able\\nto bootstrap external knowledge rapidly in its raw form to simulate the\\nmultifarious diversity of text available over the internet. Previous user\\nsimulators generally lacked diversity, were mostly closed domain, and\\nnecessitated rigid schema making them inefficient to rapidly scale to\\nincorporate external knowledge. In this regard, we introduce, Kaucus, a\\nKnowledge-Augmented User Simulator framework, to outline a process of creating\\ndiverse user simulators, that can seamlessly exploit external knowledge as well\\nas benefit downstream assistant model training. Through two GPT-J based\\nsimulators viz., a Retrieval Augmented Simulator and a Summary Controlled\\nSimulator we generate diverse simulator-assistant interactions. Through reward\\nand preference model-based evaluations, we find that these interactions serve\\nas useful training data and create more helpful downstream assistants. We also\\nfind that incorporating knowledge through retrieval augmentation or summary\\ncontrol helps create better assistants. In the field of causal modeling, potential outcomes (PO) and structural\\ncausal models (SCMs) stand as the predominant frameworks. However, these\\nframeworks face notable challenges in practically modeling counterfactuals,\\nformalized as parameters of the joint distribution of potential outcomes.\\nCounterfactual reasoning holds paramount importance in contemporary\\ndecision-making processes, especially in scenarios that demand personalized\\nincentives based on the joint values of $(Y(0), Y(1))$. This paper begins with\\nan investigation of the PO and SCM frameworks for modeling counterfactuals.\\nThrough the analysis, we identify an inherent model capacity limitation, termed\\nas the ``degenerative counterfactual problem'', emerging from the consistency\\nrule that is the cornerstone of both frameworks. To address this limitation, we\\nintroduce a novel \\\\textit{distribution-consistency} assumption, and in\\nalignment with it, we propose the Distribution-consistency Structural Causal\\nModels (DiscoSCMs) offering enhanced capabilities to model counterfactuals. To\\nconcretely reveal the enhanced model capacity, we introduce a new identifiable\\ncausal parameter, \\\\textit{the probability of consistency}, which holds\\npractical significance within DiscoSCM alone, showcased with a personalized\\nincentive example. Furthermore, we provide a comprehensive set of theoretical\\nresults about the ``Ladder of Causation'' within the DiscoSCM framework. We\\nhope it opens new avenues for future research of counterfactual modeling,\\nultimately enhancing our understanding of causality and its real-world\\napplications. Long-term traffic prediction has always been a challenging task due to its\\ndynamic temporal dependencies and complex spatial dependencies. In this paper,\\nwe propose a model that combines hybrid Transformer and spatio-temporal\\nself-supervised learning. The model enhances its robustness by applying\\nadaptive data augmentation techniques at the sequence-level and graph-level of\\nthe traffic data. It utilizes Transformer to overcome the limitations of\\nrecurrent neural networks in capturing long-term sequences, and employs\\nChebyshev polynomial graph convolution to capture complex spatial dependencies.\\nFurthermore, considering the impact of spatio-temporal heterogeneity on traffic\\nspeed, we design two self-supervised learning tasks to model the temporal and\\nspatial heterogeneity, thereby improving the accuracy and generalization\\nability of the model. Experimental evaluations are conducted on two real-world\\ndatasets, PeMS04 and PeMS08, and the results are visualized and analyzed,\\ndemonstrating the superior performance of the proposed model. This technical report details our work towards building an enhanced\\naudio-visual sound event localization and detection (SELD) network. We build on\\ntop of the audio-only SELDnet23 model and adapt it to be audio-visual by\\nmerging both audio and video information prior to the gated recurrent unit\\n(GRU) of the audio-only network. Our model leverages YOLO and DETIC object\\ndetectors. We also build a framework that implements audio-visual data\\naugmentation and audio-visual synthetic data generation. We deliver an\\naudio-visual SELDnet system that outperforms the existing audio-visual SELD\\nbaseline. Offline reinforcement learning (RL) algorithms can improve the decision\\nmaking via stitching sub-optimal trajectories to obtain more optimal ones. This\\ncapability is a crucial factor in enabling RL to learn policies that are\\nsuperior to the behavioral policy. On the other hand, Decision Transformer (DT)\\nabstracts the decision-making as sequence modeling, showcasing competitive\\nperformance on offline RL benchmarks, however, recent studies demonstrate that\\nDT lacks of stitching capability, thus exploit stitching capability for DT is\\nvital to further improve its performance. In order to endow stitching\\ncapability to DT, we abstract trajectory stitching as expert matching and\\nintroduce our approach, ContextFormer, which integrates contextual\\ninformation-based imitation learning (IL) and sequence modeling to stitch\\nsub-optimal trajectory fragments by emulating the representations of a limited\\nnumber of expert trajectories. To validate our claim, we conduct experiments\\nfrom two perspectives: 1) We conduct extensive experiments on D4RL benchmarks\\nunder the settings of IL, and experimental results demonstrate ContextFormer\\ncan achieve competitive performance in multi-IL settings. 2) More importantly,\\nwe conduct a comparison of ContextFormer with diverse competitive DT variants\\nusing identical training datasets. The experimental results unveiled\\nContextFormer's superiority, as it outperformed all other variants, showcasing\\nits remarkable performance. Vision-language foundation models like CLIP have revolutionized the field of\\nartificial intelligence. Nevertheless, VLM models supporting multi-language,\\ne.g., in both Chinese and English, have lagged due to the relative scarcity of\\nlarge-scale pretraining datasets. Toward this end, we introduce a comprehensive\\nbilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs,\\naimed at enhancing multimodal foundation models to well understand images in\\nboth languages. To handle such a scale of dataset, we propose a novel grouped\\naggregation approach for image-text contrastive loss computation, which reduces\\nthe communication overhead and GPU memory demands significantly, facilitating a\\n60% increase in training speed. We pretrain a series of bilingual image-text\\nfoundation models with an enhanced fine-grained understanding ability on BM-6B,\\nthe resulting models, dubbed as $M^2$-Encoders (pronounced \\\"M-Square\\\"), set new\\nbenchmarks in both languages for multimodal retrieval and classification tasks.\\nNotably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of\\n88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification\\nsetting, surpassing previously reported SoTA methods by 2.2% and 21.1%,\\nrespectively. The $M^2$-Encoder series represents one of the most comprehensive\\nbilingual image-text foundation models to date, so we are making it available\\nto the research community for further exploration and development. Graph Neural Networks (GNNs) and Transformer have been increasingly adopted\\nto learn the complex vector representations of spatio-temporal graphs,\\ncapturing intricate spatio-temporal dependencies crucial for applications such\\nas traffic datasets. Although many existing methods utilize multi-head\\nattention mechanisms and message-passing neural networks (MPNNs) to capture\\nboth spatial and temporal relations, these approaches encode temporal and\\nspatial relations independently, and reflect the graph's topological\\ncharacteristics in a limited manner. In this work, we introduce the Cycle to\\nMixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial\\ninvariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP).\\nThe Cy2Mixer is composed of three blocks based on MLPs: A message-passing block\\nfor encapsulating spatial information, a cycle message-passing block for\\nenriching topological information through cyclic subgraphs, and a temporal\\nblock for capturing temporal properties. We bolster the effectiveness of\\nCy2Mixer with mathematical evidence emphasizing that our cycle message-passing\\nblock is capable of offering differentiated information to the deep learning\\nmodel compared to the message-passing block. Furthermore, empirical evaluations\\nsubstantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art\\nperformances across various traffic benchmark datasets. Slicing distribution selection has been used as an effective technique to\\nimprove the performance of parameter estimators based on minimizing sliced\\nWasserstein distance in applications. Previous works either utilize expensive\\noptimization to select the slicing distribution or use slicing distributions\\nthat require expensive sampling methods. In this work, we propose an\\noptimization-free slicing distribution that provides a fast sampling for the\\nMonte Carlo estimation of expectation. In particular, we introduce the\\nrandom-path projecting direction (RPD) which is constructed by leveraging the\\nnormalized difference between two random vectors following the two input\\nmeasures. From the RPD, we derive the random-path slicing distribution (RPSD)\\nand two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced\\nWasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced\\nWasserstein (IWRPSW). We then discuss the topological, statistical, and\\ncomputational properties of RPSW and IWRPSW. Finally, we showcase the favorable\\nperformance of RPSW and IWRPSW in gradient flow and the training of denoising\\ndiffusion generative models on images. This paper addresses a multi-echelon inventory management problem with a\\ncomplex network topology where deriving optimal ordering decisions is\\ndifficult. Deep reinforcement learning (DRL) has recently shown potential in\\nsolving such problems, while designing the neural networks in DRL remains a\\nchallenge. In order to address this, a DRL model is developed whose Q-network\\nis based on radial basis functions. The approach can be more easily constructed\\ncompared to classic DRL models based on neural networks, thus alleviating the\\ncomputational burden of hyperparameter tuning. Through a series of simulation\\nexperiments, the superior performance of this approach is demonstrated compared\\nto the simple base-stock policy, producing a better policy in the multi-echelon\\nsystem and competitive performance in the serial system where the base-stock\\npolicy is optimal. In addition, the approach outperforms current DRL\\napproaches. Herein, we propose a novel dataset distillation method for constructing small\\ninformative datasets that preserve the information of the large original\\ndatasets. The development of deep learning models is enabled by the\\navailability of large-scale datasets. Despite unprecedented success,\\nlarge-scale datasets considerably increase the storage and transmission costs,\\nresulting in a cumbersome model training process. Moreover, using raw data for\\ntraining raises privacy and copyright concerns. To address these issues, a new\\ntask named dataset distillation has been introduced, aiming to synthesize a\\ncompact dataset that retains the essential information from the large original\\ndataset. State-of-the-art (SOTA) dataset distillation methods have been\\nproposed by matching gradients or network parameters obtained during training\\non real and synthetic datasets. The contribution of different network\\nparameters to the distillation process varies, and uniformly treating them\\nleads to degraded distillation performance. Based on this observation, we\\npropose an importance-aware adaptive dataset distillation (IADD) method that\\ncan improve distillation performance by automatically assigning importance\\nweights to different network parameters during distillation, thereby\\nsynthesizing more robust distilled datasets. IADD demonstrates superior\\nperformance over other SOTA dataset distillation methods based on parameter\\nmatching on multiple benchmark datasets and outperforms them in terms of\\ncross-architecture generalization. In addition, the analysis of self-adaptive\\nweights demonstrates the effectiveness of IADD. Furthermore, the effectiveness\\nof IADD is validated in a real-world medical application such as COVID-19\\ndetection. BERT (Bidirectional Encoder Representations from Transformers) has\\nrevolutionized the field of natural language processing through its exceptional\\nperformance on numerous tasks. Yet, the majority of researchers have mainly\\nconcentrated on enhancements related to the model structure, such as relative\\nposition embedding and more efficient attention mechanisms. Others have delved\\ninto pretraining tricks associated with Masked Language Modeling, including\\nwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's\\nencoder model for pretraining, proving to be highly effective. We argue that\\nthe design and research around enhanced masked language modeling decoders have\\nbeen underappreciated. In this paper, we propose several designs of enhanced\\ndecoders and introduce DrBERT (Decoder-refined BERT), a novel method for\\nmodeling training. Typically, a pretrained BERT model is fine-tuned for\\nspecific Natural Language Understanding (NLU) tasks. In our approach, we\\nutilize the original BERT model as the encoder, making only changes to the\\ndecoder without altering the encoder. This approach does not necessitate\\nextensive modifications to the model's architecture and can be seamlessly\\nintegrated into existing fine-tuning pipelines and services, offering an\\nefficient and effective enhancement strategy. Compared to other methods, while\\nwe also incur a moderate training cost for the decoder during the pretraining\\nprocess, our approach does not introduce additional training costs during the\\nfine-tuning phase. We test multiple enhanced decoder structures after\\npretraining and evaluate their performance on the GLUE benchmark. Our results\\ndemonstrate that DrBERT, having only undergone subtle refinements to the model\\nstructure during pretraining, significantly enhances model performance without\\nescalating the inference time and serving budget. Detecting diffusion-generated images has recently grown into an emerging\\nresearch area. Existing diffusion-based datasets predominantly focus on general\\nimage generation. However, facial forgeries, which pose a more severe social\\nrisk, have remained less explored thus far. To address this gap, this paper\\nintroduces DiFF, a comprehensive dataset dedicated to face-focused\\ndiffusion-generated images. DiFF comprises over 500,000 images that are\\nsynthesized using thirteen distinct generation methods under four conditions.\\nIn particular, this dataset leverages 30,000 carefully collected textual and\\nvisual prompts, ensuring the synthesis of images with both high fidelity and\\nsemantic consistency. We conduct extensive experiments on the DiFF dataset via\\na human test and several representative forgery detection methods. The results\\ndemonstrate that the binary detection accuracy of both human observers and\\nautomated detectors often falls below 30%, shedding light on the challenges in\\ndetecting diffusion-generated facial forgeries. Furthermore, we propose an edge\\ngraph regularization approach to effectively enhance the generalization\\ncapability of existing detectors. Solutions to Markov Decision Processes (MDP) are often very sensitive to\\nstate transition probabilities. As the estimation of these probabilities is\\noften inaccurate in practice, it is important to understand when and how\\nReinforcement Learning (RL) agents generalize when transition probabilities\\nchange. Here we present a new methodology to evaluate such generalization of RL\\nagents under small shifts in the transition probabilities. Specifically, we\\nevaluate agents in new environments (MDPs) in the vicinity of the training MDP\\ncreated by adding quantifiable, parametric noise into the transition function\\nof the training MDP. We refer to this process as Noise Injection, and the\\nresulting environments as $\\\\delta$-environments. This process allows us to\\ncreate controlled variations of the same environment with the level of the\\nnoise serving as a metric of distance between environments. Conventional wisdom\\nsuggests that training and testing on the same MDP should yield the best\\nresults. However, we report several cases of the opposite -- when targeting a\\nspecific environment, training the agent in an alternative noise setting can\\nyield superior outcomes. We showcase this phenomenon across $60$ different\\nvariations of ATARI games, including PacMan, Pong, and Breakout. Multipanel images, commonly seen as web screenshots, posters, etc., pervade\\nour daily lives. These images, characterized by their composition of multiple\\nsubfigures in distinct layouts, effectively convey information to people.\\nToward building advanced multimodal AI applications, such as agents that\\nunderstand complex scenes and navigate through webpages, the skill of\\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\\nmodels in this regard is important. Therefore, our paper introduces Multipanel\\nVisual Question Answering (MultipanelVQA), a novel benchmark that specifically\\nchallenges models in comprehending multipanel images. The benchmark comprises\\n6,600 questions and answers related to multipanel images. While these questions\\nare straightforward for average humans, achieving nearly perfect correctness,\\nthey pose significant challenges to the state-of-the-art Large Vision Language\\nModels (LVLMs) we tested. In our study, we utilized synthetically curated\\nmultipanel images specifically designed to isolate and evaluate the impact of\\ndiverse factors on model performance, revealing the sensitivity of LVLMs to\\nvarious interferences in multipanel images, such as adjacent subfigures and\\nlayout complexity. As a result, MultipanelVQA highlights the need and direction\\nfor improving LVLMs' ability to understand complex visual-language contexts.\\nCode and data are released at https://sites.google.com/view/multipanelvqa/home. In this paper, the LCVO modular method is proposed for the Visual Question\\nAnswering (VQA) Grounding task in the vision-language multimodal domain. This\\napproach relies on a frozen large language model (LLM) as intermediate mediator\\nbetween the off-the-shelf VQA model and the off-the-shelf Open-Vocabulary\\nObject Detection (OVD) model, where the LLM transforms and conveys textual\\ninformation between the two modules based on a designed prompt. LCVO establish\\nan integrated plug-and-play framework without the need for any pre-training\\nprocess. This framework can be deployed for VQA Grounding tasks under low\\ncomputational resources. The modularized model within the framework allows\\napplication with various state-of-the-art pre-trained models, exhibiting\\nsignificant potential to be advance with the times. Experimental\\nimplementations were conducted under constrained computational and memory\\nresources, evaluating the proposed method's performance on benchmark datasets\\nincluding GQA, CLEVR, and VizWiz-VQA-Grounding. Comparative analyses with\\nbaseline methods demonstrate the robust competitiveness of LCVO. This position paper presents a theoretical framework for enhancing\\nexplainable artificial intelligence (xAI) through emergent communication\\n(EmCom), focusing on creating a causal understanding of AI model outputs. We\\nexplore the novel integration of EmCom into AI systems, offering a paradigm\\nshift from conventional associative relationships between inputs and outputs to\\na more nuanced, causal interpretation. The framework aims to revolutionize how\\nAI processes are understood, making them more transparent and interpretable.\\nWhile the initial application of this model is demonstrated on synthetic data,\\nthe implications of this research extend beyond these simple applications. This\\ngeneral approach has the potential to redefine interactions with AI across\\nmultiple domains, fostering trust and informed decision-making in healthcare\\nand in various sectors where AI's decision-making processes are critical. The\\npaper discusses the theoretical underpinnings of this approach, its potential\\nbroad applications, and its alignment with the growing need for responsible and\\ntransparent AI systems in an increasingly digital world. When training data is scarce, it is common to make use of a feature extractor\\nthat has been pre-trained on a large base dataset, either by fine-tuning its\\nparameters on the ``target'' dataset or by directly adopting its representation\\nas features for a simple classifier. Fine-tuning is ineffective for few-shot\\nlearning, since the target dataset contains only a handful of examples.\\nHowever, directly adopting the features without fine-tuning relies on the base\\nand target distributions being similar enough that these features achieve\\nseparability and generalization. This paper investigates whether better\\nfeatures for the target dataset can be obtained by training on fewer base\\nclasses, seeking to identify a more useful base dataset for a given task.We\\nconsider cross-domain few-shot image classification in eight different domains\\nfrom Meta-Dataset and entertain multiple real-world settings (domain-informed,\\ntask-informed and uninformed) where progressively less detail is known about\\nthe target task. To our knowledge, this is the first demonstration that\\nfine-tuning on a subset of carefully selected base classes can significantly\\nimprove few-shot learning. Our contributions are simple and intuitive methods\\nthat can be implemented in any few-shot solution. We also give insights into\\nthe conditions in which these solutions are likely to provide a boost in\\naccuracy. We release the code to reproduce all experiments from this paper on\\nGitHub. https://github.com/RafLaf/Few-and-Fewer.git Although neural models have achieved remarkable performance, they still\\nencounter doubts due to the intransparency. To this end, model prediction\\nexplanation is attracting more and more attentions. However, current methods\\nrarely incorporate external knowledge and still suffer from three limitations:\\n(1) Neglecting concept completeness. Merely selecting concepts may not\\nsufficient for prediction. (2) Lacking concept fusion. Failure to merge\\nsemantically-equivalent concepts. (3) Difficult in manipulating model behavior.\\nLack of verification for explanation on original model. To address these\\nissues, we propose a novel knowledge-aware neuron interpretation framework to\\nexplain model predictions for image scene classification. Specifically, for\\nconcept completeness, we present core concepts of a scene based on knowledge\\ngraph, ConceptNet, to gauge the completeness of concepts. Our method,\\nincorporating complete concepts, effectively provides better prediction\\nexplanations compared to baselines. Furthermore, for concept fusion, we\\nintroduce a knowledge graph-based method known as Concept Filtering, which\\nproduces over 23% point gain on neuron behaviors for neuron interpretation. At\\nlast, we propose Model Manipulation, which aims to study whether the core\\nconcepts based on ConceptNet could be employed to manipulate model behavior.\\nThe results show that core concepts can effectively improve the performance of\\noriginal model by over 26%. For software that relies on machine-learned functionality, model selection is\\nkey to finding the right model for the task with desired performance\\ncharacteristics. Evaluating a model requires developers to i) select from many\\nmodels (e.g. the Hugging face model repository), ii) select evaluation metrics\\nand training strategy, and iii) tailor trade-offs based on the problem domain.\\nHowever, current evaluation approaches are either ad-hoc resulting in\\nsub-optimal model selection or brute force leading to wasted compute. In this\\nwork, we present \\\\toolname, a novel tool to automatically select and evaluate\\nmodels based on the application scenario provided in natural language. We\\nleverage the reasoning capabilities of large language models to propose a\\ntraining strategy and extract desired trade-offs from a problem description.\\n\\\\toolname~features a resource-efficient experimentation engine that integrates\\nconstraints and trade-offs based on the problem into the model selection\\nprocess. Our preliminary evaluation demonstrates that \\\\toolname{} is both\\nefficient and accurate compared to ad-hoc evaluations and brute force. This\\nwork presents an important step toward energy-efficient tools to help reduce\\nthe environmental impact caused by the growing demand for software with\\nmachine-learned functionality. Conducting real road testing for autonomous driving algorithms can be\\nexpensive and sometimes impractical, particularly for small startups and\\nresearch institutes. Thus, simulation becomes an important method for\\nevaluating these algorithms. However, the availability of free and open-source\\nsimulators is limited, and the installation and configuration process can be\\ndaunting for beginners and interdisciplinary researchers. We introduce an\\nautonomous driving simulator with photorealistic scenes, meanwhile keeping a\\nuser-friendly workflow. The simulator is able to communicate with external\\nalgorithms through ROS2 or Socket.IO, making it compatible with existing\\nsoftware stacks. Furthermore, we implement a highly accurate vehicle dynamics\\nmodel within the simulator to enhance the realism of the vehicle's physical\\neffects. The simulator is able to serve various functions, including generating\\nsynthetic data and driving with machine learning-based algorithms. Moreover, we\\nprioritize simplicity in the deployment process, ensuring that beginners find\\nit approachable and user-friendly. Despite the remarkable empirical successes of Generative Adversarial Networks\\n(GANs), the theoretical guarantees for their statistical accuracy remain rather\\npessimistic. In particular, the data distributions on which GANs are applied,\\nsuch as natural images, are often hypothesized to have an intrinsic\\nlow-dimensional structure in a typically high-dimensional feature space, but\\nthis is often not reflected in the derived rates in the state-of-the-art\\nanalyses. In this paper, we attempt to bridge the gap between the theory and\\npractice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),\\nby deriving statistical guarantees on the estimated densities in terms of the\\nintrinsic dimension of the data and the latent space. We analytically show that\\nif one has access to $n$ samples from the unknown target distribution and the\\nnetwork architectures are properly chosen, the expected Wasserstein-1 distance\\nof the estimates from the target scales as $O\\\\left( n^{-1/d_\\\\mu } \\\\right)$ for\\nGANs and $O\\\\left( n^{-1/(d_\\\\mu+\\\\ell)} \\\\right)$ for BiGANs, where $d_\\\\mu$ and\\n$\\\\ell$ are the upper Wasserstein-1 dimension of the data-distribution and\\nlatent-space dimension, respectively. The theoretical analyses not only suggest\\nthat these methods successfully avoid the curse of dimensionality, in the sense\\nthat the exponent of $n$ in the error rates does not depend on the data\\ndimension but also serve to bridge the gap between the theoretical analyses of\\nGANs and the known sharp rates from optimal transport literature. Additionally,\\nwe demonstrate that GANs can effectively achieve the minimax optimal rate even\\nfor non-smooth underlying distributions, with the use of larger generator\\nnetworks. With the increasing need for inclusive and user-friendly technology, web\\naccessibility is crucial to ensuring equal access to online content for\\nindividuals with disabilities, including visual, auditory, cognitive, or motor\\nimpairments. Despite the existence of accessibility guidelines and standards\\nsuch as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility\\nInitiative (W3C), over 90\\\\% of websites still fail to meet the necessary\\naccessibility requirements. For web users with disabilities, there exists a\\nneed for a tool to automatically fix web page accessibility errors. While\\nresearch has demonstrated methods to find and target accessibility errors, no\\nresearch has focused on effectively correcting such violations. This paper\\npresents a novel approach to correcting accessibility violations on the web by\\nmodifying the document object model (DOM) in real time with foundation models.\\nLeveraging accessibility error information, large language models (LLMs), and\\nprompt engineering techniques, we achieved greater than a 51\\\\% reduction in\\naccessibility violation errors after corrections on our novel benchmark:\\nACCESS. Our work demonstrates a valuable approach toward the direction of\\ninclusive web content, and provides directions for future research to explore\\nadvanced methods to automate web accessibility. Despite the widespread use of k-means time series clustering in various\\ndomains, there exists a gap in the literature regarding its comprehensive\\nevaluation with different time series normalization approaches. This paper\\nseeks to fill this gap by conducting a thorough performance evaluation of\\nk-means time series clustering on real-world open-source time series datasets.\\nThe evaluation focuses on two distinct normalization techniques:\\nz-normalization and NP-Free. The former is one of the most commonly used\\nnormalization approach for time series. The latter is a real-time time series\\nrepresentation approach, which can serve as a time series normalization\\napproach. The primary objective of this paper is to assess the impact of these\\ntwo normalization techniques on k-means time series clustering in terms of its\\nclustering quality. The experiments employ the silhouette score, a\\nwell-established metric for evaluating the quality of clusters in a dataset. By\\nsystematically investigating the performance of k-means time series clustering\\nwith these two normalization techniques, this paper addresses the current gap\\nin k-means time series clustering evaluation and contributes valuable insights\\nto the development of time series clustering. Physiological fatigue, a state of reduced cognitive and physical performance\\nresulting from prolonged mental or physical exertion, poses significant\\nchallenges in various domains, including healthcare, aviation, transportation,\\nand industrial sectors. As the understanding of fatigue's impact on human\\nperformance grows, there is a growing interest in developing effective fatigue\\nmonitoring techniques. Among these techniques, electroencephalography (EEG) has\\nemerged as a promising tool for objectively assessing physiological fatigue due\\nto its non-invasiveness, high temporal resolution, and sensitivity to neural\\nactivity. This paper aims to provide a comprehensive analysis of the current\\nstate of the use of EEG for monitoring physiological fatigue. Augmented reality for laparoscopic liver resection is a visualisation mode\\nthat allows a surgeon to localise tumours and vessels embedded within the liver\\nby projecting them on top of a laparoscopic image. Preoperative 3D models\\nextracted from CT or MRI data are registered to the intraoperative laparoscopic\\nimages during this process. In terms of 3D-2D fusion, most of the algorithms\\nmake use of anatomical landmarks to guide registration. These landmarks include\\nthe liver's inferior ridge, the falciform ligament, and the occluding contours.\\nThey are usually marked by hand in both the laparoscopic image and the 3D\\nmodel, which is time-consuming and may contain errors if done by a\\nnon-experienced user. Therefore, there is a need to automate this process so\\nthat augmented reality can be used effectively in the operating room. We\\npresent the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge\\n(P2ILF), held during the Medical Imaging and Computer Assisted Interventions\\n(MICCAI 2022) conference, which investigates the possibilities of detecting\\nthese landmarks automatically and using them in registration. The challenge was\\ndivided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D\\nregistration task. The teams were provided with training data consisting of 167\\nlaparoscopic images and 9 preoperative 3D models from 9 patients, with the\\ncorresponding 2D and 3D landmark annotations. A total of 6 teams from 4\\ncountries participated, whose proposed methods were evaluated on 16 images and\\ntwo preoperative 3D models from two patients. All the teams proposed deep\\nlearning-based methods for the 2D and 3D landmark segmentation tasks and\\ndifferentiable rendering-based methods for the registration task. Based on the\\nexperimental outcomes, we propose three key hypotheses that determine current\\nlimitations and future directions for research in this domain. Improving the efficiency of state-of-the-art methods in semantic segmentation\\nrequires overcoming the increasing computational cost as well as issues such as\\nfusing semantic information from global and local contexts. Based on the recent\\nsuccess and problems that convolutional neural networks (CNNs) encounter in\\nsemantic segmentation, this research proposes an encoder-decoder architecture\\nwith a unique efficient residual network. Attention-boosting gates (AbGs) and\\nattention-boosting modules (AbMs) are deployed by aiming to fuse the\\nfeature-based semantic information with the global context of the efficient\\nresidual network in the encoder. Respectively, the decoder network is developed\\nwith the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are\\ndesigned to improve the efficiency in the one-to-one conversion of the semantic\\ninformation by deploying additional convolution layers in the decoder part. Our\\nnetwork is tested on the challenging CamVid and Cityscapes datasets, and the\\nproposed methods reveal significant improvements on the existing baselines,\\nsuch as ResNet-50. To the best of our knowledge, the developed network,\\nSERNet-Former, achieves state-of-the-art results (84.62 % mean IoU) on CamVid\\ndataset and challenging results (87.35 % mean IoU) on Cityscapes validation\\ndataset. This paper presents LLM4SecHW, a novel framework for hardware debugging that\\nleverages domain specific Large Language Model (LLM). Despite the success of\\nLLMs in automating various software development tasks, their application in the\\nhardware security domain has been limited due to the constraints of commercial\\nLLMs and the scarcity of domain specific data. To address these challenges, we\\npropose a unique approach to compile a dataset of open source hardware design\\ndefects and their remediation steps, utilizing version control data. This\\ndataset provides a substantial foundation for training machine learning models\\nfor hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this\\ndataset, enabling the identification and rectification of bugs in hardware\\ndesigns. This pioneering approach offers a reference workflow for the\\napplication of fine tuning domain specific LLMs in other research areas. We\\nevaluate the performance of our proposed system on various open source hardware\\ndesigns, demonstrating its efficacy in accurately identifying and correcting\\ndefects. Our work brings a new perspective on automating the quality control\\nprocess in hardware design. The Deep Learning revolution has enabled groundbreaking achievements in\\nrecent years. From breast cancer detection to protein folding, deep learning\\nalgorithms have been at the core of very important advancements. However, these\\nmodern advancements are becoming more and more data-hungry, especially on\\nlabeled data whose availability is scarce: this is even more prevalent in the\\nmedical context. In this work, we show how active learning could be very\\neffective in data scarcity situations, where obtaining labeled data (or\\nannotation budget is very limited). We compare several selection criteria\\n(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the\\neffect of acquired pool size on the model's performance. Our results suggest\\nthat uncertainty is useful to the Melanoma detection task, and confirms the\\nhypotheses of the author of the paper of interest, that \\\\textit{bald} performs\\non average better than other acquisition functions. Our extended analyses\\nhowever revealed that all acquisition functions perform badly on the positive\\n(cancerous) samples, suggesting exploitation of class unbalance, which could be\\ncrucial in real-world settings. We finish by suggesting future work directions\\nthat would be useful to improve this current work. The code of our\\nimplementation is open-sourced at\\n\\\\url{https://github.com/bonaventuredossou/ece526_course_project} The advancement of transformer neural networks has significantly elevated the\\ncapabilities of sentence similarity models, particularly in creating effective\\nvector representations of natural language inputs. However, these models face\\nnotable challenges in domain-specific contexts, especially in highly\\nspecialized scientific sub-fields. Traditional methods often struggle in this\\nregime, either overgeneralizing similarities within a niche or being overly\\nsensitive to minor differences, resulting in inaccurate text classification and\\nsubpar vector representation. In an era where retrieval augmentation and search\\nare increasingly crucial, precise and concise numerical representations are\\nessential. In this paper, we target this issue by assembling niche datasets\\nusing co-citations as a similarity metric, focusing on biomedical domains. We\\nemploy two key strategies for fine-tuning state-of-the-art models: 1.\\nDomain-specific Fine-Tuning, which tailors pretrained models to a single\\ndomain, and 2. Universal Applicability with Mixture of Experts (MoE), adapting\\npretrained models with enforced routing for multiple domains simultaneously.\\nOur training approach emphasizes the use of abstracts for faster training,\\nincorporating Multiple Negative Rankings loss for efficient contrastive\\nlearning. Notably, our MoE variants, equipped with $N$ experts, achieve the\\nefficacy of $N$ individual models, heralding a new era of versatile,\\nOne-Size-Fits-All transformer networks for various tasks. This methodology\\nmarks significant advancements in scientific text classification metrics and\\nholds promise for enhancing vector database search and compilation. The paper presents an approach to the modelling of epistemic uncertainty in\\nConjunction Data Messages (CDM) and the classification of conjunction events\\naccording to the confidence in the probability of collision. The approach\\nproposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence\\nand starts from the assumption that the observed CDMs are drawn from a family\\nof unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is\\nused to construct robust bounds on such a family of unknown distributions\\nstarting from a time series of CDMs. A DSt structure is then derived from the\\nprobability boxes constructed with DKW inequality. The DSt structure\\nencapsulates the uncertainty in the CDMs at every point along the time series\\nand allows the computation of the belief and plausibility in the realisation of\\na given probability of collision. The methodology proposed in this paper is\\ntested on a number of real events and compared against existing practices in\\nthe European and French Space Agencies. A health crisis is raging all over the world with the rapid transmission of\\nthe novel-coronavirus disease (Covid-19). Out of the guidelines issued by the\\nWorld Health Organisation (WHO) to protect us against Covid-19, wearing a\\nfacemask is the most effective. Many countries have necessitated the wearing of\\nface masks, but monitoring a large number of people to ensure that they are\\nwearing masks in a crowded place is a challenging task in itself. The\\nnovel-coronavirus disease (Covid-19) has already affected our day-to-day life\\nas well as world trade movements. By the end of April 2021, the world has\\nrecorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19)\\nincluding 3,066,113 deaths according to the world health organization (WHO).\\nThese increasing numbers motivate automated techniques for the detection of a\\nfacemask in real-time scenarios for the prevention of Covid-19. We propose a\\ntechnique using deep learning that works for single and multiple people in a\\nframe recorded via webcam in still or in motion. We have also experimented with\\nour approach in night light. The accuracy of our model is good compared to the\\nother approaches in the literature; ranging from 74% for multiple people in a\\nnightlight to 99% for a single person in daylight. Although large language models (LLMs) have demonstrated adeptness in a range\\nof tasks, they still lag behind human learning efficiency. This disparity is\\noften linked to the inherent human capacity to learn from basic examples,\\ngradually generalize and handle more complex problems, and refine their skills\\nwith continuous feedback. Inspired by this, this paper introduces YODA, a novel\\nteacher-student progressive learning framework that emulates the\\nteacher-student education process to improve the efficacy of model fine-tuning.\\nThe framework operates on an interactive \\\\textit{basic-generalized-harder}\\nloop. The teacher agent provides tailored feedback on the student's answers,\\nand systematically organizes the education process. This process unfolds by\\nteaching the student basic examples, reinforcing understanding through\\ngeneralized questions, and then enhancing learning by posing questions with\\nprogressively enhanced complexity. With the teacher's guidance, the student\\nlearns to iteratively refine its answer with feedback, and forms a robust and\\ncomprehensive understanding of the posed questions. The systematic procedural\\ndata, which reflects the progressive learning process of humans, is then\\nutilized for model training. Taking math reasoning as a testbed, experiments\\nshow that training LLaMA2 with data from YODA improves SFT with significant\\nperformance gain (+17.01\\\\% on GSM8K and +9.98\\\\% on MATH). In addition, we find\\nthat training with curriculum learning further improves learning robustness. Kilometer-scale modeling of global atmosphere dynamics enables fine-grained\\nweather forecasting and decreases the risk of disastrous weather and climate\\nactivity. Therefore, building a kilometer-scale global forecast model is a\\npersistent pursuit in the meteorology domain. Active international efforts have\\nbeen made in past decades to improve the spatial resolution of numerical\\nweather models. Nonetheless, developing the higher resolution numerical model\\nremains a long-standing challenge due to the substantial consumption of\\ncomputational resources. Recent advances in data-driven global weather\\nforecasting models utilize reanalysis data for model training and have\\ndemonstrated comparable or even higher forecasting skills than numerical\\nmodels. However, they are all limited by the resolution of reanalysis data and\\nincapable of generating higher-resolution forecasts. This work presents\\nFengWu-GHR, the first data-driven global weather forecasting model running at\\nthe 0.09$^{\\\\circ}$ horizontal resolution. FengWu-GHR introduces a novel\\napproach that opens the door for operating ML-based high-resolution forecasts\\nby inheriting prior knowledge from a pretrained low-resolution model. The\\nhindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to\\nthe IFS-HRES. Furthermore, evaluations on station observations and case studies\\nof extreme events support the competitive operational forecasting skill of\\nFengWu-GHR at the high resolution. Over the past decade, automated methods have been developed to detect cracks\\nmore efficiently, accurately, and objectively, with the ultimate goal of\\nreplacing conventional manual visual inspection techniques. Among these\\nmethods, semantic segmentation algorithms have demonstrated promising results\\nin pixel-wise crack detection tasks. However, training such data-driven\\nalgorithms requires a large amount of human-annotated datasets with pixel-level\\nannotations, which is a highly labor-intensive and time-consuming process.\\nMoreover, supervised learning-based methods often struggle with poor\\ngeneralization ability in unseen datasets. Therefore, we propose an\\nunsupervised pixel-wise road crack detection network, known as UP-CrackNet. Our\\napproach first generates multi-scale square masks and randomly selects them to\\ncorrupt undamaged road images by removing certain regions. Subsequently, a\\ngenerative adversarial network is trained to restore the corrupted regions by\\nleveraging the semantic context learned from surrounding uncorrupted regions.\\nDuring the testing phase, an error map is generated by calculating the\\ndifference between the input and restored images, which allows for pixel-wise\\ncrack detection. Our comprehensive experimental results demonstrate that\\nUP-CrackNet outperforms other general-purpose unsupervised anomaly detection\\nalgorithms, and exhibits comparable performance and superior generalizability\\nwhen compared with state-of-the-art supervised crack segmentation algorithms.\\nOur source code is publicly available at mias.group/UP-CrackNet. This thesis comprises the first three chapters dedicated to providing an\\noverview of Gamma Ray-Bursts (GRBs), their properties, the instrumentation used\\nto detect them, and Artificial Intelligence (AI) applications in the context of\\nGRBs, including a literature review and future prospects. Considering both the\\ncurrent and the next generation of high X-ray monitors, such as Fermi-GBM and\\nHERMES Pathfinder (an in-orbit demonstration of six 3U nano-satellites), the\\nresearch question revolves around the detection of long and faint high-energy\\ntransients, potentially GRBs, that might have been missed by previous detection\\nalgorithms. To address this, two chapters introduce a new data-driven\\nframework, DeepGRB.\\n  In Chapter 4, a Neural Network (NN) is described for background count rate\\nestimation for X/gamma-ray detectors, providing a performance evaluation in\\ndifferent periods, including both solar maxima, solar minima periods, and one\\ncontaining an ultra-long GRB. The application of eXplainable Artificial\\nIntelligence (XAI) is performed for global and local feature importance\\nanalysis to better understand the behavior of the NN.\\n  Chapter 5 employs FOCuS-Poisson for anomaly detection in count rate\\nobservations and estimation from the NN. DeepGRB demonstrates its capability to\\nprocess Fermi-GBM data, confirming cataloged events and identifying new ones,\\nproviding further analysis with estimates for localization, duration, and\\nclassification. The chapter concludes with an automated classification method\\nusing Machine Learning techniques that incorporates XAI for eventual bias\\nidentification.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}],"source":["# Group the summaries by their keywords and concatenate the summaries in each group\n","dfg = df.groupby('keyword')['summary'].apply(' '.join).reset_index()\n","\n","# Display the first few grouped summaries to check the concatenation\n","dfg.head()"]},{"cell_type":"markdown","metadata":{"id":"Q8eAIjuiRDmJ"},"source":["## Text Summarization"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"udLUhEu4cQTT","colab":{"base_uri":"https://localhost:8080/","height":504,"referenced_widgets":["cc02c7b472ac46b29b348f3020012c9a","5e97872a76de4271b727f8a78593f4d5","30d33f19e39442eb90d63a24e71f40de","c5ee27338a3d47429b36acad1c385608","1d181e0bbd1046dc910538407a02ea60","de57e6276bbb44c5a4efbdb8a4e0fb08","f6d5170173864c43bcbc2d88d2bc36e5","f05ffaa8a85a441f8985b6bfe1148add","5e69c198f6fa4a8594325e4b1b8e9173","499cee988c7b4968a79a5c11e92e9658","38c99b2073b24006be124a7c7d34b3c6","de073607f554462383fa5fa60b3f6d2c","69792dd6b1be475cacab0ae3a20d66bc","a485937a0a9f4b03813862b99e5b16ea","9badc085c0bc46cebf812d3073555cf4","9da84f6ffa2047efaeb4fa1bceaa3a14","9f3fe455d7aa4642a41eae4341149fc7","c32901aaf43840ec8908815edc673cef","1ca4977bbfc4432da4f2b225494bfe2f","f47afba8f55b4698bb137591163a2d1d","28002d4d00a34707a82163bbf2b8e43f","2ff46154a9164c98b1eb44e730645c65","b506c9ed77b5419ca9ddccc7afc2a3d5","26d8546325964e1fb99817beb02b96df","8d85a403f5b445e58287e79641d55464","b7c7df7a3a5348b2abb5172d26cca15e","719ee74f798e4d28b395086a967a2e8d","42979e0ebd464373be6ec9eda5500a03","041116e0d6684bfe87f1e7e42edc0876","e43f437e561b47bcaced0bd81751eb91","6e1be574536c4f6c9e2711ec465c1369","ddb218caab8d48a386baf1a663ea53e6","dc6e6df88fd440f2a53bba68ec3ec11c","3c882052b7594b8583d39058e123e195","26a3d6b97b854286b06091e315034081","42331376d221485c9e4020e91ab9b1df","dd165c4b5ef84d6f87665f909843fcfa","10d65a12162c47ee91d048c96858de36","f43f90ed4e8c4e50b6b71e6df5befd9d","67c0a8dd3ce04084bdebcd885b7645d4","0301251ebf424dd4a4f0577d52fd4e17","004e84bce9fc47a0aaad7c415c7896c7","45d8bfd8d0f94e31bdb712b17f86b475","76bf02c48d5540a297772a6522d91bd7","a27b1a6cab1b44bc821f8b62bd4810c9","fc4fdd17d1394c5e9c141684041c4b29","afcc82dfcd0848b093607ae49431ce36","17042715442141ada04a2cb2012e8b38","1093e0baec3449abb72994ba68c0f4c4","4d551e5b3f59453c8cae72dda9fd7f34","f3e1c487d5554107be0c1a072a3941fa","f980d001c25942f8922b05f928ca1329","3a5bec70840e4c0db3dbef4ca6aa54e7","a990731e9c1f4aa4aef3b9abb0d4426f","ee55848a11554d8db0e7c03780caf147","d4b47e660ab248f5a8a70b17d242e893","26690985ef894271a817cca6d9c46fc7","461aba4120654f80ae1d35f2c741d32c","1035bab296a94aea8171c188d2cae19b","01c2b2a536f249c89af2bf9d9d198b5a","bdea11be3d2342b2b6c22c4784a8edb6","8a2932375394426592dee1419b2752c0","ff3c82e4b85a4fc093a0e34e1c72ccf4","aeb4106473e24565a3fc802e3f169437","44c16520df8d4ead9b6bac890f8d62c0","2b195b8ed7454fd9aa38da874b8771de","475df9b03def4f1e8ffdf6bba7bb2d9b","5a4f303582e14233a09f8c71fff955cb","f22ec414cf2a4ea5a3e50b0467768197","689f4178d9234ac9a4d15807b5fe7c9c","90542c38639f481f9de828db0618de23","7d8605f751aa4ab29f7521612a10323f","c047419fe8ca4bc5ab31117ade4974f0","dce5022d1313470787d14b6d95745b48","7bffdb273afa462eb407e6d21507f2a4","146859c0fcbb4b5195edf8457014040f","3570267360e54aa0b1a5a517505e3999"]},"executionInfo":{"status":"ok","timestamp":1708913109562,"user_tz":300,"elapsed":400240,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"9d5420b3-f4fe-47dc-a486-2dd56c8eb784"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc02c7b472ac46b29b348f3020012c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de073607f554462383fa5fa60b3f6d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b506c9ed77b5419ca9ddccc7afc2a3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c882052b7594b8583d39058e123e195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27b1a6cab1b44bc821f8b62bd4810c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b47e660ab248f5a8a70b17d242e893"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475df9b03def4f1e8ffdf6bba7bb2d9b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Enter a keyword to summarize its research concepts: reinforcement learning\n","\n","Generated Summary:\n","This paper presents a framework for building neural network-based learning models that can overcome compositional constraints, as well as other areas of research such as reinforcement learning, sentiment analysis, and natural language processing (NLP) and speech recognition, among others. Reinforcement learning (RL) is one of the most popular approaches for training machine learning algorithms, but its performance has been under scrutiny due to its high-dimensionality and the complexity involved in correlating visuotactile data with the dynamic environment. In this paper, we present Hi-Core, an artificial intelligence (AI) system designed to learn complex repetitive task (CRL) from a library of low-level policy knowledge (LPL). The treatment of methods, problems, and applications presented here is poised to inform and inspire practitioners and researchers to develop impactful data-driven sensing, optimization, and control applications. Safety is one of the key challenges facing intelligent vehicles in the next generation of self-driving vehicles (SDVs) in urban areas, where human error can be a major problem. The experimental results demonstrate that our SGADS can significantly improve safety performance, exhibit strongtrophic, and enhance the training efficiency of intelligent vehicles in complex urban scenarios compared to existing methods. In this work, we propose a physics-guided reinforcement learning (RL) approach to mitigate the effects of blackouts in real-time by (1) strategically moving transmission lines (i.e. Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. In this paper, we propose a reinforcement learning-based approach to train timetable rescheduling, which can achieve better performance for various problems with varying degrees of train delay and different scales when compared to state-of-the-art solvers. The learned decision model can achieve better performance for various problems with varying degrees of train delay and different scales when compared to handcrafted rules and state-of-the-art solvers. In this paper, we present a novel solution to the problem of selecting the best set of trucks for a given number of nodes in a supply chain by training them on a set of pre-trained nodes, and then fine-tuning the solution to match the set of pre-trained nodes. In this study, we introduce a new natural gradient policy type method, Natural Policy Gradient-HM (NPG-HM), to improve the performance of state-of-the-art policy models in diverse applications, such as climate predictions and weather forecasts. In this paper, we propose a reinforcement learning-based framework to improve the performance of ConvQA, a machine-learning model for problem-solving in Conversational Q&A ( ConvQA) with human beings, by learning question representations in question-answer pairs ( KG). In this paper, we introduce DuaLight, a novel scenario-based traffic management system, which learns how to manage traffic flow in real-world scenarios, and then learns how to manage traffic flow in synthetic scenarios, in order to reduce congestion and improve safety. Reinforcement learning (RL) is one of the most promising techniques for training artificial intelligence (AI) systems in areas such as natural language processing (NLP), machine learning (ML), and computer vision (CVR), as well as for training deep learning systems.\n"]}],"source":["from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","from tqdm import tqdm\n","\n","# Group the summaries by their keywords and concatenate the summaries in each group\n","df['keyword'] = df['keyword'].str.lower()  # Convert keywords to lowercase for case-insensitive matching\n","dfg = df.groupby('keyword')['summary'].apply(' '.join).reset_index()\n","\n","# Load the model and tokenizer\n","model_name = 'google/pegasus-xsum'\n","tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","model = PegasusForConditionalGeneration.from_pretrained(model_name)\n","\n","# Assuming prior steps (e.g., loading model, tokenizer, and DataFrame preparation)\n","\n","keyword_input = input(\"Enter a keyword to summarize its research concepts: \").lower()\n","\n","if keyword_input in dfg['keyword'].values:\n","    summary_texts = dfg[dfg['keyword'] == keyword_input]['summary'].iloc[0]\n","\n","    # If the text is too long, consider splitting it into manageable parts\n","    # Example: Splitting by sentences or paragraphs could be more meaningful\n","    parts = [summary_texts[i:i+50000] for i in range(0, len(summary_texts), 50000)]\n","\n","    summaries = []\n","    for part in parts:\n","        inputs = tokenizer.encode(\"summarize: \" + part, return_tensors=\"pt\", max_length=512, truncation=True)\n","        summary_ids = model.generate(inputs,\n","                                     max_length=120,\n","                                     min_length=50,\n","                                     length_penalty=2.5,\n","                                     num_beams=6,\n","                                     early_stopping=True)\n","        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","        summaries.append(summary)\n","\n","    # Combine summaries or handle them as needed\n","    final_summary = \" \".join(summaries)\n","    print(\"\\nGenerated Summary:\")\n","    print(final_summary)\n","else:\n","    print(\"Keyword not found in dataset. Please try another one.\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"aGgI4b6D1aFH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708913109562,"user_tz":300,"elapsed":6,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"fa444fa1-c70c-475e-de33-9fd4e7e83d7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Is summary redundant?: False\n"]}],"source":["def check_redundancy(summary):\n","    sentences = set()\n","    for sentence in summary.split('.'):\n","        if sentence in sentences:\n","            return True  # Redundancy found\n","        sentences.add(sentence)\n","    return False  # No redundancy\n","\n","is_redundant = check_redundancy(final_summary)\n","print(\"Is summary redundant?:\", is_redundant)"]},{"cell_type":"code","source":["pip install rouge-score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDokPgHiCAbw","executionInfo":{"status":"ok","timestamp":1708913118453,"user_tz":300,"elapsed":8893,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"5c95bc8c-84c5-4c28-ca26-333af8f37d04"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.2)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=a161514145fde1cd1bf811ef2c240338e711d7b38f76f97ff889ae14a6195fb7\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"KpqdDz962Qpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708913169581,"user_tz":300,"elapsed":51133,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"317e9d20-9372-4c15-b04e-1f1ea991facf"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROUGE-1 score: Score(precision=0.9595588235294118, recall=0.006421138091372057, fmeasure=0.012756909992912829)\n","ROUGE-2 score: Score(precision=0.6427255985267035, recall=0.004293112568117796, fmeasure=0.008529253629209638)\n","ROUGE-L score: Score(precision=0.7242647058823529, recall=0.004846606145595001, fmeasure=0.009628778806911216)\n"]}],"source":["\n","from rouge_score import rouge_scorer\n","\n","# Assuming 'df' is your DataFrame and 'final_summary' is the summary generated by your model\n","# Let's say your keyword is 'data science'\n","keyword = 'data science'\n","keyword_summaries = df[df['keyword'].str.lower() == keyword]['summary'].tolist()\n","\n","# Join the keyword-specific summaries into a single string to use as reference\n","reference_text = ' '.join(keyword_summaries)\n","\n","# Initialize the ROUGE scorer\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Compute ROUGE scores\n","scores = scorer.score(reference_text, final_summary)\n","\n","print(\"ROUGE-1 score:\", scores['rouge1'])\n","print(\"ROUGE-2 score:\", scores['rouge2'])\n","print(\"ROUGE-L score:\", scores['rougeL'])"]},{"cell_type":"markdown","source":["### Trend Analysis and Research Gap Identification"],"metadata":{"id":"j_j1NWNAYK4j"}},{"cell_type":"code","source":["pip install sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLUEI9uAi8YZ","executionInfo":{"status":"ok","timestamp":1708913182402,"user_tz":300,"elapsed":12835,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"15d2eb66-fa66-45cd-901c-3995aa92a8bc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/149.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.37.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Installing collected packages: sentence-transformers\n","Successfully installed sentence-transformers-2.4.0\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"id":"kqi91sYpAK6P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708914333823,"user_tz":300,"elapsed":705817,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"9054f758-f57b-476d-e715-36a5ed47ab4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter keyword 1 to identify niche concepts/research gaps: data science\n","Enter keyword 2 to identify niche concepts/research gaps: artificial intelligence\n","Enter keyword 3 to identify niche concepts/research gaps: reinforcement learning\n","Enter keyword 4 to identify niche concepts/research gaps: deep learning\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","from sklearn.ensemble import IsolationForest\n","import pandas as pd  # Assuming you have a DataFrame `df`\n","\n","# Load the pre-trained model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Initialize a dictionary to store the titles of anomalous papers\n","anomalous_titles_dict = {}\n","\n","# Number of keywords to input\n","num_keywords = 4\n","\n","for i in range(num_keywords):\n","    # Take user input for each keyword\n","    keyword_input = input(f\"Enter keyword {i+1} to identify niche concepts/research gaps: \").lower()\n","\n","    # Filter the DataFrame based on the keyword\n","    df_filtered = df[df['keyword'].str.lower() == keyword_input]\n","\n","    # Generate embeddings for the filtered summaries\n","    embeddings = model.encode(df_filtered['summary'].tolist(), convert_to_tensor=False)\n","\n","    # Apply Isolation Forest for anomaly detection\n","    clf = IsolationForest(n_estimators=13, random_state=42)\n","    anomalies = clf.fit_predict(embeddings)\n","\n","    # Identifying indexes of anomalies\n","    anomaly_indexes = [i for i, val in enumerate(anomalies) if val == -1]\n","\n","    # Extracting the titles of the papers identified as anomalies\n","    anomaly_titles = df_filtered.iloc[anomaly_indexes]['title'].tolist()\n","\n","    # Store results in dictionary\n","    anomalous_titles_dict[keyword_input] = anomaly_titles"]},{"cell_type":"code","source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text_list):\n","    preprocessed_text = []\n","    for text in text_list:\n","        words = word_tokenize(text.lower())  # Tokenize and convert to lower case\n","        filtered_words = [word for word in words if word not in stop_words and word.isalnum()]\n","        preprocessed_text.extend(filtered_words)\n","    return preprocessed_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVgzKYtkgHok","executionInfo":{"status":"ok","timestamp":1708914333824,"user_tz":300,"elapsed":19,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"b21950e8-7ff2-4262-f4f4-75620d6aaf1c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","for keyword, titles in anomalous_titles_dict.items():\n","    preprocessed_titles = preprocess_text(titles)\n","    text = ' '.join(preprocessed_titles)\n","    wordcloud = WordCloud(stopwords=stop_words, background_color='white', max_words=50).generate(text)\n","\n","    plt.figure(figsize=(7, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.title(f'Word Cloud for {keyword}')\n","    plt.show()\n","    print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bd6-wt8LnDdJ","executionInfo":{"status":"ok","timestamp":1708914390749,"user_tz":300,"elapsed":3160,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"b7546c76-88f0-4294-d029-e080e561de06"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 700x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x500 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjIAAAE5CAYAAACK48oHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9d5hdV33ojX92P71N7zOaUZcsWZKb3I0dU0wJYOL8EkpeAuQmgUC44V7IfS7kkptcbnJTSAIJvAnFb5IbeiAYbIxtsORuS7a6RqPpvZ1edlu/P/aZI41mRppRwTacjx8/knZba+29z17f9a2SEEJQpUqVKlWqVKnyKkR+uTtQpUqVKlWqVKlysVQFmSpVqlSpUqXKq5aqIFOlSpUqVapUedVSFWSqVKlSpUqVKq9aqoJMlSpVqlSpUuVVS1WQqVKlSpUqVaq8aqkKMlWqVKlSpUqVVy1VQaZKlSpVqlSp8qqlKshUqVKlSpUqVV61VAWZKq8qHnvsMSRJ4rHHHrus1+3s7OQ973nPZb3marntttu47bbbLnicbdt87GMfo62tDVmWectb3nLF+3alWOk53n///WzatAlN04jFYsDq78+5SJLEpz71qcvWt3P58pe/jCRJDAwMrLmNy8GnPvUpJElatO3c9/hK/V6qVHklURVkqizha1/7GpIk8e1vf3vJvh07diBJEo8++uiSfe3t7ezdu/dn0cVV09fXxwc+8AHWrVuHz+cjEolw44038td//dcUCoWXu3tr4p/+6Z/4sz/7M97+9rfzla98hY985CMvd5cuyOc+9zm+/OUvr+rY48eP8573vIfu7m6++MUv8oUvfOHKdu5l5OjRo3zqU5962YSgKlV+nlBf7g5UeeVx0003AbBv3z5++Zd/ubI9nU5z+PBhVFVl//793H777ZV9w8PDDA8Pc9999/3M+7sS3//+97n33nsxDIN3vetdbNu2DdM02bdvH3/wB3/AkSNHXlWT5SOPPEJLSwt/+Zd/+XJ3ZdV87nOfo7a2dom265ZbbqFQKKDremXbY489huu6/PVf/zU9PT2V7Q899NBFtV0oFFDVK/eJe+c738l9992HYRhrPvfo0aP80R/9EbfddhudnZ2Xv3NllrvPVar8vFEVZKosobm5ma6uLvbt27do+5NPPokQgnvvvXfJvoV/LwhBF4sQgmKxiN/vv6Tr9Pf3c99999HR0cEjjzxCU1NTZd/v/M7vcOrUKb7//e9fUhs/a6ampirmlsuB67qYponP57ts11wgn88TCARW3C/L8pJ2p6amAJaM8WIn4SsxrrNRFAVFUa5oG5fKcve5SpWfN6qmpSrLctNNN3HgwIFF5pf9+/ezdetWXve61/HUU0/huu6ifZIkceONNwKeP8enP/1puru7MQyDzs5OPvGJT1AqlRa109nZyT333MODDz7Inj178Pv9/MM//AMAIyMjvOUtbyEYDFJfX89HPvKRJeevxP/+3/+bbDbLP/7jPy4SYhbo6enh937v9857jdOnT3PvvfeSSCQIBAJcf/31S4SflfwkVvJN+MIXvkB3dzd+v59rr72Wxx9//IJjGRgYqJjzjhw5giRJi66dy+X46Ec/SltbG4ZhsHHjRv78z/+ccwvbS5LE7/7u7/LP//zPbN26FcMw+OEPf7hiu//+7//OG97wBpqbmzEMg+7ubj796U/jOM6i42677Ta2bdvG888/zy233EIgEOATn/gEnZ2dHDlyhJ/85CeVPi/4upx7fzo7O/nkJz8JQF1d3SL/luV8ZIrFIp/61KfYsGEDPp+PpqYm3vrWt9LX17dovGf7yAwODvLbv/3bbNy4Eb/fT01NDffee+9Fm3eWe/YL7/O+ffu49tpr8fl8rFu3jq9+9auLzrv33nsBuP3225c8T4Af/OAH3HzzzQSDQcLhMG94wxs4cuTImvu40nv4d3/3d6xbt27Re7jcfS6VSnzyk5+kp6cHwzBoa2vjYx/72JLf4cK79Z3vfIdt27ZhGAZbt25d9v0aHR3lve99b+W96urq4j/9p/+EaZqVY5LJJB/+8Icr73RPTw+f+cxnFn1zqlRZoKqRqbIsN910E/fffz9PP/105eO2f/9+9u7dy969e0mlUhw+fJirrrqqsm/Tpk3U1NQA8Ju/+Zt85Stf4e1vfzsf/ehHefrpp/nTP/1Tjh07tsT35sSJE/zqr/4qH/jAB3jf+97Hxo0bKRQKvOY1r2FoaIgPfehDNDc3c//99/PII4+sqv/f+973WLdu3UX77ExOTrJ3717y+Twf+tCHqKmp4Stf+QpvetOb+MY3vrHI5LZa/vEf/5EPfOAD7N27lw9/+MOcPn2aN73pTSQSCdra2lY8r66ujvvvv5//+T//J9lslj/90z8FYPPmzQgheNOb3sSjjz7Ke9/7Xnbu3MmDDz7IH/zBHzA6OrrEDPXII4/wta99jd/93d+ltrb2vGaNL3/5y4RCIX7/93+fUCjEI488wn//7/+ddDrNn/3Zny06dnZ2lte97nXcd999/Pqv/zoNDQ3cdtttfPCDHyQUCvGHf/iHADQ0NCzb1l/91V/x1a9+lW9/+9t8/vOfJxQKVd6tc3Ech3vuuYcf//jH3Hffffze7/0emUyGH/3oRxw+fJju7u5lz3v22Wd54oknuO+++2htbWVgYIDPf/7z3HbbbRw9evS8GqS1cOrUKd7+9rfz3ve+l3e/+9380z/9E+95z3vYvXs3W7du5ZZbbuFDH/oQn/3sZ/nEJz7B5s2bASp/3n///bz73e/m7rvv5jOf+Qz5fJ7Pf/7zlcXFpZqiPv/5z/O7v/u73HzzzXzkIx9hYGCAt7zlLcTjcVpbWyvHua7Lm970Jvbt28f73/9+Nm/ezKFDh/jLv/xLTp48yXe+851F1923bx/f+ta3+O3f/m3C4TCf/exnedvb3sbQ0FDluzA2Nsa1115LMpnk/e9/P5s2bWJ0dJRvfOMb5PN5dF0nn89z6623Mjo6ygc+8AHa29t54okn+PjHP874+Dh/9Vd/dUnjr/JziKhSZRmOHDkiAPHpT39aCCGEZVkiGAyKr3zlK0IIIRoaGsTf/d3fCSGESKfTQlEU8b73vU8IIcTBgwcFIH7zN39z0TX/83/+zwIQjzzySGVbR0eHAMQPf/jDRcf+1V/9lQDE1772tcq2XC4nenp6BCAeffTRFfueSqUEIN785jeverwdHR3i3e9+d+XfH/7whwUgHn/88cq2TCYjurq6RGdnp3AcRwghxJe+9CUBiP7+/kXXe/TRRxf10zRNUV9fL3bu3ClKpVLluC984QsCELfeeusF+3jrrbeKrVu3Ltr2ne98RwDij//4jxdtf/vb3y4kSRKnTp2qbAOELMviyJEjF2xLCCHy+fySbR/4wAdEIBAQxWJxUb8A8fd///dLjt+6deuyYzv3/gghxCc/+UkBiOnp6UXH3nrrrYuu8U//9E8CEH/xF3+x5Lqu61b+DohPfvKT5x3Pk08+KQDx1a9+9bx9W47lnv3C+/zTn/60sm1qakoYhiE++tGPVrZ9/etfX7aNTCYjYrFY5be0wMTEhIhGo4u2L9yvszn3PT53LKVSSdTU1IhrrrlGWJZVOe7LX/7ykvfw/vvvF7IsL/oNCCHE3//93wtA7N+/v7INELquL3rfXnzxRQGIv/mbv6lse9e73iVkWRbPPvusOJeFZ/fpT39aBINBcfLkyUX7/+t//a9CURQxNDS05Nwqv9hUTUtVlmXz5s3U1NRUfF9efPFFcrlcRcOxd+9e9u/fD3i+M47jVPxjHnjgAQB+//d/f9E1P/rRjwIsMc90dXVx9913L9r2wAMP0NTUxNvf/vbKtkAgwPvf//4L9j2dTgMQDodXN9hleOCBB7j22msX+fyEQiHe//73MzAwwNGjR9d0veeee46pqSl+67d+a5HPx3ve8x6i0egl9VNRFD70oQ8t2v7Rj34UIQQ/+MEPFm2/9dZb2bJly6qufbafUiaTYWZmhptvvpl8Ps/x48cXHWsYBr/xG79xkaNYG9/85jepra3lgx/84JJ954Yjn83Z47Esi9nZWXp6eojFYrzwwguXrX9btmzh5ptvrvy7rq6OjRs3cvr06Que+6Mf/YhkMsmv/uqvMjMzU/lfURSuu+66ZaMF18Jzzz3H7Ows73vf+xY5Qv/ar/0a8Xh80bFf//rX2bx5M5s2bVrUlzvuuANgSV/uvPPORdqwq666ikgkUhm367p85zvf4Y1vfCN79uxZ0reFZ/f1r3+dm2++mXg8vqjdO++8E8dx+OlPf3pJ96DKzx9V01KVZZEkib179/LTn/4U13XZv38/9fX1lWiSvXv38rd/+7cAFYFmYdIfHBxEluVFkScAjY2NxGIxBgcHF23v6upa0v7g4CA9PT1LJqaNGzdesO+RSATwJt+LZXBwkOuuu27J9gX1/+DgINu2bVvT9QDWr1+/aLumaaxbt+6S+tnc3LxEaDu7n2ez3L1eiSNHjvDf/tt/45FHHqkIhwukUqlF/25pafmZRcb09fWxcePGNUckFQoF/vRP/5QvfelLjI6OLvIhOnc8l0J7e/uSbfF4nPn5+Que29vbC1ARFs5l4d2+WBbeh3N/m6qqLjFZ9fb2cuzYMerq6pa91oJz9gIXGvf09DTpdPqCv5ve3l5eeumlVbdbpUpVkKmyIjfddBPf+973OHToUMU/ZoG9e/dW/DD27dtHc3Pzkgn5fKvjs7nUCKVziUQiNDc3c/jw4ct63eVYaYznOsS+UljtvU4mk9x6661EIhH+x//4H3R3d+Pz+XjhhRf4L//lvyxxurzcz/BK8MEPfpAvfelLfPjDH+aGG24gGo0iSRL33XffZXUiXSmSSZzjfL0cC/24//77aWxsXLL/SoaTL9eX7du38xd/8RfL7j/Xr+tSxn1uu3fddRcf+9jHlt2/YcOGNV2vys8/VUGmyoqcnU9m//79fPjDH67s2717N4Zh8Nhjj/H000/z+te/vrKvo6MD13Xp7e2taAbAc6BNJpN0dHRcsO2Ojg4OHz6MEGKRsHDixIlV9f2ee+7hC1/4Ak8++SQ33HDDqs45t/3l2lowqSyMYUEdn0wmFx13riZk4fje3t5Fq23Lsujv72fHjh1r7uPCdR9++GEymcwircy5/Vwrjz32GLOzs3zrW9/illtuqWzv7+9f03VWK8yuhe7ubp5++mksy0LTtFWf941vfIN3v/vd/J//838q24rF4pJn97NgpfuyYJqpr6/nzjvvvOztLrwPp06dWpQHyrZtBgYGFjlYd3d38+KLL/Ka17zmsjzHuro6IpHIBRcY3d3dZLPZKzL+Kj+fVH1kqqzInj178Pl8/PM//zOjo6OLNDKGYbBr1y7+7u/+jlwut8iXZEGoOTe6YGFl94Y3vOGCbb/+9a9nbGyMb3zjG5Vt+Xx+1QnsPvaxjxEMBvnN3/xNJicnl+zv6+vjr//6r8/b/jPPPMOTTz5Z2ZbL5fjCF75AZ2dnxc9kYeI5227vOM6Sfu7Zs4e6ujr+/u//flGY6Ze//OVLmkhf//rX4zhOxcy3wF/+5V8iSRKve93rLuq6C6vrs1fTpmnyuc99bk3XCQaDl11QeNvb3sbMzMySMcP5V/+KoizZ/zd/8zcvi/YsGAwCSwXgu+++m0gkwp/8yZ9gWdaS86anpy+p3T179lBTU8MXv/hFbNuubP/nf/7nJaavd7zjHYyOjvLFL35xyXUKhQK5XG5NbS+U1fje977Hc889t2T/wrN5xzvewZNPPsmDDz645JhkMrmo31WqQFUjU+U86LrONddcw+OPP45hGOzevXvR/r1791ZWt2cLMjt27ODd7343X/jCFyomimeeeYavfOUrvOUtb1m0ElyJ973vffzt3/4t73rXu3j++edpamri/vvvX3WIbHd3N//yL//Cr/zKr7B58+ZFmX2feOIJvv71r5+3ttJ//a//lX/913/lda97HR/60IdIJBJ85Stfob+/n29+85vIsrcG2Lp1K9dffz0f//jHmZubI5FI8H//7/9d8rHVNI0//uM/5gMf+AB33HEHv/Irv0J/fz9f+tKXLslH5o1vfCO33347f/iHf8jAwAA7duzgoYce4t///d/58Ic/vGIo8oXYu3cv8Xicd7/73XzoQx9CkiTuv//+NZsJdu/ezec//3n++I//mJ6eHurr61f0/1gt73rXu/jqV7/K7//+7/PMM89w8803k8vlePjhh/nt3/5t3vzmNy973j333MP9999PNBply5YtPPnkkzz88MOV0OCfJTt37kRRFD7zmc+QSqUwDIM77riD+vp6Pv/5z/POd76TXbt2cd9991FXV8fQ0BDf//73ufHGG5cV4FaLrut86lOf4oMf/CB33HEH73jHOxgYGODLX/4y3d3dizQv73znO/na177Gb/3Wb/Hoo49y44034jgOx48f52tf+1ol99Na+JM/+RMeeughbr311kpI9/j4OF//+tfZt28fsViMP/iDP+C73/0u99xzTyVsPZfLcejQIb7xjW8wMDBAbW3tRd+DKj+HvFzhUlVeHXz84x8XgNi7d++Sfd/61rcEIMLhsLBte9E+y7LEH/3RH4muri6haZpoa2sTH//4xxeF7QrhhYu+4Q1vWLbtwcFB8aY3vUkEAgFRW1srfu/3fk/88Ic/XFVo7AInT54U73vf+0RnZ6fQdV2Ew2Fx4403ir/5m79Z1Jdzw1aFEKKvr0+8/e1vF7FYTPh8PnHttdeK//iP/1jSRl9fn7jzzjuFYRiioaFBfOITnxA/+tGPlu3n5z73OdHV1SUMwxB79uwRP/3pT5eEF6/EcuHXQnghux/5yEdEc3Oz0DRNrF+/XvzZn/3ZolBkIbwQ2d/5nd+5YDsL7N+/X1x//fXC7/eL5uZm8bGPfUw8+OCDS8a1Ur+E8MKG3/CGN4hwOLwovPdSwq+F8EKp//AP/7DyfjU2Noq3v/3toq+vb9F4zw6/np+fF7/xG78hamtrRSgUEnfffbc4fvz4BUOWV2Kl8Ovl3uflxvDFL35RrFu3TiiKsqS9Rx99VNx9990iGo0Kn88nuru7xXve8x7x3HPPLblfZ7PasXz2s58VHR0dwjAMce2114r9+/eL3bt3i9e+9rWLjjNNU3zmM58RW7duFYZhiHg8Lnbv3i3+6I/+SKRSqcpxK71by/2uBgcHxbve9S5RV1cnDMMQ69atE7/zO7+zKC1BJpMRH//4x0VPT4/QdV3U1taKvXv3ij//8z8XpmkuaafKLzaSEGtcYlWpUqVKlZ8rXNelrq6Ot771rcuakqpUeSVT9ZGpUqVKlV8gisXiEhPhV7/6Vebm5paUKKhS5dVAVSNTpUqVKr9APPbYY3zkIx/h3nvvpaamhhdeeIF//Md/ZPPmzTz//PPVStlVXnVUnX2rVKlS5ReIzs5O2tra+OxnP1txUH/Xu97F//pf/6sqxFR5VVLVyFSpUqVKlSpVXrVUfWSqVKlSpUqVKq9aqoJMlSpVqlSpUuVVS1WQqVKlSpUqVaq8arnizr5Fp8RIYYSj6WPMlGYu+joJPU5nsJOtkS1o8urrq1wuhBDMTqYZ6puiVDS59tbNKOrFyYFCCGYmUug+jWg8eJl7+vJgOg4/GD5GwgiwJd5Aje/Kj+vo/CQvzY1xbV07jYEwAfXKOCpaJYvBkxM88YMX6dzURDZZYHpsHtdx2XxNF+u2tlLfkgDg9JERZieSmEWbXLrA7ESSXKaIL6Bz5zuuo7YpjmM7zE2leHH/SaZH57FNG83Q2HptNx0bm0g0RCttO45LLpXn8NN9jJ6eIjOfw3UFmq6w69bNbNzViaarZFMFTr00xKlDw2RTeWRZor61hu7trazb0oqqKeSzRSaHZjm47wSZ+RyW6SArEi1d9Wy+Zh1tPQ0IVzA3nebkgQFOHxmlVDBxXUGsNkzn5mZ23LgBTVcpFU3mJtIc3HeCuakUVslC0zV6trexblsrdc3xyhiyqTwjfVO8uO8ExVwJ23bQDI2WdfVsv76H+tbEFXluF4MQJsKZwLGPIUkxVGNpBfTL15aDcAZxnRmQDFT94uptLX9tgWsfxrGOI9wkstKJou9EVpavKP2LTMG2GM+naQxECKhL55aSYzOYSTJXzBM1fGyO1/9M+3d4doITyWlmS3m6wgl21DZR7w+teHzRSeEKC10Oo0gaknR59RVCCAQuEvIVqaV2MVxxQcYWFpPFSV6YP0B/buCir9Pqb0VCYlN4Ixo/e0EGIJPKM9g7QSZVYM/NG1EuVqEl4OShERL1kZ8bQQYEJdfBcl3cS/Aft1yHvGVSdGyCmk5IM1Y81hEuRcfGEYIr6bLu2C7j/VP86N+e5LY378EI6OQyBfKZIs8/egzHdonWhNENlcmROY49e5r5qTT1rQmKeZNCroRt2Ti2i+u6zIwneeqhQ0yPzmGZNpIkIdJFDu47gVmy2HHjBgy/jiRJpGYynHppmJee7MUqWkiyhBCCQhZKBRMEuLbL8ef7GTg+xvx0Gsd2EK6g/+gIqdkMiboI0dowc5MpjjzbR9/hEVRVRtW9n38mmcMseXV9XCHoPzLCiYODTA3PEooEcBwXVVfIpQsIVyCEYHp0nkNPnmJiaBazaOI4LsIt8vxjx0CCYNhPIOwD8O7Jc6c5fWSEQNiPospYpl0Wpi6tbo7juKSTeY6+OIRlXrhmUjjqZ92GRkIRP5q2TLVmYSPcWVzzEJLSBFdQkAGBEA5gActXjr60y9sIZwrXPoVwc8haN1AVZM5lvlTgp2P9/FLbhmUFGfC+NZZwcC5jlfTV4giXqUKOk6lp8pZFVyS+rCCzIGCYTg5bFNHkAAIVIWxcYeMKBxBIkoyEgiscJElCllQUyRu35RYQwkGRdCRJRiBwhY0jTHTZm6tMN0fWGsenJDCUMKrkAwSWm0cgkCUNCQmBC3gfZlXyIUkyrnBwRAlX2CiSjiypWG4BWVKRJQ1FujiR5Bcu/NpxXGzTxrYdwJsUVE3B8HkP0rYdbMvBdbwXVpZlVE1B1RaK6HnXKORNzJKNJIGiKuiGiiRJOLaLVZ60Fl4aRZXRypOGYzuYJYvn952kY30j7T2edG/4NBTllSPhrhVdUXnHuktfUeYtk9OZOWaKObrCCXqiKwsy2xNNbE80XXKbq8E0bVIzWXwBgxvv2UljRy0zY/P8/X/7OoqmsOHqDmrKmpTZiSTDp6Z4zTuuo2dbG0ZAp5Qv4Q/7sU2bgeNjfO+ffsKvfOiX2HXrZuJ1EeYmU/y//+PbWCWb9vWN1LfVoCgSw72T/PR7LyDJEq9527Vsva4bRZFJzWXxBXQ0XaWYL/Hot56lpinGrW/eTVNnHaV8iUe++SzPP3aUDTs66PFpzI4nOfXSMG09DVx9yyba1jcihCCfLlSEDtdxOXlwCNt0uOY127j2rm3IsoxZtHAsB1VTEK6g7/AIj37rWd74/9zKuq0tBEI+ZidSfPFT3yQY8dHUUUd7uBGAiYEZRvqmWL+jg2vv3EpdSwLXccmlC0QSlybIW6bNYN8Uf/k/vksmVbjg8Ru2tvDeD91Jz6YmNM2/4nECgSRMhJsFXEABSUOSPK2fECUQFkgqCKd8jAySH5AAG0QJ70MuvG2SBpQnCOEANrLSCEpTeV+5beGWzzdZmAi8P2WQDCRJLQvuJRB2uW3K+3UkyROCFf1qQMWWAiAyS8e4MIZF53t99K7tlNt18KaK8lgkA1Av+0p/af8EVnlx5AgXkFAlCU1RkJEouTZCeMK3QKDKCo7rIksShqKgSDKW62K5Tvl8UCQZTVbQZBkXgeW4DGeTfLf/KJti9YQ1A0WS8JcFGkcIXCFoCUZoC8XQ5cVjdlwX03WwXbc8iUsokoyuKEhI2K6L6drl8VA5xlBUVNkrYlpybBxxZgGoSjKaoqDJ3pyzo7YZTVYIqBoZq3Tee+YKG1sUcYTpaUyQMN08RSeJ5ebL90BHkQxcYYIkocsh/EocV9gU7Flst4gmh9CUAI4wsdwsppsjpncAMhlrnNOZh2nwX0WtsQFVNXCFTc6erghQMioCF1fYCAQRrRkZDcvNk7ensYWJT4miyX6y1iSKZOBToviUGLBydfiV+IUTZCZG5jj8bD9HDwwiSxK27bBpZzt3vmU3mq5y4sVhjr4wyOiAZwZrbEuwaUcbO2/oAbxV8MCJCb72D4+SzRQJR/z0bG3h+js2Y/h1xodneWF/L0N9U1glm0giSM+mJq6+aQNCCAZ7J3n+8RMcfWGQ4dPTjJXbee2919C5sfFluy+vFObNAgdmR5GQaPCHX+7uLEKWJXp2tNHQmkA3VOpbEjR21IIQTA7OEq+NABBJhFi3RaWlqw5/yIcsS8jlP5OzWZLTGfxBg/YNnhlJURVqGmO0b2yiVDAZPDFBbXMcRZFJzqSZGJzhjb9xKy3d9aiqAhKEY0FkWcIybean0kyOzHL8hQGOP9+PpqsIV5Cez6HpKrOTKdqKjdS1JOi5qo0nHjjI+OAMm3Z1smFnBw3tNfgCnsAoKzKbdnfx9I9e4pFvPMPI6Sk27eqkfUMjNQ1RJFkik8wzO56k/+go3/i7H+EPGkiyjGM7TI/N07ahQHo+W7lvzevqGB+a4YkHDjI1MsvGXZ30bG+job0W3XiFfoJEFts8jeuMIEQSWWlB0Xaj+m4FwC49imMeQlHX4dqDuO4MklKPHvwNJCmEax3BLv0Y4cwgKCFJcRRtC6rvLpAiCHcSu/QkjvkCkhxC0a5G87/Wa9qdw7WOYZcew9PamICJJMXRAm9C0baAyHt9sI4h3GkQEpJSg+q7HVW/ZlVDtEuP4ZgHEO4cCBNJbUPVdqMYN2AVfoBwJxBuCmEPIWtbEW4aIQpo/jciaxuQpOiFG7kESo7NM1MjHJwZYzSXRpKgK5zgxsYO4oafHwydwHQdpgtZ8rbFlkQDvckZmgJhXtPaQ3ekhhdmRnl+apSBrFfZuzMcY1dtC9c1tDOVz/Lc9AiPjp7mdHqOr554njp/kOZglHdu3IVfURnJpnh6cojnp0dpCIS4pr6VW5rPFHkdz2f4ydhpelMz5GyLqGawKV7PzU2dqLLCieQ0Pxk9jS08gcd0bGKGnzd0bGJ7TRMF2+I7/Uc4nZ4jbZZwhMOGWB03NnawvWbtizRJksnZ0xSdJAGlFp8aZa50ivHCiwhsDDmKJMnIyATVBjLWGCGtnnrfdqZLx0B4WpmCM0utbyMZawLTzZLQu8vyuMAVFpZTACQkSfG0QG4OR5gkSwMkrUEMOUpArakION2Ru0AIMtY4GWuMoFZPyUnjCAuBTcYcJ6K30RS4Gk1aeYGxElf8K6JICjEtSnugDQDLtTBdC0tYWO6Z/12urMpOCE8l/sLjvczPZtm8swN/QEcIQX2LZ8+fnUzTf2KCfLbI1Td6gsv40BynjozS0lGLZdqYpo0sS2za2Y5ju4z0T3PswBDrt7eiaQpDfVMM902xfmsLhl8jNZvj2MEhGttqqGuOUdsQZdPODo48P0hja4KrrvN+FBe7MhVCMJJL0Zuepmjb3NW6oSLJZ6wSz00PA9ASiNIZTnA6M8vx5BTj+TSOcJElma5wgk2xerrCnq/CaC7FUDaJ5TpkrRJzpTzzpQIguLWph02xOnTFe3WmCllOZ2Y5Oj9JslRga7yBXbWt1J2l+szbJn3pWY4lJ5kt5nCEQJFkNkTr2BCtoyUYJW+bHJob5+mpIZ6bHkaTFQYyczw5OQDA69s30xqMUnJtRnNpjsxPMJJLIiNxe3MPrcEYQe2Mj0zaLDKQmePA7Bh520SSoNYXYmeimeZghICq4wqB6dj8eKyXvG3R4A/Rl54lb5uENR/ro7VclWg+o/SXJHwBA81QkWUZ3SejGxqu65ZNM96KSvdpnpbPr1f8qJTyM3EsB9uykRWpolEBkA0VX0CnVDAp5kuVFPKW5VAqmITjAXx+HUn2VioLGkK3ZGGZDrbp0LGxid23ba4cA+APGnRubiEQ9iFFJLZfvx7Dp5NN5ZmfTrPvPw5Q31bD1mvW0ba+EVmS6NzUBAhG+qYwiybHn+tn6OQ4besbufqmTTi2g2XaaIbK9XdfRSQe9JQQZZo6ahf5vdQ1x9l500YCIYNi3mRyaJaJwVnq2xJcdcN6Gtouvvq0osgkasPc/trtpJN5zJL3GzVNm2LeJDmXY2oitebrClHwBAzjGhBFHPMIjnUYWduOJEcQbhLhjCHkWmRtPTIbkaRQWWMjIcm1KNpOhFoALFy7H7u0D8W4EYkwkhREVtchnBGEm0G4ybNatxDuLI75Aqrvl1DUNoQ7j13ah2uPICttCGFjm08hSQEUbTuSHAMpgCw3rHqMstKOpMsIUQQsHPMgjvUSinEdwp1DiBKS5EfIIYTIlDVSPlz7mKdJkq+cIFNybKYKOZ6ZGiaq+7ipqRPbdTg4M05vaoa2UIzJQpa44Seg6liuy2whT8zwIyFxdG4KVVY4NjdFwbG4uakTgP70PIfnJukIx9Flha5IgtFcmsdGT7Ml0UBPtIao7kMta5siukFHJM5wNknJsUmbZzQi86UCfelZDs9Nsi3RQFg3SJlFXpgepTUUpcYIMFfM88L0GLe1rKMzHCdlFtg3PsBINkVnOI6hqGyI1VLrC1JybYq2zRMTgzQFImwrCzJr0U1IKEjIgMDFBiFwyqYhGRlDCWO7BSy3SEhroOSkcYVD0UmSNkeJG13oSoiSm8EVNrKk4FOihLVmFEkHCTQ5gCIb+JU4uhzEERZZe5K8PU3WniJjTeCqDjG9HVnSKDjz2G6JkpOk5KSJaK0EtFrS5ggpcwhNDmCLEq6wcIUFkm+No/4ZCDKqpFJn1LE9uo1mXzMlt0TJLWG6JiXHrPzdFjY5O8d0aZq8U8ARF7Z3rwXhCkzT5tTRUWrqI9x09zYC5VUyQKloMTk6R3I2SzQR5NbXe2aSR797gL5j44wOzKDqCrIsEYkHueaWjeiGxr6HDrP/ocPMTKRAwMx4CkmCPbdsJFEX5viLQ/QeHmFiZI5YbYim9gR1TVEe/d5BOjc0cuMvbbvksRVsk4OzYwxnk1xf30FE9yFLEimzwA+Gj9MajBJUdVzhMlPMMZiZZzg3j4vAdl3G82kyVqkiyEwVsjw7PcxILklM92G7LvNmAdt12FnTssgHxnQd5ksF+jNz7J/oJ2eX6AwnFgkyluswVcgykJlnqpDBFQLLdZksZCg5Ni3BiNdGqcBYPs1EIUNQ1dFlBcv13oOCYyHw1Mh522Qyn+GZqWEmCxm6IzXU+IIENU8wdRGcTE3z7PQwfelZz0wAjOXSzBSy3NW6gfZQHBkJ03X48Wgvc6U819S1M1nIkDaL2MKlLz1DUyBCjeSZXYQQpGYy5DNFgtEAZtEkn/Ucef0hH5TVobLsrVRYRj2q+zQMv45juWRTBYoFE11XMUs22VQehCAUDVRUq7qh4g8azE4kaVlXTzAaQJLAthwkWUKSJHS/5ml41jdy/S9tp6YphiR52hrbctB9GrIsYVsOdc1xmjprmRye5eSBQQ49dYrel4YIRfy0rW8ECfwhH1uuWUf39jZGeic4uO8kJw8MMnp6iq3XdqNqntAVTYTYdl0367a2Eoz4Ea6gVDCRVRlVUxeNubmrjo6NTYz0TXL8uX6OPd/PkWf7qG9NXJogoyrU1ke48407yWdLFAsmxaJFqWiRms9x4vDoRQkyICMrzWi+uwAJ15lCOOMIdxJJLq8YJQ3kGhT9BmSl9sypwgXJhyQ3Vp6j60zg2ifL5iaQ5CiKthXhznvblyAQlFD07aj6DbjOGHZpP0KkECLrXR8dSa5FUtqQ5TiSUockxS48NK9YNZIUBrm50ke79BTCGWLBhCTJCWQ54Qluoogk1yFJQRzrRYS4sBnvUijYFmO5NMfmp7i2vo22UBTTcdg3PshEPltZtDQGwkh4QkXRtumMxMlZJkOZecK6zkwxR0MgxBs7twDw9VMv0ZeaZTAzz7ZEI5vj9eQsC7+qcV1DO9fUty7qR8IXYKvcwHyxwEBmbtG+2UKOsWwa07HZ29hBZyTByeQ0T08MM5JNIZXvpOnabEs0cFNTJ1OFLE9NDpE2S6TNEvV+lbjhJ1gOVCi5Dt86fZjpQhYhvO/Wct+RlXCFiStMbLdAwZ5Dl4PIkowm+1HQCap1FJx5LLeIhOcfIyF7zruSDEhISJ7QgoQhh9HkICHtjIOzKvtRZb1sOvK+z1lrkoIzj+lkcV0LgYtfTaDLIXL21KJjFXlhwVn26XGzKJKGKvvKQtjaueKCjCZr1Pvqqfed39PbdE1OZ0/zH+M/5HS2j5yTv6z9cByXTKqAosoEIz6CZZ8AKHv4Oy5zU2kMn0a87oxJIxIPEor4mRqbp64phj/oI1rjICsySBAIGUQTQTLJPKWiheM41LfEK46E/oBBXXOcdDJPPlM80yFxpm1Yu03wbLqjdcQnBzlYHGUkl2SdUoOhqKTMIgdnR9lV28LGWD2qrNAVTtAdqSGo6vgUjbF8ii+dfJZHxnq5t+uqyjUzVpHHJ/r5nS17ubNlA3W+IFmrhE/VMJQzr01rMEprMMotTev4+DPfXzZyyKdo9ERr2RZvJKDqqLLMWD7FXx9+nCemBnhd2yaiuo/Xt2+mPRQrXzfGzY1dXF3bsuhaAVWv+MY0BsL82+kXkc+y1Qu81dyPR3vpTc/w/2y8ju3xRhzh8sTkAJ9+4Uc0BSLE9AAx3Vceq/dRiRt+3r1hD64QfH/oGP9w/Elub1mP3+9NtK7jcvLFQRraa2jtbmBqdI6pkTk6NjXT0JaoCMXnI1oTIl4fxTRtBk+MU9MYJVEfZXYyxXDvBE0ddbStb0BRvDHFasM0ddZy4PETJBqixGrDyLJEej6H7vMEmERdhNrmONNj87y4/yS3vHk3kiSRnsuRSxeob0ug6yq5dIG5iRT17TU0ttcSTYSoa0nwhU99k+SM50Ox4Mhr+DVidRE27uoi0RDlh//6JEef6cO1XULRADUNMQJhH88/dgxf0KBrcwuu4zI5Mk8o6iNaE0Yua6HSczlyqXzlvsVqwoQTQb7wqW+RTV7a71yWJQIhg41bW5bsSyfz+Pw6+x85tubrehqOKAsZKiQ57plZnBlQypOdFETR1iOdowoX2DjWIazCD/AcedWyiaqA529S9pk5bwcMZKUDpDBICiB7gocQICwkuQ49+G6s0qNYhW+CyKAad6MaNyDJnee9tECAMLFLP8ExDwI2SJpnKtN6WPCZkSQDSQohSCNJGpIU8PqyyK/mylCwLcbyaabyWb4/eJz94wOVfesiCayy061f1ZAliaJjYygqPkUjb1kUHNsTeFSd2rMiKBM+P1MFg9Fcig2xWgLnBI4IIVb9LU6ZRYqORXsoVvkm+hSVzkicrFUiZRbRFYW2UIywbqDIMrIkVQIYSo5Nyizy3f5jjOfTuAhkJIazKUzXKS/AVj8vCFwKzhwZa4ykOYTtljCU8FkRRp6gAhKmm2Mk/zR5e46E0U1IbSChd5Myh8nZ07jCJKjWVPxszrlJ3qK9eAJZUgmrTRSdZMXnRpIWtEKeUOQ5F0uEtHpcYdOfeRS/GiesNRHXO5kpnkSRNBRJR1eCrFUbA68gHxlVUtFlA1VWrojDq6LIhKN+bMshly6SzxbxB30VYVdWJGobYwz2TjE7ma6cl5zLkk7m2LSzvRLCmpzJes7AqkI+4/07WjYN5dIFRvtnsCxP+sznSkwMz9LR01BxqJQkCcdxcGxnVd+0CyEDneE43ZFa9k8OENF9KJLM8eQU7aE4TYEIQVUve79neX5mhOFsElmSMF2bl+bGqTEClR8SgF/R2ByrY12khoThR5Ykgpq+SGhYLabrMJHP8Nz0MNOFLEjetmPzk56G56x2LxVHuIzmUhQcizpfkK3xhrLjnqA1GGVrvJGpQpaJQroiyAC0hWLc2NCFoahISCR8AWqMADnLpKh55kRVVykVLR779nPMTaUp5Ut0bGpi162bCMeCq3pvFVWha3Mzv/rhuzlxYJDDT59CuAJJlujc3MxVN2wgXhepmIc6NjZx+9uu5akHX+Lhf3uKH/x/+wHPtHTzm3ax+9bNaLrK635tL4ef7uO5R4/y1EOHcF0XvRzi/MbfuBVNV5kenWf/9w8yNjCF47ieQ6gis/u2LfRs90y/riM4/nw/Jw4MMD+dRlFkBJ4Advtbr0H3aSDB+p3tvPE3buGlJ07xjc897EVKCQiEDW6+Zxc7b95UGfNY/xTPPHyEsf4pT9iTJHRD4657r6O1e/WmkJ8lwp0vq7g9J1PPjySHpNR5Dr4s/Gxlzv0Bu/YgrjOMJPvR/L8GqNjFh7CK311jL5aZRCooSHItmvFLqPp1CGcaM/9vSJIfWe28wOBKuPZJhDuJrHWhGneCKGGKL3DGuXiBlyf4IKDqtASj1PqD7G3sYG9jBzISrhBEdIOkWeTgzFh5Wl6KJsu0BCM8nx9lqnDGX2umkCdlFtld14Iuq+X7KzBdu+IQvFriPj9+TefF2XGKjufQW7Bt+tKzdIRiRHUfKbPISuublFlkLJ9m3ixwXUM72xONpMwiJ5IXl6ZEQsav1NAZuhVblMoOtBF8SpSo1o6EhCr7mSmewFBCtAdvrGxTZT8Jo4eI1oIjTAAMJVy+7uKIuoBay8boG5ElBU0Ooso+2kM3IoSDUzYPKZKOT40jo9AauA5NDiAhYShRonorkqSgSgaSJBMzupCQylFRF/e+vWIEGVmSkaWFH+7l//FIsoRuqGzY3srcVIZHv3cQX8BTnzW0xOnZ2kxDS5yahjCTo0ke/s4LAEyOzhOvC9PSWUtyNouqKhQLJk/++Ci25TA+PEdNfYTahiiaoZJLFxnpn+apR46iGxrp+RyReJCm9gThqL/Sl/qmGDMTKX707efxB3W27Oqkpj6y9nGVJ8+OUJz1kVoeGj3Jnro2bNflWHKKqxJNNJadZudLBR4aPYGERGc4TlDVKbk2w9kkzjnxy6qsUGMEiWhGxR9GldYeImq7DpP5DN8bPELCCNAZSeBXNEqOzbH5yUsK1V4OIQR528JxXXRZJXqWsOJXNSK6j5JjU7StReeFNIOmQBhV8iLHDFnBUFRs4SwK7+7c2Eww6md+Ko1wBd3bWmld31jxdWldV4/P72mlFiLhzkaWJWJ1YXbevAlf0GB+Mo1lOWiaQvf2NlrW1XvCwkK/YkHWbW3FsR2mRubIZ4oIIdAMjbqmOIqqICsyXVtaUTSVeH2kkmtG92nUNcfRfRqS5Plh9VzVRqQmiGO7KIqM4ddZt7WVps66Sv9auusRQpCez3rCjqrQ0FZDx8YmVM2LzkvUR9h2fQ+aoTE/laaY90wmwYif2qYYinLmN5yoj7L+qnaiNZ6wJysywUiA7q0t1DbFLv2hXwEkdIQzhVV4ACEKCGcKSW5AVhqgsoqXvIXQOZ8rSTIAF9edwbFPeH4mIo8kx2HBf8EZxzFfwLEOI5wxkEJYhTiKtglPa3O+r6AoO/s+VtbyCISbx4us0stRTws+LwdxrV7P50eKgp5BUlpBMrycOe48rn0c8ARWSXplONj7VZWWYIQdNU24QnB8fhpZ8kzLm+L1+Cpa4eXnC01WaA/HGMmlmCpk+WbfIQDG8mnq/cGKf4qEJzR1R2p4ZnKYsVyaBn+IaxvaUCSZ4WySgzNjHJ6bZCqfZaaYJ6BqbIrXE9IM2oJRDmk6j4/388L0KBmzREz30RaOkTACpMwirCCOKpKMIatlU1jS+964Lj5Fxa9quELgIHhxeoyDM+OcTE5TdGxCmk7WMumJ1hBQdeTyHCBJEqpkoMqLrR8qPgzlzNziV+NERAtBrcHLM1O+j4YSqoRYn29RpsoGEb2l/J55DsZBtbb8b2nJuap8JvJUxVjUFwBDibKwol+LBmrxGH9BkCTvBu+8oYcjzw9w8tAwsuKFkrqOS8/WZuJ1Ybo2NVEsWhx/cQgE1DfHWL+1hbqmKLbt0NxRg1myGOydJJsuEIr42bKrg3hdGMOnUeo2mZtJM9Q3jVmyiEQDbL66nab2GgIhb1KVJdi8q4NTR0Y5fnCIcMxPx/rGixJkFmgMROiKJBjvTVd8XgYz87xr/W7q/CFs4TBXyvHI6Cne1LGVe9q3kPAFGM4mOTI3wWg+veh6Et6q5lK1Y6brMJ5P88Ph43z0qtu4u20jQVVnOJvk8YnTy5whoUgStnAuSsiRJAm/oqFIMiXXJm0WCao6Am+1lLE8de/Z5jEARZIqAttSPEdx4QrWbWtl8+6uFdtvW9/o+ZqcB93wBIyzk8athKLIROJBdt+25bzHBcI+Nu3qZNOuzhWPaWiruaA/iqzIbL22m63Xdp/3OM3QqG2KU9t04TG09jTQ2vPK1LwsQZJBCiJrGxHOGI51BOGmkJVmFH17WRgBSa5BUlsBP+cmSPf8VlqRpJO41nGQQyCpKPo1IAXwBI8UjnXC0/wgQBRxraPISj2SHEOS48hqV/l4QNKR1Q5kpcYTQjDLEUtJvF+rjKLvQlbXsRAy7doDCGccz7wleVoipw5FaUFWmpCVFhz7BI51CKQwslyPrHYACrJSD1IQSY4g4zmfS3IEUMpjW3tkyVrQFZU6f5AbGts5OD3GizNjXr4lBHFfgO5IgpZghJju877tgCvcysJFkqApEGFLvIGSY3NgZgwEtIWibEs00hQ8861N+Pzc2drDcDbJbDFHd7SWPfWtyJJgrlSgNzVD2iwCgpxV4kRyhqZyOHZXJMHOmmZ607PkLJOwZrCnrpWucAJNlpkrFegKxwmUfXo02TM11fgCxH1+VMnTHKXMIkfnJglqOptidbSFPEdqx3UZzCQZz6WxXBcJGMmmqPUFaQ/F8CtiTT40AH6lxsvhgrJEcFjL9/7c8PvVheOvJKpc2jzzCyPILNDcUUNja4Lb37gTympFWZYr0SUbtrfRs7UF4Z7xXZHL/gqNrQnqm2MVB11RfockWar4NDS21VDfHMd1FxzqpEX7vYvC3ru2csNrtlRsshebJXgBv6JS7w/RE6mhNzXNVCGL7TpsjNUT0/0UbKus/hQEVI2gpmO7Ls9ODzGQna9EOl1ubNelVM6jENJ0/IpGzjbZN9HPRD5DJOpbdLwiy/gUjYxZJG+ba25PkWRaQ1H8qs5o2otu2hZvxBGCkXySw/MTvL5tM42Bixcaq/w8YyCrPZ4QUckBA9634sxvRDVuL+9TWKqS8aMat6EaN591LuXjNUBCVjdghBeEjoU25Mr1FKUdRd/NwidakmvQQ+8vH+OZs4zw77HYFHTmfPCjBd4KZyUlOzMGbxxa4D60iq/LgmuqDKiovtefs/3MOLz8NFfme3E2uqywp66Vq2tbPL+eMork+Zr82oarKybps3vo3VGBKslcXdvMVTVNlfOl8kLpbJoCEd61cXflGFmSKlFLV9U0sjVe711z4XuPhCJ73iMBVeMt67ZWFl0S3pyhSF7P9tS1cHVtE2o5/0zc8PPezdcgS1JFk/LB7XvP8jDwhDVFklEkCVWReXPXFt7YudnrXbkPSrmNi8HL1xLhUoWHVxK/cIKMLMt479TyP0RFOUfoOAtJouLAuPL1pQseI0mSlw/kMiJJElHdz61N3Tw6fgrTcdhV14pf0TxTiaLSFIhwQ0MnB2fHOJ6cwq9qJIwAIc3Av6I24vw8OTnAgdlRJvNZjsxNMJRJVtSztzR1sy5SQ1swxi1N6/jxWC/7JwcIqjoJI0BE9xHWFye8q/MFubGhk+8PHeVf+w7wg5Hj+BWNX+vZxbpIDdPFLA8On2CikOHo3AT96Vn+b98BfjLex5Z4Azc0dNLgC3FHcw8R3eDbA4f5zsBhBGAoKr/afTVbE41EzpMxuMovLlLFVnT+SUI6TwZS7xoq5/u8esdcqKTGmW+Et9o99/gLnX+hDOjn23++78GVF2JgQSA410PjDPIqTN3nO//MdSR0ZYX5gJXnAyi/Kefph9f+mfOlZdpaWRPsoV2ESf98nHnHf374hRNkfp4JqTp76tooOBYSXuZbQ1GQAFWWiet+7mrZwKn0DEmziE9W6Y7U0BiIeGak8nXq/EF21DTTGY6TMALnbTOg6tQYQRRJ5pdaN6JIMgFVw69qBFQdQ1FpDIS5p2ML/ek5crZJSDVYF6khpvup8wcX/aTCmsHWRCNzpTxTxSyuEBiKepafjkzc8OMIlx01LayP1hEoa3piuh9dVlBkmY2xOs+EJCtkbRNJkqg1glxd00JLIIquqLhCoMsKdzSvX+RLA9ASjHJ360bWhWuI6AHaehq55903k7gE81+Vnz8Wog6LeZNUMs/8bJZMulAJBbctL2Ort3iRMfwawaBBNB6kpi5MLBFE1VYf4OC6LoOnp3nx2X4AAkGD1o4atuxoB7zozHy2xNREitnpNNlMkVLRyzUkyxKaruDz60RiARqaYsRrgvgDFyfUu65LqWgzPZlibibrRWbmStiW45WsWINpWFUVYokge/b2VHzMzm5nfGSeZ/f3VrZFogE2bG2mtiGCz7f6Gmtjw3MM908zPuolyDMMjcaWODuv7VrVMxBC4LqCbLrA/GyW1HyeTKZAIWdimV5Gd4FAURQ0XcEf0AmGfNTUhYnXhghHyn6Sl2CyX4jAnZlKMz+bJZsuUCxY2LaXkX4tBnldV+normfrzvZVn+M6LqZpMzOZZnYmQ2o+TyHvjV8IgaLI6IZKOOInVhOiti5CNB6o5L66ElQFmZ8jfKrGukgN6yLL+0H4VI0bG7u4sXFlHw/wQp9bg7FVtbmjppkdNc0XPO6O5vVw4cMqmqN3dO9cdn/cCPCG9vP7iwBounLBvsmShE/V+NWeq5fsWx+tY330TF2a2p0hNuzsuPAAqvzCYFsO+VyJXLbE9ESSof4ZTp+YYHx0npnJNMn5HMW8V3RTViQMn0YkFqC2PkJ7Vy3rt7SwflMTdY0RAiHfeVf+C7iu4OiLw3z+f/8AgNqGCLfdvY3NV7V5WZ5nswz1z3D4hUF6j40xMZasCBiqqhAIGsQSQVo6ati2s50NW1to7aghFPGX8x+tboI1SzaZVJ7x0XkOHxji9IkJRoZmmZ1KVyY11139lOrza2zY0sLWne3LCDKCU8fHK2MG6Oyp57733kww5FuTINPfO8GPvnuQJ39yAoBYIsh1t2xk57Xn/yYCFPJe3qhMusDo0Cz9vZMMnZ5hYmye+ZksuUwR07S9BZKuEgz5iCWC1DdG6dnczPotTXR21xNLhFA1ZVXpGhbfBxezZDM3k6W/d5JjLw3T3zvJxFiS1HyuLDg7a6o7F474ef3bdq9akCkVLdLJPFMTKY69NEzv0TGGBmZIzmbJ5UoIR6Ab3tibWuN0rm9gw5YWujc1Ulsfxh8wVvWer5WqIFOlSpUqF8HcbJanf3qC5544xdDpadKpPLbl4jguruPilgtsAmCDZTqetmQ8yckjozz+o6N09NRx1z07ue7WjUSi59d+LkcmVSA1n8d1BWNDc/z4gZd44rHjzE1nsMo14xb64TgulmmTTuUZG5rl4NOn6VrfwE2v2cLtr9tOJLb69idG59n/yDEeeeAl5mezmKaNYzs4C2O+gkVcXy5OHhnlmX0nOfhMP3MzGYpFC8c+87zFWdGNRcekVLRIzmUZ6p/mxef6qamPsOWqNt766zfQ2BJfNqrxfJSKXl2x//j6sxx6YZBUMoez8L6d/a5dQQb6pnjy0eP85EeHyaYKlErec3ddryYVAizLJp8rMTeb4cSRUX7y4GG6NzVy1z072XNjD6Hw5XcUrwoyVapUqXIR5DJFjh8aob93ktmpNLZ9Jg+JrEjoPhV/uUyFY7sUCxaloonjeEJNsWDSd9xBUQ5RLFi87m2711w4dkELc+r4OA/9+wFeen6AybF5LNMph9dr+Hyen1wuW8IslSdf26VUsjndO4njuPj8Ojuv7aKx5fxRaGbJZnY6w4++d5Bn9/cyPjrntaXKxBMhmtriRKIBFEWmVLJJzecYHZwlly0u0tDEEkGaWuM0tcQJR/1EE0Fa2mrwB1avXflZMz4yR++xcYYHZjBL1iLNh6op+HwauqEiK7L3rAuml0/M9jQpljWPWfICH+5+89Vs3N66au1ENl2g78QE3/23Zzh5ZJS5mQy246JpCs1tCeoao4TCPgSQyxaZncowNjy7REPT2BKnqTVOfWOUYMRHoibMxm1Lk0mezYIQ/PwTp3j68ZO8+NwAk2NJHMcF4fmF+oMGhqGCJJUF9iKW6ZTfc4uTR0axLYeZqTR3vXEnwbDvsvqJVgWZNSIQOK5D3imQd/IUnAJFp0jJKWELG0d4lVZFOcmbLMkokoIqqxiygU/x4ZN9RLQwhmygyq/sR+CNNU/azpC1sxSdoldSwrVxcctprr0x6rKOoRgEFD8BJUhIDaLLF5dE70K4wqXoFMnYGXK29xxKbqmc98VBCFGOPtDQZY2wFiKiRolqERTpyiRdvBiEENjCJmtnydpZcna+co8dFkLQBTIKmqyiyzp+xU9QDRJRw/hVP8pldgassjpUTSEcDXg5eVSFWE2IeE2IaCxAOOonGPLhD+goiuz5ruRKJOdyjA3PMTWRIpcpki0LQ7qhsn13Bw3N8TVN5q4rmBid59EHXuLpx0+STuaJRAO0dtSQqA0TjgXw+cuCTKbI3EyWybF5hgdmsEybXKZI3/FxHg8dJV4borY+sqIvgxCCbKbAc0/08twTpxjsmwIgGDI808nmZtq7aglH/ciKjFkWZAZOTXHi8CjD/dMUi17+JsOn0dJWw613byOaCBKO+AhF/K/cIqKAz69XyomEwj6iiRCxRJBI1E8o4idQnsxlRaaYN8mkPT+W4YGZSh2wqYkUTz9+kvZ1ddQ1RqlrvHC9KiEEYyNzPLPvJC88eYpCwURVFWrrImze0ca69Q00NMfOEmRKTE+kGOid5OhLwxXtHHjPatO2FnZd300w7CMc8fp+Pgp5k7GhWfY9coyDz/QzNZ4EoL4pSn1TjLr6CJF4AMPw3jPTtCs+PJNj80yOJUknCxx7cRizZBONBdi9t4d4TeiyfYdfuW/NKwhXuDjCwRY2lmuRc/JMFCaYKE4yVZpmpjTLvDlH3ilQcoqUXBMXtzK567JOQPET1+PUGjXU6rV0BjuoM2oJqWEMWUeWlEo43uXGK9RpYrqLw5llSSaoBJcIU169IhfTNclaWcaK4/Tn+hnKjzBTmiFtpck7BSxhISOjSRo+xUdED5PQE9QbdTT7mmgLtFKj1xBUg6iSeskv7dn9KjgFJotTDOQGmShOMFmcYt5KVgQaV3j3P6gEiWph2gJt9IS66Q51o8v6mkMXZUnBkA10WbtkwaxSEFJYlByTlJViuDDMUH6Y8cIEM6UZUla6LJh5IfOapBNUA8S0GLVGLS3+ZrqCnTT5GwkqwfKYLk1Ac2yv+KVp2siShGaoGOXVfJWlxBJB9t62idMnJshli3RtaGDzVW10b2ikqTVeMdVUahlZDlMTKfb9+ChPPHqcE0dGccsCzvDADM8/2cfNd21ds1ZidGiW8dF5hIDa+jBbdrRz5xt20LO5iWgsUEkf4dguo0OzPPfkKR745vNMjScpFS1KJZsDT59m+652Nm1rIRpfvoCt6wpmptL86HsHGR+dw3UFhk+jrauON77jGnbf0EMguNhxWLiCTKbAg985wEP/foCh/mkATNMGCbZe3U4gYCwqdPpKZf3mZsZH5uk/NUl7Vy0btraycWsz69Y3EK8Lo+uLv3HpZJ6+ExM89N0DHHphkJmpNK7jMj+b5fjhEdo6a1clyDiOS9/xcfY/eoxi0dMEhaN+tuxs450fuJ3Glhi6sdhMZZo2ydksX/38oxx4+jQzU16esGLBJBILsvXqjlX5RLmuy/Rkip/86AgvPNnHzJSX7TsY8rHrum6uu2UjW3e2EY4GKj4/riswSxbHXhrmycdO8PjDR0nN5yiVbPp7J/nWPz9FY0ucUNi3pN8XS1WQWQVFp8h4cYLe7CkGcgOMFybI2FlsYWO7C1oYB5eynbT8nytcbNemIBXIWGmmSzOczvajyAqGpFPnq6Mn1MM18d3U+erwK74Ld+YiGMoP88L8AZ6ff6GyTZFUolqEX+/4/9HsX1wuXiBIWSkOzB/kpdRhxgvjFJwCprBw3LLGY2GMeEKeKUpk7SyTxSl6pVNokqeB2h7bxvU119EV7ES5xLBNF5d5c54DyRc5lj7GeGGCnJPHdq2KJsYta8MALDwBLm2nmChO8mLyJXTZqCTQWgsNvgauS1zLtuhWItqlZz+1hcPx9EkOpQ7Rm+0jZSXLleDtcrVat3KPAVxRwrIsMnaGseI4x9LH8SkG9b56toQ3sSt+NbVGLZp08R+Guek0Bx4/yZEXBghF/Gzb08V1d26pCjIrEAgarN/SzHt+9w5UVSFeG8Ln09ENdVmthqLK1DVGuPOeHUTjAeamM8zOZCplU44eHGL3Dd3A2qpKC+FNHrF4kDtev4M7Xredusaot0I+S0CQFYnG1jg33r4Zv1/ngW8+x8mjYwA4tsNw/wwDfVPs2LO842s2U2R8ZJ7RoVmKeU+zEo76ef3b9rBhSws+/zLvnoQ36V2/jly2yND/6wky6fk8wwMzTE2kaGqJL3HwfSVS3xTl9tdtZ+O2FuqbogRDPnRDRTe0ZU1EwbCPjdtaaGyJ8f/9w2M8s6+XdLm22NjgHCODs1x785LTljA3nWVseJ7piXTFPNe1oZHX/vJuahsiiwq0LqBpnobwjtdfRT5XYuYRT5CZHE8xOjzL/GyWRO2FNSK5TJG+ExM8/L2DpJNeodC6pihveNsedt/QTVNLAsOvLcrJJ0le0s9N29uIxoM0tsT55v1PMDOVpliwGBue46XnBwhH/XRepkSZVUFmBRzhMFGcZCg/zHB+mPHiBHOlOZJWkqydK6+Uz09lIhLlpOPCpkRpIQM5WSdHykoxURxna2QrG8LrlwgVlwPT9Vb948WJyjYJibSVpuAUcIWLLMkIITBdk1PZ07yUOkRvppfJ0hQ5O7coIdVy43SEwMHFcqxFbXTbBXRJv+jU0wtkrSynsn0cTh+lL9vHdGmanJ3HvUDxugUh08S6pEKksqSQd3KVCq4Xy4JQfCh1mNPZfkYLY8yb81jCOu953j12ylXhLQoUyNgSWTtL2kozXpxka3QzG0LrSeiJixI+AiEfXZubmZlMUcyb5LLFn0unzcuFosj4/BpdPQ0oqnzB1aUkSWiaSiwRpLO7nm272nl2Xy/pVIFCwWRoYKZielkrqqpwzY3r2XlNF81tiUrJjHPb13WVRF2Yq69fx0vPD3ht5j1N7cxkmsmx5IptZJJ5JkfnKRXPhNlGogE2bGkmVhNElpdO5l49L4nG5jhd6xvQdLUSnp3LFhkbmiOWCL4qBBlNV6mpDxOO+PEH9IqmayUURcYf0PH5NDZtb2VseI6jZUFmdibD7HT6vOcv4IW3ewIveEJKfWOUno2NGOXK9ufivWsK6zY00tjilTJxbAfbckjO5ZgaTxJLBFnmkS1ieGCW4y8NMz+XQ7iCaDzIhi3NXH/LRuqboss+Ny+LPvgDOs1tCQCef/IUxYJJNlPELFkcemGQju76qiBzpXGFy1hhnBfmD3A8fZy0nbnsbRScAqOFAmOFcdJWBktYGLJBTI9ddNbG1SLwhJYF3x4VFVvY9GVP8+zcsxxIvkjSSl5SG4qkEFJDRPXoRQsyrnCxXIsTmZM8N/88h1JHyKzwLLysnQqKpCCXM2RWfGZe5hl5wadnMD/EodQhnpl7jnkzuSqBeCUEgrxTYCg/zGRxipSVouiUuCq6jbAWQZfXpp0Jhn2s29zE9FiS8eHZJfvNksXMeIpMKo9l2siKTDgaIFEfIRj24bqCfKbI7FSKTLKAJHmagromr8J3qWAyO5UmEDKoL5dnGBucwbFdEnVh/CGD1GyO5GyWfLaIEF4YfTgWoLmj9qLv05VEkjxHx7UgyzKRWJD1m5t56flBSBWwLM+J1jKdNVVg9q4nEQwZ7L6hm86e+mWFmLPRNIXG5jiNLXGisUBFkEmn8iTnciuel8uVmJ/NVkyjmq4QCnshxsYFhLhAyCCeCOIL6OSzRRxblJ0/U5jFy794u1Jomoq2jAZkJSRJQlIkWtpraGiOcfTFYcDTbmXLddMWjluJ5HyObKZY+bfh14lEAyuaAM8mGgsQifoxfCr5rCcIFRae43nC4xf6Ndg3xfHDo5VjG5qjbNreSktHzaoclX1+nYbmGF0bGpgYnSeb8Zy++09NMjE6j2U5qOqll8KpCjIrIBCk7TRz5twFhRipUq58adGrM6YBd8XJVCA4njmBIxwMSefammuQyyaQK4lAkHPynu+MBEkryY8mf8zxzAnyy2gvFsZZ/kdltX62CeRswmqYuB4jql1cErkFDdGMOcOPpx6lN3tqiZ8PgMyCs7FGQAkQUAOokieYFZw8WTuH5Vo4wr2gBmdhnJXxlp+pKqnIy6WjX+U4bOEwXpzgJ9OP88L8AUpuacW2z/curfQeldwSL6UOMV2a8ZIhRrddtGZm2TG4gvR8nv0PHuL0sTHS8zl0n0b3lmZuuGsbXZuacWyHwd4JnnnkGKeOjKIonuPfrffs5Pq7tjI5NMe+H7xE58Ym7nzbHgCe+tERCrkS19+5lbaeBk68NMSBfScZ7fcqAIdjAbbs7qT5na9MQeZiMQyVRE0ItVyaxHUExaKJW44EWctrphsatQ0ROnvqideEVn1eTV2YRG2oooUp5E3yueXfS/AipApFq/K71zQVf3B1/i0LxUeDQZ1SwcSxXVxHUMiZOM6laTlfDYQjnvP3ApZlY5bsVQmtXkLDMwsen19D91146pa8egpohorPp5HPlspte5FEF4rWdh2XkcEZ+nsnK9uaWhOs39y8pu+Kosi0d9Vx7MVhRga9BVJyNsfcbJZCrlQppnwpVAWZFVAkhS3hzfRnBzid7V92ApSQ8Ct+YlqUhB4nqkUJqAF0WUeV1Up0U8pKMV4YZ96cp+AWl2nNY6QwwhNzT9EZ7KDB14ChXPk0+jk7R8EpMmPP8oPxBxnID1B0FvdRQsKQdRJ6goAawKf40GQNx3XKkUNZkmbSc7I96z41+5uo0RMX3TeBYLI4yXfGvsdIYQTLXap298k+NoTXsym8kdZAC1EtWqmivmCOyVgZhvLDnMic5Gj62IoaGgkJTdYIqSHCaoiwGiGseX9v9DXQE+q+KD+mkltiojjJA+M/5FS2b1lhDECVVOJ6nAZfPbV6LX7FhyqpCARFt8S8Oc9UcYrJ0hSWay07hllzlgcmfogiKWyLbiWuX7io42pIz+cY7J1g8OQEN9y1jY4NDWRTBX787ecZPDFBJOatth/6xrO0dNTy67/3SwQjPoQQROJBbxW7im/fUO8kpYLFG995I7VNUQyfvsSB9OcBSZZQzs3o63qOna4QKGuQZEJhH109DWvOS2L4tEXnWJbjOeGugKYp+Ayt8hwX8tKsVtnpOi6lguUJa3j3wPBry5qkft5QFHmxBqPs1+Q47gXDkA1DRTvL58q2HGzrwguyBRzLqYR9A6iql3n3fLKI6wqmJ9Ok5vNY5hlBMxoLUN8YXVOdSlmWvIi0c97PfLZEcj7nZTu+xPVWVZBZARmZGiNBq7+FBl8948UJJCCkhqjRa6gxaqjRE8T0GGE1TFAN4Ff8GLKOIqkokoIrzkTYJK0k48UJBnKD9GZPYbv2komo4BSZKExwJH0UQzFoUK5sxWCBIGfnGMwNMl6c4FjmODk7V4m4iqhetE+Dr564FieseRFWqqyhlsdnuRZFt0jOzpOyUsyZ88yUZpgqTdEWaCVhXLwgM28mOZ3r52Sml4JTWHK/mnyNbIlsZktkMy3+FuJ6DN85goYQgpJrUmvUUu+ro8FXz8Hki8yWZjk7mbeEREgNcV3iGup99YTUEH7FVwmZDypBIloEdY3mGiEEk8Upnpx9mr7sadJWesk4gkqAtkAbHYF2GnwNxPUYITWEJmsoyAi8CKd8+R5Pl2YZyA0wnB9mzppfdC3TNZksTvH8/AEMxcfVsR2XJWIsmykyP5XBF9BpbIvT3tNAajZLMOz3Qi0nksTrwiRnMmy8qo11m5swzrKf57PLC/Bnl0wE6FjfiARMjyeZn07T2F5DS1fdsue+khBCYFkOqfkc87M50sl8uTyAWc4h4vknuI7AcV3y2aIXlpoqnLkG5Srra0xs5gtoNLXEL2hSOhdFkRf5ebiuwHVWbtsfMIjGA5V3aSHMNpsuEAwb5zW5FPIm6ZSXYdgpt6FqCjV14Vd0yPVyLDyfbKZIci5Hci5LJlWkkC9VtCdWOVeP63gJ62anM5w6Pn7ulRDiTDHKlYhEAwSCZ75rhbxJNl0gly3iDxgrZgj2wuWLZNJemYoF/AEvu/P5vgmu4zI9mV5k/gI4cWSUb//LU8s7dq94LcHkeJLx4blF20tFi3y2tFYF5LK8ut6gnyGSJKFLOm2BVjaFN2ILm7AaptHXQFuglRZ/Ky3+JiJaBF0+v6PawsQ1XZrhRPokAhjKDZFzltqj806Bl5KH6Ah2UG/UX1HzkitcZswZZswZhvMjpKwUACE1SJ1RR0egg63RzXQGOkjoifOG97rCLTsuTzKcH2EgN8j6UA+JS9AIjBfHOZHpJWtnl+wLq2E2hDdwa93NtPhbVszHI0kSPsWoaIfa/K3k7Byma1XGu4ChGKwLdrE5uonEJWiSziZtpzmdO81zc8+RttOLNFYSEjEtSkewnd2xXWyKbCKhx8+bW8gRLkWnwNH0cV6YP8CJzAlmzbklxx3PnCCmxWjxNdPob7j0iLGyo6Dh01DKRe8WVtTCFZSKFrbl1fNRNQXlrBXkwodQUeTyZHmmDo9ZtLBKZz6ym6/uIBg2OPr8AMnZHNPjXr6VuqbYJfX/SrAwhnQyT3I+x8xkmrGReSZG55mZSDE/lyOXLVLMm5RKninBth3P6dJ2sW3nsjhT67pGrCZYMVOthUXmywsIUeGIn/qmGD6fVnEcTc3nOHV8HF9Ap6YuvOT7sCCYTY4nGTg1RamsGVBUmVDYR1NrAv+rwNEXyouikkU66dVZmhidZ3xknsnReWZnMmRSBfK5EsWCl8LAMheetVOug3RxD3uhTpOmKViWQ6loMTWepL93kp7NTZX8Lef21bFdBnrP+KKA57AcSwSpa4iet0SC6wqSs1kK+cWmxhOHRzlxePSixnEutn1+DeBaqAoyF6A90IYiKST0ONtj22kw6vEpvotyXq3TawknQnQE27l/4J/py50uR6GcwXRNTuVOkzK9lfulRvucD1e4nMz0krPzFcdeGYnu4DpurruZnbGrUM9T5fdsvEk5RkyLsTG8YdH2i2WkMMrxzIll960PdXNVdBvtgdUXO9NlnUZfI9ck9lBwihxIHqzsEwgyVobHZ/YT02OXTZA5lenjpeRhZsylzrOKpLA9uo3b6m+hK9i1qnslIxFQAuyJ76JWryGiRfjhxINLjis4BU5lTxHX47zWuKsifFwsgbCPSDzAxPBcRbtiWw7jQzNs2tFBTUOUQMSHWbTIpQsUcyVCZ6Xcl2QJ3adRKpqUyoKL47jMz2Qwz1otBiN+Nl3dyebdXeSzRb7++UfZ/8ND3Hj39kvq/5XCcVyOvTTM4w8f5YWn+kjO5SrCwM8gYzywEB1jXHETTSwRpL2rjtr6MKZpUypaZNIFfvidFwhH/V4BzGXMJLbtcuTAIE88dryyLRjy0dgSp6W9Bt8rOJvv2QghmJlI8/TjJ3nsh4cYHZolnzNZ0KxcKRpbE7S0J4jGg8xOZxDCc5Z9+HsHaWyOodeqSMrib4frej5XP334SCXEHiCeCNLUmqC+KXreRbIQgkK5dtOVQrii7B916TqZqiBzAfyK3zOR6AnCWuiiMtWe7SCrywZ1Ri074zswhcVAbmDRsQKB5VrMW/OkrDRxPXZ5BrIMLi7TpZmKz4iExN7aG9gVv5qeUM+aTBJnH7fgn3KxCCHIOTmSZpKMtdTRWkKiJ9xDR7B9TRqrhWO7gl2czvVzJH0E8yy/G8u1KhFAncEOguqFowJWwhEueSfP8cwJ+nJ9S/bLyOyO7+Lq+E6afE0VB9/VjgGg0d/AtsgWxgpjnMr2LXHQninN0pvt5Ub7+kp26ZWwLYexwRmefOgwAycnSM/n8Qd1ZidS7L5lI/Utcdp6Gujc2MRTPz7KUz8+AgLitWE6NjRWNCZ3/PIehvumuP+vHkI3VIQr2H5dN9uv66auOUa8NszJF4f5x5H/wB80SM3laFhIiy8E+37wIideHEZRZWzLwTJtNu5oW8Ud/9ni2J4j5CMPvMSRg0OMDs2SThUqeT4Mn0Yk6qemPuJl+Q0aGH7Ni3rRvZX13HSGl54bWBSRcjF4WjD50vXzF0KCRF2Y1751Dz/41vMMnJqkVLQ4dXycr395Py8+28+6DY1E4l6JArNkk5zLcfLIKMcPjzI65AnzsiKzaXsrt929bc3msJcDIbwIqyd/cpynf3qS44dGvFpLBasitIYifhI1QeI1IUJRfyULsKZ7/i3pZJ7+U5OcOnaueenCSBL0bGrm5ru28sA3n/NqOM3meP6pPnLZEuu3NNPSnqhk6M1lS0yMznv3vdxXJE/gvfE1m9m268LfTSEoaxAX++KEo/5KBe9LJV4bxjB0LseL+8p/i15mVFlFldVLmtTORpFkfLKPTeFNjBbGGMwNLpn0FxLSpa+wIANUomf8so8mfxM7YzvoCfVcdKTRApeiiREI5kpzpKzUkvDkhWzE9UY9US12UdePahFq9RpiWozp0syZyDJcck6O6dI082bykp655ZoM5gYZLYySthbni1AllZgW5arodrqCnQTUtRcLBE/Ibgm0cE1iD1Ol6SWCTMEtMF2cpi97mo3hDcTO8y5JEhh+jfrWBIGwD8u0UVSFaCzg1VHxa9Q2Rrn6pg1Mjc5TyBVRVIW65hgtXXX4AjqO47Lt2nWEon6mx5LIslR29g2gagqarrD9um7qW+awymaqhpYEifowsZoQsiITqw3T3FnrnQtE4kGaO5av5v5yMjo0y3NPnGLfj48yPZGiVLLRdIWGphid6+tpbquhtj5MJBYgEDTw+TU0XUVVFRRVLpcGmODk0bFLFmSQPGHmSmpvwROiQ2Efe/Z2MzuVRpZgqH+GTKrAsUMjTE2k6DsxQTBkICsyluWQTRcZHZwllcxhWw4+v86WHW3ccOtGNm5rXXNtqUvF81Fa2yKrWLA4fmiEpx47wYFnTjM345m6w1E/jS1xunoaqGuMEEuECEd8+IMGetlBV1EVVFVmdHDOE/ouSpCRaGqLc+3NG5iZTHP88Aiz0xlmJtO8kO1jbHiOmvpwxSm+UDCZn8kyOjRHIVdCkiTqG6Ns2dHOtTdtoLW9dnX3XGKJjLFxawu7b+hZc+Xu5WhuT1DXEFmT4/BKVAWZl4lWfzMNRj2arC0bxZKzc5eUwG2tRPUY1yauYV2w65KFmEvF892ZJWcv9SHSJJUaPVGu43RxWWxlSSashak1apk155aY9+bMeebMeVoD5y+mthJCeFFGR9PHmDXnluimAkqAzmAn60JdlxxVFNEiXBXdzlOzTzNrzi6J7Mo5OY6kj9LoazivIKOoCg0tCeqb47iigCQpyNLSaKFt13TBNYszv7rCxHWLyLJKfXPsjIaljBAurjCx3CTbrmvlquvXlc8rIUkqEgpSWcu544YedtzQczG34meCEJ6Pz/HDI+x/5FglnFTTFOoaolx93Tr23r6Znk1NxBIrC8Kz0xnSycKiaJRXA4ZPo7Wjlpvv2oKsSBSLFpNjSYoFk9GhWcaG587kRpElFFlC1VQCAYNQxEddQ5RfetPVbLu6nZq6S8+QvVZsx8WxV28ucV2XTCrPTx46zKEDQ8zNZD2h36fTs7GJa25az/W3bqS2IXL+XDrlsgIXSyQaYOOWZsRbd6EoEi89P8jcTIZspkjfiXH6Tkx4910CWZJQVBlNU4nGA8RrQmzY0sLdb7ma9q46guELR15KEui6inpOrpgNW5q55949aPqlBxBcTqqCzMuELutEtAhxPcZUcXqJVsZ0LawVwnQvN4qkUGvUsLf2eoLK5dE8XSoFp7jI7LOAKqkk9MQFHawvhE/xEdEiy65iM3aGjL26rJvLIRAUnALH0ieYM+eX7I/rMfYkdhFSV5/zYyUUZAKqn9ZAK9OlaaZK04v2l5wSp7P9XJe4ZpVXdMmbJ1GVOH6tY1VnmM4UrltCU2pR5RCc41jsCpOSPcJ8cT9x30341DYEDgV7EE2OocoxFOnKlOe43Ajhqe5PHR/nxJEzTo819WH27O3h199/G4GQgXKBkNqFisI/Kz+ay826DY3Isowsy3znX54inyuhKDKqrmCZDpLkOZYGgwYNzTHWbWhk81Wt7NjTRTjqR3+ZTEqO7VQcX1eDZdrMTKV54ek+Zia94ABVVehaX89rf3kX19+yEc1QL6ihcF2xqPr3xRAIGly1p7PiNPzYg4e9/mheKLVtuciyhM+vEY0HaGyJs2FLM9uu7qBnUxOhiH/V1bYlScLv15eU2iiVbPI5k+grzCT4yurNLwgLkqxf8RNRI0wxveQYyzWXzZtyJag36ugKdhJUgq+ISsoCcVbBxMVIkoyhGJfcT1VSVxSGvAKbF3/vc3aO8cJ4pX7S2UhIhNUw3cGLy0lzLpIkIQuZBqOeiBZZIshYwmamNFtOCmijSGA5XrI5RfKjKl5dH9vN4rhZHDdP3jqFLhpRJD8gUJUYsmTgChPTmUFGR5EDSJKGaU9SsE8jBMiSH4GNIgWRJRVHFJAlvaLdEcLBxcSl5FXpcgsgx5AkGSFcHJHDFSYIB5BQlRi2m8Z2015SQjmOIgeQL6Ge1KWykCRsdjqzyBGyo7uePXt7CEV8yKswl9i2Qz5bwnVXnw/klcRQ3zRPP36SJx87TqlksW5DI1ft6WTr1e3gKQaQFS8Jnt+vE4r4iMaCxGqCazYnLaS8PxshQLhrd6jOZYpkzgp5vxCpZJ6h/mkKObMiiOiGxrU3radrfUO5ztCFx1IsWotyuVwMhYJJ77ExHn/4CMcOjWD4NDZvb+Xq69fR3OaZX6WyL4ymq/gD5QzAiQChsH9VRSIXkGWJRF14SQ6nharW0VjgyvtkrYGqIPMyoss6fsW/rHOsLRzsS6zrs1pqjVpa/a3ndQb9WeO4Nq5YLgkhl1x9euE65/MpuBRn5aydY7w4QdEtLbmOX/GXo6Lil01olCSJGqNmWQ2PQFBwCyStFBlrGr9sUbRHUCQDXWlAkcNY7jwlexzHTSNJKpY7B7aMhIblzhAxrkaWfJjOLJY7h+sWkKUAmhLDdpMUrCFkScentlI0B/GpbahyhILdj640ospRJElDkhQQbkWgKTnjKHIQVY4gcMmZJ7weCwvbTePXOsvC0yy2O0fUuBZFvjyOhheLK7wK0Llz/FrqG6N0rW9Allc3SZeKFjNT6SsaFXIlcB2XUsnmhaf7ePonJxg6PU0o4hWFvPmurWzY0lwWPC7fLCdJlLU/UkWYEK5LqWQhlvlGLMeCSTBVDpVfLflsiemJNI5zph1VU+jZ1LxsuPlKpOfzZNOrF6DOxbYcpidTPPbDQxx45jSp+TxNrXFuuXsbu65bR0Nz/LL4rSwgKzJ1jVHCUf+i+z47nWF0cJZ1GxquuE/WWnjlzFy/gKjltPrLIXARq0infzmI6zEafFc2+d5aWelHIoTAXiGr7VpwhLtinSNFUlYddr4cOSfHVHEKx106SUW08AVzxawVL/Q9QkBZ2Wk4aSaZK41RoxUpWAOoShRZDqBjU7BOU7K9EE1DbQYkXFHEdMZJlw5iKC2ARN46iSSpFK0hBAJDbcJQGrBFGlkYCCwy5iEkSQVJImseJaSrywgfMkLYFK1BNDmOrtQBEpnSAVQ5DmWhxhYZFMmP5cyTt04R0q9C4uXNAiuEoFiwsKzF744vYBCOrm6V6rqCbLrI0OnpS16l/6yxbIe5mQzPP9XHyWNj6LpKS3sNu67vZtO21ivSpizLqKqCrquUymn9bdvzXTk3qmYlhBDksl6NoUxy9b6HluVQyJcW5deRZYlIPLCqTMpCCITr5dGZnb74en2FfImRgVkef/go2XSBRG2YdRsa2XvbpjWVpVgtsiyVS1iECYQMsmlPcJ8aT9J3coLrb92IpkuXVXi6FH7+c0O/gpEk6bJoFy6ViBqh9jLlTbkcSEiVMgjn4uCSs/PY7qVNAJZrUbDzywpEuqxjXIIPTt7OM1WaXuJEDHgZgtXL70ztk5e/XwsUnCI2fvzaOiRJxbSnMO1JhHAo2aNoSoKo71qC+mZkyY+hthA2rsavdSFJMpY7TdEewXaTyLIfSVJw3DRBfTMhfSuG2shCpaiF2XwlYVOSZGTJQJVjSGeZiQQCv9ZB2LgKv9aFT23DcQuYzmxZyxPkXP+bnzUSLKuit89JA38+zJLF1HiKIy8OUcj/bPzgLhelgsXpkxNkUgWEK1A1hfqmGP7AlS0joWoKkVgApZz0r1gwGeo7U7n7QliWw8mjY0xNpFYt/EDZcVaRF8mnQgjMooWziuu4riCXK3H65EQl/PximJ/LMTwwjWXaCOH5y9Q3Riv340ogyxKtHbWsW99Y2TY5nuTE4REmx5OYpZ+N68NqePln0SovKzIyPsWHX3l5VfZnI0kSMT22rIbBdi2mS9MU3cWps9dK1s4yY84ue42wFiakXnxERd7JM12awV1GkPErAUKXKZR/Aa9GlI56HlOVV+k8i+OmALfse5JCkhQMtQXbTZMqPke69AK2m0WSNGTJhyx5Ap0qx/CpLbiiCMJFlSNoSoJM6SA58xiWMwfIaEotRXuEZGE/RWsAVxSx3SwlZ4KiPUTWPELeOoXpTFG0R8mZJyjYgwjhJRaTJB1Z8nmmKGQcUcAVeWTJVxaLXl7vWEmSiEQD+HyLBd10Ms/UePK87+SCg+/hA4M8u7+XbKa4yGTxasDLqeJWqiFbJZvBvil6j40xMjhDoWAuytx8ufAHdJrbayrlDHLZkhfyPZ5akn32XCzLZn4myxOPHmNkYGZN7Ro+jUgsgHRWskHHcRkfmSeTOb+pyHUF87NZHv7eQQZOTVEsXvzE77pept6F25pO5ek7McHJI6NMT6Y8M5u79vIWK7FgHly3oZGtV7dXNC+maTMyOMt3//VpBvumF5U+WA227ZDNeOUcLqdZtWpaugTO1BoqUXKKlFwTW9g4wsFxHe/PcrK5harFLt6P3EUwlB8qh+e+fB9nTdbQZO0V5R8jIVFr1BLRwsjIi9L628Jh3koyV5ojZ+cIaWtXq+bsfCXEerlioHEtftGlFQSCkmOSttKLajktkLUzDOQHkWYv5xpCkLcLSxx9z2bBeVxCw1BbkKUAulKPhIJf7cQVFpYzgxAWfq0NXalFkXz41I6y5kRFkfwU7SFARpUjKHKQoj2GKkfR5ASqHCag9WA5M1jOfMVXRpbUsg9NO4ocwnvCKj6tBUUKlsOvVfxaN5ocQ5ED+NRmLCeJIgUw1DYkScVx87hyEUW6uLw7lwNZ9nwHIrHAIt+BseE5Dh8YpLYhQijiXxJWvVCXaGRwhv2PHOfwgcGKMPBqQtWUcs4SHVmRsCybseE5nt3fS2o+R11jFMPQlpgcJElCViRUVcEf0AlHA8QTnvPvanxqwhE/G7Y0M9w/TT5bwixZjI/MceCZ0+g+la71DYTCfiTJa0sIUTbhFRgfmef4oRFeem6Aueml5U7ORzDso6EpVimyKIQXyXT4wBC1DRGCId+yRU3z2RJTEymOHRrmsQcPMzWevKRUz4GATrwmhK6rmCWbbLpI34kJfvrQEVo7J4nGg+j60kKQkuxplHTDc/6NxYPEEiFCES/Q4EL3vbElxuar2li/udlzes6bJOdy7H/0mJfQMpmjua2GaDyAz6ctinJybLeSATqXLZHNFEjO5ZibzrBxWyuNzbElUVEXyytn9nqFsyDpLggnjnAouSYpK82sOcNMaYZ5M0nWzlJwihSdIgWngFkWbmzX8f7ExnFtLGHjuM6yE+nPEl3WL8kf5EogIVFn1JLQE/gU36JEbwKB6ZoM5Ido8jfRo3avOivuwjOcKIwzXhhfUsNJwhPs6o1aai6y2KUQAltYlUSD5zKQH2QgP3hR174UTNdCSIZnstFakVA951tAV+vQ1dqyQC3wCiF49zPu31u5hlCbCBqbgHL2ZiER0rd6/yqbSA21AVHWRElnaYh0pZ6QvmVRn4L6+kX/rg3cWfm7YoSYzH6LgLYOXWkgXTqA7aZwRAwFT5BxXfdMWKs483y9+j6e+cE6p5aL67qUijbFguVVn6b8MZfKhjFpYdKVl7X/y4pEc2uc+sYogaBRSWbX3zuBLEs0tSbo7KknFPFXJhXhCpLzefpOjPPgdw5w6vg4+VyJcNRPLltclYnilYLPr9O9oZGmlgQDfdOkk3lKRYtn9/Xy7L7eFc+TZQnd0AiEdJpaEvRsamLb1e1s391JKOxDPbcS+DlEE0F27Oni2f29zM9mK5WjH/6Pg2TSBW5/7Xa61jd4JRIk756bpk3fiQme3dfL/keOkk4VUFUZn1+nWFidSSoc8dPWWUssHiCbLmCWbEpFmyd/csLL2Bz20dpeg7TwrpSrWo8MzvL04yfY/+NjjAzO4Avo+Pz6RZsSYzUhL/FeQwTL8t7fuZkMD/77gfOep5THG40HaGqJs2VHO9t3ddCzuRG/3wD5/MJMIGiwbkMDr3vrLr79L08zMjiDbTnMzWT57r89zfHDI+y+oYdN21qoK/8mZFn2ShzkS6SSeaYn04wOzjJwapL+3knGhub4T//l9URigVXltFkNr6wZ7BVOwSkyUZxgIDfAQH6QicIkGTuLLSxPSBEOrnAr/3vVbN3y9HDmP4SobHs58UwSKsorwE/nXGRkmvxNdAW7OJI+smT/odQhwmqIJl8jQTW4Kg96FxfTMXl67llOZpd+dHXZYH24h1qj9qKFO+89eOU5cLo4OMKl/NRZ3it1mVSeSzj3XVnu3bn090mWNGK+6yg54+Ssk+hKoqzhiVaOSSULzE6lmZ/NlicYC7NkeQUaixbpdIHTJyYWXXduOsNjD77EsUPDhMI+DENDN1R0n4ZhqPj8GqGwn6bWxIoJzDRDZeO2Fob6u9j/yDEASkWb0ycn+MJfPEhjS5yauhCBoFE2L+SYnckwN50hncxjWw49m5q4/XXb+Y+vPcPY8Nya/DZeTmRZwhfQufONO5BkiUd+8NKqfIPccmFRy7LJZ73keQef7aez5xj33LuHjVtbMHwr+6WFQj66Nzey+ao2ctkiU+NeTpdsushTPznBsZeGicaChCK+SnmEdCrvhVunC+RzJpGon6t2dyLJEo/+4NCqxquoMtFEkBvv2MJPHjrM0OnpsuNwkUceeIlDzw/Q2BL3/HcUmWLBZHYmy9x0muRcjly2hKIo3HnPTooFk58+eLgcbbW6+72Aqio0tyf41ffdwr//6zMcPzyyKtOMY7sUcp4GKzmbY+DUFM8/eYptuzp4833XEY0HL5iUMZYIcd0tG0nN59n3yDF6y7WbSiWb3qNjjA7N8tB3dTRNQVUVZFnCKVf9dhzX8x8zbcyiTalk4V5EhfcLURVkLoAjHNJWmqH8MKdz/YwXJpg1Z5kz58lYGSzxynF4uhiks1bfrxQWVgit/hY2RzbSm+3FOidSad5Mcih1GFmS2RrZTKOviZC6cmn6jJVhtDDOscxxjqaPMW8mF7eJREgNsSt2NfW++ot2wrZcC+sSHZGvBAv+Jd79WV6IudBbcO4RKy3kLk/orYKhNiNLPhxRQJEDaEp0UQ6ZodNTvPhsP73HxnDK1aRt2/X+Xv54Zs4Jec1mihw+METfiUl0XUVRZVRVrqSS9/l16huj3PnGncsKMgtmkO6NjeQym0jO5eg/NUk+W6KQ9ybo+dks/oBeqTWVz5sU8yaWZWP4dK7a08n1t27kqj1dHCyH0qbWEEnzcuE4Lulknt6jYxw+MMjg6alFPj4L5qNz3xPXdStVsB1b4NgmxYJJJl0gNZ8jFPbhOoKd165bsW1FlQlH/Nxy1xZA8NwTp5gaT1X6lEnlmdCSGIaKJEs4tkupaOG4Loah0dAUZe/tm1m/uYnR4aXV4ldCkiSCYR/X37qRXKZYziM0i+u4XgRUusDkeBKfT0eSJWzL8SpgF01kWaa2PsKevT3cePsmRoZmOXpwiPHR+TVp4WzbqZjHDh8YZHY6jXvWfZflheihM/dd4EVLLWgsXdPBMr2+5bJFigWTQEDn5ju30tpZe972NU0hlghy3S0b8QcNaurCHHtpuOzrYi7SMi1oNFdK/qcoMuGoH39A9+qDXSaqgswKCCGwhMVkcYq+7GmOpI9wMnOq7PuwupdQkRRkSUbB+1OSJOSy4CBLEpawMR3zVS8MXSlqjRo2hHpYF1zHcH5oUckGW9gM5YdJW2kyVoauYCc1Rg0+xUCR1EpuHkc4lJwS06Vp+nL9HJg/SNbOLtGaRLUoPaFutkQ2EdOi53Zl1diujfMK1Mi82pAkCUUKoMgr+8NMjac4+uIwB54+verrmiW7sppfDt3wwon33Lh+xWMA6hqjXLWnk2LB5LknA4wMzpBOFijkS5SKFoW86SUnUxV0QyUSDxCJ+mlqibP3js3suKaLUNhHa0ctQ/3Tr2hBZiGEeGosydFDwzz12AmOHRohmy4QT4TKidN0dF1DUeWygLswqQqvvpHjYlk2ZslmfjZHaj5HPlciOZfj2X29xBJBejY3l00TywvCiiKz9eoOHEegqgrHD4+Qms+Tz3r33LYdcpZTLqKpEAz7CEV8NDbH2LClhdtft51wNECxYFX8aFaDrqtlwbWIosi8+Fw/yXlP22IWLdLJAmkKKIrXrm5oNCbi1NSHWb+piTvesIOWthocx6WpNc70RGpVgoyX+0bQ3zvJwWdO8+z+U5w4PIKqKjS2xIklgmWBQCln7JXOOdfFtl0s0yafL5GczZFK5igWLEYGZnnkgZdo66qjtiGCz3/+KE1Zlj0/pIiP2voIsXiwIrTnc54Qb9sOju2Ze2XZM89qmvf++/w6gaBBNBagoTlGY3P8/CUd1khVkFkGzxnXJWWm2D/zBPtnniR9gZT1Z9Yh3opNQcGnGPhVPwElgCZr6JKOJqtokudgm7SSTBQnmTVXv0L4RUKXdZr9zbyu8Zf49th3KeSGznH8tZkxZ3l46hF0SSeqR2nw1RFUQiiygiMc8naeieIEaStNaYWSD4qk0B1ax10Nd1Jr1J43jPlCOMItm3BWRmLpqvVK83K0eaWRy46MlzMEVVWVcubZ8x8nSRL1TTHecO81bL26gxefG+DFZ08zeHqaTDKPZTkoikIgbFDXEKGzu55N21vZfX030XgQVVMoFS2aWuPEEiHGR+ZXnfFWKtfSWUBRZM9HY42P13MEPXMtRVWQV0hhXypZPLO/l+/8y1OMj8wjhKCpNc7Oa7q49bXb6eiuJ1o2ryyHbTnkskWmJ9I8+ZPjPLPvJCePeCaKqYkU/b2TDPdP07OpCekcU8fCPZEkCV1X2bO3hw1bmzl1bJwXnu7j1LFxxkfmyaa9KDDd8GoMNTTH2Lyjjat2d7LlqjZPQBIQjvkxfBq27aCc5Qt1rmBzdrsAO67poqO7nquv7+aZfSc5dWyMybEk2XQRSZIwyqUB6hujbNzWylW7O9iwtQXD52X/DUf8tHTUcPzwKLbjIpcjoVZ65K7jUiyY/MfXnuWZfSe9YpUSbNpaz7U3b+D6WzfR3J7AMNTKtc5GCIFlOp7WsHeSJ39ynGf39zIzmcY0bYb6Zzh9coKOdfW0dZ1fK7NAXUOU2voIN9y2kaMvDnP80Cinjo8z1D9NOpWnkC1hlhxUXcYfMIjGA9Q1RGnrqGHdxkbWbWykc12955N2GZMmVgWZZbBci1lzjgfGf8DxzElyzsqZIP2Kn1q9hpZAi+egqsWJaFGCagBVUpEl2fsf7yMlLRhzJIlj6eM8Pfcs8+b8shEuVbz72x3q5va6W3lGeZZjmePLZvy1hEXSTJK3c8hS2XFQCBzhVhyuz0VCwicbXJPYw674Llr8zZecbXfhea9ER6CdjmA7ce3SikWulQZfA83+5p9pm1eaXdd307W+4dKrR5+FLEnoPo2mltU/n8aWOOGIn53XdFIsWOUwWS9P/0K6eJ9fIxA0CJ9V70bTFa67ZSMbtraQz5WQgM71Deeth6MoMtfdvIHWjndXtgWCnrDkD6wt99HOa7vo7K7n3vfcBHgRSbH48qkBnn+yjxef7WdmKo0QgtaOGm56zRbuvGcHsZoQPr9+3uRoiiITDPnQ21XuetNOZFlibjrL7HQaISA1n2Oof5rOngbmnQK2cInpfnyKtqx85g8YdG1qpLElTiFvYpZsHMfxSmXInonLVgQDVpLZiIkpHAxUTNfG3xVk73/Zxa6aVhpCYRIxL/JxvJBGliSiuh/fClGcobCPDVuaaWyJndWu63mXlTVBuq7iD+qEwr5FNaVaO2t5069cxy13bsNxXWLxoOecvAJzs1meePQ4vcfGSKcK+Pwa7V11vOHea7hqTyfRWKAcqbTyfVfLZqHNV7XS0p4gmyly8JnTlTINE6NJpidTqxZkFpAkia6eBhqaYlx703qKRRPH9nxihCuQZKmcyNB7/w2fhj+g4wvoV6S0QVWQOQeBYNac49m55zieOcFsaRbnHFOSIimE1CAdgQ5aA600Gg3UGAkiWoSQGsSvBDBk/YJ+FhPFSQzZwHuyVUFmOVRZJSyH2BLZjCqrRLUop3P9zJtzizQsAs8UaDkXNtMt1Dtq8jfSHVrH1sgW2gJtBNRLz6Wjyep5HYUTepwNoQ10BldXkPFy4VN8BM+T+ffVSLwmdEWymq4FSZLwB3T8AZ3ahrUlOpRlmZq6MDV1YUoli/lkvrJ6P197C+ecjesK5pM5fIZGcJlw4OWIJ0LEExe+f0LAsZeGOd07WXHsXb+5mW27Oir+FRdaXUuyhCorqJqCP2jQ2llLfVOUudkMwhEU8iZzM1lKtk0WE7csyACUHBvT9ZzVhRCokkzONjlZmKY1GqO+MU5Y88acMgu4QqDJCrIkMTVZxBQupmujK56JPxA1qN9cw7rGJlqCUXRZxRYuGbOEoahEywrZ+VKBkmMhSzJhzUBXFGzJpWQ4hBuDRKUQAir98isaPtU7OWuV+P+z959BlqVnfif2O/6ce65P7115b7q72qC7YQcYYAbDwdAsqRXJ0IpBrajQB0YsFdIHRUghReiLtCFtSEHtkrEMTsSSHHLEGQwGMzADoLvRQLvq7vJZLk2ld9eb4199ODezKiuzqrKqMqurgf4jEp2V997j7nve87zP83/+fzcMcP0mWcNCFiAZMkanyWC3jalqaPLDF02VUpP3374Ri/j5IelsgjMvj3HwWB9dPdlHX/OWR5VuqHF5M5tgYLidqZtLG4FMpdzYwiN7FNb3a6fMXes6elp8EcjcBzd0mWvO8V7hfYpeaUsQo0oqWS3L/tQ+XsidZSw5Qu4JNUe+wM7RaXZgKRYZLU0zbFAP6psCmXtLJ+vlp5iLFGfDVFlFlzR0RSeh2PRZPRxI7edU9hQpNblrOjrrZcPt/LMg7ozK6VkGEnsj5/5ZwQkCSk2HquuiKzJp0yRn7b7IouMHNH0fLwzJWiaG+psxhVVrDpeuzHL29DD5J3AWDsOIG7eW6O7KMLLDQGYniEX8Qu5MrrK2dLe8PnKgi/7h9icuD1gJnUxunZwfc2hcNyCIIqqhE3d1CgBBwW1QcBtISNQCl6xuUvc93lq4zStdw5iqhqWoVH2XpWaVRuBjKSrDqTZUWSGM4u0KAbqiYCgaiiS3ysBxJ6kfRVQ8B1vTCaKQYuiz2KhQcBsI4Ei2CzeUqfgOq80akiShSrH79/px9VhpuqwUbuiz0KxSDzwsRSOtmzRCnzWnTsGpI0kS/XaWvJl4YMdoFEXUqk1uXJ2jUYulHBIJneMvDJPLJ5/4uqczFonk3fHhe+Hnzu9rO/xmzAK7iOUWKXS+ubDtgyirZTieOcYf9n+XhJJ4ag2WL/IwO4ciySjIeJG3pby0rocTijAmTwtQZQVDNkkoFjk9T5fZSZ/Vy77kGB1GO5Zi7brbtyZrGLKOIinblrN84T1QY+bzjIVKlb+5fZtfTd+hO5nk9ZFhfvfggd3fT7XK+MoK85UKv7N/PwPZJydmP09YXavx1z+5zMhQB/kHlHceBt8P+dV7tzh9cpCRoccrEzwMYRhtdLp492jyZHIJkk+xGo9CET9AWxOgosgYhooiS8xWyrhhQJeVQldsxktL3K6u0W2luVJc5Fiumw7Tph54LVK4RNX3eG/5DkEUsuLUqPku3xs+jr+N39n9kIi3MVkrkNQMVFnhVmUVRZKYb1RYdeocyHQyWVtjoV7Gj0JulleQJIm8mdg4rtNtfWiywnStwGS1gKmojKTaAInx0jKLjQpOGHCzvMKbPWOcUnuxte2DTt+L3dHrVWejA0jVFNo70zvyeHoQgiAiCu8+dTRNeWT79ecBXwQy92GhuchsY3bbIEaVFA6k9vNa+yvYio0iPVzE6VEIRIAXxdLsX+DhKHtlPi1d4L3C+8w05nCimBdhyAb7kmMczxyjx+yOW/9apD255WWlSAq6pGMoBqZiklRt9FawsduQJAlDMcnqGQruVuXgetCk5u/cfffzgs6kzZsjI0RC0PB8wj1Sre1M2liayuHODtrsz0ep7Or4PLduL7O8GvNB7ITO2EgnL54doViqc/3mIh+en2Lqzhp//sNPaW9L0tGe4utfOYKmykxMrfDphRnK1QaeF5JKmgwOtHH21CBBEHFntsAnF+9wdXyBUrnBzFwRSYKvf+UIPV0Z6nWX8ZuLTEyuUK400VSF7u4MJ48N0NP96EBwO5J4GAqi6PG1b9bbsEuFGguzhY171TC1mPOhKCRUvZXRjBGIiKRqcKqtj3rgYmsGkiSR1Ax6rBRZ3aLoNpmuFdBllUgIUpoZZ1taXKWHnx9osoKt6hiyiirJdJo2N8qr1H2PLiuFqagUnDpzjTIJVcfWDNwowFb11nF5GIpK0WswWV1jMJmjJ5Emb9goksSyU2W+WcGUVdK6iSrLhI/qmrrvuIWAMAgf2Nq8k+u+OFeksHrXvDKZNjdlaD6v+CKQuQ+r7gpL7vK2r+X0PEP2IAOJgacOYgD8KMBtpVG/wPaIREQoQi5VrvBR8Tw3qrc2Mh0pNcVQYpCX289xMHmAdqPtuTDhtBSTvN5G2atsyRxV/Solv/TMj2m+UmGhUmW10UAISJsGvekUQ9ksThBwY2WVZhBwurcHgFtrBQqNBqd6e3CDkOVajaVaDVmScIIAQ1XptG3G2vIYqoqt6wxkM/SnM8xXt7r81j2PiUKB1XoDNwjRFJmOpM1QNkvGNGn6Ph/PLWCoMbeh7Di4QcC+9jaGczlkSWKmVGauUqHmubRZibh0pWkEYUTZcZgqlSg7cYDrBTEnoied5lBnBwCzpTLzlQrNIC5PuUGApWkc6uyg07axtN1rB70XN24tMTm1Qi5nYyd0EgkDy4r3pSgyCUvHsuIumqRtkMsmSCXNjW4WTVNJpcyWJUBIoVjng/MTHD7Yg9kS8kslTUBg6Cq5bAJJktBUBd8PWVmrcf6TaVIpk3zOxvMCrl6bJ5+zSafMh3JqZFlCM1QMI26tXm8bXl2qUFqrk8k+fvZo6tYyEzeWWFupbtg0pLMJegbzNEVAxXMoek1makWSqk4kBIai0GklyegWuqwAEqosU3AbdPoOqiyT0S1qvksoIixVw4tCil6TRuDTbhTZn1GRQii6DZabVaZrRSxFR7GSuKFP2XMo4WCpGklNp+o7OKFPr5ZGliQSqo4uq9QDD11WCEWEoah0WUmyuokmKzH/TjOpBx5LzRqRECRUDVvV0SSZZuijywqGoqJu02m0DkWV0XUN3dA2fJR8L2Bueo1cW/Kxid31msvkzUVmplY3cWI6ujNb+FafR3wRyLSw3npX9suUvOK27+k1e+gwOjGVp4tg1/flRe4m+f3nGZEIcaMmbhg/KHTZwFQSyHuQ1bgXgQgo+SXeX/uA8eqNjSBGQqLH7Obltpd4MXcWU3k+SGcQO1x3Gh3cqd/ZohFUCSqseQWCKNiVYPhREEIQRBFXlpa5sLDAcr1OFEFS1znZ001XMknD9/l4fp6FSi0ODgS8d+cO85Uq+9raWGs0eG9mho/n5ulJp6i5sUbKQCZDR9ImL8soD5mU/TBkqVrjpzdvs9q4G8h0JW2+sX8fCU2j5nr8xbVxspZByjBYazSoOC6KLDOQySDJMrPlMr+cnma6WGIolyWXsMiYJn4UMl+t8qMbN1moVmlLJKi5Ln4UcqK7m9F8Hk2Wuby0xHt3ZlAVhbV6nflKBVvX+a9efIG0YexZIFOrOeiGyqkTA7TnU9hJA8uM/YiymQQnjvUTCcHP3x7nS6/s5+jhu91lQgja8jYnjw9sdAW9+94t/urHl3C9gLa8zb7RTjo70rz3wW1OHBvgu98+tfH5UrnBwmKJq+PzfO+7Zzm4v4vVtRpXx+dZXCrT35t9RCAjYxga2TabhG1skERvX19kYLidts4UVsLY1hF8/fghbr92HJ9GzeVXPx/nyqd3nb9NS6erJ0v/SDtNEdAMfaq+w2KjynAqj9XitEhAUjVIqPG/83qC5WaNdjPJYDLLiXwPN8sr1Hxvg/wbRhFV32G+UWHAziEQ1Px4bCw2anRbaTK6SdmLybnN0GexWaFDJFFkGVPRkJFwQp8+O00gQqarRQxFxYxUUpoRi2lqBpaqkdZMjua6+WDlDnP1CjU7Q28iw3AyTygEs7USpqKS1kxM5cHjTVUVEkmd9q4UK4sV3Na1+/TDSfId8TU3W8HwA6+7iD2+mg2POxMr/OQHnzI7tYrnBsiyRMI2GBhup7M7+8Dj+Lzgi0DmHoTE/kletLXzRUKizWgj9RSuyJv2JUKqfo2iV/rMrQp2AjdqMl75mIn6ZSIRMWQf4lj6ZRLq3naNVPwqHxU+ZtldbpXhYhiyzmBigDO50+jy461O9hopLcWA1cen8qfcr53YCOoUvQIVv0JKS6FJe/PwXIcfRazW61xcXCRtmvxXL74AwJ9cvMStwhp3yp0MZDK8MjjI+bl5/vsPPkQI2N+e51sH99Nh21Rdl4bnI0sS//jsGWxN452paf76xk1eLZcxVZWU8eCH4VqjyY21NW6srvKPzp7hQEc7C5Uq//rDj7i5ukbOsjY6OBRJ5nBHJ2f7e4mEwFRVdCV+7YX+PhK6xoWFRVbqW8tza/UG3ckk//TciyiSzL+7cJHlWp2FaoVO22a2XCaMIv75669Rc12+f22cW2sFTvV0k9D2bgy9+fpBro0v8LO3xqlUHI4f6eX0yUFGRzof+dkwjLhxc4lff3ALzw9RZJm5+SKO67dcph+sQwJQr7ssLVdYXavy/R9+QjIZq+g2mh6+H+LvQJhNkmD/oR7mpte4Xp4D4MKHk4goQpYlzrw6hm2bKMr2BxKFEcuLZcYvzfHeL8YZvzy7qbyx/0gvB472kc8lERL8/uBRIiIUScaQVTKdgzHnTZJ5sWNgQ69rJJVHCIGuqGiywoFMB0PJ+G+yJKErCr2JDBERqqTE40jAix2DHM/3okoymqygyjIJVefvjJ5EAGtOnb+cucYrnUNISEzVCiw2qvQm0pzrGOJM2wAtey5USUaRJF5oj49LkWQEgg7rCFHrOAxFpd/O0GWl8NsjJAlMRX1kF3IqZXHqpVHee+s6ruNTKTd460eXUZQ4M3b45MAD+S1CxMHjnckVPnl/gvO/usX4lTm8llu1nTQ5/fIoA8PtWLtIDv+s8EUg04JA4Ic+QcutejtYion+FGJp92LVXaXol55LX57toEkGA4n9uFGTkreCH7mIZ2B42Qwb3Krdph5szlyltBRZPYulWM+d0FtaTdGf6MdUTOpBY9N4ihCU/DJXK9c4kT3+VOJ7O4EfhixWa8yVK9Q9b4OTcGttjaxp0vDi4LA/k2a5Xudnt28jSxJ5K8H+tjaUVhbAVFXylkXWNEnqOnnLIqXrFJpNmr7/0ECm4jqUmk06k0nyCYucZeH6AZ1Jm6rnUWg06UrGAXG7bdOXSZMxt2bYDFXF0jRMdfuHQMYy4gxRIhGv3g2dmufR9HywIWdZTBaK/NmVqwRRhB9FnOntwdS2ujXvJtrySY4d7aO7KxN3J12d5f3zU3cDmVYkEgQtjzYhNlbZS8sV5haK+H7Eay/vR5ElPvx4irn50qZZSpLioCeMNn/eNDWymQRJ2+TlF0cZHorLbCISdHamdkwsPnVulKWFMnN31qhVHFzH48bVeWpVh/feuUFbe4pUxmq1j8fBi+cGNOoulXKTcrFBYbXK0nyJaqVJEMTCdV29Wb701cMcOTmwoZ+j3rcwUe7x7jKUu48sXVGIhNgw/pQlBfW+0vKWB70Ub+/e7UDMp0vKd8fwmbY+GoGHH0XkjQRdVpKUZqDJCoZyl76yfp3v354iK5vcrlVZiQOzbT77IGTbknzlW8eZv1OgVoltBeo1lw/fvcns9CqdPVny7SkSthGfpxRzaJymT63qUC01KRXrrCyVWVuu4rQyYOmsxYEjfXzj90/T05/f07H/rPBFIPMY2E111MnGNMvO0nOTjXHDJgVvmUZYJRRBy1DSoNPoI6GmUGWNdqOHkrdKsE3GKhIhy+4sjSD2nxIiIqGmyGrtmEqCFXc+7iiKXILIJ6mmaYZ1VFmn3ejBUpLbkm+9yGfFXdmUjYnRUuNEPHeBjKGYtOvtdBldNILmlvJh2a9woXwp1q5RErvW+v0gSLGgKQI2VrP729roSibJmCaKJOFGEUEUtbQ24pZY/z4yp7hvtK5v75F6Fjyo7BBv5N5tJnWNlPFk2RFb07E1rXX8oMoyiiwRRFGs96Jq6IpCzfNJaBr72toYafFv9gLrZZXZuQJrhfqGmZ7j+BjavdIBYOgqHe1Jbt5aptn0yWYS7BvtIBIC1w2oN1w8L8DQVTRVwbb1TcctyzK5nM3KWpX3P5rENFTGRjuxEwY93Rn2j3WSsIyYkyJBJASWqZN4hDT9OvqH2jlzbpRysc6VT+5QLNQptX6uX5kjk02QTFkYZizQFoYRnhcHMrWKg+8FGyRVRYn1cwZHOzh+dohT50bp6s0+0TW+/7vbjVKtpWgcznYxWy/hRyEZwyJnJDayhjvZQ+zSsPXYHufoErbO/sO9nHvjAJIEt8YXqJQaLMwWWZwvoevqhk2BpqmbApl6q+MpCNad6EFVZbp6c+w/0svpc6McOTlA4jG5Nk+G+HuPIgeBixB3O8kkSUWW0k/9vX0RyLQgIaEpGupDOB8PUoh9HEQiwo08xivXmWsuPNW2dhO1oMzF8q8oesv4kYsm6yTVLC/kv0piB+W0SIRM1K6w4EzTDGsEkU9Gb+dw6gW6zUEuld8jiDyqfol6UKE/Mcaqt4CtpDmRfYV+az+KsvXaRyLCDbdmf2pBjYJXpOSVN6kobxhF7DH35GFQJJmEmuBg+gAlv0yjuTmQqfpVrlaucTp7koyWIaPFQmp7ccyaotCdTNKbSpE0dP7o2DFkSSISEZqikNR1hICJQoGJQoGedBoJKDgOlxeXeG14CAE0fJ+VeoNis4kXhKw1GtRcj7yVwFRVvCCg4jjUfY+m79PwPcpOTJxMGTo502K5Xmet0aCt0WCt2WCpVuNgRwf5e/Vmtpns1wOCuudTdT0avo8TBK2SVyvLJMSGYd12EELghAGmpvLa0BBdSRtDVVBkmSiKkOSd2QM8LgTwyYU7XL46h6zIiEjQ15vl0MGee05ZIpNOcObUEBcuz3DxyiwHxjoZGWqnLZ8kl7NpNj1+8fY4yaSB74eMDHWg68rG5zVV5sihXq7dWOAHf32BXDZBPm8z2N9GT3eWl14Y5dNLM3zw8eSG6vXvf/sk+bzNo1hu69YAJ18cIdeWRNNVxi/NUlip4vshYRhRKtYprtU2uTqvC7LJsrzhB6RqCsmUyeETA7zy5UO89Pp+dH1vM2KPC1WWyRoWWWP3dZAeB7IsY5gyX/+9U2Tzsa3FzavzOI5P4IeEQcjKYrnVkXT3c5IUm0hKsoRhxiRtw9BIpkxeefMQ5948wOET/bEGzjOYJ4UQRKJGEC4SRAXEPbxBRU5jasd5WrnfLwKZe6CgoMsauqxty5MpekVqQe2p9lENqoxXbjBZn3ykf9OzRCOscqt6gbP5r9Bv7SOpZpAkmYSyMw6MIqkcz7zKkfRLCASRCPnx0r9n1ZunyxwAIK93kde7aIRxfXxfMh7AC81pus0hDLaWE1RZJa2lqQRV4O534oQOl8qXccImJ7Mn6LV6yWk5LNVEeeTUvPewFJPT2dNM1aZZaC5sasOOiGiGTX65+i6qpHImd3pPWsEBNFmm3bY529fHpwsL/N9+8TayHBvmneju5nvHjlJoNPhodp5is8k/PnsGgF9MTPD21NRGV5OmyNQ9j//xo/PUW3yZY91d9GfSaLLCteUVfjB+nYlCgarnkdR1poslvnlgP4O5LPvb2zjU3s4Prl2n6V9BV2IS74H2NjqTNqXmw20G/DDkz65e5dP5BRaqVWqeR9X1eGVwgJM9PRsls+0gADcMCcKQmVKF//H8+Y2yRFfS5u+dOE5XMom5B2RfCfjm14/x5dcPsj5Za6qMeZ8WSFdnmu999yx+S5zMMNTYlVuReensCEcP9bQeUPJGYBd3J8Wf1zSVr75xiNde3kcYRSiKTCYdP4iTtsHZU0McOdS7ya06nTJRH2KHcD90Q2NorJN/9F9/lemJZW6NL3BrfJHFuQLFtRr1qovnBkRRFJtl6iqGpZHN2bR1JOnuzTE01snw/k46ujKk0ha6rj3S1+q3HYmEzguv7mf0QDfTt5a5fnWO6dsrLC+UKBXqLWf12J5B0xR0U8VOGmTyNu0dafqH2hje18nwvi4yuQR20tzWm2mvIESTUu3f4QY3iISDLN0tZxrqCKZ2FHi64/kikGlhPTJNa2myWoZld3XT6wLBbHOOZXcFP/JRpYd7XGyHklfmRvUmb628zYq7SiieH0XFpJrlePZVgshjqn4NU7HptYYxZBONh6cfhRAEwmeuOUEjrG7U6AveMp1G30ZBwlRsVClWvQ2ET1LN4EcetaD8EF6Sxb7kGAWvQDO82zYoEJS8Itcq4xS8Aik1jamYaLLayso8xH9EVlAlBU3SMRWTtJYmp2dp09toM/JxN9FTrhAUSaHTaGdfcowVd5WZ5sym10MRMt2Y4YPCh4SEnMqeamnbPN0NvX4di16RglckjELGkqMc6eokZ5ms1BsbD47OZLKlwmvw0kA/QRTRm4qzb+cGBhjL58lZFk4QYKoqPekUrw3FGRpDUehM2qQNA0WW6UoleWNkmDN9vQRRhCrLpA2DrmSShKahJWW+vm+M1UZjIzBqT9gMZDJxZsjQ+YMjh+hI2qS34ccossyJ7m56UimafqzUmjZ1elNpcpZFytD56tgYiXuCkRf6+qj7Hp1Jm5ura/hRxImeLkZyeSQJbhcKFBpNJoslUoaBqcWtrouzRc7/6hZRGBMq9x3ufSIRsvX5IZt5tN6Npinb8lUkKQ5Eko8gZMqyRDq9fQZBUWRs29ixdcHD9mGYGoapYVoanT1ZDh3vp1Zp8sE7N7h6YYZGzeXL3zpOR3cGw9JQVQXT0rASBsmUSSZns7pc5oN3rlMpNXnxtf30Dbc/lbgexF5NH757k+X5Ev3D7bz4pf0Y5sP9nz4vkBWZZMrEtDRSaYuegTzlYp161aHZ9AmDOCuGiN+rqJv9vdIZi2zeJpOzd2xMupsQBHjBFJrSh6kdRZLuPk8UOcPTBjHwHAQy66uLiFgvRKwXzu9/H7EBYCACIhE9sO7+tGjT22g3OrYEMgCr7hpT9Wnu2DMMJgZQeXQwE4mIQIQUvQK3axNcKF3iWmV8i1DaZ42kmuF45hWm69cpeEvUwjJlf5UTmVcxH+HREwiPil9kunEdISJsNY0AvMiJg7XWd6xIKqqsobTKQIqk4hPzae53nr17XDZHM0eYbtyhHjQ2hPAAfBFQ9EsUH1OXRZUUVFnDkHQs1SKn5Wg32ug2u+mzeuk0O8lqWWz1yQXXZEnGVEwOpPZT9Iuseqs4obMpYKsFNcarN3AiFwF0G13k9By2aqPtMFCOREQQBTSjJs2gST1sUA8bLDYXWfPWMGWTIXuI7lSS7tT22bWMomzox6zjYEc7BztihdiVeh1VVmhP2Lw6NIitbw1su1MpulMPLkHqisLR7q4Hvm5pGq8MDW772rp664me7gd+HmKi8L040Dp+Lwi5vrKKF4YYqkpHMgECCs0mXhgi3xO0Bn7I0nyJd//mKoEfkEgaDO3rfCo11d9EJNMWybTFQMtraXWpwtzUKoos8aWvH2FkfxfmNvwbIQQ3rszywds3mJ9Zo7svR1tX+qkDmXrV4ZP3bnPzyhynXx7j5IsjGIbGnjgUfkZQVeWZ+4vdnZfXS/ZPuB0CDO0ASesbSHug9bVngUwkWgz81sR97wR+70MrLkMIvMijGlRb8vNbH2qhCHFCh4pfJRKixYnYTL6VkDbG7QZXYt1vZ4cXr8PsoNvs5mrl2pbXfOEzXhlHk1V+r+c7JFV7w1fnfqyflx/5VIMK7699yPnix0w1pjfes+6I/TxkZmRJwVISHMm8QCACZhu3+fO5f8VAYj/tRi8gCEVIKAIiQhASgQha30tjg+g7ljzGwdRp3MjhRvWTTVfm4XmS7WErNkfTR5ioTVILasw25p46CAxEXGZwcCgHFRadJajG3Ja0mubltnOczZ1mxB55aq2X0eQIzbDJZH2KmcbsFnuCalDlSvkqN6u3OJM7xfHMccaSo2S0dGvMbsMZWb+nhMCLfKpBlfnmPDONWe407jBZn6YaVDFknUOpg0RPOb5kSUJT5I026M8bFFlif3sbN9bWeO/ODOdn54gE6KrC/rY2TvZ2bxucfYGdQ1XlVuAifSarfkmS0DUFO2liWnq8/89xDBMJv8UleRbNIBIgI0v6liDDDQLCKG5xV5RHN7uEraYBIUBT1vWlpLicJASRqCNxd1EgIQMPN0rdCfYskJltzDHbnGOlVYrxhIcf+fHvkY8Xea2/+wRRQERM6iz5JdxwqxfNmrvG+4UPuFYdR5EUFElBlVR0Wdsw6tNkPf5360eXddr0PMP2MH1W7zZHuRW9Zi+jyRE+LHxIPWhseWiW/DKfli6y7K5yILmPgcQA7XqehJpolUxaAVdQZdlZZq45x53GDCWvRPUefo0iKYzaI7TpeT4ofPSZZmgEsOrO887KX6BIKuv9KfuTJ0mpWSIiqn6Ri6V3mW9OUg1KSEhU/SIHUqfoNAZo03sIhM+V8gfMNm6jKwYgsHbIsXkYJCRea38VS0nw67X3mKpP7cn1CkVENajy67X3WXAWOJ09zQv5MySfQitHRmbIHuL3er7DXyz8JbPbBDMREW7kcql8mdu1CWzVJqdnSWtpEi0n9di7KSSIAnwR0AgbVP0qtaCOGzl4kYcXebihixt5uxoc92fSZKx9+GGI+Tk0aZQliXbb5ruHD/HV0dGNVaWEhKXFqsTKF0SNp8KrXznM8bPDRJGgqzeHpj/boLetM83f/V+8geP42EmDRNL4TAn/T4uV+l+xXP8+flhmr4MZVU5h6wcYzPyv0JTNBsj//oOLXJlf5jsnDnJ6oJdM4uGZsxtLq/zi+iSrtTrfOX6QM0N9rVciSs3/HxXnr9GUHiBuzNDUAbL234en5DXu2ay05C5xtXKNqfoUoQgJWqv5jd+j+N+BCHf0UPKFT8kvU/LLwN1Mi9oKauIf9e6/5TjQ6bP6sFV7x4GMpZj0W32cyZ3m4+KnVIPNcuuBCCh5JRpBg4pX5nZtgqSaRJe1ls9PhB/5NEOHalCl5MWlj3sfLDIyx9JHOJE9gSHrXK5coRE0P9NgxlJsRuwjrRZBCUVSSalZ0lobEhK6bNJtDpLUshvt15aSJK3lMRQLQzE5mj6HGzVQpTiobNN76DB6sdRkKyjKIEsKGa0NIQSGYhEIn4zWhrGNMm8oQupBnfnmAnPNeSbqE1SDyp62rAcipOSXuFUL8KMAW02wL7mPnJ59ou1JkkRStRlLjvJmx5c4X/iYW7UJ6uFmQTeBoBrUqAY1ZFdm0VnEVCyMlhnmuodUJEJCEeJFHk7k4G5joLnbMDVtT4iwzwpSSxytK5mk69ll5X+rkGtPkWt/fLHQ3Uqc6IZKz0B+F7b0fMALV6i6V/GjrRSH3YYqZ5ElnUhsbXBZKFe5sbTKK/XBLXIM28H1A5YqNS7MLPDSSD8AEhqmfgIlyCIIW7yYOD+vyNndOYdd2co2qPhVFp1FZpqze7J9sVHqePjKU0Km6u+800iWZDqMdl5uO0fBKzJZn9rSqSQQuJHLTHP2sc5PQiKhJOgyOznXdo5DqYPUghpteht+tPSZuSJLQFrLczb/5Qe+J6EmOZA+/dDtHMm88MDXxpJHd3w8QsTXd9Vd405jhvHKdW7WbrLqrW3Sk1nPyCn3tF4/aFKMmVfx/4ct/6Y4u7H15oWYv3KzdouUmiShJEirqVjk6gmgSApJ1ebF3AvIKMiSwlR9impQ27adPyKiETZp3ENufhLEPLLP3nvqecKGFUmxTrnYoF5zCPwQRVVIJHRUTXkgX+teuI5PvepQKtRwmj5hGCu2appKMmORzt51h94uM+C5AdVKk0qpgdv0CFpmgHG3j4Jp6TEPJWWi6VunaSEEIhLxNsoNGjUX3wtjnRgZVEXGsHQy2QTpbAJVu1siDYKQSrHB6nIFw9ToG2zD90PKhRqVchO3pf6qqgp2yqSzJ4Ouq8j3dDjVqg6FlSrF1c0LPU1XGW7xYx5NtI15jqVinVrV2dA9Wf8u0tkEmfz2BFXfD6hXHJYWShtCb+vId6ToHWx7oG2C6/iUi3VWlyvk2pJk80lqre/CaXpEkYhLZQkjvn65xLbHIETsf1QptT7reARBhAi3f+C3dabpH949Z/LnEaoioykyxUYTx2/ZyUgaCeNFIu0gkXAQwkGSTGTJQv5NIfs+j7AVm33JfbzZ8ToAV8pXdyVboskaQ4lBvtv7ewzasRhaIHyG7EEqQRXX+2wCmecNAsGSs8zbq7/kg7UPqQbVLVkYRVJIayk69A4yehpDNlFldRNx816sk8nDKKQRNqkFNUp+iTW38MDv1os8zhc/YSAxwHBiCFt+fIO8dUhIWIrFK23n6E/08fbKO3xc+pSSV3ribT5yny3n7881WWAPEIURlz6a4te/iD1/ims10mmLfUd6OX5miO7+3CNJjYXVKpfPT/GLv77EzOQqtUoTRZVp60hz+uUxzr15kBMvDD+wvFEq1Pjkvdt88PZ17kysUCrW8b0AO2XR0ZVmeH8Xp8+Ncezs8LamfkLEwdDVT+/w4bs3Gb84w9pyFc/1UVSFVNpkYLSDl988zEtvHCDfnto4p2bd5aN3b/KX/+lD+gbb+Kf/zbcprFR456dX+OiXN5m7swZCkMnZHDs7zN/+R1+isyezibw7dXOJH//5x/z0+59sOq58R4r/43/7DxjZ34W8TQB2L9bFFC+fn+b65VkufzxNpVQnmbYYO9TDuTcO8qWvHyWV2dqNVas4XPxokn//P7zF9O3NJr/f/N5Z/pf//FtYlr7t97i2UuWXP73CD//jh3zl2yd443eO8ekHt3n/7RvcmVjBafqkMxajB7t5+c1DvPrVw9uXqgQUVmt8+MubvP/2ODOTq5SL9Q0bgPvxnb/zEv/s//B7D70mn3/EwaMXhIQbLt0KmjKIzxxBMIEfzqIrAyhKH6rS89Ct7RRfBDLbQJIkVBQOpA6gywa9Vi8fFz+h5JUeuIJ/GBQUBuwBjmeOciR9hMHEAKZiIkkShmwwnBjidm2SItubVf42oRE0WHAW+cnST7lZvU09rG8EMTIyaS3NscwR9iX30Wl0YClWK4CRH6qcKWitYhGtTrIAL/KoBw0m6hOMV68zUZvcEjAFImC2OctUY5qjmSNPfF7rk6CCQrfZxde7vsqB1AFuVm9ys3ZrV0jMEhKmYtCudzBsDzFqDzOSHMFQviCyrmN1ucJHv7zJ2z+6xMpShXTG4vCJAaIwol51ePdvrpFts1mcK24bQARByMT4Im/9+BIXP5oiDMK4QyehE/gha8sVPv71LRZmCsxOrfK13zt5l3zaws2rc7z/9nXe/elVVE2hZyDPgWN9RJGgUXMpFmpM316mozvDviNbS+K+H1BYrvIX/+EDrl2coVpukEgaHDs7hK6r+F5IpdRgeb5MpdzASmx+oMf3QmxTUCrUuPDhBG/99SWcpkdHd4b+4XacpkdhJQ6MMtkEqro5G9kzkOON3zlG32AbTtPj6qd3mLy5RBTtvPBbLjV460eXaNRcFFXm4PE+RATLCyVmJldYW65Sqzqce+MgQ2ObfakSts6+w7384f/8VVYXK6wtV/jkgwnWlisbjtoPhohF2qKITz+YYHmhzOSNRXJtSY6eGkQIWF0uM3VziXKxwcpSmT/4+y+TvK+9/eqFO7z31nU+eOc67Z1pXnh1H6m0RaPhcemjSRbmiui6yktvHGBwtJOjp7bvyvtNQRhFrNbq3F5eI6FrGK0xI/BouO/jeJcIojUUOY3rXUdVLmLpJ0kYr/LcCuIN2YOAYH9y317t4tGQIKfl6E/0Pfq990GWZDJamn3JUVJqkryeY765wJq7SsmvbHRY+ZG/Ud5a5+zoso6pWCRVm4yWoV1voz/Rz6g9QrfVjXGPl4ilWOxP7SMUIcV7VufdVhe95s54PY9Cm57nRPb4FsNLSYqzBAOJgV3Zz9MiFCEr7gq/Wv0145XrlPy7+jISEn2JPo6lj3A8c4w+q5eUlnoqIbn11uUOo52EkiAUIbONuU3lnjg7tMJ8c/6pApl1rF/zXrOXtHp3bCw2Fyl4Rcp+mVpQoxE2cSN3Y3wJBDJyq21dQZM1TNnAVEys1lhLa2kyWoa8nqfb7KLT6CCrZ5F3IXX7mwDP9Zm/U+AXf3WR+ZkCowe6OfPqPnoH2wi8gNXlCpM3lrjyyTT1qrMlkAlbwc6vf3GNaxdm0HWVl79xlM7eLFZCJ/AjCqtV3n9rnPnZAu+/dZ39R3oZGOkgcY+Gy9StZS6dn2Z1ucI3//AsYwe7SbW0Zhp1l+JqFc8N6Btq21ZCfnWpwvlf3eKDX95AkSXGDvVw9PQQ+Y4Uuqbi+wHVcpPCapX9h3tbbcjbY36mwHu/GMe0dPYd6aV3II+dNPHcgOJaFSuhk0gam8pKAOlMgv1Heunpz+N5Ps2Gx/JCGdfd+ULPc3zuTCxz9PQQYwd76OzJICsyy/MlLn8yzYUPJvjVz67S0Z2hqze7KSOkaSrtnWlOnxvDaXrMTq0yM7VKpdR4yB63Ym56jSgUHDzez75DPeQ7UiBJFFervPs3V5mdWuXj927z8puHULW45BcHQYKbV+f59IPbNGouJ749wsFjfaQyCTwvINdm88HbN1haKJFvT3P8zBBD+x4sP/C8YHxhhdliGTeI58DJ1SJVx+Xi7CJeEJIyt18UCQE11+PS3CKzxQqD+SwZy2y95tN0PwZJxtKOocg5AmUNP5im6V0kYbzMcyuIty85xr7k2F5t/pnBVm1GkyOM2MMsOIvMNeeYa86z7KxQC+s4YRM/8hEIFEnBkA1s1SajpekwOui1ehiwBkiqSdQWx0IIQaXm4PshqqowkBpgMLF9tB5FAsf1qVSbNJoeYat+m8/YpFPmjpj5nWYnnWYnL+VfBKDecHFcH5DIpMwtq63PCo2gwXTjDu+tvU8jbG7KjiTVJEfTh/lK55t0GB07bqd/GGRJRld0huxBZEnGjVyWnRWCcDNvpeQVWXXXnnp/90KSJFJakpS2j9HkCF7oMdOcZcFZZMVZoegVqYY1nNDBj3wiEaHICpqkokkapmKR0pKk1VjAscPsoKulf/MkYo2/DahWmsxMxEHE0L5Ozr15kG/8wWlUNeaPuI7PxI1FJm8tIi1tw2txfJbnS3z4zg2QJF772hG+9w9f28LFkCSJn//wApM3Fxm/NEsmm9gUyKwtxxmEhG3wylcOceBI3yYeTBTF2YIwiLaYHkaRYG56jXd+fJnCcoVXvnqYr//+aU6+OLLpfUIIojC2bbg/CLkXhZUaE9cX+Uf/m69x+OQg2by9aRvr53M/NF0lo6tkWiJ+bR0pdFN9rEBGkiU0XeW1rx3h9LlRjNZD0nV98p0p5u+sMXFjickbixw63k9P/10yr6zIGIpMR3emtTFIpk1U9fHmBd8LSOcS/NE/fI32rvSmYKnZ8CgXGyzNFZmbXiPXnoxfb5X1ZqdXWVkoM3IgLkEN7bubNUpnLEprdeZnClRKDXRD2zQGHgVZMlDlFJFwiK1mw1hvixD2sCnkxtIqv7w1xUq1juMHzBbLVB2Pd25O8smdedQHKAILBOWmi+MHpE2Ds8N9dGXWFwIhXnCHpPUVMom7pbVi7d/j+FfYja6sL0pLj4Eus5MOo50TmeNERBulivskg+Ku/Bb5VJGUTR5AEH9tH12YZnG5QlvO5uuvH3pgMOEHIbcml/npO9f49OosjaZHPmvzR985wzfeOPxECbmrNxe4MbGMqsh8/fVDtOWej1aOueY8t6oT1MPGlhLPgdR+DqYO0m507IlJZJue52DqAG+tvAP38ccbYWNL99puQiYWzxu1Rxi2hzb0l7YfX/Fv6/yC9ZLaerbmeTPQfJ6wNFdiemIZEBw5OcDQWOem+043VDq60px8YYRKcevKvlSsc/mTacqlBsfPDnP01OC2hNaRA11cv5xn+vYyt67Nc/jEAN39d183TA1dV6kUGyzMFGJRuI70xuuSFKvxKtsEIIEfsLJQ4saVOVKZBCdeGOHIA0oWsvLosZBrszl6epB9h3tJZ59cAPJJkLANTrwwTFdPFv2erJGuq/T25zl1bozlxTILswVmJlc2BTK7he6+HIdP9NPTn9sS8PUPtdPdl2NtpUJhtYrTjIO0SAjqVQen4aPpKt39eTRj86M03xG7gQdBSHEtzrA9Dky1j4x5FjdYIBR1gqhGGNXi/4omWyapXcJgPsOBznYcP2C+VKXuerhBwGK5hiQ9vGnG1nVG2vOcGxvg908eojv9qOfK7s1VXwQyO4QkSSgo25YyokjgByGfXp5B1RTOHn94LVQCjhzoYXSwHUNXH+p70XQ8fvrLcYJQ8I3XjzDYF/f5jwy2P/EwGB1spz2XjFuDE08nW76bWHKXmHPmtm2vHraH6DG7n1rC/0EwFIO0lt4QOLz3GLzIx428TWWu3cT6ileVvrgd9xLVcoPiag0kiY7uDOlsYksmRdNVuvvyWNvcF426y9z0Kp4bcOWTadZWqqT/ZOvDv1aOHYrDMKJabuJ7mx9iB472MX9njb/5wQX+4t9/wOXz04wc7GbsYDcDIx0k09a2QQxAudigWKgRRYK+obbYxFHbOiftNCNnJ016B/IY5lbjxr3O6mm6SmdPbguHSJIkEkmT7r4cqqpQKTUordUfsqUnRzqXoKM7Lmndf75WQscwNUQkaDZcwg0naQkzoaHrCmEQUSnVN15bR73q4jQ9FFnCShgoj5kpSurH0JUOIuEQiQCBjxDBxu+RcIkih1A0W+9pEkat/4omflig4U/ihasIvEfvsIXh9hyZhMnZ4T5KDYc//fgy1xdXeXVskIPdHQ8sLUlIGKpKyjJoTyboTCVRNwQ0FQxtBNe7wlq4iCp3EIRLhFEBXR1hNwKaL2bOXUAQhqysVbl2a5FMynp0ICNJ9HRmHrldIQSeFzJ+a5Hjh/p46dQQB8a68PwQ5QGthTtBWy753GRh7kXRK7GyjTWEjEyX0UVWf/Q1e1IoUmwYuh2fJCJ6anXc3zT4YRkvXI5T3ffGnRLoSie68vxpejiOT7PuIQF2ytwoZdwLRZFJZSw0XSG6r43WcwNKhTphGOJ7EtVyA6exfadhJpegszdLV292i71B/3A7L75+kGbTZ2ZyhfFLM9yZXOH2+AJDox0MjHTQN9RGT38e6b77vFF3aNRcZFkiv17qeAoYpka2LfnQ8tNeQWl5CKnbBGK6oZLJxm3PTtOn8YDr/LSIvYi2z0QpLdNOISAIIqJovdQGpqnT0ZMhlbGYm17lxpU5RCSwbIPAD7l64Q4zU6tYtsHYoW6S6Z1ZMERCMF+vYCk2WfNILBu3pe07QgifUDhxQCOc1o/b+ptD059CNP6GIKoSip0HMtmERTZhtbLBcHlukarjcry/m9f3D5G3t16rjeywtP0zSZJ0EsZLON41gmiVSLgI0URTBzC0I/xGt1+vZzlqdQc/iECI2IDM0PD8AE1VSCXjwVEqNwjCiEzaQm1F1mEY4bg+jhtgGip2a4Xl+QGO4+O4PmHY8gBSZAxdJWkbyLKE4/qUyk2StkEQRLieTyRiGe6EqWO1Vi9CgOP6rBZqXL2xwMx8Abc9zcJyLNpn6CpW6/0AQRjhuj61untXq8DQSNp3W/vW69LVmkPT8VlarVKqNGNjxjBiaaUSb9fSkWQJ3w+p1d2N44d4UEWRwPcDiuUGqaSJoasEYUSt7uIHIbIkYRoatm1sccCt1hwaTY9U0qTpeHgtR15VkUkmDAzjbhYpigSeH1BvuIShIIwiRKtzQVVkEpZOIqE/cJDfe871oL6lhCMhocoqKS2JpWxvircbiK3mxbbZoC/0WDZDiAgnmKHQfIswcrg3kpEkmbz15nMZyERhfA8ByLK8bVlIkiQ0TUGWJaL7YlcRiZZWCwyNdXHixRFS2QePyQ1vnPbNi4ZMzubUS6MM7+vi/bevc/HDCSZvLvH+W9d5/61xRg92c+6Ng7z+jWOks4lN/JkgiOJzkEBRlac2RVRa1gLyZ8GpkuL9b7drRZHRzZbIaBgRBXvDC9F0FeMxg0FJklA1hbFDPUzfXuGDd67zzk+usDRfoqMrg+t4fPjLmyzMFhkY6eD0uTFy+Z0tHCMRcWV1GVvTGMnkSKg6lqqhK3dtdiRJRpIMZB6cTa8pndS8G9S9G4Ti8cvi6x2g7Umb3kwaXVVQFQXtCWxKZEknYZxDUwbwgjsE0TKa0oum9qPK7fxGZ2RcL2ByZpW/+PFF7swViCJBd2easyeHuHl7icG+PH/0nTMA/MkPzrNaqPEP//bLdLbHzP1SpcnFqzN8dPEOp48N8PXXDwMwM1fkowvTfHRhmtVSDQmJjnySowd7+c7Xj5HLJLhwdY7/7x+/zR986ySzCyUuj8/RbPr0dmd4/dx+XntxjHTSxHF9Pvhkih+9dZWZ+QLlioOuKXx0YQqA44f6ePnsKC+dGgagVGlw8eoc3//xRSrVJj1dGV46Ncx3vnZsU61eAD/82RU+/HSKUqXJWqHGz391g08uz2AaKkcO9PLlVw4wPNDG7akVvv+Ti5w43Md3f+fkxqTgej63Jlf4f/+bX/C9b5/hxJE+FpfL/PmPLjK7UCRpG5w80s93vnaMjrbN3Rl/88txfvHrG/ytb53i1+cnmLyzShQJerszfOvLxzhxuG8jaPK8gOu3l/jrX1xhrVinVG5Qb7gEYURXR5qvvHqQr33p4EbQ+SAEItgwBL0XsiSjy/qed974wqcRNghaxO17ocv6pk6z33ZEoknVvchs5d9yP/FQQsZQesiYZz+bg3sIFFVG0+Jx5Da9LSUfiEmL6+J090NWJAxTRZIluvpznH557KECZ5IUB0z6Nnoqmq6Q70jylW8f59wbB1heKHHtwgwf/PIGM5OrFFZqVCsOX/32SfqG2u5+TlPQVAURxTwNb5tz+LxARALfDba91lEUi82J1gJW3SPLgyfxf1vHsdNDIKC4WuXm1XnGL87E7uCWRjaf5LWvHeaF1/YzMNKxrajhdhACSm6Tt2YnccKA/dk2vtQ7xHAmR0p/9jSAkY48sizRmbTRnlAQdB2q0oki5xCESJJKFNUJo1UU+em7uZ7bQObW1DLvfzJFre7w8pkRMpkEjYbHh59Os1aokcveZdeXK00KpTpBGK0bLRNFEY2mT7HcoNG8m1rTdYXe7gznlGFMU8P3I2bmC1wcn2N4sI2jB3pxHI/ZhSLnL9xhbLiDb3/1GEIILlyd4+bEMinb5NUXRtE0hbHhdr4SHOTClRmmZtfobEvxytlRANrzSXq67pZDbEtn33AH3/2dE7z38QSuG1BruNwvJCoBp472092RZrVY44//0/scOdDD6WMDZFIW7Tmb7s40hq7S3ZXBdQMWlsrML5Xo7coiSbBWrHNregVVVbBMjZRtIHdl+cYbhzl/cZrFlQqVmhPbv9+Hat1hamaN985PMNSX5+iBHuoNj4vXZrl8fQ7TUDnTKp/dmlrm0vU5wkjwpZfiVvsbE0v85U8v8/q5/ewb6Xho++c6HkRUFS3jzYiYALtXZNaKX2W2MYe/jdJuQrFIqqkviLQtOMECzeAOQbSd7pFMJJ5PYUc7aZDKJBAISoU69Zqz5T1hEFFcq+G5wZYOGNPS6ezJomkK9ZYS7ZO6NkuShKoqJFMWCdsgmbbI5pMMjHbwy59c4ZP3b/Ph2zc4fW5sUyCTyiRIZSzCULA0X6JedRBCfC671IIgZG1leyKs0/BYW64QBhF2ytxoT999PPl1c5o+hdUay4tljp4aZHhfF+1daVQtVkRebxvXjZ13ESqyxNmuPgZSGSqeix9G3Citcqu0Rlo3Gc5k6bZT2NqzWVjt68zTk0mS0HWsHQZjD4IkqUj38ACd4BJ+OE/a+j2eW6+lJ8V6mWHyzipXbyxw7GAPr7+8n862FDPzRS5fn9/24btTZFIWB0a7ODDaRT5n4/sh7354m5tTK0zPFhjoyd0VjBKCfcMdnDkedyfU6h5TM6vcmFji5TMjaJrCQG+edMrC8wI8P2R0sI3feXN7vRHL1Bnsy9Pfk6NYqnNnrrDlPesD/tC+bg7t62ZppcKf/uUnjA618+bL+zdlT4SAzrYU/b1ZHNfn5sQy3R1pZFlhebXKxPQKB8e66GxPYicM7IRBZ3sK1/Uffg1b5y4EHD/cx77hjo1gcXmlyvRcYSOQmZ4rMrtQorcrw8tnR0jbJvlsgp//6gajg+30dmUxHnUDSDEPRpU1VEndpOMSEXtX1YM6buhibuPJ9LRY168Zr17Hj7a2j6a1DHk9t80nfzvhBDO4wfxnfRiPjWxbks6eLAiYmVphZbHM8P6uDfn5KIpo1F2mbi1TqzQ3tSIDJNMWo4d6+PXPx1maK3Lr2jxHTg2iG+oWcm4YRoRhhKLIG+3Z63Nb4IcI2MjUyLJMwjawEjrdfVkW7qzx6QcTzM2sbZHft1Mm+Y4UmVyCwkqF2akVVpZ66ezObnrf+v0b53el2NPoOQt23KbfEp2r092f28hKR1FEqVDn9vgCnheQ70jR2Z15xNaePVaXytyZWKawUuXg3+3n5S8forfl9/Sk11qWZPZl2xjL5Kl4LteLq7y3MEPBaWCrOivNGkPpHMPpHD3Jx/e2ely0J23ak0+maO6HSyAiVKUDL5hG3LfAabofEUQrpK3vPPVxPpeF/zCMWC3UqFSbvPbiPjraUiQsnc72FKeO9pN7ijZBO2GQTVvYtoHjxA/0pG3Q1Z6i3nBxW6laVZU5fXyA0cF2EpaOrqkMD7SRsAyK5a3twZ8F4skJjh7oRVUUrt5YIAgjwihiea3KzHyRY4d6n4jYm0rGWae+7iyWqWMnDPaPdIIE1epdD6Ba3cH3A3q7s+iaimGopFPWRumt6TyaaCa1/mfKxrY8mIiIRWeJoldqTdC7c+1jXkxELagxVZ/mSvnqJi+ndbTpebrMzm228NsJJ5jB+RwGMp3dWQZHO9EMlRtX57g1vkBprY7TjAXdKuUmS/Mlrl+c2VZYLZNLcOTkIB3dGZYWSlz4aJLZ6TWq5SZO08NpejQbHo2aQ3GtxupiGdfxt5ROims1ludL1KoOjbq78Vmn6dOoe3heiCRLcSfRfS3UeqvT58DRflwn4NLH01z8YJJ61aHZcDeOIT6f2IPp/o6ap8H6/ReGEYEf4vsBvhcQhrEcBYLW3+PXgiAkWn/tPjQbHuOXZ5mZXKW4WqPZiK9DtdxkZnKFi+eniKKI3sE2+obaNx1DFMUlwMAP8b2AwI/LgULEZanACwj8+O9hsP7a7s7ZpWKDtZWYf6JpCr4XUC42qJQalEt1KuUGtaqD23rO7GT/QgjcIKDme1Q8l6rnEkQRqiQTEXFhdZG/mrrBx8tzu3ouewHXu4rjX0YIl5rzE8qN/0yl+RcbPw3vQ/wwlkN4Wjx3GRmICbSuFyDLEumUidZa7SiyRMo20HYg4CZEzAC/f+wsrVa4PD7PxWtz1OpOXJOsNCmVG6Rsc5MIVNo2Me8piyhybNUa7cAF9Fni8P5ubk4uM35rkVrdIQgFa8U6YRixb7iTTOrxSbKqIpNNJ+5e65a2BQLunZeTtoGqKszOFzl1pB/XC6jWmlSqMVn6cdq7c3qWdqNtW82Wi6VL5FsBxW6VeCIiGkGDd1be5XzxY2pBbUuAKiPTa/Uw+JyoHz8PaAazuOHCZ30Yj41E0mBwtINXv3KEix9N8taPLjE3vcq+Q70IIVhaKDE7uYqqq9tyGlRFIZuz+db3zvKT73/CrWsL/Hf/lz/n8InBDWPFRsOluFpjYbaAhMQ//t9+nZ7+/EbGJooEP/qzj/n417foHcjT0Z0lk4tNHT0nYOrWEjevzoOAl988tK2j9NBYB9/63llWFktMXF+ksFLl4keTdPfHbeNhGJtCTt9e5tiZIb78uydo60w/0j9qp4giwfJ8iUq5gecEeJ7P/EwBp+Hh+wE3Ls/SqDpYto5h6mTzNqmMtaXDStVk8u0pfvz9j7l4fpKhsU5MS+fOxDI3r8xTKzc5fW6M/Yd7N3X9CBG3wi/NFVr7D1iYLVAq1DdsIi6dn2p1pmkkUybZnI2dtlB2oK2zUwyOdnDwWB8fvXuDP/3jd/nL//hh/D235sqEbdDZm+XYmSFOvTQaZwMfgVBE/Gxmggsri1Q8h+F0jpd7BuixU5iqShhF/PG1T1hq7NwI+bPCOh9GEOD6N1HkDJo6vPF6EK4idknc77kMZFRVabW9CTwvIGoFF0KAH0Qb/16HLMUrehGJjeAujCLqDW8j6BAi7qh5+/1bzC+WaM/ZHN7XjaLITM+ucfn6wqYOAKl1HJu6Alq/bhs/foZZ22w6QVdHmjtzBa7fXsb1fBzHZ3SwnXTq8dUuYXP3Bmwmxd27shgb6qBQavDxpTsEYYRpqDSaHi+fHWGgN0fiMToCuswu+qxeJutTW16bb87zaelTVEnlcPogCSWBKj/Z8PWjgIJXYKYxy0RtgqvVcRadxS1eR4qkMJQYpD/RT1pLP2Brzw4CcIOAiucQRNGmoLvNSmAoe3s7RyIgiCp44TJ+VN7Tfe0FFEWmqzfL7/zBaayEzsSNBW6PL7AwU8QwVayEQXd/nO340Z+d5/5VkCRL6IbK0dNDRJGgozvD5M0lbl6Z49a1+U2Bgqop9A60bdFnkaS45VeSJKZuL3NncgVVUVBUGVmW8Tyf9q40Z1/bx8tvHqK9c+u4s1Mm+w/38vv/xTkufzzN3NQqt8YXmL69EpfJ5LiMJcsSsiLFnUG7dA1FJPBcn3d+cpmb1+YJg1iBeP5OnJkKgpB3fnKZZMpC1RQMU+PkCyMcOzu8iRgtK7GWz3f+zotM3Fhkaa7I+XdvEoQRzbqLpim88KX9vP6NYwzv2yxcGAYhc9Or/Oc//hVBEBIGEfWaw+JsAdf1uXN7mb/6Tx+hGSqqqtDTn+PEC8McPTP8WOq6D0LsieVw8+o8E9cXsZMm+fYUVkLfFLA6TY+pm0sszhYJ/JDT58boHWx76LYFEEQRA6kMWaOLHjvJYDpLxjA3yLb7su3Y2qN5h7uB5UqNxUqNcsPhSG8n2YSJ0upYFUJwp1DmyvwSEysFQiHoSNoc7G7naG8XmtrfOiMFTR1EVwYx9aMb246iBkG4xG9s15KuKaRsE01TmZxZi1f9ikzD8ZhfKtFsbk7/r9ea1wp1OttTyL5EpeowM1/cKBUJIQiCiE+vzKBIEm+c28+JI7FR23sfT3JzcmVzXVNix5R2SZLQVYUwjHC9MLaiv6dmvpe16fWAo78ny+JKnkvX5qg3XZIJg+OH+zC0p5Cr38HHOtpS5DIJVtaqpFMWyYROPmvzzS8fictNj0EQ6zQ6GbKH+LR0kUbQ2BRY1MI649UbOKGDEzbpNDtJa2lM2USXtS0KyuvKy5GI8EWAH/l4kYcTOVT9GnPNOW7XJrhevUEzbG4bxKTVNCezJxiw+tCfg66l+VqF2VqZ5UZ9Q/UXQJFkXu0d3PNARggPx7+DHxYQj6FN8TwhlbE48eIIiirT1Zflzu1lalWHhG3QN9TOoWP9HDjeT7lYo1xs0NGdQbmn5VRuyeK/+KUD9PTnuXR+koXZItVykzAIMUyNVMaiszfL8P4uMrnEJjE0SZI4dLwfIQQLswWqpWZMdpViMnEml2B4fxcHjvbFDtLbiGWqqkImb/PGN47R05fnxpU5ZiZXqFWbhEGEoikkbIPuvhz7DvXGD29p8+fbu9IcPjmApqnk2pLIO1zsrPMHm81Yk2cdXX05uvru8siiSOC5AVEo4hL+PeWtbN5m7GAP3X15zr62j+6+LDevzjN5Y4nCWo1sv83ASEesOHyoF8vW8dwAvaWe67mxl1SpUCcSAkWW0XSV4X1dcdAoxdn4ZsMj8AOshIbrBnGXlB+3z6ezCY6eHqJvsA21VRZSNWXT9U6mLYb3d+G6Pn2DeayW71XgB9y6tsCH79xgbnqNMy/vY3h/F+ns3e868EMKq1Umri/y65+Pk8klyLenHhnIyEitIMZkIJXZVrbieHsX2kNEVHcT04USH0zOMlss05VJkrHuBoIr1TofTs3yV5dvcGNxhUgIutMpzg71kbFM+nIZLE0jEh4J/SU0tRddHdr4fBiV8IN5fiMDGan1xXW1p2jLJnj7vZu0520URWZppcKFq7M0m5sJmZm0BfNwcXyWvt64q2B6tsDHl+5sZATWRY3CIMKyDdIpM06RrlaZvLPK1OwaffcR5nYKRZY2OCGFUp16w8Vq7VeSpI3yzHqdNopizZJICEQUZ4rkMD7vJ9WFGOjNU640+dMffkK15vDS6WGOH+rbUP1c36+AjX1LkSCM4nr3eqvo46JUaVCrO3R2pPkvv/cSQ/35DU2K9ZXhTgOpdqONUXuEUXuEW7XbNMLNPIWKX+Fi+TJXytcYsgcZtocYsAbIG3mSSgJDMVAklYiQIArxIg83dCn7FUp+iRV3hdnGHAvOwhYvp/uRUBIMJgY4l3+RzueEH/OLmQl+MTtJ3fdI6cbGykiXFY60dZI391ZiPhRNat44fvj5y8asQ5IkFEXixAsjnHhh5IHv+/v/5MsP3U4qY3H45ACHT+685Lh+Hxw9PcTR00OPePfDIcsSZkLn+AvDHH9h+LE+m7ANzr66j7OvPr6h73rJ5B/9s6899mfXcfD4ACP7u2nUXQorVUYP9XDypVF8L2R+poCuqyTTFglbZ3G2SLXSJNkiOa+XlfKdKf6b/+sfEYYRCdvAtHSqleZGBiwIIjzXp1ysoxtqy95AolyqEwQhYwd7eOMbx2g2XGoVh9JanWybjabfDRyGxjq3uG5D3K30Nz/4lPFLM+w73Ms//RffjrW17iN8R5Hg9vgC1y7OMD9TYH5ma3PH/ZAliWPtcflcuWc+jlrcJFmSOJTveOJr/7iYWi3yq1vTzJUq/L0XjyPL8kYm+Pz0HD+5cpOPJmfpTNmoisJytcYPL19noC3Dlw+MMJDPIksatvnylm1bxgtYhuA3WhDvyIEewkjw83ev82/+5D10TSFh6Yz0tzG/tHkiPXt8kCiK+PjSDJfG57FMnUzK5CuvHuD6xBIQ3/iWqfHK2VEuXpvlX/7btzEMjVwmga4rvHJ2FMt6snSdYWgc2tfN+O0lLo/P83/+b3+IrqucONzH6WMDHNrXDcDFa7Ocv3iHucUS07Nr1Ooulqlza2qFjrYkZ08M8fKZB0+uD0M6adLZnkKSwDBUMmmLXCaxERhNzqzx7oe3mFsocWeuwFqpjixJLC5X6OpMc2isi2995egj9rIVEtBoety4vcT/43/4KZaho8gSmqbywolBTh8bYGTwwVob96PL6OQb3V/Hm/eYqk/jRNu0yBIy31yg4BW5Ur6GJqubMjL3+hRFItrQqPEjHzd0N9kNbIecnuNY+ihvdrxBTs89N+7RNd9jOJ3jD/cdQZHvlgtkSaLH3vsOhkg41P3rBKKy5/v6Ar+5KK5WmbyxxOpyhYGRdkQkWF2qcGdyBd8NWF2uYlga/UNtLM+XSCRN5P4cmbzNtYszOI2YEH3hg0kkSeLAkV4GRtq5dnGG3v48hqVRWqtx/fIcbZ1phsY64vJXK5jwHJ9cW5Lrl2cJ/JBazWFloczLXz5ET39uk+/TdgiCOOCSZZn2rkzcXr2tuGJcQlMUGd+PicmPgh9F/LvxC+zLtvFa391g9/zSHNeLq/zt/ccw1Wf32C41HJwg4EhPJ0nDQKJV/goj3p+cZaZY5kR/N//kjRexDZ33J2b4wcVxPpiY5UBnOwP5bGtLcffcZjyNis9mPLeBTFsuybEWCa9QqiNErAEThoJSpbnpvYN9+dhN2DapNz10TSGftenvzTE82MZgb35jJXb6+ADZjMXCUhmQyKStuIspYRCGEe35JKah8V/8wYsM97dh6vGgliTo685x7nScwbg3WlZkiXTK4sWTQ7TnbArlBoosMdCb2yQEl0lbDA+0kUqajAy0EUZxHdvUNVLJuG35ftgJg+/97ilGhzseyjfRNAXD0BAC+nty9HZtLuvYls5gbx47YTDQm4vVkmFDWbjrHtO644f7sG2D9vxdHxet1bUlKzLppEkURVRrLuO3lqhUHX73K0cxTQ1FlgnDiKbjc+HqLAlLZ6i/bcftnwk1wag9zGvtr2KrNjdrt6j4Wx+cTuRsG+Q8KSQkDNmgP9HHwdQBjqSPMGQPoj1HTtKyJJE1TA7m2h+qlLwXiDlmTWreDYLoi0DmCzw5fDfuZopNOjMYlh4HMrdXMC2NerWJrMTju1F3aTY9TEujuy/H4myRVMYil7c3CMeNuksQRFSKdfLtSSRZ2ijXpbMJ0tkEYRixMFtARIK2zjTZvM2VT6bjrjLiLI/reIRBxEMEczcQBnFnVhiED2y6qZSbLMwUaNZd2rszO+LnCCG4Uy2TM61Nf1usV7lWWCYUz7bRpO55BGFEfy6DocZzYcP1mF4rMblSwNI0Xh4d4NRALwlDo+56fDqzwGyxTLkZz8+R8Kg776Krgxja3Syg598kCNdIGOd4Wib6cxvIaJpCT2dmkydRtebw0cXpLSZpqaTJkQM9HDnQs2U7R/Zv/tvIQDsjAw/PEHS0pbZkESQpDkwGerfqicTlMDh6sJejB3sfuN2d7Pt+JG2Dv/P726uk3stFdD2fas3B8wPGhjoY6tssEd/Vkd4UrDwMp44OcOro5pS5oauMDnUwOhSnNf0gZK1Y5/KNeXw/4H/9d79MyjZQFJla3WXyzir/8o/fZm6xxPbR+PZQJAVbsXkp/wKmYqDJKpP1aWp+DSdyCHfR80hCQpVULMUkqSbpMDo4kT3O0fQRusyuz0a2/SHQZYWS63BpdQlb0+IMlBSfx3pXw15BCI8gKuMEdwijra3JX+ALPA5SaZNMzqanP4eixgvU9TZlRVNIpU3spIGsyJTWahsmnkIIFFXBSujYSZMgCPG8gLWVKrWqQ9BaoKmqQldPlp7+HNl8Eqfp4bkBVsIglbbQdJUgiGg2PWRFJpm2YkLxDm55pcWTqpQaLMwWuH19YcNKQiJuNHEdnzsTK3z6wQTNpkfvQJ7u/gdrUTlBQNVzWWzUKDgNZqplLq3G1YRIRMzVq4TbKCDvNYJW23jaMuKuXaDquHw4NctavcFYRxvnRgewdBVVlmNuTDbNrZU1nGCdn+pSc/4G23y9FcjE5+F4l3H8cRLGizxteem5DWS+wONhcbnC1RsLFEvNWHRvm4BrNyEiQb3pEgQhsizfIyomqNYdbk4tIysyVssv5XFhyAZnc2cYs0e5Xr3BB4WPmKxPUvJ3j5+hSAo5PcvB1EGOZY5yMHWApGqjSMpzqeEbAR8tzfHjqZsMpXNYqgqShK4o/LOT5xjN7J2/kR+VaW4EMV8YaH6Bp4W0aRGe70gyeqCb29cX4oyKF1KrOkRhLFJYqzSRZYn+oXZmp1a5eWWOSrnJwHA7q0sVxi/OsrpUZnC0k1TaWncy3Ni+qimM7O/m0sdTXL1wh2zeprsvh6LIrC5XMAwVyza2GHxuB11XOfPKGKvLFS6dn6Zc/CHHzgzR1pFClmXqdYc7t5eZuL7I4lyJRFLnzCv7OHis/4HbXGrU+PXCHf7s9jVuFVf55fw0/+nmFQD8KKQ/meGNvuFnvrhSZRlNUTYlncpNh3dvTtPwfAbzGY72dqG2KhSqLKOrCk3PJwgfNk8IQlEnjEr8xurIfIGdodZwmLqzxlvv3WRxuYxpaPzR751meKBtixHkbkNVZQb78hw90Mul8Tn+X//6Z0Br/pAlZFnm9Zf2cfJI/2MHMuvvV1BIa2kOpw/RbXZT8AqsuKssOouU/DIVv0I1qOKEDn4U82DWMzaSFGdbVElFl3UsxcRWbZJqkqyWJa/naDPayGlZUlqKtJoiqSbjIOY5y8Ss40R7FwlVxQ1DEqq2sUJSJJmMvvuKx/fCD1dp+LcRfH69fb7A84H27gyZvN1qbojnqUw2wYFjffQO5pEk0A0dM6GRzdl4XhAbWyoyA6MdtHWm45JQGGHZeiub4xH4Ie1dGQxTI9eeJPBD7FZpX5Fl2rvSnHllDLfpoWoqCdvAc31cx0dRYoPPndz7uqlx9tX9yIrCxY8mmZ1a5eNf37rHHTvuJG3rTHPszDDHzw5z8Hj/Q0tLHVaCL/UOMZjK8h9vXGIgleFMZ1/rVYGt6XRYNvpT+h09LlKmgSrLjC+sUHc9Vmt1bi6tcXVxmbGOPKOdeYx7MsFeGFJ1PGRJIoyWqDYXqDu/pOG+hxfcptb8aeuMfBARujrGb2TX0sMQtxnnOHd65KnUfX9ToMgypqnRnrcxDZWOthRnTwzSlrP3/GEsSTLppMWxg70YusrcUokoFCDFZahcJsGRAz2bSoNPAk3WyOk5cnqOvrCXalBj1V2jFlSpBTXqYQMv9FqE3pDonkBGQUGVVTRJxVAMTMXCVhKktBQZLU1aTWMp1nMbuNyPkUyOjGFSdh2qnouhqKR1g6xhktT3tj3cC9doeLcRu1ja+wK/fZAkKX6g3/dQ1w0NTVfJ5BKxhELrlrRtY2NhBJBKWyRTJkKwwbuLAwix0fG67X5lCSuhY7YaOjZ1U64nBHbI41MUmc6eLKdeGqWtI8WdiWUqpQauG0DL5HLdSqJ3IM/IgW5MS99iY3EvEppOQtPpspOsNOsMJDOc7OjeuGYSn43FRF8uTWc6yfjiCj+8dB1DVbm+tErVcTne182BrvZNnbYNz2etVieha+iqgSyrqEoHipRBkdtQlXWDyBBV7sTQDvEb3bW0HUxDY/9IZyyV/wVIWPpndj3W76mx4Q7Ghp9NO6ChGBiKQbvxcC2G31QYioqExFqzyZpTx1JjcnVvMo2yh5OcEBFetErTn0R8UVb6AnuE7QIRZRsV93VO4jriB+nOxv/69rdohj0BunqzdPVmeeG1/U+2gW2gSDIvdvWR0LRNDSVhFBGJ2KrgWQY0Yx15jvR28smdef7T+csEUYQsSQzms7ww3MdI+10KQxhFlBpNlio12m2bjNmLbYxiascACUs/SWKjDVtCQuY3vmvpC3yBL7AZF1cWeW9hholygb5kmpIbdwUcbe/km0P76Uw8vqfWTiCEhx+u0gxn+YIf8wW+wN5BAroSW0tcNd+jEXjxa8+QwTfcnuOrh0ZxPJ+P78zjBAEjbTl+9/gBjvV1Yd6jMFxxXBbKVZYqNb56aIysHXdeyZJB0vwKitLG07pcPwifs0AmzgH6YQU/KuAFK3jRKkFUIYyqBFGdUNQRwkeIAEHQSiHKsYU4GjIqkqSjSCaKnERV0qhyqvWTQ1faUeU0svRsJKB3E5HwCcIKbriIFy7hhWsEUZkgqhKKBlHktq5JFKdR0ZAlDUVKxOevZNCVNgylG0PtQpPbkKTdi5o/L4iEhx+WccN53GABPywQRGX8qEIkXITwiAhBRPHYQkOWDBQ5gSIn0eQsmtKOoXRiqN1ocmaTff2TYrJSJBKCv3/oJElNxw0DJspF3l+c4bXeIXY7LydESCRc6t51Gv7k51bN90EYX15hvlLheHc3Gcvk9lqBd6fucKdU4oX+Pk71djOYzX6mx+iFa7jBYjwOowJ+WCSIqkQ0iYTfmusEkqQgS/E4lCULTU6jyGk0JY+utGMo3WhKBll6eon+Z4EwcgiiEm64iBss40eFjbksippEwtuYy+L5XUNGi+d0OYUqZ+O5TO3BUDrRlCy7qVuyV5Akadvs6kS5wK1Sge+OHcLYY/7jvdAUhf5cht87eYhzowOEQpAyDQZzGVKWsYl8nNA0Xt8/TE8mRV82w9CGhoyCqnQjS2YrCFu3HPIRREgYT51les4DmfXApRQHLuEafljAC1daP6t44XogUyMQNcKo0bpAQSuYEUjcE8hIKpKktQKZ1D1BzPrg70BX2lDlzMbfNCWLJmeB54sIKkRIKBy8cBkvXMYNVvDCRdxgsfW3NYKoQiAqhFEjvvlFgCCM/aYlFQkdRW4FMnIaXWlDV7ow1R4MtQdd6Wz9dKBIViuw+exR927iBLP4UemB79GVdlL6CVTZfmQgEUYN/LCAEy7gBku44QJu0ApkoiJBWMYX5VYgsx4oR0go8SQq6SiS3QpkMmhKG4bSian2blxHQ+1Bk3Mo8uObeAKUXQdNkXmld/DuHyWJv7h9Df+hHQIPQsvJWDRj3xNRJ4xaP6JOENUIoyp1/xY17+qOt1n1LrJY+89PcDyPj4Q6TNI4goT+2PfmrbU1Liwscrizk4rjcGN1lY9m59AVhU/mFzAU5YGBTM27TsO/TSTcB2xdQpOzZM1XUOSdBQ+xiGOAHxZa428ZN5jDCeZww0X8sBCPxahKJJyNQAbEpnG4EcgoaTQ5h650YChd6Eo7mtLWCm7aUOUcimQ+F/d0JHzCqIEXLuGGK635bCmez8Ll1mKi0jr35j1zWdSa39cDGRtVTseLMrkNQ+1uBTPdrXuwC03OP/E9uFdYatTww5DeZJrJcpFGsHnRcH55nulKiW+PHMB4tnxfbENnf1c7+7seLh2iq2r8vs621r0Y348CH8e/gq70o8t3BV+9YOqujsxTBpjPXSCzLn8s8Fs3q0PNG6fqXqLqXaLqXsEPiwh2vjoURAgRAA7hIzu9JFQpiaH2YGkj2Pp+ktohbP0QipxAlkxkyUDisxFKi69PRCRcgijOvpSc9yk7H1HzruGFK+yknS32TAkBlzCs4oVLm16XUFHlFCnjNFnzJTLmGUy1H0VKIksakvSM76b7UGi+zXL9B9S8Kw98T8Z4ibH8/56ENoKyTSCz/uCIRJOmP0PVvUDBeZeqe6l1PXZyHeOAORJNAspbKi8SCrraTUo/Qd76EinjGKbajyxZrYfPzseQKsm4IqDoNFFlOfaT8X0M5cHbiceLiF1oRQiEG7/H5+7hBSutAK710AzmcYJ53HCJ4CGB4oOuyHL9ByzXf/CYn3sy9CT/HiPaEKqs8biT4VKtznSxTMo0mCgUmK9UyVom/+Xpk/y7Ty9yZXmZbx7cnv+w1vgZc5V/ix+tPWDrCkn9MMe7jiKL9od+zwIBrUWJHxaouBcoNt+h5HyIH60+JFi6dxshCI9Q1IEibjgPm51cUCQbUxskpR8lZZwkbZzEVAdQJfuR298LxGMzJBQufligGdyh5LxPxT1Pw7uNHz1a0h/uzu8RzVZGemHTuUvo6EobaeMsWescKf04ptqHIlutefyzD+RuldaoeR6diSS/mJ1kuVHbMIkEuF1e27ApeF6xrml1v7idEE3K9T8haX4dXRth/T5tuB/j+JdJGC/wG6kjI/Cpe9cpOe9RbP4aJ5gniMqtwMbdY8KhIBB1omAKJ5in7HyILFuoUhpbP0DGOEPaPIOt7eezSVMKvKhA2fmAQvMdKu7HhFGNMGoSCYfd6MmP9xLgRxXKzvvUvEss1f6ctHmKNutrpIxj6MreaZbsFiLRxAlm40mLrV1ukXBxwwVW6n9JqfkejWCqlbnazesY4gVLFMP4u0poo+TMV+iwv42hdiHtREa0hb5UmvNLc/zfP3qH/lSGsudQ932OdXRjaw/qWhJEwmllmRY3Mk1OsBCXIIMVQtFECK+1eAhaGc34999kyJKEKktossy1pRXCSPDGyDBDuSyqIuOHT6OiGpflwqiBKgdIPKxULXDDJQrNt1hr/IyGP9HKMjd2da4LRZOmP4EbzFFovkN74ht02N8ka57btX08DgQ+XrhGofkWxeavqHlXWtnAOOOyu/tZodB8i7L7IabaR9o4S3viGyS0UTTls3e2z+omhqwQCcFEqYCmKIxk7hJpy26TsufwvJfGtoeEJOlbFr9xWX53qAvPVSAThBUa/m3K7nlq3lUa/m2a/iyheNYiXFHrRvLiDE4k4bKEH60hSzqWNgTa4xuuPQ2EiPDCVWreFUrO+9S8qzT9KdwdZg6eDGEsWhTW8cMSgajiBHOk3RNkzJdIGcdQpL1v9X5ShKKBE8wSiVNbXnOCOSruBQqNt6h5V3CCeUJR25PjEASEokYY1lrW9SWawR3y1pdJGccx1a2K1NvheHs3lqoxUS4gBHRYNgeyFgdy7WSN7XVkgqhMofk2FfdTnGB2o1wURNWNUuxvK4FXVxTCSPDx3DyfLizSm0pxvLsLQ1VbxqpPJwcvRIAfldBE+7acOyEiQlGn4n5Csfkryu5HNP1pgqgK7IUUfZzJjTM8lVaJfe89uu7FeobQCWapuBcoOx9Q92/QDGJX9b2Zy0TrHqwShvHY96MizWCajHGGjHmWlH4MeLYdQfeiN5ne6AgaSmfpSNicaO/eeD2IIm6V1nhCT+FdQ5zFZrOs/ANwfxdaJByiqIEkWQixvvCO+I0QxFtP77vBAjXvGiXnfUrNX+GEcztKqT4bCCDAC1eJhIMiPzsNm/j6+K3Sx6cUnLcpNT/Aj4rszWT3gOPAxw1mcYP5jQAqjKok9SPoaucTkqM3D+D1e2O3JpMwauIEM/es7uIdNP04hb3W+BnF5i+JHqNM+bQIRJWaP04jmNyo92O+iKH0PPK8B1IZsoZJbzJNodnA1nTaLIusYWEq29/KoWhQdj+h0PhFXG74AhvoTiXpTiX5cHaOIIzoTCbpTNo0PA9T1UjqT0eMjfkuJYTqAZvLN5EICKMqZfdjVus/ouj8ekt5d+8gIUkaptqHoXY/+u27BCGiVrb9FmX3Q4rNX1JyPmjN88+uZBKJBk1/kqY/jRPM4IXLhFETW9+HJmd3hZj/uFj3VgqjiLNdvbRZCYbTdzMyNd/F1vRNLdnPAkIIGp7PYrnKWr1BvaXYu5MK19HeTvpyGUBBU/oJwhVqzlsocpowKhNGJVSlh899RiZ+SIf4YYHVxo9bnIedkgqfPRTJxFKHsHdJxOdRWA/yvHCN5foPWGn8kKY/uef7fTginOAOS7UFqu4lelP/gHb762hynniC3PmgFCIiDsYkQIlVY4WAVlD0tG2GkWji+LMbgcx6ULjS+BEr9b96KL9mbxGXe1YbP8IPi0TCpSv5t0A8ul6f0g0O6ga05ria5zJXqzCQymA940nu845j3bE4149u3OblwQHO9PUgBCzWavSlU7TbT8cdEYT4UWlLmUSIiDCqUvPGmSn/99S88TigfUaQUNCVPKba88xKxEIIIuHihavMV/8nis1fxlyWzxQRde8ajj9D2TnPYPa/JmOcQZXTPO5ctluQJYmTHT1I0l2+qCRJHG/v5mhb157qRW2HMBLMFsv8xYVr/OrWHWZLZequTyTEhkhfzLe8G9nIkoQiy/yfvvs1/jAXd8olra9Tqv8Hyo0/RZYSRFEN2/wyGftv8bkXxAtFnaY/zVz1j6k4nz4HA/thkEgaR7C0EWRpb1VU1xGJJg1/irnqv6HifIIbLj6T/e4EgoBmMM187X/CCefoTf19dKUdiZ1fmyBaJgiLgMDQxnC860SigW2+xG4M7lA0cMJZBHFmzw2XWK3/iNX6j2n4E0+9/d1AzbuKVJfQlDxp4zS68nhifwv1Kn926yr/4NBJ+lNPp6L824acZXG2r499bW0kDR1b19EVhaFslrxloT6lHLwQIUFU3JJZ9qM1is1fMVf5Yxr+5DPPPMuSSVI/iirvrR/bvQiiKlXvArOVf0Pdu4kfPogk/ewRigZ1/yZ3yv+STvvbdCS+ja6081nwUSIh+Hh5npxhMpTOobUWJxI88yAGYHxxhb+5dou/uDBOJAR5O0F7UmJytUhXJomt64SRYK3eoOH5pAyd4fYcpwd7ONC93uWkYKijtKX+CVn7e4RRA1lKoMrtqEobn+uMjBcWqLqXWG38iFLz13jhynPu4yKR0k+Q0IafCcs9iOrU3EssN/6KUvM9vHB1l66PstHL/3REQtEKtCZACCQkOuzvYKmDO25tFEIQRhWCcBVVbscPF1p10925YSPhb+huOP4sFfdTlus/oOFPPOEKWEJCIb52u1PbDUWNmneDxdqfosppFCmx6fotNWos1KsczncwVSmxWK9u+vxUpcjF1UX+wD/81Mfy2wZdUdAthay1mV9kaRp+GN3ffPHYECJsZdzWM4IRoWhQbP6KlfoPqXnjsefMMyyrwN1ARnsGgUx8j1cpOb9mpfEjKs7HhKLJbpTF43tRat2LT7O9iEg0qHvjrKIiREin/R00Jf/MFq13j0QwVy3z8dI8lqrSl0wzls3TmUg+hNC/d5hYKXBxdpEwEnzr+AHGOvKsVOv8q3c+5OxQH0d6ukibOlXX49rCMtNrJQRwtLeL9uQ6BUMQiToSKorchiTZLU0ZhUg0UaSnp2o880BGiKjVUn2Z1caPWK7/RetGf/KbORa4M1oXR7unPXidER23LMc12pBIeC0dAndDWOnhkFGkBEn9CKba94j3Ph3WWxIb3k1WGz9jufaDVkvlzm/UeMAkUKQkimy3RLL0DR2d+LoIItYFtXxC4RCJRiyeFzVbE+xOjtej4U/g14oocpK89SYJbay1n4dDlhNIkk4oanjhLJFwkCRjF5UrI0LRbLUUL1Bo/pKqd/HRxyWZrYAiiSIl4msn6a1xpYJYb/dcb712CEWDMIq1Vx5HGgAgiEoUGm+T0o+jy20k9LGN14pOk9ulAvsyeS6tLnJlbZmscU+gU69ScJoEDyCmSihochpd7WxN+I8PIdxWJ4mzo/crUgpFfjYtvaqcYS/KvBOFIooscby769FvfgDisnmppT0UlxNr7mVWGz+l6Ly/g3Eite5ds/WjtdqF4wd4jGhTK/16Z+fDsjyKbJHUD7dE4vYO8VzvUW3N9YXGz1tz2c4hbWjDpFr3otHSy2ldi1YgE2102sXXIIwaGxy0nS7YIuFQ9S4RigaakiVrnsNQup8pZ0ZCwtZ1KqUCk5UiK80GVd+jx07RmbAZSmWfKU9mvlxhqVJjpCPHH5w8zKGeDi7OLvLHv/6Eg10dfPXQKENt2TiTdGeeH1+5yfnpOZwgIIzWpVQCHP8aUVRp/VsghIPUEspLGK/wudOREQQ4wTxLtT9ntfGTXUiryuhyHlMdxNIGY9EnOY+qpJAxkCS1FbzEgzuMarjhCm7Y0soIFltdAg8e7IpkYmr9JLSRZ5COjVdtS/Xvs1L/K0JRffRH7oMip1qt4qextUNY2gC60tUist1Nlwsi/KiEFy7j+DPUvRuU3Y9p+DdbejQ7w3p742zl3wBgKF3IO5gkFSmDrvYRRmUa3gUMdRBTO/S4p/vwYxMhZefjVpfSJzv6jKF0kTSOkNJPYmv7MNTeloDY3TZNQdAS0Yu7H+r+OFX3ClX3Am64zOMF5jF3Z7XxE3SlY1MgY2s6XYkkiiwzV61QbDY53nb34SqEwFRV5AdMBPHq+zCSpOGHpcc4prvwwkXq3k2awdSO3m/rB0nqzyZDlDSOtFbNu5t2f2dyCkNVnyqQgZBggyMT4kerzFT+NRX3ApFoPOKzMW/MUHuw1GEsdQBN6UBXMiiyjYQW5wUjh0DUYk2pYB4nmMMJZnGCWbZf/Mgosk1SP4Am720pUhCX1mYr/5aK+9FjBzEgoyoZUvoJ0sZJkvpBTLWvJc6Z2pQZj4TXmsuWaPpT1Lxxys55msEUQVTe8R5jJeubTJX+n4zm/gV5603UZ9jZpUgSXx/cx1cHxlhrNvh0ZYFfzk0zX6vQl0rzv3vxDWz52WVmqo6LQHB6sIe8baHKMhISuqoQRNGGRIEsSbww1EfD9ZleK/GTq7foTqfozaYRwqXefAs/nIs1kwjwghlkKYFtvNISxPsc6chEwsMJZpmp/CvK7sdPrBVgKL3Y+gFSxtFYB0Bua62erfuidbklziM2sjFChAjhEQp3YyXth2s0/RmawXSLyT6LH61u7E9T8uTNN1Dl7B4TwAR+tMZ89T9Qdj56rBtwXewqb8aia4bahyZnWhkZcyMjA+tdQQKEHL9HsuKHt36YtsRXcIJ56t44BecdGv4kYbSTYEoQhGXWGj9HxqAv/T/jQUrIQkQI4YCktMSq+lDkHEG4iBtMoqv9Oz7vHRwVheZbrVXqgzMKutJJSj9G1nqVhDaCrnTctwq8VwdBgNBQ5BSyZKIpWRLaPvLWl3H8GSrupxSdX1H3bu44swW0JuBrNP0ZTLUXSVLosBIkNR1dVjiQa6cvmea1vqGNz7QVVrhVWkVXNvM53KZH4AUIZNKJF0jqRx/rWO5FxblAJPwdBjIyGeMs3am/9UT7elyoUnrHsvvFZpOqu7OF00q9Tj7xdOqvQsRk3/Ws5Urjr6l7NwmjB7f5m0ofSf0QSeM4trYPVcnGitH3zm33ZGTieW09M3jvnFai6U/RDCap+7epezeJhIMm50moY8hSgr3yvWmdPQ3/Nou1P6HujRNGOw9iVDmNrR0kZ71KUj/auhfTKLKNsn4vth5d63OZjIGu5FHlJIbSS8o4SYf9rQ2hy5IT60TtxGYjbrAosFT7cxDQYX+LZ6Xqvu50DbEZZkymFYRERJ+JGJ6ELMkokrLRTq3Icdao6fk0fW/TdcnZFoP5LO9PzlBpxvOtLCXIJv8BopW0iAPwOk3vI6LHDm63xzMNZJxgjrXGzyk77+MFSzxOuUSREuhqF0n9CLZ2gIQ2RkIbxlB7Wjf5k51KTMir4ekruOESXrB0j8ppnLEx1R5y1mubVuQP36YgFAJZkjZ5UTwKXrjW0jb5OU4ws2NOjKkOkjZOkDFeIGWcwtIGUOUkD1+lSi0lRr21ok2CEpOzLG2EhDaCqfVTcj6i6l5okWMf/n3FrZU3UeUkST1WQ9aU7VZ98UpNiJAwKuOHi8iSgRfMoci7bXwoHtLWKiGhkjKOkTbOkDHOkDSOtSTMH/ZwlFoTjgySioKF1ur+MNUBDLUXQ+1mrfEWNe/qQ9RfNyMUdRr+BFXvEobaiYSCqWqYahyAHm7rQAhB1z3mkEIIvjl0YFO5CUBEguJymdW5IrIik8wmyHW2k+lIbesy/DC4wTKKtPPvRVfaSGhjj37jM8b5uXnOz85jqI+eK8aXVzjZ+3StybGoZBEnnKcZ3GGt8bNtuW6yZLYC6aMk9UMk9AMktBEMpbdV1ny81WrskeXj6QdxgkXcYC5epAUzyOgkjaNxGX4PH8xOME/F+Zi1xlt40eqOyjsSsUZXLDp6lpR+HEvr30HGbf1+NJAlA1W+m0Gx1GEsbRBLG6LkfEjVvdSSIXhYUCAQwqPqXkJX2jHVAZLG4cdqZHhShFHE9eIqd6plVpsN3DBgKJ1lX7aNbju5Se33WcBQFVRFpuq4hFE8/2uKTFsywVq9wVp9M9dQU2RMTaXquLhB/J1Lkopxj+7aupaQH87g+td25TifWSATRnVq7hVW6n+JFzwesVeVsyS0ETLmC3QkfhdLG9o0WJ8GkqSgKRk0JYPNvtaxNvGiVarOJWreVVTZJmUcR5G2rtCEEESIu0JBxMzzotsgpRnY2k59VgIa/gRrjZ9T92/viIwqoaEpOfLW67QnvknGPLsr1gmqnETVD2DrB0ioY6zKeYTwcIL5R35voahQ866xVP8+fXK6ZeuwWWNGbAhz1QmiFfxwHkVOEwkXhb1xcN4KCUVKYGnDdNrfJW99KRY6fEqocpKUcaw1RnNQI1Zf3uHKwwlmKDsfk7fe2OIfNJTKbnl/VyLJ744c2NLRIKsyzZrL7M0F1hbL5LvS9O3vYfBAL8lcAs3QUJ6h+dzzgAvzi/zl+HUOtD/cMwZgpd54SmXfdbJvgYr7KUFY3UZaYn0MDpExX6Qz8R0S+thTz22SpKBICpY8jKUNb+i31LxxwqiGKmf3jMS6LqlRdS9TdN7FCWZ29DlZsjDVPtoTX6cj8U0S+oEd8eweBU2JvfJs7RAJbYxlOUmx+W6rA/ThGQ4/ir87Telo+aXl95wvEwnB5dUlLqwu0gx89mXzvNQ9wFgmv6E18yyRtkwsTWO2VMbxYxNmQ1Xpz6dZLFeZWF7jWG8XCV0jEoJCvUmh3kCS2FjECxEShMubOGHxInZtwyvsafHMApmad52S8wE179pjdsvI5Kwv0Wl/m5z5SosMurcTsCyZmEovht1JW+JN4vSayXargkBEuGGAGwashzLN0OeDlTscz/VwILMzT+IgqlF1L7La+PGOSZWG2klH4vfoSn4XSxveSLfuJpLGUVQli6H2cqf8/8EL13hUZsYLV1lp/CUZ8wy62tFqZbwLCR1DHUIIF6EOYRsvIKETRCvwlGqqO4WEhqWNMpL95ySNg7vewaFINh3277QM7lzK7gc7+pwbLFLzrsbCi5LFven/9dv9/lG4XVumpqv0jnYiyxKX3h2nsFxhbaHM5JUZTn/5KN1D7SRSz5dx3l4ja5m8MTLMv3jz9Ue+97/71Xskjad72IuWiOZS7ftsd89IKKSM43Ql/4i89QaqnGx14uw2JCR0kvoR4lEk79F+ACKCsBzbyzi/3vGnEtoonfbv0ZX8g7iEv8vHJ0s6GfMFNCU2Bp4p/+sddYw1/WnWGj8lZ76CLBkPyDDvHiQJMobJ748eYl+2jYSqoSkyymfkB9WZtEmbBlfnlyk3XcJIYBs6x/u6+Y8fXeZn4xNIksSpwV7qrsevb9/h/ckZMpZJwogXsJFoUKz9Mf49gpxRVAFJwtSO8Llov14XdSs7H1B2zz9WEGMo3bQnfod84sskW6aNMfa2VhmvgpX4ZnpE/X2uXma8vLSp08YNfa6Xlhi0szveZ9n9KF65RTV2EqGa6gA563W6kn+AqfY+obLuoyFLGobSRdY8hx+usdL4Kxr+7Ud8KiKMmqw1foEmt9GW+PKmV9evLxgtr6HYKVVVOp9RICORNk7RlfwDksYhVDm96yaYsY+IQdZ8GT8s0fBvEUTlR45/QUAQlWn4EyS1Q6j3+MD8YmaCUAi+MXQ3TXunWuZnM7f59vBBuuy72axGpcn8xDIrcwUOvbgPVY8zdVEU0ag2cRreb10gc7CjnU47npgfhfZEYkclqEcj2pbYq8ltpIyTdCf/kLRxEm0PxuA61rN6D/d72h2EwqHgvEXdu96yv3g0EtoB2qyv0p74eitbtPuPpbj0pGOqQ+StNwijKiuNHz2yqSEORldYrv8limyjKSd2/djuhSzJnOzoxlBU0rqB3CoD130PJwzIGdZj0RWeFsPtOc4O9eEGAbIsEUQRGcvk1bEhPpicZXxhhf/8yVXeujFJEEas1RsEUcSbB0YZyGUBkCQNyziFHo1s2rYi59DVAT4XgniRcGkGU1S9SztOMwIYah9Z8xyd9u+T0Ed3rZS025AlCU1WMBVtI5QxFZXeRAZbffSEKVqut2XnPDXvGjvhDcmSRdo4RXvia9j63ns+KbKFqfbTbv8ObriAH5Xxw9VHfEpQdS9g6wdJGSdaHVObB2wkGi0CmIoipwijMkIEaFIc3OxVDd9SB8mZL5OzXkOTc3uqC2SqfaSMY6T0o5R3VGIShKJBw7uFqfahcjeQmawUCaK740MIwVqzwS/npvlS79CmQEaSJXRTI9Oeon9/N1bSRFZkfNdneWYNTf/M3UmeOcba8riZnZW0j/d0oe5am+vmhYkqp0kZR+lKfpesea61yn8+/coeB+u6TcXmOzSDaR49l8kokknOeoXcLpV2HwVVtmPDX/vbNIM7VFzvkU0VYdSg5PyatHEKSx3a06yMLEl021ufdfO1CnO1Cq/1DiErz44n05tN8fLoADnbpCNpx0arqsJQW5avHhrF1FQmlgsslmtoikx70masI8cbB4bpTAn8cBlEiKp0och3VaRj4noKRc7xOcjICEJRpdD8JQ1/asclE1kyyBin6bK/S8o4tmcrld1Al5Uip1vYmrERKQdRyFi6nYy+vZHfvYg7ueZaxoWzO9ijjKn2kjXPPVPXWkU2SeqHyJqv4IZLFJvv8qjMkRsuUvfGafg3SBsvbJQE1+Ws/WAeP1xFlnR0dRDHH0cQoCkd7N3QlMma58hYL2GqvXu0j7uQJAVT7SdnvU7Nv0EYPporEwmXRjBBJjpLEEV4YUDN96h4Ln4YMluNJ15BLJjXCHzC+zoaNEOjZ7QzlgvXFDzHBz9ENzX69nU/t0afe4me1IMXQ+K+6/fK4MAeHYVEQhslb71Bh/1N2CX33+cBYdSgGUxTcS/hPXKhE8/zljZE3nqTlHHsGRxhDFVJk5KPk7O+hB+WHqktJfBxghlq3lWS+kE05fiuH1MziGX/E6pGzfc2LVgArqwtc3VtmZe6B9CeYSCTTVhkExbH+zcT32VF4XePH2S0Pc/Hd+ZZqtaxdY3RjjzH+7rozaYJw+s43no5KWpZ0sTkXyECYBFVacPSz/Dc68gEUY1i8+3HMESTSOpHyZmvkTZO8yw8jZ4GuqygyZtVPBRJpstK7UhSOnYnfmvH2iOypNOe+AZJ/dgzSRXfj6x5DjdcotT8YEc15oZ/m2LzV6T0U7ApZRzi+DdpepcQ+ChyDiE8dHWIvRqWsVBgkpz1JZLas1PC1ZUOsuY5Fmv/sZXKfkT3V4tYHYomJbfJ5dVlfjh1nU+W52n4Pu8vxgFvEEUkNI2xTBuWunksrMwWcBoulm3QOdjOzU+mUFWF/aeHkdXn+576LHAvWX8d614yuwcJWUqQt96kLfF1nve57XHhhgsUm79qyTU8ei7TlTxd9vf+/+z9WZBkV5rfif3OubvvHh57REbkvgKJHQUUUHt19cZms9kjdotsE43kAzVG6UHSUDLNg8z0JDNJYybTy5iNzYxmKKM1m2ouzV7YXdXV1VVAFQqFPQHkvsa+++5+96OH6xEZuUSGR6Z7ZiQK/7IsZEZcd7/3+rnnfOf7/t//j6NP94Xf9yAIDErOd2kFtzq8zd2lCereOVLGYbJW7wOZz9aWaQQ+b05M84NbV1lu3dmif7myjheGj+zI3ktYus7JsWEODQ0QxTFCCAxNSzqdpKTpfk7L+wUJ1aBGFNcSgU5thCBaRggLx3oex3yefa0jE0TVLZfR7uqlEilsSs63yFnPdy11/ySR8A4U9dDl3MYCc80qQZzwIF4bnn4g2VehCOIq5fbPOhb2u3wWJpY2Qt56CUeffCK7akMrkjaOk7NeoO6f27W7yguXqfvnCeINhBjc1i0hsYyjSJkCok6KUUOT+b5dly7zFOyvYOtTj9XBPCEJlrD0MbxombCjcLkTYhXgh8vEsUtaNzlSGOA3D51AFxIvCnlz4iCwKYZnMJrOULJTWz+Lo5hb5+dYurWG1ATFkTzrC2XGD48g5Bdj998LLNbrnF9e5eczs7SDYKu9FCBjWbw8Mc6vnjjWs8+TwqbovEbWerbjS/bF+S4UMV64RNX9RceC4MGQIoWtT1F0XsfUSk9gLhNY+hBZ8xQN8yR1/9NdX5EIX14hiKoJMbuHlQJDapiaRqwUFzdWCZXiwDbvNENKgn02XKQQ2IaObdw/jHDM59G1MZRqU2v/ObZxAss4jZQZ4riJG3zela5PN+hrIONHqzT8Cx2S4+61aU2kSJlHyNkvYRu9E0XrN0IVU/ZaLLXr3GpsUPNdFIrj+UT3Y6eHNIpb+NEyreDSA0WyNmFoeTLmmaS1V3synCEpTBz9AEXnq7SCq7sGMpFq4IbzNP0rSMvutDAm5F5Ln8bUxzrv29uJ4X4wtCKl1LcwtaHHWq4UQiKFhaVPogfXdw1klAoJog1i5eMYBgeMPAeyeYI4IoxjfmXqaOd9xY5ZA93U0A1JHCsCPyRTSJMpppG/ZC3XD8L19TI/uzXDSqPBpdU1TE1nJJtmud4gY1kPLEXtHQJdZjsWHocfu4dPv5Eopi/QCq51JXRqaoNkzJPY+gE0uXsJvtfY1J1JNmXPdxXIhHEVN5hNiPjmyfvKcTwshlNp/Ci5D7rUGLYdzpRub4Lbgc9so9q3gM8PQy4vr5EyNPIpm8FMYjHS9gPCOMbSdXRN7olobBnHsIxjRHGNhvsjLPM0OefX2LQNUs02bnCxJ+ff10DGixap+58Rq+4IdqZWouR8q9OF053+yn5ArBSN0OdQZoCCabPYqhGqmJxpo1A77ryCaD3xKYpqXQV6pjZI0fkqmszyJOvqpj5M3n6V5ca/J4g32C2NHMV1qu67HXnx7YQva0tqPfF88kDITqt7ryExZImi83rHn+fxIuHKTHQlqqiICOIqMZseZMl3/ezgKEpxz2Syye/YnOSklJz6yjGOnJ1GCEF2IIPbcpGP0aPlacDV9XVmKlX+5dff4A8/Psd4Psd3jx7hJzdu0fA8pjtdF72AFFanxPgKljbWs/fdL3DDRdrBDJHafUMGCeE+Z73Q0WV5cnOZYxwkY51F1LWuOmr9aIWq+z6OMYVG7wKZ8UwyL4RxzIvDY4xncpzeFsi4YZg0zvcpkKl7Pm9fuEAxVeP5qbGtQGap1qDWdhkrZCmknHsUxLuDIo7bxHGjU3bUgEQMNVH23ec6Mn60RtO/3LX4nakNMZj6DoYs9PO0eg5T05jOFGkEHoam4UYRZa9J4kqx88DzoxVa/lW6VTg2ZJGc9QKaeDyGfDtBEw62PoatT+LHG7taGERxi7r/GaX4O3f9vEIYl4miMn40QxitYehjFFK/1fNzNuRAR/G42EcNjZ0hkJiy1JGG3w2J35a6awMwYDsolTz2d3M6NvlYcazw2j6aLtF0Da/lU12tcfWTW6TzKZ756vGeXdPTjlgpNCEYy2WxDB1T0xjPZfnNk8f5H9/7gCtr63zn6OGefJapDZGzXkQTWb5o3BhIhBz31pU6lniAPYFncTt0menMZQfwoqVdG1KCuEwzOE+sfqMv56MJwaujk/cQep8fGuVYsYSl9WfJ9oKQT+eXGC84TA/e1tT6/ueX+WR2id958TQvH5zEfAjbDoGOZZ6g5b1Hy3sPXRsmjJYAgWWcYl93LYVxDT9c6bDXd1+oDVnE0aew9QNPVTYGAJVMitfr61vZmFAp7qUP3gkvWqEVXO/KlVgTaUxtBFsf65tmTLdIlENTpMxjtMPZXQOZWLm0gmsEcZlY+VtpdYVPFK/jh7OEcTlxqlUhsQp6olC8HZY+vCUa+GQ6dsR9VY53xqY3WLxVBrte2eDCxiq3ahW8KCJWClPTKDkpfu3gMUZSGUI/pLJcJe5YFJSXq9gpi+ufzTJ2qDtxxl8WSCGQUmx52HhRSMsPSJsGrSDo6TgxtUFy1lk06XwhO8YSo8r5ro7VZR5LG8HUhnjSHVtC6BiyQMY6Tdiu7xrIhHGdpn+946wd9kXzJmveu/5lTYuMafXtbkUq0YAZzIR3hNkbzTbzlRp1199ys94rhDDJWF/D00YJoxVQoOk5dH0USz/OvtaR8aNV/Gi1K6l9SMTvHOPwYyVh9goRikbgsdCqsdSq4egGYRwTxWpbYeA2bntNrNMOZ+kmtZaQRUf74EX0cBBCJ20cpa59jBvOPPDYTYVTP1oljJuYWhLIJF1XMmm51kdRKkTXBunkHOjlJGdqQzj6VM/eb+9I1KH30p2hVIQi2tq1fr6+wrtLszSCgKuVddK6QbYjmvX62IEtDyalFIEX4LV8fNfHMHUs28BynmwAvN9QcGxGMhlafkDRcai7Pn9z7Tpp06Tp+wyle5f5NOQAafPkF44bszmXeeEiXrjU1WssbRhTH943c70m02TNMzS8z3f1RYuV21nb1rD1CWSPnbFjFNcrG+RM+w5dqHW3RdVzOZgrdtUNu1coBVGk+mJMKYSBbZ5B14YIwjmCaBlDG8XQJ9G1oZ58Rt8CGS9cxu+iE2cTljFOyjy0+4H7ELGKaYY+g3Yy8dUCFz8OCR/QKpcYylU67bjdtCoOYemPZmLXSwh0HH16D3wThRcuE0RrW6VDTQ5g6olzr2Oc7JBgo75M9kkg2H/dmJ0ghEATzp52cJsZmc14bq5RY8jJ8H9+9WX++0/f5+zgKEXb5j9cO79Vu7Yck4mjo/huwPiRETRNohka64uVL4m+d+FYKUnVu2HI82Oj/MWlK/xff/gjhII3Dk5zuNQr2wrZcUg/+FiMBx8vFIoAP1oniMtdvcLSJzC1Up/Pq3towiFlHEV2Sd7d1JVJGYd6LtQaRjH/5tKnPDc0yt89kkhEKKX4YHmB91fm+d+98AapPnDdhBBYhkbbD6l1DCIFgs24ZrOcvZdAZ7MRYTPYTZzZa0RqA6lS6MrraMs8uvhpXzMyYZcDG8DUhrH1folQ9Rem1JhMFyh7LdwwoBl4HMkOMmind8wp+NFaR1GyO36MqZWwtP1TGhBCxza6I69uwo9WtsjBsXIRSAQmujZEpNp44Q0ALKP3Aa0uc51U9pOCACHZexr19sShSwkdsm8YR+hSoklJ1XPvEdDSTQ3UbQJwOvfFLGk8CqaLBUazGRzDoJRyKDoOXz80jR9FTOZyjOe7H9sPgqEVMbSBJ6L71G8oQtxwsaNY3d0iZ+mjPfc2exRI4eAYh7qW+1AqwgsXOhuvif6eHMldbYU+Fbe9K13hYWFokolCjoVKnR9dvI4hNfIpi/lKlabnc3lpFVOTZLuw99jEidEhxvJZFD4t92e0g3NE0QaaLOD5F3C1T7DN50hbX2PfCuL50RrBLtLPtyEwZQlLG+nX6fQVAoGtGUykCxiaTjsMGE/lKJg7Lx5+tEYYPbgNdzsMmcfYJvH8pCGQGHKgU+pKWOi7wY/WCKIyiogoLqNUSBRX8KNFpHDwgivIvpTONHSRxXgC3Urb8ai6IRnDJNAilFJoUnKlsoZA0PD9rZ1SHCuiMCIKI9S2mvbSzdUtVd8vkSBtmliaTt33cIMQQ9OYzOcJ4wjHMHqm82LIwn0tOr4IUCrAC5e69lUCMOXgnjZA/YYUJqY21MnISHYVrCTCDRc7vni9QcVrM1uv8tnaChc3Vql4bdbd5J6GccxCo07OtPvGkXEMg9cOT/E3F8pcWVmnHQSkTJNrK+tU2y7v3Zzjxlp5T11L//C155JARgW0/Y87nksvockCUbSBH17H9T8jbb3BvhXEC+Iy4S4k0E1IYaFr+X01uPeCGEUrDMiZNiUrfY/K6v0QRBt7ehA0mUPvs/PqXiCERBMpdJlGE3YXHkLJNSfBrUIpn0i1COMyYbSGJnMowr50MUhhoslU3wQW41gRqTghsckkTdqP7MeBbJ4wjhECJjM5btYq1HyPiUxua4IJvID1hTJhEBJvC2QuvX+d4kjuy0BmG+qex1K9wbX1DVodQbzt+93DA0UGUo++49Zlbt96xT0qYhXhR6tdieBtwtCK+4brB0nzgi7S6DKNFOauhF9F55rj3ee8btEOQuYbdT5eXWSl3aQeeNR8D0gCmbF0jldGJtD6JKHgmAZvHpum4Wb4xc05VutNorhOudXGC0OWaw1qbQ+5B0HNWtvr/C3CD2fJON8in/o7W78vN/4NbvA5+7r9OozqXav5GrKELjI8rW2JfhQx09igFQaUrDSHc7vVf1WH9d79w6/LzBPRP9kNmsigy1xXHkJh3CCKWwgMTH0ahYfSp8lYryKEQRit7ckdvTsIdJnpkzZNAj8KaXkBURSTcSwsXacfVZyvdRR9pRD89pFTXK+WaQQ+B3NFBuwkSKut13nnzz9E6vIO3ZjZy4scefZJkp33H84vr/CnFy7xw6vXGHBSOIbO9i/uO0cP8/Lkowcymkh12Xb/NCIijCt7UmjVZf6JS0jcD7osoMk0cbSLJ6BSBFG1a+/AbjCSzvDd1BHeGJ/iv/3kXU4MDPGdA0eAZEhqQqJJgd6nrJ6hSQ4OFPmnbx7kN86e4OZ6mUrL5S/OXeLmepnXj0xxbLhE2uqe43V8ZHCXI3o3SfYtkIlUo6uFWiAxtDxS2k9tDV+XkoKZ4kpthpuNDapBct0H0gUG7fvvPCJV39MuRgqnazLa44QUNlI63VSWiFSDSDW3lH1RFogQpXziuJaIB/ah9T5ZSEz60erZcD0uLqxyZXGNrGNhaBoHSgVOTgz13Ktn0405sSbQmc4VaIfhHQS8dCHN2a+fwk5Z6Mbt7Nb0yQkyxf23eDxJ3CxXWG02+Zdf/xqDmRS2fud02KuuJSmsp09SoksoIsK4Sqy83Q/uQJPpfXk/NJFCsvt5KWJCtbdr3g1SCKQQOLrB1ycPMeikSBlJZn9zBunn+ig6hNu0ZTJZzDGQdgiimPMLy1TbLsdHBvnKocQFu1vknc1jNSzjCK7/KUE4h64NEUZLRHENSz/MvtSR2WQoR3Gryy9aoMnsvhzY3UIiyBgmWcOi6rdpBB4gCOKda63d35/kE6Qw92XrphRW11LdyTXf3sUo5eJHi/jhrU7GRKLLYsf9ulfotD33SXun6QXU2x71tocXhARRjGMaoIb6JpGRyKsLsqZFM/B5f3mer45PMaClMC2DkalB7JQFKEI/iTArq3W0L7uW7kArCBDAN44cJG/Z6H26P0IYHQXbLx6Uignjxj3ijTtBoHUCu/1HfNak0+U6FBPFjZ4GMpuQQnA4P4Ct61sqvgqI4pg4jhOD4j4GNJqUpEyTlJmsNaV0ipxjkXdshnMZBjN7zyxKYZKyvoIXXCCMVgmj5Y5B8CFs4xT7VEdGoVRErPwuFX0Fukjvy0W6WwghsKTO8dwQjfB2itV+gApjrDyU2t1xFZJWZymMvogvPSr2stuMlUcc374/UdzAC67R9H+BrR/vBBz9aC00+qYgGscxGctkJJ+h3GojheipzoMXhbjeziXaa5UN/vLm8+S0MAABAABJREFUFU4NDDFgp1BKEUUxbsujXXdpVJpITXLz/CzF4TyHnnk6OwP7gYxpUnCcRPNJxUgl+iIBL4T2xBVs+4eYKG53aUMjECKZyx6n11m3kMLuch1Se7jmvcMNg3vmkIrnUvddprKFvujI7IRiOsVILott6Dy836yBY76IoY3jR3OE0SqGPoapHdjPOjKqY4neHYEnIUZaj93GvZcQgKXpjKXyd7THPWjAxSrsStE3ySjo7Ff+kBBa15OSIrwzuBUCXSuRMl8gbb2CJjLQhwlOoCP6dP+G8xnqbY9ba2XKjTbHRgcZK+Z6xpG5US3z88WVHX+/0KwzU0+UfgFCP2RjoYzvhSzfWmV5Zo1sMc3s5cUvdWTuwsFigbVmi7+4eJk3D04xVShg6U/vPPQkkOiXh3QjIyE6meUnrea7E4TQ9zCXBXRVT98jgijifz7/Ic8NjW3pyAC8vzzHe0vz/B9eeoOUfHyb/jPjwwykHQ6WiveUXvcChYemDeBoJVAx9Di4731pCdXJxnSnjwLs64W6G2ym+vQ9rV4RSnX5IAitbwvxoyLRgul2QCogJlZJd5JAR6kIP1xAqbcR6Bj6GGnrld6eoxD0hX1LkgoeLWQxDZ3TkyPkUzaFVO80W5ZbDT5aXeR0aRhrh9bHTf8lAMMyGJos4bs+6ZzDxNFRrJTJ8NQguYH90ymyH7BQq/OzWzNUXJef3ZohY5rIbRnBr0xN8g/OPvMEz/DpQFJW6mbjKkikGvZpIING9+uQQhGjVNTT7JIC2lFIEN9eGwRJQ0kr9PukIrMzDg0WGctnSZkGpvFw4YJSLSrNf4ttPkvKerkvX3+fMjLdDuwEAu0LqbHwYMR0+/AL5H599klOrPvvTnWCGUWMEDqazKDLAkoFBPEKirjHgUxCLO7X7au3PfwwIu/YTA0Wel6aEB0C4NHCABnj3p2YpelcLq9tdTPohkaulCEMQnKlDHGkCMOIbCGNYX2ZbdiOgmNzcKBI3Uu4DorEc2YT/ZBr/+Jh83nuNgMv+/g0PioebPJ7D1Ry3b2+GokgjBVeGGJqGkEcE8QxT2I4Fh7CJPJuKELc4HP0Pgq67o+Zbb+O676i2yzBpu/QfsZeglYBShGrFkJIdDlIynwOhKTtf/bUcQnWGy3W6i2EgFPjw4klgBT3uNc+LMbTOb4xeZA3xqdJ3yeQGbBTXK9u4NyV9pVS4rV96uUmzWqbTCGFlTI7ZPz+dkA8LXht6gAvTuxsW9Gr7/CLjb1mO/ulTdsL7G70eyd6H5IJEm5lw/eYqVcp2jZV36Pue3sSo9tfkGiyiEAjjtt3Ed+TLN0+tCgQHb5L9yemVMdT5pcIeyGgxirct/dHEe1B+6WTvRGKMFoljtsE0TJeeANN5vDDGXT5JG0E9o5CyuHGSpkrS2u0vQBd0xgtZDg03BsV5qlcnrHM4I4ii1PZPP/49AsMp+4sGzWqLS69f51r52YYmiiyvlhh8tgor/+dF3tyXl8EGJq21dJ+P3wZ7O2OJIdhdjWXJbSDvWXrHyeSeazbeVZ0Skq9DS40Kfn21BH+ZuYa/5d3/pqcaVHzPY4XB/n21BFM+fQFMwINUz9IEK/Q9H6GqR9kMwgUwkbXxh75M3oeyAhEwkrvOpBRxKo/xKn9jISA2s2gVB2S7P4MZFDRnlovN7kxuhwkFu1kIMsiUqYwtJG+Ctf1A6ahMZRL0/YDdE0+ArN/h/eXGo6edIVtuC2awZ3CY0px35Rz4IdoumRgJMf06Unclrdl/PblAp1gsVan6rqcHB66pyToBiErjQYLtTpp02Qin2OgB2n2LyKk0LscU3GnU3N/zmVKhV3zFpPNeu9boaUQnCgOIoXgWHGQoOOpdiCb53hhsG/Kvv2EIiaMVwijZfzwBm7w2dbvDG2CfOp3eNSAsD8ZmT201ymVWKN316r9xYEUZtfaEskDFvacWNYLxCroWtUzaSFPNDV0bYBYBWgyQxQn1hQCvW96L/2CLiVDuTTGto4gx+zPNdyolrlVq2z924sSMby0YfI108LWdQI/pFFusjK7RrvhYjpJOcm0TEzr6bq3/cbV9XXOLS7TDAIkgoJjM5LNkDIMrq6vc3F1jcVaHV1KzowMc3J4kOHMl4TpOyE7m49u5jLVmS/251ymlN+lJIboyE70fvmUQlByUhRthxeGx2n4HinDwJRaRz/q6YNAdjLtm2e/fefVm+xc7zMyQiDYFIDa3YAr6clvEO9B4vqLAClSexABjBMNFuWh7TOp81h5RF1KdSfXfDvjIpAEcR3Xv4DCAxSGNorF4T6dbe8hhcDS9S3p7lipvnErLpXXeHdptpOFUay0m7hhyEQmxzODI5ScFG7D5dq5W8xcWqBZa4GCWxcXMAyNwnDuDtuCX3ZcXd/gP35+gY8XF1EKnh8f5VeOHeVoaYC/vnqNCyurjGWzzFaqLNRqNH2fXz95/Emf9r5C4rmW2YPAXUSk3GQTs88CmUi1iOlmHRJoMo3og/aZUopQxURx8l9DSoIoIohidCnv4cI9DZAyRTHzBx1+3maTiwR650nXt7uiiQyacLowE4wT36G4d74VTwN0metaEReSrFWk2mjsr0AmUu2unW81mUbK2+cfxhsE4RxBtIxlHE5UgmWhT2faH3hhyEK5xq21MnEMC+Uqh0cG+O6zx3r+Wb8yfZTXxxK/JIXCjyIul9f5wczVrVJuKudw8pUjHHrmAHEUd47tdD+ln1717H7ADyN0TfL61BSWrrHaaPJnFy7xv37tFWqux2g2yz95+UXqnsefX7zMpbV1fv1Jn/Q+g0DD0Ap7EjTdVDXX2F9l5ChuEXWxDiUZhlxf1OgjFfM3M9f5xdIcVyvruJ2sa960eWZwhH9+9pWuTIn3I6K4ShSvEcUVdDmApg2iid4YRfeltASbi5azq5mgQhHEG0SqRWfK7f0p7TsIDC2HJrsPSqK4QRjXMbXdDCkfF5KUYKSaXbuc6zJzR0ZJqZAorhNEy2gy0/FZerq+f0PTKGVTiKQZi5VaAz+MiGNFIl/Tu+sZctIMObf9f2KlaAY+rSAg7pDBNV0jU0ijGRoL11aYu7JI4Cdl24kjI5x69WjPzudph2PoTORyvD59AEvTeGdmls+XV4iVIoxjHMNgqpAniGN+cOUaTf+XK2vcDYTQMLSBPS3qYVwlUi0M9osJbjKXhXGNWHVndKzLfJ8CGcWna8toQnCmNMI7izM8OzhKpGJqvrfVdfg0QSkf1/8MN7hIGK0ihI4iTLLvxkls49lHll/pW0YmyTikCVjb5ciYIK4Sxg1iFew7qwIvConiGEPT0ITsmU6IIYuJkm2XCOM6YVztyWf3AokIm98pC3aXkdFlHl3evuak/KiI4gp+ONfp5Hq6Sh9SJqWlrJNMakO5NFnH7uhL9DYoW2rWqXq3d4yRUszUq0RxdE+luVltsXB9mQu/uEq90kRFiiiIOPnKpqPu0xUw9gMpw2AonWIoncLUdGxdp+n7zFSq1D0fS9dR0PG34alcRPoNgY6pDe6JpB/EFaK40cez2huUSkRcw7jeVZlcCImhFfeUUe/+XGC13eSl4XGeHx5jpdXg7x87zWKjzvmNlafyuY2VT9P7KUG4gBAWujZIFK0RholxpG2c4VEFcfsWyBiycMei9SAo5RPGVcK4hqntZv39eLHutqj7HoNOmpxpIXtU1zW1IXTZfVotjGsEUaUnn90bxARRhShudd1+bcgBDHl7F5YQ5hJ/pYQwHHdJtts/qLc9Prm1yCe3FhHAWDHHoeFiXzx7fjBzlZ8tzGz9O8nEKIZT6XvaiBvVFvnBLC986wyLN1eJ/JDSeIE4ir+0KuhACEErCPhkcQnbMLiyvsHFlTX+6JNPuby2RtYaJ4pjBBDF6kuBvPtACB1LG9sTdy+IyoT7KJCJVUAQrxOpNt1ZLWhY2ghal+vbXmFIufU8B3GEpekoFPWnNCMDIV5wmYz9bXKp32KTO1tp/iGu/zm9IPz2LZAxtSEMWez6eD9axQ3nehbItMOAtXYTTUqyhoWjGzQDHy9K0uyGppHRTVbcJmEckTNtcoZFjMKPI4IoouK1WWzVCeI48ZlQCktPGOSGlKy5LfwoxNJ0iraDtof0mCGLGFoBIcyuun78eB0/2tlz53FDEeKG84RxrevXmNoghnZ7TAhh45hnMPQxBAYg7uDQPA1ouj75lM3rx6cZzqW5uVrGDyMipXpu7vZsaYS8uY0sLRLX9bF0ltFU9o5jh8YHSGcdausNvLZPda2OlPLLIGYbnhkdoekH/NWlKwRxzGA6ze8//yxRHJOzLdpByP/tRz9Bk4Kq63FiaH9tsvYDNjMyupZDYHQ8iB4ML1okiMuP4ey6Q6zatINbxF02LQihYWmjXW/U9wIhoGSnSBkJD8bWDf70+kXW2i0E+1WBZzdIpCwgpdMpISVq61KkkDJHL+gEfQtkLH3kjkVrN3jhEu1ghpz1/CN/dsVrs9RqMN+sUrQctLQkUoqP1xZwdB0/iohUzDOlUWq+y1KzThBHfHXsIH4UstiqM9eokjUt6r5HGMest1vMRzVG01lGnAwr7QZzjSoKxaCdpmDZXX8fCXfCxJBFTG0QL1xktyHqh6t40dIj35teQakQN5zdUyBj6cMY2zg+cdwiVi6ayKBrg09V63WSjlbMrldZrjYwNIkA1ustDE3rC9NnOlckbzk0A59m4OPoBoNOirHMvZm9wA+JwhgrZTJ9coKb5+eQmkTFCtFrsZunFJP5HExNYmiSIIqZzOc4NFCk5rrEwEK1xvmVVcI44pnREZ4dGXnSp7zvIIRAEzamHMTQCvjR6q6v8cIFgmj9MZxdd4hUm1ZwjShud3W8QMM2JvaUUe8WmpC8NnaAQSdF3rL52sQ0C406Kd1gPJ19oIDjfoYQOm3/HFHcQIoMsarhBueJ4zr19l8n5Tp9Gts48VDv38eMzEhn0eoujvTCRdrBjY6C7aO1ZTUCn6VmjVv1Cm4YUjAdgjjiw9V5jheGaIU+q+0mw6ksdd/jZr3MrXqFF4YmaIcBM/UKn5eX+crwAXQpqfse12ob+FGY1NWdNKtbgUwiAjidLezhDJNrM7QSjj6FFy6x2z0KonW8cIlY+R29lSc7oGMV0gpudLmzEkhhYmrDd2TpIlUnCOeJ4waO+QxSOoCxJxL0k8KmuFy17bJcrRPFinLTRaHQNdkX4SpNCMI4YrXVpB54WJq+JZhVsBxMTUtSzwqWZ9apbzSwUiYjU4NouoaK1ZeBzDZkLYtTw0OcGr5XTdoPQyZzOQ4WizQDn+lCgZFM+j7v8suOZCxZ+iiWNtplILOIH60Rq6Azlz3Z8RjFLRr+pS46bCHJLqSwtPE9cRy7hS4lr48nnYmxUnxv+ijXq2UMKRlJZZ9KZV+QaCKHF1zCC66gyxJhvAIopEjT9N4CIG29uf8CGUsbxdSGO+nG3UsnbjhPM7iCwkPwaN0rw04GNwq5Vttgvllj2Mkwls4SKzicG+gYw62w0KhS9tqstBu0Q3+r/pgzbZ4ZGOXMwAgzjQrXqut8ur7Mb0yfYMTJktIMThSGWW41mWtUQcHZwTH2SlO2tGFSxlGq7vu7KveGqo4freCHK5j6MGLPn9ZbKAKa/iX8LnZWUphY+kSHFHibIKeLHL66QcN7l1i1kDKNrg2RMs/289R7AkEiJ/7qkQM8e2AUIQTFtIMQ9G3X9P7yPD+eu8GV8jpjmSw130OXkueHxvi7R04xls6CgiiMuPHZDDcvzBOHEZlimtCPOP7CQaT+dO7oHjeWG00QcGp4CIVCit5pXnwR4egHsPUD1P1Pdz02iCv40TJBtI6pDfOkOxUj1aDuneuKt6PJNLY+gaEV+9K1tB0CMKTG0UKSxe4H7+5xQAqTtP11HLNjjyIEbFnuCOhsyg1t4qE/o2+BjBQmljaEY0zTDq7vSgiN8fHCZSru+2TNsxjaw7fmNQKPSqe7o+y2aIZJICWE4vzGMm4Ustyq40chQRwTxvEd1oxSCEyZ+LBIBIN2ml+dOkbFbzPTKGNqkrrv48UhrdAnVNFDEQFNfYS0cZTuGNsKP1qn6n3EgHwTqT25QCaKW7jhAm4431X3gRQpMuZpdJm/YzEQwsTQR0mZZ9FkASE05BMO0LrF5nVkHYu0ZSCEwNS1vjrUztSrmJrOf/n8V0gbJn4Ucb26wdvzN/nO1JHOiYHUJSdeOszIdMLpyA1kEEKQLWYeajEWiD34Aqptf57OiRfg3dk5DE3y26dPPelTeSpg61PY+oEuj1a44SIN/yJFp/REOxXDqNaZy+aIlbfr8YYskDFPogmn74Ht5vvrT2kAcxs6lnF0Vysb+QhdYH0LZISQmNowGeM4bjjThYeFwo9WWG/9DbY+iS6zD10+kUKSMy2OFgYZdjKMOLdTgIbUyJgWWdOiaDkIYDiVwQ1DHF1PUnhOhiCOMKTGgJ3ipJQM2mmuVNfIGBZSSAwpmUjnsDUdXUj0hzhXQxZwjGlMbQg/Wt01cxVE61Tcd8lZz6NrhT54r3aHINqg4X1GEG90ZS2hyzR568V7OFOKuCNX7hPFdYSQCGFzO6Tc/w9w2w/wwxApJMWMw0K5hiYlo4VkzPVysvOjiIJl88b49FbBVpeS79+6uhVIi07mYPzwMKXxIlEYYacsDMtAe9hsjJB03x6ZeIPFKtyD2uvjQSsIiOOYjJXspJu+T8u/Pzn14soqeXt/CbZ1C6XUlqfW9vFXb3ugwDZ1dK23PkGmPoxjHMCQRYK4xm7eeW44R837hIL9FZTQn9hc5kaLNP1LRKq7LipDK5E1n3ukRfeXDYn6c381g/qqd2zpo2SsM6y3fwzsHu360Tob7R9Tcr6FqQ2ji4erSRcsm5xpMZ0tYEgNKQQLzRq2ZvBMaZRDueLWQ7wlJLYtEBmwb3M0JjN56Ag3bXcYLloOw6lM4vX9kKUETaYwtVEy5klqXpsg3njg8UG8Qc17Hz/6+1j6cEdA7vFCqRg3WqTivtOlom8iHpWzXkC/S7VXKY8gWqEVnEeXRUCS9I3FgHwKwhgoN9us1ZpIIcjYJhcXVnAMg5F8Zg9ZjO6QMgxqXsxis44hJaFS1DyPjG5S9z1WW00MTSNvJoGL2/ZpVFs0Ki1SWZtUzsFOJWNmL4tYwmPovjYfK59YufsukFmpN2gFAadHhgFYqNW4sraBJu9dRudrNcw+WU1EcUwcJ8GGrifEcKUgjCKklFtB6mapWwqBlIK4E6AkEFu/EyLhUwghQCX8nkbLJ2UbmIaOJgUKWK+2EAIGc+nO978Z8CRlUiEEURwTdhShNSmSTrcuxoouM1j6BCnjOHX/HLF6MHHWCxdo+J8SxjUM8fg91jbl8lvBNer+J129Jmm7HiZnnUWTT2eQ+0VFfwMZbZSsdRZNZomiNrtF6YoAL1xho/0TdJkjZz//0J+9WV/cfAQzhsnzg2MULPuOSetRlsuHycLc8x4yR9H5Gu1wdtdAJlY+XrhC1fsAQyuQMh6/J1GkWrSDG5Tdn3fUmB+MJOt0EEc/cI+AVKzaCGFg60eJaRHHza4NKPcLbEOnHYSUGy0mSjnKjTZxip4HMZAsgB+szPOLpVmG0xnqvkfFdYlR/JtLn5LSdaZzRf7xmRdo1VpcfO8alz64QX4wi6ZJpk6O8+xXT+w50SWF2WmP7/I8VZsobqLL7O4HP0Z8uLDIUr2+Fcj89OYM//qjTxhMp++5J7OVKsPp/pB7Gy2PatMlCCPGB/PYpk7bD1hYq5FPWxiaRhTHtLwApRQZxyKXtmm2fdwgTAKgTht9xrHQNEnb87EMgyiOmVut8Pa5GzxzaJTD4yVKuTR+GOIHIaauISXUW24iExDFiVFhPo2ha7Rcn7nVGkJAPm1TyDg4XZqNWtooBec1WsHVXQOZSLVoh3OU3Xco2q9j6Y+/IyyMGzT9i9Tcc10db2qDOMY0lj7ecb/+EvsFff02pLAwtWFy9nPU3Pfxo91UfhWKgLL7NqY2hKWPYGqjD1fXv8sp1NEMThSGyBjmne/3CCtOL1KzusxQsL/CRvvHuOHMLloGili5bLT+Flsbx9Yn+k44uxs172Mq7nsdW4LdxaMcY4q89RJS2veUCjVZQJcDBGKBOGqgyQKGPsbTUFLaRD5lM5RN0fR8Pp1ZppB2mBzI049rOFYcJFIKLwpJ6QZ+HBHGibGcpelIIbayhO2GhxCCdM4m9ANabkCr3t4qOewFmkyj7UW5NdrAi5ax9NG9XmJfcXZshKOlga1/CyEYyWb5/eeeRburk+s/nb9IxuotXyuMIpY26iys1Wh7AYWMQxwr5lerLJeTzrfrC+t4QUgUxfhhRNo2KeVT5FI282tV0rZJGMWs15I2/7NHRkk7Flfn1zgwXMAydFpuwEq5gTd5W/FZCMHSRh1D19B1ycVbK9imjh9E1Fseb5w9RNsLWKs2abZ95lYrHD8wxEC2+w5CUxuiYL/MWuuvCIP6LpsSRRCtsdr8c2x9vGNz8PiyMoqQsvtT6t5nXXYrQdo8ScY8s+8yjV+iz4GMEBJD5ik5X8cL5jsdLruxIRWt4AZl92eY2iCl1LfR5F7cVe8PQ9MYdPZf+6QUNo4xTdY8jRveohVcf+DxipCGf56q9z6OcZCsdbrzm/4u/rEKEo5O+x3q3sd0E8RIYZMyjpG3X0Jwb5pe63QpmfpkQvzVRrC0qafKpkCTEqWg6XrU2h7jxRymodEPns/RwgBFy6YR+DSDAFNK0oZJxry94Nq6gQB0UyOVc8gU0lRWqliOiZ22Hup0dJlFk90/O4km1C1y1nN7/7A+4mjpTp+y8VyWr04f4FePH73Hsfz6ehlL721pKQhjZpYrNF2ftG2SSyXZlOVyg/m1GscmB5ldqbBSaeAFIQLBWCmL2dKoNV1uLG5wbHIIpWBxvUYcKw6OFdE0yeJ6jYFcCtPQ0TUtEUt0TGxT3yodtbwALQhptH1uLG1waHSAOFasVBs02x6Nts96tUml4bJabTI9mrx3t9BlBkc/TNZ8hjCq4kWLDzw+jJtU3Q/JWR9gaqVtGeb+zmVR3MaLFtlo/S3N4Cq7z2UCTThkrWfJmCf7em5f4uHQ9/yYJrMMON+k3H6HZhcpxwSKmvshsfKw9HHS5nGMTlfLk8J2aehekuSSzJFO3n6VdjhLK7jJbg9WpJpU3HfRZY6UcbAj89+/e6NU4lBecd+l4v2Cdnirq9dZ2ggZ8yQZ8wT3m5yUitBlAd16BdCA8A5uwNPQ7tpwPWY3Klxf3mAgm2Juo0raMjk0PNDz6bhoOYRxzFq7xUqrQda0yFk245ncVkZmE7lihmq2jlLguwETx8YYOTCIfAg+V+KRlU3M3nbpPICEyNkMrnaO1fbt93h8cJDhTPq+ityj2cw9wc2jIo4Va9UmYwNZDo4NkEvZGLqG1ykXHRwdoNp0iZSi0fKwDI3p0QGkEFuBy1AhjWUYbNRblOstwijG9UOCMEYpcEydfNoibZsUsylSnaySEGAbGrGCIIxQMUwOFbBMHTcICWOFYxnommR2pcJAziGftu/JVD0IQkh0maHkfAsvXMCLlnnwXBYRqTrrrR+iyxyWNo4UVl/Hi1IRfrRCuf02Ve99/C5ERgUGtj5J1jxDyjjYt3P7Eg+PvgcyAg1dFshbL+OGC9S8D7p6XYxP07/M9fL/g7Hs71G0v4pjdNve1x8oog67vvdBQ9Y8Q8u6Ts37ADdcYnfW/ywb7Z9gaAUGU7+GrY/1/Jw2kZCMP2K+9v+lFVzr+nUDzjfJWmfZaYcVRMvEcR0hLAx9HC+4QqzapMzn6cc97gdafkDaMpkeKuAGIW0/TMiSfeg+/mRtifeX57le2WA4labue7y3LDldGuabk4fuyDiuzm/QbriMHx7mxW+dwcnaWM7DlUoEOrrMY2rDHfHGBwfaSWvtBdrhbMeHZ38SI0ezGYbS6ftWl7995HDPeU6GLjk+OcTluVUuz60xlE/z8skDDObT1Fsef/mLi6xVmkRxjGncNTULEFJw/uYKYRSxVm0yOVzg+sI65UabtUqTk1MJ90cpiKKYc9cWiZXiwFCe9VqLW0tl6m2PwXyaIIyS99x2jbFS+GFE0/UZyKWS94kVutb9jZDCJG+/TM37mGZwtStblWZwhfXWX6NJh6HUr+3Jt2mv8KIlyu5Pmav9q64tX3SZZTj9d3CMQzxNZe9fJvQ/kNnKOLyEFy3SCq52ya9QRKpJ07/McuNP8MI58vYrZM2z6DLTcU7uH5RK+Dp+uIIbztMOZ4niJlnrGfL2Sz3/PE1myVrPUkp9l6XGv9tVnyVWHu3gFiuNP0OpkIL9BmnzWM+VMlvBTaruu6y1fkgruNoxVnswNJHCNqYp2K/i6NPc/fBvdgx4wQ28MAmMDG2EMFpFk3kwX+jZ+fcbOcci51gsS42m22Igk2Ig4/SF7Hu9skHD9/j21GHypo0bhczUKry3NMdLwxN3BDK6qdOstVlfLJPK2LRqLbLFDKXx7m1DNiGEwNAGcPRp/HB1V/FGhUc7uM5S448Zzfwujpjq+/P6MLD0nc+pmOp9e62uScYHcwgpaLY9UpaBbeqMFDOYhka16XJguIAUIuGyaJJCZyxZps7Sep2hQpqBbAo/jCjlU3h+SNP1CaOY0YEspqFTyDq8+dwhHNMgl7KQUpK2TZ4/NoEXhDiWQRjGDBcyaJrk9MERSrkUV+fXCMOIN589SLXp0vYDak2XgdxeAoukS7FgfwUvWmal+afsRieIVZuGfxGaAqVC8var2PoksodjJlYBreAaG+2fsNH6MW44z26bRQBdFshaZyg4r+8L8b4vcX88ttklZRwmb79Cw79A1X2/S4KVQuFT897HixZxwzkCZx1bn0rco7VC0qIt5CPpECgVo4iI4iaRaiX/jRv4cZl2cItWcCVRHVYhUuh9CWSEEKSMgww636HufUrTv7yrtkGkGtT9T1FEhHGDKG7iGAfQtUKSon3Ie5JY2tfwwiUq7rtstH9M1X2vK5drgYapjzKY+hXS5kkMrbDDkQqIUMpDKZ+AGNDRZO6JlhD3ipxjU8qmqTRdQDE1WGS0kO1LenzdbWFqGr968PjWz95fnuf7t67iR3eWfFJZG93QaFbbrM5voOkaIB4qkAEwtWHSxglq3kddOZT70TKrzb/A0kZRdtwhptv7qsy00WrR8H0GU2ksXaPiuizU6tRcbytbk7N7R6aXUpJL22RTiVaSUklGJGWb5NI2sdpZQdg2DS7PrnJobICDowNbx2y1aG8rAZm6xkvHJ4njpLV683OfOzp+3/PKp5OMmRAC09Q5NDbA9cUNZKcdey9IzkuQsZ4hiMvU/U/xwoVdxeaCeJ2q+wFR3CaMG+Stlzp+RnnEI+jMRLFLGJdph3NU3J+x3vpREjR15XKtkzIOM+B8k5RxpC8mkV+iN3hsgYwQOhnzDGOZ36cd3MAN3a4Wxk144Twr4QKrzb8mb79IwX6NvP0qafMYGg63Bbu6GfBq678JJyMgiuu0gms0O0FL079MM7jWyYwkg97WJnbdjT4KdJkjbZ5gJP33WFR/RMP/vKvXNfzztINZKu4vGM38DgXnNSxtjOSeCO6+J3dPggluK7IG0Tp17xxLjX9Pzftk17bw7dBkmoxxgonsP0Tbof02+XydlPUCtnGiI6onkDLTSSs/PYJ4SinGClnGCsm1qm0/h97zfJJ0f3yH1sj9dD50U2fs8DB2+nZZx0o//KJsa6NkrTMsNQxgd5fgWPm44QKz1f8BP1pjJP33sI0DoO59TrffozvH5NZPd/h78j4PK5x5bnGZCyur/PqJYwxnM5xbXOJff3SOT5eW+bunT/KbJ4/z/HjvS7bJ5d6plpyYL+48VmxD5/TUMMWsc0fQstP4EkKg7aEkBHB0osTcis5HV+YZyKUZHcgyVLi9eKuO9o3qRGCio29zP5jaAFnruWQuq/8RXrSw6+fHqk3N+4BWcJ28/RIj6d8ib7/aMWd8uLnMj5apuO+w3PhT6v5nXXI0E+haon81nP6tvpa7usH9nwu4f7Zru7p2l+9PkiVPhGvv953uMJYET0zMcDsea75Xl1my1mnGs/+I5eZ/6nqhvo0kQ9PwP8cNZ1lrfR9DFrH0kY630xC6zHW6nOytdHaiHusRExArjyhuEsRVgmiDIC4TRGXCuEoUt4hVu5OVaXfKKP0LXO4HTWYopb6FFy0RK49WcLWr10WqRdO/xGz1f2Cl+RekjMOkzRM4+nTiRC6LHV0PedfrGgRRGT9a6ZA0L9Pyr9EO5/Cj1a4sCDYh0Ck6bzKa+V00mb1vp9J2KCK88CYt/0M0WUQIE1MbJ2O/ytMQxAAsVup4QYhtGozkMlyYX0HXJEdGSw9cmB4Gk5kcn6wu8d988DZj6cRrqeq5PDs4Qtq4k/9imDq5gaR00Ki2yBbSZAoP37Wna3lSxmFS5mFa/tWuM6p+tMZy4z9R986RtZ4hbZ4iZRzE1EYTAjFJpmj7a5QKiZTbeRbbRKpJFDc7Wcc6YVwjjKsIdLLWcxSd1x/qmhbqNa6sr/P79rOsNBrMVqrYhs7//mtf5bOlZT5aWOxLIPMwMDvE36Qjrj9I2yYHRwcYLmYwdO0e/ZjZy4t8/s5lLn94nQPHxznz+jFOvHxkx/eztFGG0r+GG85Tbv8UL5rv6jyiuE7V/QA3mMHS/z1p4zgp8yiOPo2pD2HIQiew2D6XRYRxkyDewAuXcMNbNPzLnU1z0jHbjQVBAoFAYzj9dxhMf6/Tsfek56OIWCXrVzd/ksaRa10FbrHy8MIFVpp/hqGVkMK67x9NWIg7fmbe5/l9MnisgYwUBoZWYiD1DcK4hlIRzeDiHt9FdSayGjCHwMTQckn3i8yhiRRS2kiMrRKFUtGWbLoiJI7dTrDSSCZH1dxFv+XxQQoDSx+h5HwTpQJi5eKFS11YAUREqkE7bOBFC7SDmzT9i9tKcBmkcJI0rZBsXzCS+1DFj9bxwgX8aK1rbYVNCGFSsF+j5HybrPlMV+3ySvmd0lJEFFdRKkAKo7M72A9x/s5QShErxfXlDZarDXRNMpRLM7deZayY5ehoafc32SNOl4aRQnKjVqYVBEghmM4VOFEcJG/dSaj12wHLM2ss3VzFsAyWZ9YZmihy+NnEWXevmaJN9/KS803CuE57F5mATSh8vGieIN7ADRep++extGF0Wej41Rhb7fZJtjOxrYhVgMIn6qgEx8oljj0i1SZWLaK4haEVO7YXDxfIuEFIy/exDJ1rCxs0fJ9nRoZ54+AUny0ts9bsRrn68UBKSfohydrdQtc0dEfb8XNSWZv8YBYQVNZq1MsPniM0aWOLSYbSvw7EbLTfwo+Wdz0PRUgYlwnjCu1wlnZwC8v/DFMbRJf5LV0jgb5lQBgTEsdtQtUgjMr40RputEAQbexxbhcYskjBeY0B5xukjSM95ercc60qwo/WaYe38MNlYkKUCpK5f9vfk/UrSJ4NOr/v/FvR+fvWz8MtakA3wZtSPl60zGrzr9BkCiH0zvqZ/JEkystSGB0OZufvQkegd37e+fddx1v6JDnr2b7dv008dgaeFCYp4zCl1LdRKKJmEy9afmhFV4WPH611Ibb3dCHb+fJDVafcfgsv3N2LaROx8vCihfumc5MBJ7d8jvaSfrw/BJrM4OgHGcn8NgXrlXs8lR4EKVMY2ihhvIEQ2p4UZJ80lFI0PZ+NRosojmm6yfdj6TpaHxywjxRKFO0UI+kMC40aectmOlvgYP7e+91uuizdWOXm+TmmTo5z68IC7YbLoWcOPHS5S5c5Sqlv0/Av4ofLewp2Y9WmHV6nHd4ZACUKqYm+dkxEkgHtbkzaarIrx+KdsCnLX2m3+Wx5BSkEX52eYjidRsq980MeFoEX0Kq71DaSa1Fx0kqtGxoDYwXslEUcxTQqTZrVNoEfIITASlnkBjKYtoHb8qhtNMjkU2SLGZRSrM1vIDVJOp/CckzWFsqEfohu6nhtnyiISGVtSmNFhBTU1hs0Kk3CICSOFaZtkM6lKAzlABicGEAzNGrrja1z3Q0CnaL9lSQjrlw22m8RxY0uaQWqM25u0g5v3vPOyVwmOhvViF7MZYYskrWeYyzzDzpGt7lHfM8HQxHhhnOst35Izfuok1FxiWKvE8AnGZZkI9sfR9qEY1nt2qphOwQaQphJYNnJ2shO5kaTKQbsN7+YgcwmMuZpNJnFkEXm6v8zXjhPv76opxOSjHWGKW0QlGKj/XbXqdkHIYnae3B6HQh00sZJDuT/KXnrRXTZvTmYLgeJZQuPa0RxGUs/hGUcpHuDwicLTUq+cvQAZ6dGkVJSTCdk1m68aR4WhpRkDYuC5ZA1zR07b6IwIjeY4cTLhznx8hEUYDvmI7WFS2GTNo5TsF/Dj1aoeR899HVsYjPT+CSefEvXCaKYH127wUfzi7w4McbJ4aGO55B6KEf7h0F5pcaFX1zl7f/wHkITW0FGfijH7/yL73Hk7DStusuHf/M55966wPLMGrqhc+jMJG/89iuMHRzi5oU53v6TD3jhm6d57TdeQCnFD/7129gZm5e+/QwHjo/xwz/8KeuLFQbHi9y6OE91rc6Z14/x9//Fr2E6Bp/8+Dzv/eAc64sVAi9g7NAwz3/jFN/+/Tce8Qo1is6bGLJIFLeoeh8RxuVHfM+EZtDbucwgb7/GeO73yVsvIOhvBgySRpMg3qDpX6bmbQYSD+KE7S8oIpRqd0pY2yeWpFvZ1u9PMO81nlggI4SGpY1QSn0LTaZZb/2AqvsBoao9qVPaV0gM4HRMbYjx3D/C0sfZaP9tTxaP3kCiyywl59uUUt8lZz2PJjNdES+ViohVeyvtaRnHMfRJpEghRXafF5USbGY1MraJYxqdFmVJEMXESrEHQdSu8eHKAh8uL3C9ukHWtPCixLH99MAwXx2fpuTcJiQapoHX8rl5fo6VuQ1q63WkJmlU2xw4PsbI1CCZwt4IjMk1aww4XyNWLkFUwQ1nu3JA3494dnSEdhBweXWdFyfGeXkymXQX63WG0mkG+tCCfT8opWg3XOauLvE7/5tfZeLICKtz6/z1H/6MtfkyhcEc1bU6596+yKlXj/Ldf/gmrYbLT//j+8xeWsC0DeJIJZmU6HYWKQwioiDaIoqGQUQcxeRKWX7zn30bAWQHMhhWItlw8tWjjBwcIvQTi4T3vn+OSx/c4Fu/91Xg4YjrW69RBinjCFOF/5LV5n9mo/02reDyI9+7XiDRScoxnPltBpyvkzFObmV7+o+ElLtZVn26cXcAFoF6PNf0RMUdNOlgiwkGnK+jiRSmNkzN+wQ3nO0YEu7vaLTfEEIghLmljKvLDLrM0/DPE0RlFLu3wfYDmsjiGFPkrBcodUTvdm6zvhdJq3uZIFohjDeI4xa6ViSIllDKxdQn+nfyPUKsFGEUs9m04YchfgiL5TqWoTM1WOj5Z14urzHXqHIoV6RoO7TCgNVWk58uzPDs4OgdgYzlmJRGC/hukoErDCZdVbqpY9oGco8dLdth6xMUna8SqSZrzb/uPK9741TtBxzI59GEIGfZHBooMFUooAmBqem8cmDiDuuHfkMIgWEbTJ2c4PgLB8kU0mjaz2k1XMorVVbnNpi5OE9ptEA6l6LddKlvNKitN2jVdiB0qq3/24KTsRmdHuTY8wcx7aSMq5QijpNgqlFuJn5cUlBZqSX+XHHy70e9Pl3LkpVnQSl0maPiDtDwzxPFzT11sPYShkysEXL2Cwymvttps95fZqdfYnc8cZUqITQsfZjB9HdJm8exW3/Nhvs2bjBDGNf3SUCjoQkbQys+gTa8zs7fPIGlDZMyjrLU+Pc0/M/xtngKjyPqFUhhocssKeMoRftNhtO/jqkPI8VeJ3xFHLcJomW88AZhtIpjnMIPZ9BkkbT9WucT929mJopiGm0PqQmark+97aFJySe3FhnKpfsSyGy4bQqWwz979uWt9uv3l+f5f334U7y7dGSEFAxPDTJyaAjLNnu6uxRCI20cR88UQSnK7k9pBdc7fJWnZ1eZsy1y9hAnh4fu+PloNsNo9vFqhpi2wdDEAHbKRGoSqUnsjI2KFe2ml/BjKi0uvX+dlZl1AFL5FKmcg9QEQgri6HZ7tFIQ+AFGcOcUn845DE2WkNtShipWtOttzr97lasf3cDO2EhNsnhjmVTWSQIbJXrQnJK0nOfs57H0URzjIMuNf08ruEkQrT/GLlGJJmw0mSNrPksp9Q2G0r/eIZ8/8SXxSzwE9s23JjBwjGnGc/+IUupbVNx3WW//mKr7QYd1/qQmSIEh82TMUxSdr5Oxzjyh80g8b3LWC6SMw1Tc91hv/ZCN9k86wV5/dzRSWKSN4wymfoWC8xop40hHen7v7aACE8s4BEIiZRoVu6Ssl2h6vyCOmyTXsr9F8fwoYrnWwA8j5jeqLJbr5FIWN5Y30PtRVwIyhkmo3fkcmFKjZKfQ5Z33a21+g9pGA9M2mD41id7ztl2JpQ0xmf8nZKzTrDW/z1rrr5+IZMEXAh0LgvshlbEZmixRGMnzK3/wJs99/XTC44liTNsg9EMWb67gtjx8LwCVcKTWF8vEdxOWN20Jtn1UGETMXFxgbWGD4akSv/FPv43b9PBaHos3upPx3ytMbYgB5xvkrOfZaP2I9faPEkG8x7Bx1WWOjHGSocxvkLNewNEPIIXNfmgj/hIPh/0TyIgOCx0dW59iwHFIGccYSv1aom0SXKcdzOBFix1l0f4MdoGBLvNY+hi2Pk7KOISjH8TWxzH1UUyt9221XZ+bkEgsTG2Qgv0Ktj5G0XmTpn+ehn+JdnADP16jN/cm2bU4xiHSxjHS5nFSxlEcY7qj1/PwO9YkO2BgaKMo5eNGF2i4PwWiTllp/5N9TV1jrJDFCyPSlsFYIQcohrJphvP9281/vr7C//P9t7ZKS0vNBlXP5c+uX6Rg2Yyks3xv+ih22mL+2jJLt1bx2j66oZMfzDIyNdiT80i+Q4kusuSs5zFkiZz9EnXvHA3/Am4425FI6D0kFrpWIG0cx9JH+/IZ+wl2xiadT/Hc108xc3GR+avLSCmIY8UL3zrDwTOTDI4XyRXTfP6zyyzfXMV0TLy2j5O+j8/VXckVqUmKI3kEcOPzOb7/r36CbunU1hvkB3MopQiCkOvnZvjkJxe4du4WXtNjZWaN6nqDs2+eID+Uw7S67zgUQkMjhdQsis43cIxDDDjfpOFfoOlfwg1nCR6ZELx5uRqazJAyjpA2jnfmsiM4xhSGHECTj4cL9SX6h30TyGwiUbh0cOQUtj5BrJ6jFZyiHd6gHczQDucIo0qn7NTo2Ap4nfY+v9NzH28jT4mOToWGEBKBTPrkhYkU9tYfTThoMo0hC5jaIJY+hqWPk9IPYuljaPLJKjtuYpNwaekjmNogGfM0TfMYGT8J9txwniCuEMY1orjR0d1wO/cm7NyXTZ0WDSG0zr2wkvsg0+gih6EVMLSkfpw2jpEyDmNqQz21D9BkBl0bwuhoRuhyBFOf3NIVeRBSxjFKzjdJG8cefKCQOPoUVo95N4amUUg71Nse9bZLEIYYukbGtrYch++ExNBKFJzXuq7BO/rUHfd72ElTtGzKbhuFotVxTT6cH8CPIzbcNik9WUzsjI2dskBBfaOJpsuHNo3cGclyaGpDGHKAtHmMlHGItH+SdnADL1wkjKtbWk2J4KTf0b0IYdtYTFqw5VYLvhTGtnHpoEkHKVLoMt0xsCzhGAdJGUd7fE3dIWOeYCj9G0Rxfddjs9ZzDwy47LTF6PQQZ988uUXATuccnn3jBGOHhsgW0lgpkxe+eYab5+eortU73XEJd0U3dHIDWZ756nFWZtcJgwhN13j2jZOMHx4mnU+BEBx+dgopBamsc0epUWqSwnCeI89No5s6QgoMU+f4i4fID2aRUqLiKPF8ckwmjo4ShzF2xkKIh1ev3vThc4xJLH2UjHmKtHGcpnklGT/RIkFUTeYy1SSO20TK7XReRtt4NdvnMuuO+VyXOQxZxNSHtgIZR5/C1HsT0N+NWCmiKOaDz2cpZB1OHh7Z5R5oWNo4RecNLG24L+f0pCCERtZ6/vF8ltpZ+3jfQamYWHm44TxuOEs7nMMLF7cUesO4sjVZbooJyc7EKIS5NTlqMtUZ4CUMrYSplTDlELY+ga2PoWs5nsY0Y6wCwqhMM7hGO7hOO5zFD1fw4/XOZLDZKaQQyC2FxuReDGDqg9j6BI5+CMc4iK1PPJLPyS8Dlip1Prg+z6WFVQ4PD6BJwfRQkbPTvVeEnalV2HAfLNKWMSyOFAbw2j5rC2XW5stbMvIDowUmjiYTa787MqK4hRct0Qqu0gpuJUKL4TJBXCVUdaK43VmQFAKxJaalCRtNJAuQLnMYWhFTG8TURjoK3uOY2mCiKrqPfJu+RG8RxW2CaJ1mcHWbOu8KfrROqBrEsUvM5lymb81lhixiaiUMLZnPU8ZhHCNRN0+C5f6OmTCKcb2A/9N/8yecPjLKv/iHX+/r532JBPsuI/NgJIRTxziApY+SU893dnZRp589EdRS27yUNl+XRO2bxWHZyc7onR2gtk3N8Cm7Jdsg0NG1AXIyQ8Y81VF7DDvCd/G2bMwmNh9s2QlYtI5io9GRn35678XjghCCoVwa29B5/uA4hiYx9f7we0bTWYZSD7YZ2NSw8d2AuSuLXHz/OoXBxMQyjmMmjow8lhhdChtbn8TUhshZL2+pkyrVUe+9Zyxuf0Y3n08NNnfa6FvBzm7WF1/i6YcUFqY+gq4VyFnPbZvLNuf47ePn9tjZzMwk81lnHhMGT0O5+ks8PJ6qlWrTWVWQRN9fYtMYrHNfOqlahI7G/iiFfVHR8gLmN6rMrFdYryfKvh/dXEAKwcRAjhPjQ7u/yR5hasnC3g3clocQgkwuRRRE+F6A2/KSDMhjyGQkgYj5EB1tX+JLbI4f2ZXVyZd4clAqQikXFZeRcgBxFwUjjmvE0TKafgAh7sPX6hGeqkDmS9yJxPPmCo5x/A7ehRvcQAiz57yQL3EbSin8MJFyt43kMWp5idKoGzx5gThd10jnUuQG0pRXa5i2ie1YT2PF9Et8iacOQkC95XF9bo2NaoswjHAsk1IhzehgFiEFK+t1bs5vcPbEOGkn2ZhHcczVW6uYhk42bdH2AvwgotX28cOIXNoGpWh7AUEYMTU2QC5jY5k6URxTrrZYLTeoN1zixKScYi7FYDFDMecQx4qltRrL63UGixmq9TYt10cIwUA+xUgpS/Z+BPH7IAliWqi4ShxXEDKDILX1c4FGHK0SBOeT34l8J6va+83Nl4HMLtg0B1QdyXKlkp8lWlPqjuQmolO8EmKLBCe3Wd33eifsBbMsVP9bDhT+K3TzJJup1o3WX6LLAYazv9fTz7sftu6PUsTx5r1Rt6XDt90j6GjDdLomNu+TFIkOxmZZ5GngPqRtkzMHRjk5MYxSnYKlFARRnGi8dG7Ak7qWgdECjWqL+WvQrrscPjvF8FQJ2QcPqH5j815uPnvxNq2U288ibB9pdz+PbI6zzjP5KCTVLzLU1rOsiNXtf6vbB9x3zkNskncFstNKvnm/fxkhhWB+qcJfvnWBD87PUG96jJSyvPrsNL/97bOYhsa7527x3//xz/h//9f/BYcPJIFMGMb827/8iMFCmtNHR5lbrrK60WBuqUy14XJ0agghYGmtRrXW4vd/82WePznJ0ECGIIw4f3WRH79/lUs3VgjCCCklZ4+P8/WXj/L684fwg5C3P7zGX759gW+8fIwL15eYXSojBLxw8gC//vXTnD4y2uX35qHiDeJ4AyHSgIZSMUq5ROEMQlhE0RJheA1Nn0DKNkKkkTIhNfdybHwZyOyCZttnZaPOjYUNltZqrJYblOtt6k0XLwgTSfo4xtQ1TEMn7ZjkMw6lfIqxwTwHRgpMjRZJpyz0R1BT7R6KMK72Jeq9Hxotj5Vyg/nlKkvrNdarTSr1NtWGi+snu4kwioniGEOXmLqOZerk0jaFrMNgIc3IQJYDo0VGSlkyKeupShosVxrU2i6GpnFwuMinM4sYmsYzB0Z50nP4yNQguYEMvhtgp0zs9NNbjo2VYq3cZGGtysJqleWNOpVaMs4aLRc/iLcyZAIwdA3L1El1nsdC1makmGVsKMf4cIHBfBrL/HL6uxttL2Bpvc7M4gbLG3XWyk3WKk1aro/rh/hhSBBEaJrE0DRMU8M2DXKZ5Hku5lIMFzKMD+UZKWUoZH85S9xxrIhixXApw7/8J9+l7QX84tOb/PyTm7x8ZorRoe7NKF3P58XTkwwNZPn//Ief8+aLR3jjhcOUay2u3FplcqTA0EAGXUqmxgb4ne8+R6wUlqGzUW3xzsc3+PF7V3jtuYMAtL2QSq2NH4T8znfPkklZzK9U+f/95YccnRpkYjhPIdfN95Z0A6u4TRB+iml/DSgTBue3gpg4XgUiomiZMLiEEA6282uARS+1wnryJCsVE0WzRPEqIDGNM9BxWd7MEvj+5wThBcJoDgBDO4hhnMIwTj7y5398aZ6LN5cp13bu6BgeyPK9106Ssg20BwiWxUrRaHrMrVS4Mb/O/EqVpY06a+UGtYZLveXSdANcLyCMY6IoRimFpkl0TcMyNBzbJNOZQAdyKQaLaSaGCkyNFZkaKTA6mBgrPsxCp1RMEK3QCi5Td3+BG9xitfHHmNpwZ0caEkTr2Pqhvb95F2h7AeuVJrPLZeZXqqxs1FmrNCnX29QabRptn5brb6U+oygmipNdnZQSXUp0XeJYBinbJJMyyaVtBnJpBotpRkpZJoeT4G8gl8LegzbFXhHHMV4Q8aP3rnBrceOBxw4VM7x48gDjQ7k7zilWikrTpR0EDObSLJTrZOzug8ha0+XmwgY/++QG3TQQWqZOKZ/mmy8fJZ/ZWf+isloj9CMMS2dwosjSjVVadZfhAyWEvP9OeWm9xs/P3WS92sJ/QHnsyIFBTkwPMz020N1FPgTiWFFvuSyu1ZhdrjC/UmGt3GSj1tzaSLTaAW0vwPUDwkgRRRFxpwsqeR4llqnjWAaObSTBc8ahkEsxMpBhbDDP5Eieg+MlHMvom5BhL1Cpt/nrdy9RqbcJwvuIX4qOiekz0zx3vPuSchTFtDyfG/MbzCyVmV+tsrJeZ73apNZ0abQ8Gi0fLwgJw9ubEimSe7wZ0Dh28jynbJNcyqKQdSjkkvlveCDL2GCOkVKWwUIGbYfx90WCAkrFNKcOj3J0eogoipldKvPBZ7NU6m2K+e4DvJRtMjaUZ3w4TxjFDBbTHJosYa8anLs0j+sFW58ZhDFLa3Wq9TaaLmm1fG4tbKBU8kwpknlP1yUnDg1z/OAwubRN2rGQUtJoeTTbfpeBjI6QWYTMEcdVlPJBBUTRPKb1WmImGa8DEk0OE6kIcInjClKW2HeBDCj84DN8/xOktDH0Y1vdP0oFRPEqrvfXtN2/IQyvAQLDOIVjfxcph5Ay/0jdQp9fW+RPf/IZNxZ2XoxOHhzmxZOTjA7mcO4zYcWxouX6rJYb3Jhf59Nri3x8aZ7ZpTL1lrfrOYRhTBgmrXfVhnvH7zQpmBwpcurQMM8cHefM4TGGBzJk0xaWsdfrVkSqhR8u4Ia3iFWLdnAVP1omYfOHOMYJbOPgHt93Z4RRjOsHbFRbLKxWuTa3xvnrS1y6tcpauUHb687zKYqSwM8LkkwX3OnPY5s6I6UsJ6aHeebIGIcmS0wM5Snl0xiGhtbjsohSEEURP/7gKm99dI0o3jmQOD41hGObFHPOHYFM2jLRpKTe9phbrxLFMaamdT1Rb1RbfHhxlv/pT9/tysnXsQwOjQ9w9tg42bR9j9O2UgoULF5fobpex3RMhg+UuPbJLbLFNEOTAzu2oC6t1fh3P/yE2eXKA7/T33jzNIWs05dAxg9CGi2PtUqTW0tlLt1c5sKNZS7fWqHZ9h/4HW1CoYjDiCCMaHsBlfq9XkT5rMP0aJGTB0d47vg440N5hgeyFLI2mpT7bqGtNtr8p598xq3FDVzv/kGmrklSlsEzR8YeuFnbDJirjTYrGw1mlyt8eHGOz68tMrtcodHFfBcpRRRHEES0Cag13XuO0TVJ2jE5OD7AkclBTh0a4YWTBxgqprHNLz6Jd6iY4cjUIJoU6FqSrU85ZrIJju5UXFZqc2OriOP4jk1NJm2Rdkx0XcO2DAq5FPmsk3BvopioUwp0vYAL15eYWdyg5QYYuiSKFbWGi2MbiZt7520dy+DU4VHyGQdNk6Qck2zaJlbg+d1x/JI1O42QBe7nfC2EjRAW4COEiRBWhz/Tew5hjwKZGN//BNf7WzRtmGxGY/PC4riG6/4VrfafEYa3kLIISHz/E5Ty0LRpbOt1hMj35lR2gBdEzK9WKeQcnPvs8oMw4sbCOn/6k89459xNVsuNnlnER7Hi1uIGM4sbvP3xDaZHi/zud5/npVOTjJa6TzEmkNj6IezMQdLWWRZr/x1juX+OYxwBNv2JJL1sN3S9gJuLG/zlzy7w7me3mF+pEkdxz7WVXT/k1mKZmcUyP/zFZY5MDvLV5w7x62+cZqSUxbF6G8hIKUg7FvmMQ8o2Hxiwtv2QhdUqfnDnbjifshkrZmn6Pu9fn+P42BBHRrpf4Mu1Fgur3Svgtr2A5fU6TdcnimLk3a3eHXn6G5/Ncv3zWaIwIlNIE0cxJ18+fIfHzt1wvZD51SreLmTloUKGUv7BbeAPi2rD5eNLc/zVO5f45PIc9ZbXs+fwjs+ptzlXb/PZtUX+499+ysunD/DdV4/ztRcOk0nZT7ws+DAIo5i2H+IGIWntwVlBpeCTywv84N1LvP3xdTw/Kcv1+nyqDZdPLi9w7soC731eoFJv8yuvnWRs8IsfyOiavGOjujWkFGwf1Hd4Riu2yvGbkFLeYV+xnXdJJzgJo4i1SpM//v7HvPniYf6LX32BoWKGRsvjf/qPP+fSXVYTQghsy7j9Ppvn1+FEPSykLKJpE3juj4jjdRQRUqbxvJ8Sx+tochQpR+k1q6VnGZkoWkRgomtHgMQWXqmIOF6n1f4LVNzCsb9HOvV7gKTZ+iOC8DKu97eYxjNI2d9Axg+ShejI5CBsE1ZVSjGzVOadczd566Nr3FosU6m3+zJ5KqDt+txc3OBf/+f3OX99iTeeO8QrZ6bQte52gbePEZjaCKXUb2FqIx2thM3f3P7/hz7XTpR/7soC7352iw8vzrG0XqNSbxNF/fXSUSTB39xKhR/8/BLnrizw1c59OjY11DMS4eZ7DBUzDORSDwxkXC9gYbV2T8llqVqn3GyTcyy+feYIWcciZRldL4QbtSTLtZfxFkQxCytVxko5SoW7AgoBUpc8++YJpk9PEIURQkpQMDCa3/G+Ndse5Xob1wt2XdAGC2kGuko9d4c4VjTaHu99PsPPP73JhRvLrJYbNNt+X57Duz87UCGfX1tirdLggwuzfOOlo5w5Mspg4fEaR/YCbTeg0fJI71DedL2A+dUqf/XORc5dWeDW4gaeFya79T5CqaREcurQyI7n9kXE/Z43BRi6jm3paJpkfrlCqZBG1yS3OlzM7IP4bPd5hKNYJcGoirEtg3RHxfvTKwvMLlX2dH7dQqkApZooVUVIM9Eh00oY4iUM4wSKsHOync2WCkDYnUxOb3cKPQuL4riSKHNqoyTZAEEclwnCKwTBFQz9GJb1Bpb1OiAJwotE0QJhcBGl7k1L9hp+ELGwUr1dT+x0PVy4scS7n93ipx9f58rM2q670UdFFCuabZ9rs2udScel7QecPTpOKZ++I0LeDZrMkrHOAoowWu+IRSWQMvXQdvRhGLFea/HJ5Xne/3yGc1cXubW40fMd225ouQEtt8riei3h5lSbrJabvHBiAsd6MNdpLxgqphnIp7i1tLO3i+sFLK5W8e/iJwig1nYpN9vkHZtqyyVWioHMgx+tzXUjKdftzZMoimPmV6ocmxq6J5ARnSAvO5ChUW1RXq6Szqe2vHl2Qq3hslFtPrB0I4XAsQwKOWerXfRR4fkhM0tlPrw4x3uf3+LizRXWKr3LhnYDpZIyS6PlslpuUGu6rJQbPH98gsOTpaeq+6bt+TRaHiMD9z77jZbHpZsrvP3JdX768XWW1+u4XZYRHhWGrlHIOhwaL/WV99ZrlOstbi6VuTizcscmRgjBqyenODn9cLYCUgpGSjmeOTbGh+dnmVksk3ZMgjDCNDRSewz2dCnJpCwOTwyyvF7jL9+6gGVq1JoeURTjWEbPmyjiuEwcrxDHFXTjFELmEcJG0yxggE19s9t6Z2z7WW/Rs4yMwgOhI7ctnlG0gO9/hFItTPM5TPO5rcyBpo0i5QBBcB7ojmPxKPCCJG3u+ok/TRBGrJab/M17V7YyMY8bC6tVao02q+UmKHjhxATFzk63q4lTxUSqjR8uEsUNtrsOW/qBTkt291BKEUYxKxt1Prm8wB//8GNuLWzQaPt7ep9eI44VF24ss7ReY3apgqFJjh4YpJBz0LVHJ4wlGgsPzjC4fsjyem0rBb8ZcGZsC6VgqVzHMRLC6Hgxx0Bmt4xFMgbLtRZrlcaezjeKYuZXK/dwsbYj9ENW5ta5+fkc0ycn0HRJ6gHXWGm0Was2d/w9gK5LBosZsikb8xHdtOM4uf5bSxu89eF1/vzt8yyv1+7hDjxORLGiXGvz1kfXWemQ+y1TZ7SUxdC75z09SbhecA/HJY4TzaOrs6v8zXuX+at3LtJo96dktxMyKYuRgSyDxacry1Vrely4tcyfv3OBesuj7Sd8oFgp/qvf++Y9gcymDMOB0aSTaDuyaZvJkQLZtIWuSyZHCnz7Kyd46/2rzC2VyWUcDowWOXN0jIMTA2RSFgP5FGEYkbJNLEPn4PgA2ZS1pTMzOpQj5RgYhkapkOb15w9x7tI8Pz93E9vSefbYOEenhpAdgrWUglIhzcHxAbRtXbSGJpkYyTNQ6L6bT6k2Km6QcF6fQ4jN+eXuYKU/wct29CiQEQjMbSZeCcJoFs//ACEMDOMUun749iuECUISq3pHtry/8Dsch82MzEatxb/9/kf87Nx15laqff/8ndBs+3xyeR6lErLWr79xuuuyhBvOslT/Hwmi9Y5+w+2FKu98jdQeAxmAcq3ND969zJ/87aesbNT7XkbaCyr1Nh9cmGVuucI//q1Xee3Zgwzsgf2/E4a6KJUEYUS53qbR8gijCFMmj46haUyVCjiGnpCRBThdTARK0en2anVNrttEFCvmV6r3JVhuQkjBwGgBJ21x5vXjGKbxQH5Mpd7eNaAyDY0DI3nSzqPvqIMwYmm9xh/91Ue8c+4m5Vqr7+WNveDqzCrNls9Kuc4//q2vMFLM3DHx71e0vZBG686NhxeELKxU+aPvf8T752dotLye89t2w8hAlunx/nW59QtDxTRvnj3EgeEC9ZbH+ZvL/PUHl6k07iWQQxLEOJbB//YPvnFPc8LzpyY5fXQMc6txQfG1lw7zlbPTSVenEIlBJwpNyjuyqJudXv/Hf/ZdLFNHSsHwQIazx8cxOjw529T59mvH+forR4njGCEEuia3nitdl+hIfu3N03zntRN3ZH0KuRT//B+8udXt1w00bRJNGyfJtvS+XLQX9Ky0JOUAUXSLMLwBhITRMkGQlI9M80V0bQLB7XS0itso1U5O4THsdIIwYnmjTssLuLVY5hef3eLnn91kZaPxUCUTTUt4B5sCXQ8LRUKKuzq7ylsfXSflmLx+9mBXrH6lXPxwnlL672LpU3d40Bh7dFL1g5Bqw+U//OgcP/vkOqvlRk92x0KIrYcwiuJHu1cKXD9gca3Gf/rxZzTaHt999Tj5rPNIHU2DhUxXnI84jlmrNKm3PEr5TiCjS4ZyaXKp22Pb1LsJZBSLazXK9+mo2Q2bpaVqw93W7XAndEMjcAPmry5T22gipWD8yAgnXz5y3/esNlzWKw/OyJi6zuRIYc9p77vRcn1uzK/z7374CR9dmqfSaPckiNE6WbJN4cpHQRQrVssN3j8/i2OZfO+1E5w8OLKn0u+TQMsLaLRvZ2RcP+DG/Dp/9P2P+PzaEo2W/9iDGIDRUpZDT2EgYxk6w4UMWcciCBMpiXc+v0VtBz7dZtbufqVXQ9e2go7O0ZhSYu6hczWzbZ7RJHe8nxACy9TZrehrmfo9WRcpBSlnb8+1EN1bpvQbPQpkJIZxvFNKep9m64+J4hU8/11AYFvfQtPGO7oyCWJVQcWNpPX6MdyMKFY0Wj43F9aZW67wtx9cYX6len9NBhI+QGZTDyHrkM84WKaGoetbwm6QiB75YUTbDag3Xcr1FuvVJtWGu6cAqdb0+PzaIpapMzGcZ3K4+wUjZZwkZZ6+4/7uBVEUs7LR4K2Pr/H2R9e4ubCx4325H6QU2KbBYCFNIeuQTdukbQPL0NF1DU2TSJEEbEEY4fkhbS+g1vSo1FtsVFs0235Xi5lSyQ7z/I0lNC15cL/+whFyafuhOTPZtEU+m3SzuX6w4yIYKxL576a31bUTRDErtQaLlfpW4DdayHJ0tLTLdSRZlQdpH+2EOFaU6y0qjTZtz7/vOLEci+JIHrfpEStFHMYPVPWt1NuslXcJZAyNAyPFRwpkgiDi2uwaP3r/Cj87d4Naw+2qpXoTKdskn7HJZxzyGZu0Y3bGmdwq/0RxIoWw2X5db3pUG23KtRb1ltd1gO4FIUtrNX7y4VVyaStpfZ948Pf6pLHJkdncYN2Y3+DHH17l55/epN4ZCztBAIahk3FMMqnkei1Tx9C1JEgUoOKkQyaIYlwvpO36NN1EO+rujr6t9xWCkVKWqbFin666f9CkRDPl1saymHEwdPlUiXb+MqBngYxlvkIYXqPt/hXV2v8dlA/CwjSexba+iaaNAJsaBhFRtEKs6mjaGDwmYzClFB9emKWxVc659xhdS8Tachmb6dEihycHOTw5yKHxAfIZh4xjYlsGpqFtZRlcP2StIxJ3dWaNCzeWuDq3RqXepu0FXZdnVsoN3j8/w7GpIewXDCZHjAfvAIWGJtIE8Tp+tHiHmq8mHDT54Hr0lp5E0+Wza4v84X/+gHKtfQ+hdSdoUuB0FpaRgSxnDo9yeHKQAyNFhgYy5DM2lqFvXcNmRqXWcFmrNphbqnBtbo1LN1eYXalQ66gBB+Hu98vzQz69ukiz7TM2mOPEdCLs9DA8BtNIlIZL+TSL67Udvy+lkl369u6metvj/PwK716d7fAOFK8fn95qwd7pfGIF8yvV+2qcdAM/SPg15Xr7jsBCdew0TMdk+uQEB46PA9CsNpH34RNtStBX6m3WdwmqTENnYqSw1RGxF2yOtdVKg3c+vcmfvXWeWrO77kBNJgFr2jaZGClweGKQg+NFDo6XGBnIks/YpGyjE8gkqXTXC2i5PuvVJnPLibjl1dlVbi2WWa+2aLl+VwFNGCXZr7c+uo5jGYwO5u4Y0/sNrns7I1NtuPzi81v8559eoNq4/72WQmAYiTpvyjYoZJ1EfG0wx2A+TS5jk0lZW0FiHMe0O/e2XG+zutFgZaPO0kadasPF66h5+0FIGCYSDY5lMDyQZWTPUhO9QRhFuH6I64cEYbSVsZNCoGsiUYE2kixFr7WqNuH5Ia4f4AZhMu5UsgFMtH9MLFNH1+TWd5QIr/pYhk62c/83rSI2ai1cL0DTZKKx1el43ez+a/sBuiYZyCabraeA2vXI6BlHxjTPEqsGCA3P+wApUpjmSzj29zp1tM3JLyKKNwijGZTyMI0zCLGzQmmv8f6FOVS8c+p5fCjPy6cP8J1XjjM2lCNlmxhGkhLU7vIEgts10fGhHMPFDM8eHeNX3ZPcWtzgh7+4wnuf32JxrfuulGrD5T/+6BwjAxmGihkc+0FBnkIRslD9b9FlHkMbZFM/Jme/Rin9m7t+ngLe/3yGP/3JZ6xVHty1sh1SCkr5NK8/d5DXzx7i5MERbFPHNDR0bTMLI+55iCxTp9TJ3BwcK/GVZw/Scn1uLZb56SfXee/zGW4+QNhwO3w/ZHapwr/+iw/4R7/+Ei+dOvDQPIZs2mJ8KMdqufGAQCYJNuvbuCn1tstA2uG1owdYKNeJ4ojhfIa4U/PeCUop5lcrO2ZkpEyIeYlo3/3PZ6PaZHm9zsTQbemCOIpx2z66npSW2s1kUbv80Q0yhRTPfe3UnedB0h1Wb7odkcKdYRoak0N5HOvhMjJxrPirdy7y1kfXqTfdrss/hazD2WMTfP2lI0mnVi6NoUt0XUPvjLPtgYUUbGUT8hmbAyNFXjk9RcsLmFuu8M65m7z98fX7dqHthKuzq2RTFkcPDHH68EjPurZ6jbYX0GgmHJi3PrrGu5/eYq3c3PFeZ9MWh8dLvHxmitOHR5kYLpCyE9K6piU8jTu6ttRm6S7JRkcddfOm67O8Xufa7BqXZ1a4eGOZ+dUqURQzNVpkqJjBeELKyWvVJh9fWeAXF2eYW61Sa3oEYUTGMRkqZDg0OsBzx8Z55tAohQeoZT8Krs6v8f6lWT66Ms/SRp0ojsk4FtMjRd589hDPHB5jdFun2V+8e4EffnCFU9Mj/O7XzzI9Wkyy0WHIf/enP+fn528xUsjwX//Bd5kYymMaGq4f8Oc/v8B7F2cZLWb5l//Lb/XlWvYjehLIJIPcwTSeRcoitvVtBAaaNoKuH0AIa1vZQyJFmnTqf4FjfRddP4jssxjedrg7KJamHZPXzx7ixZOTnDw0wtRokVQXLb5bxocyCXYcDLIpi7RtkrZNpkYL/OTDa1yZWe1KATcMI5bX63x0aZ7hgewD5cZ1OUDe+SZR3EAIHSmcLdVWUxvd9bOiWHHp5gofXJjlysxq10HMQD7FyYMjvPHcIU5MDzMxnO+q2yrx7hNILdmJbC4FubRNxknKeEcnB/ngwizvn5+lXG89cLFTJI7TF28u8+5nt8ikLE4f3v2674dMymJsKM9n15Z2bKJTSrF2V0YmSTVrpG2TWCmqLXdrh7QTwiii3vJYrzRpuvcGD4auMVRMk3YsGi1vx0B4vdpieaN+53v7EeWlRLBwY7nCxlIFO21z8/M5xg+P3PMecXw7y/QgISzHMhjIpTqeYXtfkOpNj8+uLfLhxTnmlitdlRFNQ+O54xO8emaaZ46MMjlSIJ91ulDD7jyTiW4/m3SzbNom45jkMjaHJ0r87JMbvH9h5oHdX5vwg4jr8+v82VufUSqkmRpJyqb7DS0vYLlc5+rMKu+cu8m1uTWi+M5AWJOCbNrmuWPjPHtsnCOTg4wO5ijlU2Qc66FKtMUoZiCXYnwwxzNHRll5ocnCSoW55QojpRyTwztrGPUTV+bW+Pnnt/jJuWv4QUTGsZgYzCW6K0HIcjkRl8ymLY6Ml3oeyNSaLh9emecnn1zn+sI6fhgykE3kEFw/5PLsKksbdeZWq7x+ZprjBzrcRgVNN+Dy7OrWHOGHEYtrNRbXa1uZ47m1CsWsg2k4hFHMzaUNyvUWI8WHk954WtFTeT1NG0bbhWSaLPwpbOvNzZ/08hQeCqVCmjOHR/m1r57k9OHRR1YtlVKSy9icOTLGQD61pSR8fX59V/lvRTJgz11ZYLxTMjF3SGXrskDe/uo9P4+Vi7aLwGAUxTTbHu+cu8Fn1xa7msyBJOt0bJxvvnyUV89MkUvbj+yoLKUgl7HJZWxGS1lGSll0TeOTy/MsrNUeuMDGcVIWef/8LMVciqnRIo5t7DlFnE3ZjA/lH7hIKxSrlQaNlrt1TjnHxjYMbMNAE4Jq26OUTT0wG5O0ctepNV3C+5TRTENjcqTAQC7N4lp1x0BmMyNzBwRouiQKI1Cg6RqGpZMbyJDO3ztJx3HM8kZ913GZSZkMD2QxdW3PZZXNDqUfvHuJ63NrtO4TvG2HlIKUZfDssXG+9fIxXjkzzfgeTPYe9L7ZtM3xlMXEUJ50ykTTBO9+dotG29+1BLxRbfLB+VlefWY6uR/7cLFouz43Fzb4/s8vcv760j2ly5RtMFrK8eyxcV4/e5DTh0fvqzmzV+iaJJe2yaVtDowWE1XfepvZ5TKWaTAy8GTari/NrPDO+ZtcnV/nOy8e49jkIIWsAyop3yxt1AmiiFI2hdEDKYftcP2AudUKf/nuRS7PrpLP2Hzl1BRTw0V0XaPaaHNjcZ33L83x9qc3EAImhvI4ppEYnaZt5teqeB3JED8IubVcJo4VWSfZUMytVjk4mtAeolixUm5sbYR+GUpKm3gC9q/irv8+WVimzulDI/yD773AM0fG7mtf8LCQUjAxXODvfP0MXkd2+uKN5a52o9dm1/h8ZIlvVpqMlLJIee9DpkkHTd5rDukGMzv66WzCC0IW12q8/fH1B3pUbYeha7x4cpJf/eopvvpcf0wpi7kUrz4zTanjTvz9n1+i2d7d++XSrRWK+RQvnJzkyGQJzdxbIJNJWYwN5nbxqEksBepNjyhWaFKgSYljShzTYCjXXQDcavvcWFjfse3aNDQmhwsMFTMPbM3eqCUZmc2gSgiB5ZiMHRrGdwNGDw6haRqaISkvV+/bfh3FiuWN+q5+Yrm0zehg7qG4IfWWx7W5Nf72/Su0d/AJ2g7HNJgaG+APfuMVTh4cvqNToxcQHSL/G88dYqiQZqPa4vLM6gPb2SG5V5WGy88+uclQMcNgIfPAgPVJoOUGXL61wrXZtXsCM12TTA4X+NqLR/jd7zxHoeOz0w/omqRUSN+rPP2YMbtSYWGtRimX4ve/8xyHxkp3bHJipYiimDCKH1kb6W6U623O31zmrXPXmR4p8r1XTvC733gWQ7vNr1yrNYn/5B1+cXGGtz+9waunppgeLVJI2xSzKT66Mk+7o7btBSE3lzawTY3xUvIsLq5VE25eR5dqtdKkmLUZ6pOFyH7F/rV7fUx4/vgE33z5GKcPjTyEgWN3MHWNX/nKcb7yzDRDXQpCxUqxsFrj55/epO3uTTCw5v2chn/ugccsrdf5wc8vJbyYLkiPKdvgpVMH+Parxzl7bHxP57NXSCGYGi3ynVeP8ytfOdHVa5RS3FrY4D+/fZ56c/fA525kUxbjg7ld6/hhGFNrulTqe+822kSz0368k6qqqeuJW/po8YE6ObVOy/Tmjm07dFPHsHQ0vcOZKmXI3GdRieOY5fXdMzLZtJ0Eeg+Rfbt0c3nLz2c3HxcpBEenhvhf/eYrHJks7cIRezTousbkSJE/+M1XmB4rdhWUhFHEBxdmudpR5t6PUCohKd99p08dGuE33jzD3/vmWfJpe98SlnsJ20pI4M22z8xK5Z5gVXDbE6nXQen8WpVz1xaJYsVLxyc5e3jsDgFPKQW5lM03nj/C9EiR9WqLdz67Sb3lkc84DBXShFFMudGi4fqdQCbJcE0OFxgqZFjsPLueH7JWa9L2fHJpm6Gn0F7jUdDjlVsRx22iaJYwmk9sC5Do+mF0fRop81tdS5u2BIk7pv7Y66dSClK2yYunDvDc8Ym+kveEEAwW0rx06gCr5QZ/9tbnXb1uZaPO+xdm+erZQ2RSVof8GdL0z2NoJaRwaHof3/O6hvv+A8Xw2l7A7HLiL7XbLhSSQGx0MMevvn6Sk4dGer5DvhubhmZHDwzSaHlcvLXMzGJ515LERrXJJ5fnubmwgW0lXKVuYega2bRNIetQabR3bCVVJHXv9WrroUuQrXbAjYV1XP/+C6FpaEwM55kYznNrcedsWRjFNJoui2s1xgZz2NtMNZNF6vYzpe8QpEexYmm9u4zM2B4zMkopGi2PKzNrfHZ1sSsO1pEDg3zlmemOu7fVty4SuC2xcPrwCC+ePEC95e1KNFcqMZy8PrfOjfl1njk61rfz6xVMQ2NkIMu3Xj7G62cPbm2m+j3lhrGLH1UxtAyasJDi8RcAjh8Y4trCGm+fu8GfvP05l26tcniixPRIkcmhPCnLSEwZ+3AvyvU2sysV4jhmcrjAWCl3R7AkRNIxdWS8xEA2xdW5Na7Mr+F6IYWMzXAxgwLWay3qTRc/jLi1VObIeIlSLkWt5fLZjSXqbY+WF7C0UccPIwqZFMNfBjIPg2SCiqJlguASnv8eQXiVOFpN+DD295Cy0DGGVCjl4wefouI6un4UXZ+AXWV8egvL0DkyOcjpQyN3dH30A5v+N8emhlivNvnZJ9epNXfXs6g2XK7OrLJcrlPIJQ7NihgvnEEgEaLJRuv7GNoQUty+f160iBVP7/i+a5UG12fXuLGw3pXWzUA+xbNHx3nlzFRPlHS7RTGX4sTBYb718lH+7Cef03YfLObl+iELazXOXVmgVEjtKZBJtHB0RgazrJQb+MHObdG1psdqucHxqaE9XE2COFY02x63FssPKC3pjA/lGCnlKGRT2KaeZDPuc2zTTQQeB3KpPXvYbFpSrG7Udy3f5dIJf2lvgQzcWipzbW7tXi7PfaBrkmePjvPqM1OPrSSha5J8xuHl0wdYXKtxa2FjV8G4WCluLqxz/sbSvg9khBAUsyneeP4wr509yPRY8bFwJ5SKCWOXVrRGWhhIzUCpmFiFKGI21wyBtu3fAk1YSSuxCoiUj0BHisS7TxETKR8NAynNzusffDHHJwepNg5SrrWZWU7KTJ/dXOLEgSFOHBhmcijPcDHDYH6zVbl3N6fl+pQ7CsD5tH3f+UhKQSmXImUZeEHESrlBEEaU8imGChk0KVivtVivtRACljbqnD0yxtRIkdVKg7fO3aDW8qg22yyu14hjRTHjUOqyzP1FQU8CmSRbHOF6P6XR/Ff4/gdsGkVJOYRhPLPNGFKhcGk2/5AgOE8q9Q9Ip34HTdv7ovAoyDgmX3/hMGNDD1f3fxgUsg6HJko8c3ScTy7P70qwjeKYWtPjyswqw8VMRy9EYGjDaDJLGG/QCi4yYBzE1G9PqHpwCXkfbZ7NtP612TXO31juWrDv0ESJ77xyLBGd6+MO+X4YKmb4u19/lg8vzLG0ViPYJfjz/JCff3qTowcGOTRe2tPEpOsJN+XG3PoD9V1qzd1VcHdCGEXUWh4Lq9X73n+t084/MpAll7bIpW0G82kWd7CLSFrXNzh1aIS9huNxxzE30VXZuUyidcjYwwPdc0IS9/SYjy7McWN+fdfgQEpBLm3zzJFRTh28t7uq3zh9eJTzN5b40ftaV2KQNxfLXLixfAc/aT/CNDQOjBb5ve+9wGAh/UjnGas7lbmlEJ0g437HhkTKS4IRIRFoxCrEiypEBMQqTHyRhU1MiFIhAo20MQpKw4+btMIVDJnGEGmk0AhVm3a4ga0XSckhuuFZDhUyfPvFo5w5NMoP3r/Mzz69yafXFnnvwgy2ZfDy8QP8ysvH+e4rx9Bkb4XuojgRZZRSdGQp7r1XArBNA0PXUUrR7jiRp22LgVzSKFKut1ncqGNokqbrM5BNMTVSRNdkoidVT7hyyxt1NCkpZBxyaXvH89ocs7FKyo+b3aSCp8cc9W70KCMT4/nv4Hp/SxQtYFmvYxjPEgaX8YNP7zpWIkUWXZ8mim4RBJ+g1K/25jS6hBCCTNrm+ROTDDzmyHUgl+KbLx9lZrHcVadQEIZcuLHcaXMuINBImSeRmICi4HydvP0mpn677dgNbu7YtaQU3FjY4MrMalfnW8g6HJkc5OSh0bvktR8PNE2STVs8d3yC9WqTq7NrDzw+jCKuz60zv1Kl0fLIPuCBvhuGLhkfyu9aZqw1XFYrzc4ecm/YqLVY6eyc7oe0YzFUzGy19mZSFmPDeVar9+cytb2Am4vlh3Jtb3kBS2u1O1x974eBfJpi1kHbQ1eHUkn33cVby13pKDmWwVefO8SB0WLfCKi7ff7kcIHjU0NcurWya7a01fZZLTdYXK1R2oPR3uPG4YkSX33uEIXsoxus/mDpY/5s4T2UUhxMD/P64EneGDp932OFkARxkw3vEqbMohkmflRntvkWKb2EF9Vxw3UMLYOjlwjiFmHUYjr7HdrROu1wjTBu0wyXyRkH0KTFhneZvHEQS8uzlyfP1DVGihl+47VTvH56mvm1KudvLvPh5Tmuzq/RaHvUWi7ffP5ITzq4NqFrGqahbxF1gyi6x75EAW0/IAjDLXf5zc2CbeocGM7j+QGL61Usw0CTgqFCmqnhAqAwdMlGrcXcSoW1apPBQnrXTHSoIhba6/y7uXdYdSuM2kVeLZ3gheJhbO3R7EeeFHo0Y8R4/vtE4Sy6Nkk69Y9IOb+NYZy658ikzGKgacMIkSIMZ1Dq8borZxyT8cEcI6UstvV4J6Bs2uLs0XEGCml0fffbH4QRV2dW2egIpwkh0WUOKW10bYCC821sYxpDG9z6k7ffIG09c897bWqGLKxUWd/F6XgTB8cHODRRemjV3EeF7NSRnz02zvTY7l4tSkGj7TGztMHscmVPn6VrCTdlN+XaWtNNDBYfwrRmvdJk6QFllmza6nSpJbujbMp6YFt4kpHZmW/zILRdn8W1nW06NjFUzFDotJR3OwbaXpIpWlyrddV1ZlsGL546wGgp+0TGmaZJhooZjhwY7EonJ4qT9uIrs93pQz0J6LrkyOQgL586sGU0+ChYdit8XL7Ox5XrXK7Ps+7vPI4FEoQgVmFSTlJJWWgryyJThMqjFa6iCwdTZoiJCGOXejBPzZ/FjSr4cYNAtTtlJg1HL2HKzJ78lKVMLAbGSzlOTg/zlVNTfO+VE/zO15/l0NgAc6sV/ubDq2w8AoH/fkjbJqVcChBUGi61+zQhxLFirZroSZlGEnAlKspJpubAcBHXD1lcr7NRazKQS5FLO2RSFvm0QzGXotH2mVtN5vShTiDzoEeoFXnMttb4xfol3tu4wvsbVzlXuYEX7c9x3A16FMgoguAiSvmY5guknN/EMp9Dyp19SaTIIYRNHG8Ae99NPgqKuRQHxwdwrL3rjTwqLENnbCjPaClLpguCcRjFzK0kCrB3Lzi6zJGxnkOTGdS2/2XsF0iZ9waRQRRxc2GdlXJ9RzLr3Tg+NczB8YEk/fgEs45HJweZGi123SI5s1Th6tyDszd3YzMjsxuZud7y2Ki0iOJ41y6cu7G2SyBzNxcl09E82Wlxdb2gEyx0J7m/HS03YGG1tqslxHAxQzG3N6Gwesvj/PUlql14KWmaJJe2OD41lGh8PCEUsw7TY91nhJptn0s3V3YloT8pFLMpDk2UODxZeuxt4ooIpSJQCj9uEKr2Fhcmo4+RNSaxtQIAaX1k69+xColin1C1CVUbQzpowkAXDlljkowxgallYZdQRqnb3VvbnwtNJpyoU9PD/Obrp3n+6DhKwcWZlfuKUz4KSvkUB0cH0KRgZrnMwlpSTt6cMza1Ya7MrbFea5FJWRydHNzaXNuGztRwIfH8Wq9RabhMDuXJOCa6JrFNnfFSDi8Ime8Y0A4VMrtmZII4ohm6NEOPMI5wI5+q3+TePrenBz0LZOJ4HSEcdP0g3cTKQjoIYaJodgb440Mx53BovPREUtiQZBkOjQ8w3IVIlFLJgrNRbVGpP6gUpYC48+f+CMKYq7NrlGvd+/scmigxOVzo+vh+oZhzGB3MMVTsbse+uFZjZrG8p8/QdY3xwRyZlPXAz3C9gGqjTcv171FN3Q3r1SZLDyi15NIWIwPZrYVnMyOz01iNYkWz5ScqwbtYDNyNluuzsLp7RmZ4IEsxuzeSd6Pl8fn1JVpdnFPm/8/ef8dYluX5ndjnXH+ffy9eeJveVWWWr65qb6ZnusnxMzS7BDnEYndFUhS0giRgIUDmn5UggJCAJbECKeyu6MldznKGHNMzPaaru6vL+/Q+w7vnzbXn6I/7ItKFeZEZabqnvo2ojnxx/bv3nu/5me/XtZkYKpLPOE8kfbmB2y3m/UadQm4sVrZto3/SODYzxNRI8YGUmB8WftykGS5QDa6x0v2IZjiH3Iy8b3V9b3+WsybJmKOEsoNScvOvuxX2boVm17/LDfxOaCQZgiT6+WDb3wkT5TzPHx7DNHQ+vDTPJ1cXCePbz1rijRTwxsdXublcZSCX4gunpsn2JriOZTA1nERklipNaq0u08NF0j1vNcPQmRgsEMWSpUqDettLnLp3ITJZw2UiVWbMKZE1Uoy7AzxXPIitPR7Pw0eBfcurCHSUiED1N9OXsoNUPkKkkzDkY0Qu7TA21P8Laz8hhEChmBguUM6nuUR/tSpr9Tar1SaDxds1PUG0QsN7k3bwGWG8hiLp6jH1AfLOlymlvn3XNqIokVnvx3HZNDSK2VRPKv/J502FSHLDB8dKLK83iOKdZw+VHmHoeEHfZnCCpA27mHUpZByqOxT8+mHEwmo9GSjc3QffDcO39Xr7PluBO5FNOb3UUnK86ZTFaDlR1BVsnc2SSrG41qDe6pLP9F8TlBCZRh9EJkNhjxGZVsdPohV9pF2KOZfDk2Us8/HLMNyJtGsxWMz2jsPf1Quq64fMLlXxHyCt96ghBBwaH3hiqTpLy1B2TpI2hjA0F1vPo2FwMPtdXGMAWxWw9CyxDEibQ5vrWHoWpUq4RpmifRSBhqml0IWJVDGm1u99mLh//84PP+P8zWXGy/nN+hFD0/DCiJtLVT68PI8mBF85c/C+aGAcSxodj/VGhzCMCaKYKwtrdLxEoG52pcoHl+awTB3LMHrKxjauZSJEUrx+bGqIX3jlGB9dWeB7715kcb3BzEgJw9CotTyuL6zzweV5Rks5vvjMDFNDhc16K9tKIjJCwEqthWubvHJikkzvfWzqGpNDBS7PrbG43kAIwUgpSy618zvA1HSmUkP83SN/CU8GpHWHYaeArT+ddV79YN9MIzVtgCieJ4qX4I72unuhlESpDlF8Eymr6PoEiMc7UGZSNsOlBxP32g8IYGQgRyHX/yy31ujeV9cSySp174eY+gCKmDBewzWPEsUVwvjutIpSiiCKmV+t76oZAkkL8ORwgVz6yc6S70Qpl2JypMjbZ2/CLnzZCyJqzS4rlRYj5WxfSr8bbfKlfJpSPr0zkQkiFlbqDJWyfWkQKZWkW6qN7rbic4IkKjB0R0TGMgxyGZdC1qXa6G5Z1KuUYnG1Qa3ZZWqkuOuxbKzT8UKW1ncmMpqWaCDl91A0HYQR9Vaib7NbITEkUafJ4UJfNWOPEhueWRsp591SdUEUs1pt4QXRriahjxsbopLlJ6Ssq2sWKa1Myijf9bmlZ7b8HcDUbh+rSQbYWFc8ULREkBTSzq3WmV+r41gmttlzmUbRaPtYps6rJ6f5+vOHevUst9ENQs7dWObNz24QRjFhLFlcTyIfsZR8cnWRZjfA1DVMQ+fIxCCnD41yaCwpqbBMg5GBLN9++RiWaXBpdpWPrixwZX4dTRMEYUQ3iJgaKvDKiSlePTl1V7mBbRqM9tzWu35INwjvisiYvYiMEEmas5hNWrZTzs7jqSY0sqbLmeKjUWd/Etg3ImMYh4niBcLoAlF8s1cf45MQmhilfKRsIlWbKLpBEHyMlE1c91WEeHzaJEIkoeyBfOqJKlsOFjN7Ghwa7e59LcFSdfGi6wxn/yZBvEInOMdw9m+w2v5t9HuuaRRLOl7Aer3dV2GobRpMj5WeimjMBvJZl7HBfN8DRtsLuLXc01ix+g+bbkirX92hxsYPI+ZXG5w42F9aIZaS5fWEbGw3QJqmTi7jUMi5m/VImiawTYORco7lSnNLIiOVYmGtvie14SiWtDr+jsrOek80sphL7Ukwst0NqDQ6fae6Uk7i42Q8oYnFBoQQ6LpG2klqEHYjMnEsaXZ8vCAijiXaU0L4NSGwbZPhgSz5R+Tm/Kgh9lTOu80WhOD0wVHCMObGUoVKs0O76yM0QdqxGC1lOXVghOePjHNyZvi+yJUfRsyu1nj7/K27Ph8spIE0HT/k7PWlzc+lUkwM5jeJDCRjzUvHJ8lnXD64NMcHl+ZYqiQR5Yxrc2K6xKsnpnn24AijA3f7iRl6EhU/MFKi3uoyVs4zOVTYtNExdI2JwRxTQwVWay0GC4kezqNSqH+asU9nrOPYXyGKruP5P6BW/29wnW8SRbOARKkmUXyLWC4RBJ/R9b5HHN/CMp8j5Xz3sbpfO5ZJyrWw99FT6UFQzKbI9nRZ+qmzaLT9LbRNNDThIIRBos8j0UUaJT2kuHvZrh+yvN4kDONdQ+YAlmUwMVzYV++ph0Xi99N/qLzrh8wv1/asSzKQT+06k/WDiLmV2o5eSHcijiWzyzXqre2jPKVcinzGuW9ANw2N8cECV7epb7ozItMvGq3EZmGnaIzduwdSjrWnQu96y2O91j+pcmyzN7F48o4pQiTH02/9nFKKVtuj64dPTeTSsnSGB7K4jvUXwoZgJ7x0bJLTh0aJYpkU2vY+F0KgawJT17dtIChkXH7x9VN884Ujfe3LMg2cbdrwZ0aKjJVzfPvlY0iZlNVqQqDrAsswsLa5dwTwd37ldYIoTvScUretJQxdY6iY5b/8pdcIeno1u6WVflaxbxEZ0zyM6/wcipAg+AAZLyNlBSmbeP4bBOF5QCBlFRmvYtkv4jrfxTAO9mwKHg9SrnVXr/6Tgq4LUo5JJmX1pSfT7vr3pYQ0YWHqQwh0BII4rrHc+pd0wyvo2t16CF0vZLXa6ruzxTQ0hkpZ7D1EMh41HNskl3awLQOvD9+ejY6efgTO7sRAPs3ALgrGGzUy/RZ6xlIyu7yzdtBgMUMh495H1BKhvu27qZSC1VqLWrNLGMUYurYr2as2u5st/dvBtgwmR4q4trmnOot6q0ulz/Z+ANc2KOXT6PqTH3Q1kSg87yXt3PbChNA+JWKqlmkwVMxgmfq+1sdsxEh+GnpbNk7btowH1vjRtSQ6l94lVdMPTEPvpS77X2fju9tO3E4IgaGLHcXv/qJgX4hMUltQwLJeAmEghEsczyGUh66PopRPHC8ghIumFbDMUzjO17GtV9C03O472EekHWtPaYZHgY0b1LFNsmmHRtvbNUrS8cL7QvWGViTvfAlDK6D0CMc8RBgvYRuTOMbUXct6QUil0enLIBLA1PVemPLpmGVCMgNxbZNMyqbdDXYlKH4YsdaT/N4LSrkUA/k0mhDbOpUHQczCah0/CFFK7TpgxLHaNSKT6LXcnwowdI3xofyO7frtbkC12aXR8vqykag2OqzXdyMyJlMjxT1bH7Q6PrUdzvNeLK41+cH7V9D1/VVWfRBUe3LwUdzfPaNIonN7bX1/lDANnYF8GrMPAbxO5LMeNFn16rSiLr4MkUphaQZZ06VkZRl1S1haUoi90zNRDVrc6qyy4tWIZMyAneNUfgpHt9C3UQC+E7GSdCKfS815VrwautAZdgpMpsqU7NsTs4VuhdnOKhW/ybBTYDxVZtDOEcqYZa/GetCgEXYJZPJsakLD1k1yZoohO0/RyvQt/BbIiHbkUQ1aNMIO7cjDiwOiXjeVLjQMoZMybApmmgE7S8nKwjZdUF4ccLW1xKpfx9UtDmVGSBsuK16NxW6FRtjF1HQG7CwjTpFBO48QglrQZtGrsOzVCGWEpRkUzDTT6SHShoup3f9db0z0LjcXuNpa3PYcM4bLsFNgJjOMpT0cJQhlRDvyWfXrNMIOncjHlyGxkmgIdE3DFAaObpIxXPJWmqKVwdKMvu6R7bCvyTTDGEPXh3Ds1/GD9wnDi8TxAkp1EMJG18oYxmFs+xU0bQDxmIt8ISEPT4sKp2MZZFwbQdLJtBP8IMLzw7sk0U19iHL610hsC8pYxhjd8BK2MY6h3V1kF4QxjZbXd7uwoWsUcynMp4jIQKI5UsymttTVuRdhGFNtdvsmbxvIZRyKuRSWqeOH0ZYkM4giViqtzQ6GnaIJG4XWCyt1Gq3tC60HixnyWxAZ09AZG8qTSe38vNQaXZYqzf6ITLNDZRci41gGU72IzF7Q8cO+Cso38PGleT6+NL+nfTxNiGK553vsUcLQNXJpZ9u26413iBcH3Oqs8mH1Ku9XrjLbWaEatAlVTM5wmU4P8Wxhhi8PnmLUTcQoDaFvDuL3Ysmr8r3FD/iz5U9oRl1eKB7if33kLzOZHsTtgziEMma+s86/vPkD3l6/iKtb/Nzw83x37KW7iMzZ+i1+d+4tPqxd4/XyCb479hLp0hGWu1XeWr/Ax7XrXG8tUwvbxEpiayYFK8OhzDAvl45yujDDeKqMIfQdI/PdOGDNq3Ozs8LZ+izXW0vMd9dZ9xt04xBQWLpBRncZcYsczY5zujDDc8WDZAwHQ9wfEWtFHn+4+D5vrV1g2Cnw16e/yohb5M2187yxcpbrrSXShsOzhRm+MnSKrww+g0JxubXAny1/wltrF2hFHlnT5Vh2nF8cf4VjuQmKZmbLyZRE8cbqZ/yzG3+GVGrLcWYmNcRXh55l2C0+EJHZvJ9kSMVvcrO9wvvVK1xtLbHkVakHbbw4wBA6jmGRNhzKVo6p1CDHcxO8MHCYspVFfwhV4UcwousIkcW2XsUynwMSk7CEnRoIYSGECzyZAdI2dcwn3B2xAdPQ+1YW3iQy3FkCJ5HK7xlG6hhajrR1GiFMxD3XN4giGm2vb38lTdNI94SXniboeziuMIp3LK7dDhuuyCPlHPMrW+usKJVsv9ro0Or6OxZVbvihVJsd/HD7QuuN1NK90Hu58GzK2bEItdrosLha58TM8K41LdV6h0pj5/SPbRlMj+6dyHh+2Jd+zM8Koijuy9n7cUHXtJ6r8/Y3gUTxg9XP+POVTzlXn6UbB4QyIlYShaIWtmk3ZrnZXuHNtfP8lckv0Qg7uIa1bbv5sF3gldJR3lq7QDPqsu43ebtykaKd6YvIeLHPh7VrrPmJzpKlmZwpHGDE2b4Trxq0mO+sowuNf3XzByx1q7Qij6B3LqA2Rd8qfoMLjTmeLczw6xNf5GBmhJSxfZTzx6vneGP1LGfrN/HjkEBGhComlvGmeFwcSfw4pB62ud5e5v3KFU7kJ/nNyS9yIDOCvkOMsRsH3Oys8MPVc5xvzDLXWUv2EbZ5d/0SjbBD0cxSDVu8s36JH6+eoxP7SKUIZMiHkc9a0ORvznyD1waOo2/x0AsS3ZghO08nDujGPqGMd5047xWxkvxk7Tw/XD3Hp7UbdGOfQEZEMiZWEokiUpIgjGiGXda8Oldbi9zqrDDk5snqzkPZI+w7kUlYoY4QGeDpsxI3jK3Nu54EEiJj9pV4lj2n4jCKMfVEwtqPFqh0fo+B9C9iG+OAgS62/kqjKOla2i4sfCeESDQKHMt4Kgow74Smib6JTBRL2l1/z4OMEElXw/hgnpXKzqmp9XqbRtvbkci0uj5zyzU8f+voTrJPKBcyW25HkJj/FXMp8hl3W3uJarPTE9vb3QWq2uzuGJHZIIzDpeye04ueHz21arePAlKpXeu1Hic0QU/mfut7oBP7fFy7zo9Xz3G2fota0CalWxzLjTPhlsmaLrGSNMIOS16Vm+0Vvrf0Id3Y7xGdrZExXWYyw0ymBmnHPpWgyfuVK7xePsGAld3WYBJ6cgBxwCe166z7DRzdYsQpMJ0ZImtu/2yt+Q3eqVzis/pNrrYWGbByHMyMMOjksTUTLw5Y9Rvcaq+y6tfpeHVU7QaWZvCfTH+NCb28bUrD1AwCGbLq1cmaLhOpAQbtPAUrjaNZKBTdOKAatLnaWqAatJmN12hFXQ6mh7E0k6n09mbI60GT9ytXaYQdRp0ix7PjeDLko+o1qkGLm+0V/mT5Y9b9Bp3Y53hugolUmaVulZudFRa7Va63lrjaWuRQZpRR937CJxC8WDpMwcoQyZhQxXRjn7nOGn+w+D6BfDgxR4WiHrb50ep5fryWELI1v4EmBEN2gWGnQMnKYGsmsZJ4MqQVeSx3q7RjD01ojDklbP3hyj32yf1aEgTvILQshnEAwf0Fi1ushZQdYrmMjKso/KRoVctg6GMIke114+wvdE08NYOzoWubQmf9IFZqs5gTxKaOTN79MjbjO68rJX4fBbKQ6AwYhoaua0+8KPpeaD136H6+w1hKunek4/aCtGsxMVTgs6vb55YhsRyotzwmd2iManV8bi5UtrWF0DWB61gUci4p5/4HeuNZGigk3VTbEZlas8vSemNHTryR5qo1uzS38H7ZgOuYFLMpMilnzwrYQRQ9kInlTy2U4qkqge11w2z1Do5kzLrf5E+XP+Gz+i3qYYec6XKmcIDnioc4nBmhYKWJlKQStLjRWubD6lUuNxdoRt1elGNrWJpBycpwIj/Bql9nrrvGtdYSC90Kg3aenLl9ytOLA1a9OtdbS7QijwE7y9HsOANWdsdBrh62udDwcHWbqdQQzxUPcjQ7zphbwtFMunHAolfhQmOOt9YvstytsuY3eGvtIl8qn6JkZbclShOpAZ7JT+PHIaNOkYn0IKNukQErS0p3UEjakc+a3+CjaoGPa9e52V5hxa/zYfUaY+7AjkSmHrS52lrkWHacL5SPczAzQqdXg3Oufotm1OXdyiWkkpzKT/Ol8kkOZkaY7azxo9WzrHg1unHAQnedZa96H5HZ+P4PZ8c4nB3b/LwTeXxSu8H3lz9+aCLTjjyutpb4o6UPuNxcoBv7FMw0BzMjHMmOMZUaZNDJ42oWkZJ0Y59G2GGus04j7DDiFhh1Sxji4TI0+8QUQmqN/zumeZxs5n+FoY/1Bo+NOncduLuTQqmYKJ6l2/1DguBdYrmGEA6mcZhU6lcxjZNoWvLF7GflvaYJnpaORKGJPQ0SSirCSLJRRK8JE0MrAhpKRdzrOCHumGlImURl+hnTNU2g6/0TrMcJQWIi2U9b6YbXysaMeS/3Udq1GB8q7NpSu1Zr09il66zV8bm5VCWItn5pWKbB2GCOjGPteD+UC2kGi2ku3tz677Vml6W1JlIqNLH1+cZS0Wh1aXa8HclGPuP0rBL27rEVx4poF/+mz/FosVM0Zq67xtvrF6kGLdKGzXR6iL998FtMp4ew7pGpP1M4wBfKx/l/X/j3fFq/uevAZ2smLxQPcbm5wK3OKq3I41x9lhGnuCORWQ9aXGrOUw87RCpm0M7zQvHQrumGQEaEMmbQLvDXp7/CmcJBBuy7OzafYZpXB44BSbporrtOM+pyuTXPeGpgWyIzkx4mb6Z5beA4U+lBzG3qR6RSPF88yL+++QZL3SpdGXCltchCd33HYw9VTChjvj3yAs8XD1Kys4QyZsWv04o8Pq5dZ9mrcTAzwkulw/zC2IsAHM6O4sU+b69fpBl5rPvNzXRcP9CFhqkZD/1+VyjmO+u8tX6Rs/Vb+DIkb6Y4npvgbx/8OQ6kh0gZ23dUbaQyt6ol2iv2NeQRx0v4/o/wlIeMqyBA10ewrVfR9fFeuilBEH5Ct/sHdLr/HpTsvS0lUXSNMLpMOvVXSbm/2qun+dmEJsSe2jyVUsRxvBlh0EQK1zxCJzhHLFtY+iD0yIsuMph66a51oz4LfbWexsLDCVI9GgiRRLL6VfpUKtFwkUptmUPeDmnXZnwovyuR2Ugt7YRmx+fGDhEZ1zY5OD6Au0U05k6UCxkGi9lt/+6HEfW2x1q1RSmf3rKoPYpjFteau4rVFTIuwwPZB7oFpJR79qD6HI8Hy16NT2o3Np2Ox90y3xg+Q9nOY2wRAbc1gxGnwEulIzSjLhebOxdlm5rOsdwkE6kyn9Zu4suQj6vXOJod48gdUYF7seJV+bR2g0BG6GgM2XmeLcz0VTcx4hR5vniQZwsz25IlWzN5rXycm+1V5rrrSKVY6taoh9unVzUEeTNFxnDQd4gYCCBnpphIlRlPDXCltUgtbNGKdn4v6EIjbdiMpwY2j1sTgolUmaJ1e6w8mBnZLLZO9ieSglknT6cd0Il9OnH/xfX7iRvtFd5Zv0ikYjQhOJWf4q9OfZWp1CD2Lt+dLvSH6lS6E/tKZMLwAkq2UKqLVG0S64I8QfAJKfeXsawzm+3WYXiOIPwYISxc97vo+iBSNgmjiwTBh/jBO5jGUSzrRfZ1QFVPVRB4T1BwV0RFERLG60SyihfexNCym9PnlHWKvP7atuvuiI1Z+NPHY25jD8f2IN+3a5uMlLM4Pc2h7WqL1uudHYlMFEuabY/F1TrRNrU2jm1yYHwA1975wR8sZO7y2roXicFowOxyLRF93IrIRHLTLXsnFLIuw6XsA0nDS6U+JzJPKapBi+utZaKeJ96gneO5wgHSur1lGlkTGrZmcjQ3xoXm3K5ERiDIGi4z6WGm0oNcbMxxq7PKXGeNetgmb95//wYyYsmrcrE5T6Rihpw8k+lBSnYWo4+BbsQtcCo/RcFMb9mGDGBoOlOpQfI9wqBQNKMuXrz9cyCEwBQGu1VvbCyXM1ObBMSPI4I72o63ijiYmkHeTJE2bIzecQsgb6TuKo4etPN3XTchBKZmkNYdNCEIZfTQKaIHQTPssuRVWehWiJVkxClyJDvOsew4KcPasSYK6N1v+zPI7HNEZh4lG+jGeI+wKJRs0vX+AE0roWlFLOsZAKLoBnG8hmk+Syb91zGMGWJZJww+pR4vEUW38IMPsKzn2TeTbjYK8/Ztcw8FBX0V325AkKR9xOa/dXQt09tWRChvuz1L2bl/3X4jEioxXHtaGV9ybHu7bnt9XCxTp5BNkU87WKa+rfBdrdmh2faQUm56Nd2Jjpfou9Sa3W0vp2sbHBwf2LI+5k4Uci7lfBrT0ImieMvteUHEjcUKU6PFLQuHo1iytN6gvUsxbv4hIjLssWREiD3cm08ZNG138cGnCc2wy2Jv4DGETsHKMJEqY+zSdjvmDiT6KLtAiOT9dCgzwvHsBBcac9TC9iaZyefvJzK1oMVCt8KSVyFWipn0MAczI323ApftHAfSwzsOnBvRFeeOehtfhpuEbjsopZAoulES9fDiAF9GRCpGqiTSu9HWPNtZw5NJpEuhiJVCKom2TTTH0nSyZgrtnvHN1s27akayhktKv7u7ShPaJmmLldyxdulRoRI0WfebdHtkcCI1wFRqkIz5+AX69pXICJHGss6Qzf49DOMgKEkQfkaz9Q8Jwo/Q9dFNIqNUEyHAMo8jRBoQaKKAbb+G5T1PGH5MFF0iMaDcPyipUE9Jq6SSCrmHmaumCQxd34y62MYMk4X/fd/rmobWV72DVIo4fsoKGHtQShFHsu8CXk2IvpRut4KhawwPZJOOo22ITLOdKC4Hkdyyu2et1ma12trxSjq2ycHxMu4uCqKubZLPuuR7ztxbaZd4fsiNhQovn5zaYgs9IrPW2LU9Op9NiMyDDNGarvVd+yVE0km4nUT70w5dSwrjf1qIWCf2WA8aSKXImQ5Zw8XWdu8YKVoZMjvUO9yLA+kRjuUm0OYFEsWN9gpn67c4lZ++b9lrrSVutleJe8/04ewohzOjfe8ra7gMOvldo4eWZt6VIpKqv/eIH4dcay9zqTHH9fYyi93qbXE8GRDImECGhDLelRjdCV3ouJp1z70j0IV2FylzdPM+UicQSaQH0as1f/zv6jW/QT283Xgw7BQZch6f3dCd2FciY5rHcJxvYxqn0LQ0oLA4Rcr9ZTzvz5FyZXNZpWRvnHTYmDMLAUoJDGOKKLqAlDsXSz0IYqn2FAV5lIil3FNRZCKCd5uM3L7/d485aJrAMPRdl0uOSxHFW8/4nzSUgiCO6YeLakJstqHulchsGAiODea4mLZZ20Fyv90NqNTbjAzc7wO1XmuzVtt+XdsyKGRTlPIprF30jURP32ZypEi7G9Ddgsj4QcTNxQpdf2utjziWLFeaO7ZHu7ZJPuOQfUDpc32DcPeBYi7F66cP8Auvn3igfT1pCAFjg3mKe3Cyf5KIlMSLQxQKSzOxtf7sJ2zN2DZtsxVc3WLULXIqP8WV1iLz3XUuNObw4gBLM+4aqC81F7jZWUEXGnkzxVRqkCGn0Pe+TM3A1a0d32wPMpFZ8Wp8XLvBW2sXmO8mXTbdOEgiOT0tFk0IDKHj6haGiPFl2HeaRyDQtd3fyZrYfSL2JN7V3TggkLffMynd7ksv6FFg34iMwEDTihjGDJqWR/SYb/LZEeDPkLLJ7UuevIST5e5hpFoJhNmrs9nfryiIYsI+5ccfNaI4EQjq9wx1TbtLHyKM12h675F1XsHUbzuutvyP0IRDyjq++Zmhabi20V+3j7ytWSOlfGra1SGJFnX9sC8lVU0XmzUuDwJdE4yW87sO6O1u4io+VMrelwRdr7dZ34HI5NIOQ8VM35o9addmaqTItbm1LcmKF0TMLtUSzaB7vjspFUEYsVZtb0t0hIBSPk0+4z5wlMTQtb5FJzeiXi+f2jqC9Dn2F1Ip4l7U4N6Z/07Qhd73spDUpAzZSZHwkldj3W+w0K1wtbXEdHqIjOEQyZh27CUty14tqcXJjjPiFvc0ICY2Afs3J49kTDPq8qPVc7y5dp5zjVnakcegnWc8NcCgnSNjuDiaiakbmEJHFzo32yucb8xxq7Oy+07opfvZfWr5tJYrJimt26OXoek7FkU/SuybaaQQKRAGqHvZqAIilPJQqoWUDUBHEQEqiczctzkTlEDtIUzXL4IwItyme+RxI4ziZEDpg8noehLCNu4YXIJohdX2v8MxD/SITLKhevdHmPrAXUTGNHQy7tYFffdCkZCsjheSciyeIh6DlInIXT9qvYauk0lZaA8ogKhrGqPlHNltzBo30O76rNfaScrynud4vb5zRKaYSzFaztHvqyrjWkyNbN8WHkYxa7UWjbZHEMY49u1zj+KYdjeg3vK27aASCEYGshSyD57ntkwDu0/F6iCM9+yF9dMGzw/xwwgpFbn03nV59hOCXnRC9eoF+0zdb7pG0//UsmhleLF0mB+tnaMSNKkETd5Zv0Spl6YKZMhse5VFr5Jox1hZXigeomzvzX9PIPY1tefFAZebC/zJ8sd8WruBJjSG7QIvlA5xpniAw5lRBu08acO5qyX7Byuf9eqB+iMyP+0Q9yTzkpqhJ1Pkv280VtdKxHKFMLqAwzc2P1eqSxB8TCzXIbbwg7fQtOGku4kY6HJvHYxSHiARYv/NHf0gempenGEYb9oO7AbHNPqUildEst6zLbgNy9TJZZy+IjIAMpa0Oz6FjLNrC/LjRCwVrY7fl6mfaWjkMy7GAwoH6brG6GC+DyITsFZrb5myrNQ72wrYQWJQOTaY73vKlUnZTI+UsHZQ242lYrWaCPXdafjY9ULWau0dr50QMDKQJZ9+cNkD1zF3NLi8E54f4vk/2+J5t5ar3Fio4PkhX3/5CNnUk3MrNjQdWzOJenUd/aZBwp7c/F7i42nD4VBmhHF3gGWvRi1o8+baeb5QPsaoW6IVebxfvUotSJ6PlGHzXPEgZevxGgnfi1rY5o+WPmShu45EUTRT/JWpL/HSwBFGndJmJOveR3aj+PcvCu6t3fF6abcngX0iMhqm9QKx98d0ur+HlDV0bTip3I7n8f0fITBRskmj+Y8QwiaKbgExfvAxrvMdNG2ARNgtIAzPI1UTQ59iv4NqHS/YNqz+uOH5YaKu2sfNb1sbRCbGj+ZpBR/S6P6ETnCB+fo/wtQLKKVQREjl3xWNAbAsg0LW7Vu3Jool6402g6UMT4uST+JvJKk0uttGFO6EaegUsg8+A9Y0wUAvzWIa+rYEuNVLLd35EpNS4QUhtWaX1g4GiqWcy+hgru+7POVYjA3lcSyzV1O29XLL602qjU7SedRDxw9ZrbZ2bI0WQjAykCP/EBEZ1+6fyARhjBeE+EGIaRo/NUWze8HIQI5MyiaOFY61/5OzvcDVLYpWhm6c6I+0oi6hjHYVJauHnT1rlWhCYGkmpwszLHtVLjUXmO2sstBZZ9wt0Y48Pq5epxa2KVgZDmZGKds5rIeUq39YeHHIleYCzbCLrZkMO0VOFxPPp51Uhr04pB317/r+046ilSF7h27Pqt9g3W8+kWPZNyJjWy8Rx3NE3izd7u8htAJJ6qgLaDj2lxDCIQzPIeUalnkchSCO5+l6f4hhHABMpKzhB+8hMDDN4+xn6zVAu0dkpFQIsb+qwf1C9VrAN1yC++HwKdci7SZ5YyF0NOGgayk0LYWupdFEBoQCYmzjGVLmsbvWd22TciHTt89UGEvWam38bbp1ngSiKKbjBbS7QV81MrZpUC5kHjiiJBCbnUK5tM36Nt5E7a7PWq19VwdELJPr1+xsnwbThKCYSzFc2r2tdQOmqVPIuBRzKRbXGtuS8pVKk2rz7uPteAGrtVavI21rCCEYHsju6B21GzKuTS7THxGSStH1Quotj1I+jdZzEY9iydxylbV6h26vMNmxTSaG8oyWk86IeqvL0nqT5UoTIQRSSoZKWY5MDqJrgvV6m+VKi0q9jQLSjsVgMcPkcIFmx2d5vclavY0QSXRU0zTyGYdj00MYuk6l3ubmUoVOrybLMgxK+RTTI0Uc26Ta7HD++gqFjLPp6xWEMScODDNQSBPFiWbP8noTqRSlXOoushvHktnlGmv1dtJFJpII3ehAjoHC9npBD4OM4TLqFFn2agQyoha0WepWGXGLmDvUmSx7VapBa8/704XGM/kZzjfmuNRcoBV5XGstM+wU6MQBN9rLdCKfg5lhTuWnSBvOvomkPSgiFbMeNAlkRNpwyFspylZuW3E+pRShiqiGLdaDJzOQPwmUrRwlK4MpdEIVM9dZS1rQ4wBLe/DaxAfBvhAZIQSWdQKlmijl4/k/QMoaALo+gG19kZT7K2j6AFF0mTC8iGkeR8oqrfY/pd3+1wgthcAijtdQdHGcb+6/GB7g+xFdLyQIoy0Fwx4Xgigx1uv4/ZnrZVLJ4CCEjm2MYxvjuOYhJDEj2b+JY870ltzIXN593VzbZKiU6bsIM4xiVirNp8ozp+sH1JqdbTVU7oVjm4yUcw9OZHqXsJh1KRcyOxCZXkRG3rZCCKOY+ZU67R2iMY5tUMwlHUv9QhMC09QZKee4tVTdnshUW1Sbd88Ou17Iaq29Y0TG0DWGihmy6f4iKlshm7Yp7aGLp+MFrFRaSRqwR7TDKOatz25ydW6NVjfA0DWK2RRfeu4AIwNJ6uHa/DofXpzjyuwahq4TxjHPHx1nZrSE0jUuz67x0aV5FlbrKKnIpGyOzwwzXMqyWm3x5ifX+fTKIuVCmo4XEMUJERofLJDPOKxUm7z5yQ3W6208P8Q0kuv+G984g22ZLK01+Z+//xHHZoZQSrFe79DqeBRyCdEMgoirs2u89dkN2l7AsamhngmnQSwlbS/gnXO3uDa3RrPjJ/VJ5SyvnJoil3EeWDZgJxSsNNPpIT6tJz4X636Tz+o3yVvpLaMySiliJbnWWmLZq+15f7rQOJQZSfRFDId62OFya4G8lUIXGvWwsymm9mx+elsbgMeNDRktIZI2Z8nWNicbOjMrXp35zvqerAJ+2lGw0gw7BQbsLCtenYXuOleaC8x115lwB3YlM0kGIcFm7dYDYh/vGoFpnSFnHCKT/ltIWUMIDaHl0EQBTcsAOpqZwzJPAxZKtdBEnnbnXxAEnxKrJYRwcO1v47p/Cct6jv2OyCiSF2el0WG4lEXXn0woe73Wodn2+27/z6UdCvfMkk19kHL6FzH1QXa7To5lMpBPk+r5+ewW0QiCiNnlGh3v6UjDAdTb3ubsth+4lsF4Ob9jPUk/KOVSDBYzXLy5dRFf1wupNboEUURKJe2sUSRZWK3T2kavJalFyVHIunueueiaYHwwTz7jsFrdepa8Wm1Rb9xNZDp+wHpt+9SSZeqU8imyGQfLfPBXQzGbXK9+0eoGLKzWOTA+wAZ9iqKYz64uMjVS5Je/emCztT3V09pRCv703cs4tsl/9stfIJd2QCS1ZIauUW12uHhjGV0T/G/+6lcRQvH9dy5xfWGdW8uJXUSz46Npgr/x3ZewTYOffHqd987PsVRpYJk6B8YG+GvfTs5DSvjgwiz/5vsf8QuvnWDwDn8+XdN47ugYB8YHNgt6DT1xD3/t9AyObXJ1bu2uiF27G3Btfp1rc2s8e3iMV05NIaXi337/Q67MrjEykGNiqLBnn6vdMGwXeCY/wx8tfUQ3DpjrrvEnyx9zLDeBo1tY90RlIhVTDzu8X73C9fbyA+3T0gym04Mcy47zTuUyN9rLuLpF1nCRSvZatUsczow+tHngfkAXGgUrTTf26fZMIduRR95M3Re1Uij8OOB7ix/yae3GExGme1IQCKZSg7xYPMIfL39IICM+q9/iX998g/90+muMuTu7WsdKEql4UwrgYdz99q1rCUATLkpz0LQSSvkk3UwmcJvp3+lorZSBaT5LOv23cJ3VpMhXWBjGJIY+jfaIfJZaXZ+VSovBYubeJpPHhtVqi3qr/3xqPu1QyN49y9V7Xku6SO0qBiWEwDYNRso5liu779sLIm4sVHcVTnucqDc9FlbryD4FDV3HYnKkgP0QgzIknUU7DcxSKbwgolrvkrItHFsjjGPmV2q0u1tHZDQhNvVH9joT0TWN8aFCMnhvg2qjQ63V3XRLF0LQ9UPWa+1tU0uOZTLeq795mLBwLu1QLqQxDY0o3t2otNHqcmOxwivP3BZLs0yDLz93kPV6h48uzaMhOH5giOnREqah0+76dP2AUi7FxFABy7z9jvF7LeYLaw1Wqy3SzgUAbiyuowmNdidA1zVc26SUT1MuZLBNnVzaxdQ1PD+J2NZbXT67ukTcU22+Mrt2OyK4cVIChksZRgZyDNyjWrtBvNI9uwjvjuhZ1w9ZXm+SckwG8inKhQxSSUq5FFEsWa22GB/Ms98R6YzpMpMe4kxhhk/rN2mGHS43F/g3t97gdOEAM6khsmYKidx0KT5bv8m11hJSKWzN3FNB58Z3MpMe5lR+mncql6n4Ta60FknrNrGSHEkPMZ0a6stX6XHA1S2OZydoRV3W/QYrfp3fnX+bF0uHmU4NkTYcYiWphS3mOmucq8/ybuUSzahL3kzfJRL3tEEpRaQkgbwt+yFRRComknFPibhfMUvBZGqQ1wdPcLk1z2xnjVrY4sPKVQIZciQ7xrhbTtJPmkGsYvw4pBsHrPtNVvwahtD58tApxt0B0nsQXLwX+x7HS25cPWnH3nVZE10vouuv7/dh7IhG22dxrc7xmaEn0pGjgIW1OpVG/0SmmHMp35M3j5WHF97ANQ+CEgTxKl54FVMvYxljWPrQ5rKJgqrGzFiJm4uVXYlMEMUsrNSpt7oEYfRQM/T9QqXR5tZSta+IjG0mxc1DpexDf8fFnLtrhCGKY9ZqbcrFNI5tEkWS+ZWdIjKC8aE8hezeybqua0wM5cntUMfS9UMaLY9Gy+uJtSW1KOv1zrZq0q5jMjlcfOiUq2Pfvvar1dauhdm1Vperc2v4QYRUqidkqPHSiSkuz65yfWGdetPj3XO3UEpx5sj4piCiuMedW/UEqaVSSKkIo6SYWJBEwEq5FJmUnbSmWwbZlI2uJYKJGzpNcSyptz1uLFT49Moik712d88PCMP4rvtPIMhlnF0NP7eCQt0n1iiEQCnZN1nfKyzNYMjJ87WhZ/FlxNn6TSpBizdWz7LYrTKTHiJnppFKUg/bzHfXudJc5GBmhJyZYqlbY9Wv7Xm/w06Bw9kxilaGZthl2atiaQYSxdHsODOZoafG6iFjuLw6cJRFr0Iz7NIMO7yx8hnrfpOZ9BBpwyVWMdWgxa3OKhcbc7i6zcHMCABvr198wmeQdFAFMuJyr2h5w8YgUjHdOOBWZ4VIJs9lO/K43l7mByufkjVdjJ6ZoyY0UoZN2coxlR7ccj9FK8PJ3CRfGzrN2+sXkxSkX2N9pcHV1hLjbomSlcPqERkvDunEPmt+nWWvzoCd5VR+imG78FDn+whHp70+iI/vJq63uswt14ifgFWB6vkEzS3XdmzLvRNCJM7HQ/cMpkG0wHLznzOW/y8AqHXfYLX1P5Gxz1BM/QKWO3TX8qahc2SizKeXF7i5WGUnxLGk2uywvN6k0fIo7yFV8CgQS8lqtc31hUpftg6FnsR+ZpfW6X5QzKYoF9I7amhEkaTSaBOEyWw9iGIW17d3mRYbEZkHITKaYGwoSS3tdEyNjs9KpUk+6yBIIjLVRnfb+961TaZGijgPSWSEEGRTNsemh2h3AoJwZ9LcaCWkodFJSFdSx5XUAp04MMLxmWGaHY//17/6Abm0wwvHJ3sERKPdK2DOphIDPV3X0DWNwWKG4VKGgXya3/zmcwhNoHokybFNbi1VkvqHbV47a9U2S5VEwPOrLxwm7ZjYpsFbn968/3y3iIcmBf1JpG4jwhOEiW5UqretkYEs75+fo9ro0Or4SKlYrbWS+62YeWQDe8Zw+frw6U3TxEvNebpRwEfVa3xQvbp5TrrQsHWTvJni50dfpBklA/qaX9/zPtOGw5hb4nhugrP1WzTCDl4coguNI9kxplJbD5RPAlnT5YuDJ7naTlysl7pVVvw6S8sf3VHTITZVfVO6zdeGj3K6MEM1aPPO+iXUnse//UWkJLWgzb+++QaXmvN4cYAXh/gyRN5zbOtBkzfXzvPm2nkEAlPTcTQTW7eYSg3yevnEtkRGCMGAneWvTH2JjOFiiE+50JglUjHznTVm26t37W/jWdGEhi40Rt0ihrZzx1w/eMREph+/ng19w8eHaqPD9YVKX50vjwJRrLixUNm2vuFObAwKhZxL+p5BWamAMF5BqhA/uoUfzTFT+r9S7f4xYbR437ZMQ+fw1NCeCjGvzq9zdLn2xIlMpd5hca2RCM/18Y4YKWeZHC7sy77TrkUxlyKdsuh44Zaz5SiWVBodgjDa7GBptLqE0dbF0poGY4O5B4rICCHIpW0KWZeUa21Lllodj5Vqi0OTZbwg3FV/JyEyD5+KA8imHZ45OMqFGyvUdon+xVLR7PicvbJExrUZGcjS8QP+xR+8z1qttdmRNFLKMj1aAhIy9/WXjvDplQX+8W//BMs0kErxzKERvv3qMYpZl2cOjfHplQX+23/zRhLpAGbGSvz6N87sanExWEyzXGkyv1rnn/3+u2RTNvWWx0g515fisVKJM/rv/egsV+bWWau1iWNJo+3xhWdnODY9xMzYAIcny3x2dTGJNgH5jMOhiTJj5dy+18fcCUPofH34NNPpIT6sXuPT2o1NGf5IxmRMl8lUmZO5Kb5QPsZMepgLjTlK1v02HP2iYKb5Uvkk851kP45mMpEqM+qWyBhPi8hDYjDp6Ca/NP4FDmVGeWf9Ehcb86z5DbqxjxACV7cZdgocyYzxQukQx3MT5M00Z+s3MTRtM9rxpJCkkGLWggZrfgPJbXPLHddDJZpBStKJA1KGTSPauslhAwKBpRl8ffhZjuXGudJc4LP6TW62V1j167Qij0BGm1YSKd1myMkznR7iZG6KA+mRh7Y22Ccik1ycMLpBGJ4ljK6gZBsI2Y3ICJEik/6b6Hr/JmEPi1YnYHGtwUq1iWXpj1XboeuHzK3UWam2NttKd4JpaEyNFinlUve1Tie3ZUgUV4hkA03YpKyT1LtvINX929Y1jeGBDKO9SMC9XS1b4dLNFY7PDPPcsXHgybSrA1y6tcLsUrUvRV+AyeEChybK+7JvvVe4OTKQ29ZAMoollXoHP0xaxNeqG9GZ+7dnGjqlXJpCNvVA954QiZdRKZdEirYnMkktmFSKZtujuUMHFUDKMZkYLmJbD59uzaYdnjkyyh+9fYGlNbFrOrDrBbz92Q2mRgqMDCTpwDNHx2h1/M17rpRLMTFc2Pz30akhUo7F9Hpjk6iM9tY1DZ3Dk2XSrpW0Z5O8iQbyKXRNUM6neP7YOH4Qo/fCMgfGE6HBieECGdfm5MFhTEPD0LVNohSGMQP5NELAUCnDL37lFAfHB+4z/BQiIYYnD44wPlTAD5KahFzaYaycI+WYWKbBiycmmRwuJFpDAgbyaSaGCo+0o3Lj+hWtDMeyExTMDMdzE9SDNp4MkUpia2avK6XIVGoQWzc5nBnlu2Mv8Wx+mmJP92UvMLTEbdvoeTalDScRwLNzm5/1g6PZMX5l4jVeH0z8uU7kJnetEdzAV4ZOMeoWEQiGncJmOuhOJA7eyd9NoTNo53m5dJR25BH2lOtNzUjMKnu2BXkzhS50DmVG+DuHv0usJMdzE/fVm6R1m68OPsOR7Bg324sse6sseatkDJuMmUIAOTPFmJvjTGGc47kDHM9N3Ef0ptKD/NLEF3h98AQ5M8VMeviuvxuaRs5M86sTr/XVNt+OPN6pfMaoU2bCHSLT04fJGAmh3Qkb16toZXB1iwE7y3R6iErQoh118eOQWEl0oWFqBqZmkDEcilaWsp0lZ6YeuuV+X54WpUDKZXz/J3S7f0AQfgaEKNVBKb9XL6ORWBX4QIwQLprIoxvTpNxfp0+PuX1BGMVU6m0u3Voll3ZwSo+PyDRaHh9eSMLJ/aS2TEPn2PQQpdz9uhJCmBhakXZ4Dg0TxzzYU/Td+qHWNEHatZkeLTI5UuyLyMwuVbk+v8Z6vUMx56I/ZiIjewaWn15Z5MZiZdflBUnb9eRIcd8iMpAMSpPDBVYqrW2ITNxLLUW0uwHLlca23UFuTw8l7VoPJVdfyqcYHshumyZsdnxWq02kVNRb3o7CfKahkUnZDBYfXHfnTqRsk5mxAcYG8yys1mm0dyZRfhDx6eUFXjwxycGJMoWsy+unD+y4TimftK6fPjK25d8Hi5lta5sK2dR9xfMTQwUmhgqb/86kbA6MDbAdBvJpvvbi4S3/tmHw+cqp+92e78ThiTKH94lw7xUCQdZ0yZouh7O7k5IRt8iIW4Tygxl8BjJi1a8TyggBZM0ULxQPU7D2Fu2dTg8xnR7afcEt8FLpCC+VjvS1rC40Bp08g3twdB5xS/yVqS9v+3fXsHlpINn/u5Wz/LvZW6z7NcbcckJkhCBtOJzIT5A1TV4qnWLAymHfE7EYc0uMuaUdjl0nazh8begZgt71zhjbNxY0wjZ5S+dAeozDmUkKVv/aVnfC0S1GdIsRp7j7wvuIfcrpSDz/J3S6v4Pn/wCQGPoBdH0MIWwM4wCmeQxdn0bTcoCBro1g218mk/4tNH37l8WjQscLeffsrfuEzB4llFJUGh3e+OAq9T5IBIBl6ByfGb6v0BfAEBnS1inq3R8Qyho5+7VeAEyyUyTs4HiZYzP9vQhavTbRTy4vbNZ/PE5EcUy12eWTSwvMLtV2XT5xrM4zMVSglN8/UbGUk5Cj7WbKcS8ic5vItLZNXaacZJB/2FqUgVx6RzG9VidgpZooDjfa/o4RmUzKZiCfxrXNfTEJ1XWNtGNy8tBIYsGwC2KpWK21+fTKIpdurmzWmHyOnw0opaiHbd5au0g97GAInaKZ5tn8NHnzp8M5/HHiheJxfmn8q4y55ftITL9QKFb9Gldbc9xoL+y4bM5M85uT3+Kl0skHJjFPEvsUv5QE4UdIuY5tv0I283fRtBLd7n+g0/0PpFO/gWmeQIgMUq7jeW8QBO+hVBfTmEbw8AWZe0XHC3jv3CyvnJpmerRIuk9J9YfBer3D5VurXLyxTKfPtFIh53JosrxlLYWpDzKQ/mXy7tfQtSyWNogQJoOZv4a2g0/V1GiR4zPDFDIuzY63a2To+kKFP333MicODDNYzGA8Ru2d1WqL//jDsyytbR/huBOWafDqM1N3zaz3A65jMTlcwN5Gk+Z2jUyMlIlqbLTN8aZcK9FMeciUZqmQYnhge1+aVtdnrdpCxopGu0ur422/rT1qv/QDTdN44dgkV26tcfHGSl/ljx9enNusE8lnnLtMUj/HTy9akcd8Z53zjVk6kce4W+aZwjSObnG/a9FfDGwlXLqfkEpxo71ALWgyYOcTgb9Htrcni32rkYmjWUDHMk9jW68ihEsQvI8QKXR9BNM4iq4PoVS3pzPTJZYrBOFn6PoU8HhZeRRL1motPro4x8hAhuePTz6yfSmVFFpdvLnMO2dv7VqrsIFCNsWJmWEG8uktRd00zcYSY+haFqk8op6asiYsNG3765nppZdeODHBe+dmabS3H+AA6s0uF24s86OPrvH66QOMDeYfaSHiBir1NueuLfPGB1epNHYuOIMkDVcupHn++CTD5f2dVaTspDXZ2iaKEm1GZGL8IGK50tw2IpN2LGbGSg8dkSlmUwwVM9t6LoVhTLPj0er6VOsdWp3tyXMhl7qvK+5hIQRMjhQ4OjXI2auLzC7Xdl1nrdbm40vzfO8nF/j6y0d6xPkpsl//HHvChnrrtfYSH1Sv0gg7CJIaj+cKBzH3oWOl3+MIZcT55nUqfp0hp8Ridw0v9kkZLgczYww7ZVzdphv7zHaWCGSEHwd0Yo9a0CRWMePuEGcKRzE0g0CGrPoVrjRn6cQ+UknShstkaohxd4jUHXUtjbDNXGeZG+0FYhWTMzMseet3vUe7sc+aX+NGe4FKUMfVHZ4rHKVgZu/yn4pVzKpX5UZnkXW/RqTixE5Fd3ixdBxXd6gEdS40bvBR9SKtqEvBzFDx6wghOJAe50QuSdtWwwbz3VXmOsu0oi7HstPMpMfImbej2Ukrd8i7lbNoQielOyx0V4lURM5MM+4OcTA9gaHpeLFPJWhwuXkLX4YEMqQTdTE1g1G3zGRqhHH3wdKCO2HfiIxUTTSRRtenEMJOVH0xEMJEqTaKiEQgL4VtvUAYnqPrfR/P+2Ns+0voPP70kpSKDy7MUcqnmBwpUci6+/7SVD1Ni6W1Bh+cn+PDi3N9rZfoXmR5+eQ0acfc8mGPpUcQLxJEi0jVRt2hKmmb0xjW1iF9TUuMAb/64mFuLFRoezt7F0WxZLXS5I9+coFc2iHtWg/UbdMvlFJ0/ZCLN1d469MbXJld62u9fMbh+MwQR6YG71NBflg4lsloOUfKTsTi7i1ejWXSkeL5IUHP3mGrSJfWq52Y2ge9lmzKZiCfIuVYm/5hd2JDqG+93mGt3t6xRqaYSzG4B8+nfpB0VzkcmxnizNFxFtcbRNHOUbU4ltxYqPAffvgZhazLmaPjPWuNJxuZkVIRhImtCEKQss27nMU/B0Qy3ux42SiuTpSB27y7fpn3KldQKIpWhsOZUY7lJh6br5JCEciQ9yrnuNy8xculU8x1lmlEbSzNYNlb48uDLzLqDuDFPhcaN1j2KkQqQkNjLagmxEaGnMofwkBn2VvnQuM6F5o36EY+sYqxdYu5zhJfGXqRcc3cVCq+0V7g49olrrRmMYVOycrTiX38ONzsJIpkRC1ocKU1y6e1yxhCZ9Qpk9Kdu4jMut/gYvMmH9Uu0oo6SaRaQNZIczw3jSEM6kGLy61b3Oos4cUBjbCVdFwBrm5zPDeDQNCOPBa6q3xUu8TZ+lV+YeR1SlbuHiKj6MY+3196G0u3mE6NMt9doRN7WJrJdGqUEadMWrhUgyaf1a/yaf0KKEUn8pjrriCRvFw6SdbIPM1EBjbaqMWdEtPCRAgLKau9It/b0PVxdK2AH7ydKPo+IcwuV3nr05sM5NN8+7XjO6qlPiiCIOZ7b17g7c9usl7rTztmo9bj5Wem7uuI2IAfzbLY+CeE8VrPSPL24F1wv0ra2r4obyCf4gvPTvPDD65SbXR2Lfz1gojPriyS75G9b75ytK/zeBBIqbi1VOVP3rnM99/uX1xqcrjAL7x+gtxD+ARtB8PQEvPIjIttGfd5HCmVODm3uj6tjs9atb0lObQtg3zGZaCYfugw70bx9vhQgdltfJeiKCGhK5XWLi7c+x+R2cCx6SGaHZ8ffniVVsffNZXpBRG3Fqv8j7/7Nr/6jdP83KvHnnj7fxhFLKzW+ezqIoamc/zgMAfHH//k62lGJ/ZZ6laph200oSGV7GmUXOBc/damV9OLpcM8U3gytTHtqEsn9sibWb468yJSKT6qXeC/v/Y7TKZGKN5RH3K5eZMxd4hfHv8aw06JUEZovdZrgPeq55jrLPMLo19k1CkjEJxvXOOfXPtfGHEHSRspylYeheLHax/RjDp8Z+SLnMofpBY0+cHq+7xfOU8ok+aBjJHiZP4Qx3MH+I/mG5ytX9vyHD6sXeBC4zq2ZvGbB36OopUllDGduMuAVcAQOsdyM8ykx/jtuT+lG/scykzwevkMGgJNaJtdXuPuIKNOmWdyh/hHl/8ttrZNPY6CZtTBUTEjzgC/MflN2pHH95be5P3KeV4deIbx1BA3O4v8ZP0Tvlg+w8ncQaSSfFy7zCe1y5zIHeR4bmb/vsw7sG8WBZrII9U6sbzdWaKJNEKkCcOLWNZr96wjUSpCqd1btB8llIJbixV+70fnCCPJa6dnmB4t7ku4U0rF3EqNH35wlTc+uMr8Sq3vdY/NDPHs4THyGXezPfT+Y/cJ42XK6V/DNqcRdxgumLsUUG/Ip//86yfoBiFvfnxj1+JKqRSfXVlExpJm2+dLzx+kmHUfqvPmXlQaHc5dW+J7Pz7PJ1cW+jatnBwpcOboOCcPjuBYW0ewHgaJeJpguJQll7a3NWucX20kLsjR1joSpXzidr0fzrBCCFKuxcxokZVKc8tjCuOYlWqTtVprR5XhQtZ9ZI7LrmNxcHyAX/n6af74rYssrO4uqBbFkqVKk//4w7PcWqrypTMHOXlohHzGRdtOxW4fEcWSerPL3EqNS7dWuTq7xtxyjVbX58zRcSZGCo/8GH7aUAmavFO5xHuVK3TjxEcukBGVoEkr6pLSLabTQ3x16FmOZsefiJSDQpE10pzKHyRnphOjTqdM2S5QC5vUwiYpPZnMlu0ik6lhRpwBbN3a9A1SKJpRhxWvwqe1K8kAr1kIIWiGbQIZUguadKIusZmlGbapBS1yZorjuRlShoupGYy5g+TNzGZUSgiB3hMi1IW+bUv5XCfxvHq5dJKilcXRbCxN4ehWsl5vO6Zm9LalYWg6lmbc1w6eCPsl7eSa2LmJXZEQn5P5A9iahWHqFMwcjm7TjDqEMiKWMUEcYgkDSzOTVn7d3AhzPLII3L4RGV0fJZYrxNGtHjnR0PQiuj5MEJ7DDN5FCAtdG0CqLmF4njheRNNydw3ATwKtbsCV2VV0TdDseDx3dJwD4wMUsu4DhbRjKWl1fK7NrfPBhTl+9NFVri9W7vJa2Q5CJPL6Z46O88zh0b4MD13zMCnrJGIPN8mGFskzh0a5tVRlaa3J1bndUzjVRofPri7S9UOabY8TB4eZGiluFoo+yMtJ9gTRZperXLi+zPvnZ/nwwhy1VndX8buN6/Xc0QleOjnZk+N/NBAicSfOZVyWK1trM1ybW8Pzo21JYTmfZmRg/1I4KdtkerTEx5e37krwew7Mq9UW/hZt40IkbtX5rEN6m8jfw8LQE6Xdr790hPmVek9hePeaJz+IuDa/TqPtUWt2ubawzoHxAcbKeYZKGRzbfOhUsFKqJ2AY0Gh7VBtdas0O1UaH1VqbpbUGN5eqzC/XqDW7mzYf0TZE9S8yIhnTjLrMd9dY95ubBoop3WbcHeBAZpgXi4c5mZukYD4a0twPLM2gZOUwhI4mNFzdJmW4BDIkiMNNIpMxU+TNDK6xEeFN3sWxkgRxiBcH6EJnKjWySTqG7BKHMpMcSI+R0h0kEl+GhDLEFMZmR5De26ejW3t6bwO0oiR6PuQMYArjLgL0qJEzMwzaSdu3oRnYuomlmYQysRcpWDkmUsNcas7SiroYQmfJX2ciNUTJ7r+Nfa/YNyJjmoeJoivE8SoKD4GJro1gGkfwvD+h2/0eMq5gmIeQsobn/YAoXsS2TiPE/qdztoOhayjFfR0wQZholSyuNbg6t8bXXjzM4clB8hkH2zKweq66iS/LBoPekCJP2oTDKMYPkvTCzcUKf/7eZd47N8vSerPv4zMNncmRIs8dHefQxC6ha6GjizRBvIweZRDidkpF11IY2s43jqYJSvkUL56YoN7ssrTewPPDXUP/9ZbHRxfnubFQ4eVTU3zh2RlOHRzBdczkWhk6hqGja1rPC+c2uZFSIaUkiiVBFG+2K88u1fjJJ9d599wsN/vQi9mAZRrMjA3w2ukZTh3eWk9kv6AJGB7Ikc9sf79emV3bsd5ooJBmeD+JjGMxPbp94XDXDzl3bZlKvbNl15emaQwVM+TSzr5G1rY6zhMHhnnt9Aytjs+HF2bxd/FgguR+Wam0+NPKZd4/P8uxmWGeOzrOqUMjlAuJm7tp6L1nU0PTRK+AMqnS2KhR23hOEw+m5P6LYpl4Mfkhy5UWc8s1bi5WuL5QYW65RqO1vZ3D57gfKcNh0i1zIjfJqt/AjxOPq7Kd52h2jNOFA7xYOpTI1D9BX6VIxbSiDjkzs6lkG8gQXeh3CfMlnkNbTyQ3xN0mU8P80thXyRgpDE1H9vyMdKGjIXq/JxGWWEm6sY+tWUglkwiGitlrRsLsFRq3osRDSe8VVCsle7Wpt9+5iVCk2jdXbl3omNq97xq1aXI2aBc5mp3iPy78kNnOEnkzMYz82tCLTDyC2pgN7BOR0bDM55GyhpQVlApASAxjCtv+El3vDwmjCwThJ73iX4lSPpZ1hpT7K2haYX8Oow+UC2nCSG7rc1Spd/jJJzf4+OI8Y4N5js0McXxmmOnREoPFTCILf8fMVSqFH0Ss1RJDw0s3Vzl7dZHLt1aptz2CPlMjGyhkXX7jm2c4Ot2PoaVCEbHQ+O/QtSymPojoSQPlnNcpp3+pr30enhwklopby1U+ujhPrQ+NG6kUtWaXH7x/hffPz1IupHn28BhHpgaZGikyXM5SyLi9NM/t9YIootHyWK21mV2qcvnWKhduLHNrsUqzs31KZitoQlAuZPhPv/sip4+MPXQX0G4QQjAykCW3A5FZXm/s+FoaKKQZKW/fMr1XpHodUNu1cvtBxI3Fyrat67omGB3MP5LasK3wtZcOI6Wi2mhzZW59TzYhjbbPRxfnOHd1EdPQGSxmGC3nGB3MM1jIJFEl18I2DXRNI5Iy8TgKYoIowg8iOj0zzbVam9Vai0q9s0lYolgS9whOHMu+zEk/x20M23m+OXyGLw2eQqrbDju6EBiankQPnoIG4HrQ4uPaZV4sngABy36Fxe4aeTND3sz2yMX20BDkrQwlK8+KX+HHax/zWvk0JStHIENW/AolK4+rO+hCp2BlyRguzajDpcZNjudmaERtlrx1KkFjW5mG7TDmDHK1Nce7658x7BQxNINIxnRjD1e3MbXb19nWTGqySfMxOXJ7sUcz6nAkO8krpWc4mp1CILB1a7Pw+VFg3yIyhjGNK34Bpfye6J0OGBjGAbKZv4/v/4QouoyULTQtjWEcwrJexLKe78spe7/wlRcOI6XkrU9vsLDW2LLTww+Sl54XRFQaHc5fXyaTsnHtxOzNMnX0OyI7YRj3vGwCaq0u1UaHRtvbs4Pt2GCeV5+Z5rljExRz7q6zFlMboOB+k1i1ERhowmFDKcA2+m8nNw2dqZEif+XnnkfXND66ON+XoeVGV0wQxrQ6Po22x2dXF++6VqZ5OzITx5IwSgaXjetVb3WpNbu0u8GeBg5NCI5OD/HNV47w7OFR8hlnX+pOdkJCZHLk09t3RO00g9c1wUA+va96LZapUy6myaUdTEMn3III7mTroGsaY+Uc2cdEZFKOxQvHJ4il5N//2afMLlVp96GpBEn0MwjjTTftjh+yWmtxZW4N1zaxTB1D1zfdrJVSxL0IYCxVj6zEvec1ouuH+EG05TX7HHuHrum4mo77BHTB+oWGhkSyHtT5X+b/jE7cJZIxXx96kanUKK5u09rFWwiRbOf54jFszeRy6xaznaXenxLS9s3hV5hwhxMjU6Hz8sApzjeu88fLb/PW+qdkzTS1sMlEahirF+FY9iq8XzlHPWxxrnGNZa/C95bepGwXOJyZ4nhuhpKV40zhCKamc7k5y7+++Ue9954gpTt8a+QVBu0iWs+9+mh2mkbY5uPaJRa7a9i6ybP5w7xYOgnAh9UL3GgvsOxVWeiuEivJil9h2BngdOFoIsi3XQHwFlCAHwdcby1QD1u8VzkHJFYHpwtHOZpNtIP2G/tCZBIr+lyPwNwNjTyO/TV0fYgouomSLYSWxjAOYuhT6I9Z1Xd6pMhgKYOua/zJO5eoNbvbvui7fkjXD1lca9z1+YbL7oY+zF4Jy1Yo5lKcOTLG1186wuhgri/jPl3Lk3NeI5YNYtVGqRBNpDG0LMYer2s2ZXPm6BjVRgch4IOeK28/Z7ZBaBZWGyys3n+tND3x4oil3JdrpWmCI5ODfPn5g3zlhUOUC4+nPVcIKGZd8hkHy9Q3B9T+1k1akYtZl7S7fw+yrmukbItyIU3GtfqynbgTmiYYLefI7YNLeF/7E8n+Xjs9Q9cP+dGH17gyu0q9tffOxY0JR7Wxt3P+HP1h8dYazWqHofEi6ZyL2UfEU8aSZr2D7Vo4+3if7ycc3WbcHeRme5FYxQzaaU7lDzHslDC1pEh1MjVMzkwz7Nz/Ht2Idsykx7A0E01o1MPmZnoqa6Z70acNHyI4nptBFxrnGzeIVYyr2xStLNOpEcZTgzi6hRcHvbFUMJkaYdgZSCIswrwrkjWRGt48jmVvnVDFaGi9Yt/bHUkagsnUMI1eNCaQ4X0ifKL3P1e3eb50HEsYOLp9l6O7EAJLN3m5dJID6bvT96NOmTOFo5TtIpGMWfNr1IImU6mRTVfrWElutBfJGmmKVo7J1N2+UPuBfY/F3y5y3CAHCiFMLPMMlnlmi+VjkrbtxxNy9IKQ6dESY4M5FlbrnLu2TLXR2VM0QCqF3KdZnBACy9Q5eXCYLz1/kFee2dmb5U5owsLUB4hklTBeR6oOpj6IpQ2ja3srphNCoGsaX3vpCJZpEIYxH1yYo+MFD1UnkFyr/QnRC5IIRCHr8u0vHOPLLxzadEN+HBBC4Ngm+axLPuP25V6+AV0TjA3me11o+1uLIoRgtOekvVcio+saI48xIrOxz6FSlt/81nM9SwTB2auLeH70eTrnKcKF929w9bNZXvv508wcH+uLyIRhzPXz84xMlRmZfPra0yWSvJnh9fIZXi/fPx4BpA2X54vHd92Wq9scykxwKDOx67IlK8+rA8/y6sCz2y6TM+E7o1/cdVua0DiQGedAZnzH5YQQ5MwMr5VP81r59JbLPFc8xnPFY7vuM224/CfT37nv85P5g5zMHwTgZnuRa+151vwq//nhX6ds5REIOrHHv5//cxpRi7nO8k8HkUmCSyFSdoAAxW75Pw1dKwGPR1wqEQ6TTAwV+S9+/Yv8m+99wE8+uc56ffcuikeBjUjIr33jDM8e3pubbBivUvPeoBMk4TtduNS9H+OaR8jZr5J1Xtrz8eia4IXjEz1Dwhw//PDqfRGpJwXHNjk8Weav/vwLnDo4sqX/1ONAIeMyVMrsicgYusbEcGHHQuEHhSZgrJzbs0ihEALbNBgqZfc1StTXvkmuyTdePsrYYJ433r/C99++RL29e6fa53h64XcCfvz7H/Pqt555KonM53h08GVAKEN0Te/ZTiTRmG7ksepVKVpZ0sajEVLdJ/drhZRrBOHHBMHHxHIVVIBi94psIdLkMn8Pw9id1e4Hun5IEMbYtsH0SJHvfPEkxVyKNz++zq2l6mPLlwsBUyNFnj8+yddePMzxmSEyKXtP0v+RrNP03iHrvIytT6AJi0jWaPrv4UW3yLI3IrMRFUu7NgfHB/jOF08wWEzz7rlbnLu6RNsLntggc2CsxJmj47x0cornj02QSzt9taY/CuSzLsOlLGevLvW9jt4jMrl9VhyGjYhMfs9ExrENBosZ0o712G0ANu61DTVm1zYZHsjywYU5Lt9aZa1P4cjHCU1LFH3L+fRdBf8/K7h5cZEbFxZYX6rjpC1uXVoiuiN9evPiIlc+naXd6BKFMemcy9hMmWMvzBB4IQs31jj/3jXOv3+dKIhZnl3HMA1e/dYz5MsZaqtNPn7zMu1Gh8ALMUyDwfEix56bpji0fwXwn+PJoGwXGHXLzHdX+L2FH2JqJqCIZIylGcykxx6Jqi/sq2nkB3S6v0/gv4MiRggT2L1KXWj5x6rs2/Uj/DBRaLQtgzNHx8j2ilPfP5+0/u5UN/OwSFyBEwPCF05M8trpGc4cHd8siN0LpPLw4zlGrN8ibZ3a/Lzpf0gsdxcd2w5CJEWZJw+ObAql5TMu1+fXE4XYrr8vtS67wbENitkUwwNZXjg+wSunpjlxcBjbNJ5o+2Yh4+7oOr0VdE1jcujRRGSEEIwO5MjvkcikHYvxoTyW+Xj8brZDPuNy8qDNaDnH8ECW4VKWizdXWK22aLS9PdUi7Tc2LCWKOZdyIcPoYI5Th0YfqV7R44aSCt8LufzpLJc+uonjWqQ8h2b1bjIZeCHNaptmrUMYRCzeXGPhxioHTo4TxxK/G9Csdmg3PJq1No1KG9M2iHuTwyiKadaS9f1OQOCFXPr4FgMjeQqD2Ud6DwoEuqZzMD1BN955vFG9VmLuaRMPpYdCYggLgU6sAgLZIZAdUnoRQ7MBRSi76MJCEwbaNp06927r3nOXKiaSHp24SkovYWjOfWJ2TxsKZo4D6XHqYZvl7jrtqJvolQmDo9lpDmcmGXhEWjL7RGRiOt3/gO+/icDBcb6Brg2iaWnYRexOCAdNK+7PYfQBLwjvejGahs6xmSGmRoucPDjCH755ng/Oz1JtdohjtW85e00INE2QzzgcHB/gV79xhjNHxh5Sel0g0FEqRKpe54dKfDfYp5t+bDARH3vpxBQ/eP8KP/jgKpdvrdLx/H29PhsQgk09kLHBPC+emOTbr53g4FiJzGMqSN0NhazDUCm76SfTDwxDY2zo0bQ5CwFDpSyFbApd1/puaU45FhNDBaynwGF6QzTv5187wYsnJvn0yiJ/9u5lzl5bYqXSRPa6jnZTn35YCHH7WdU0Ddc2OTI1yEsnJ3nh+CTHDwxj6Noj75B7nIhjSXWlwezlJRzX4rf+618EBb/9j/+UCx/cABKhh5kTY0wcHt5sdPiT//kdfvA77xP6EYXBLKdeOcTIVJlP377Caz9/mm/+xiub+1BKURrK8a3feBVNT67dtbNz/IP/6p/z1V9+4bGcp6NZ/OWxL20ez/1Qvf8qIhWgCwNN6WwUx3ajKrEKSRkDmJpDJ65R9W9SC24xmX6ZrDmMUjG1YJ6MUcbWs5tE5jY5AhD3bMvd/JNCJo0RKqQZLXOj9Q4zmS+QN8fQxNMdBdSEYCY9xkz60ep5bYV9q5GRcR1dG8O2v0w6/ZsI4fQ0TXZ74LUtu50eFYIgIoju13axTZ1nDo0wMpDlqy8e5uOL83x4aY75lfqOHjX9QNcFM6MDPHNolNNHxjh5aISBXIrUQ9Yl6FqGlHWK9fZ/pO79CE2k8KN5hDCw9f27mXRNo5hL8Y1XjnL66BjX59f59MoiZ68uMb9S69vNezck9RqZTXuGQxNlxobyPT2aR6sRsxcUsi7DA9ltXafvhWMlKZyNFulHAcPQKGZdBgvpvgUY025CZMwnlKLbDvmMywvHJzg0UWZhtc61uTXOX1/m0q1VVipNOt7uCtkPAtsyKOZSTI8UmR4tMj1aYnKkwEA+Qy5tk3ZtDF17CpRQ9hdSSqprTSzbxM3YSXRAQLaYpjCYRB7jSHLhgxt89vZVojBC0zWunZ3D74YJcd7lOZBSsjS7zg//40dEQYQQUFtr0qonaSql1COPCiok7WidSHrYehZbT85NqpBYhnhxA00zkCriVvs9itYUJWuKlDFArEKinl+gJnQC2WG5e5ZmuMKgcxTXyCPQCFWXWAWJMN0d0Rg/buDLNgpJSi8RqQCQ6L3upsTUsk0rWsHWsthaGlOk0DFQKu6VaXyO7bCPyr7HCMNLgIcmcmhavpdeeroQRPGWDryappFNO6Rci2IuxXApw/EDwyyu1Vlab7JabVFrdmm2PdrdgG4QEkWJcJZCoWsahqHhWCYpxyLbC0cPFNIMFTOMDxWYHC70Ihz7o+xq6gMU3Z+jG14ili0UMZY+jGsexjWP7Ms+4HZnVbmQppBxGCwk53P6yBhL6w1WKi3Wa22qzS7NTnJ9Ot2AMIoTcbFeGkrXBYamYRo6KSe5TpmUnZgo5tOUC2mGSonI2fhQgVLP2flpg2OZHJ0a5L/8jS/2RWRMQ6eYTc7xUfgEbbR4PndsHNPQabTvD52rnuiWuKNjarCY4VTPm2on1NebScdW2sa09p7WazeSVEIq62I65q4Kwqahk8+4m/fF+FCew5ODLFeS53DjXqs1uzQ7Ph0voOsF+GFE2HsmY5nMbDVNYOhaovyrJ/eeZRqkXYuMa5NOJf+fTdkUsi6lfIqBfDr5KaQoZlNYlnFfBCby3yYKPkCpJkIbwLBewrC27oLZDyRCmc9Rb+2e9s6lHY5MDZLtM4IphMB2jF56KNwkJVEQE3rJpG99qcb8tRXq602eefVw77M6K3NVYCMRk/zEUSKzcCc5aVTazF9bYfbyEs+8eohcMc389dVkOake2G5PyTZR8C5R8M62y2j6EJr5DLE2QyDbNMMVvO4FpjKvoAmNZrhM1b+FY+RJ90hLPZgnpReRdxCIdlRBqghXLyTWA3GbSPo4WgZdJP5ukfSoBrM4eq4XaZG04wqNYIlYBTh6Dlcv0onWiVVEyhhAEwaxCglkGy9usu5dp2TP4Oq5TWXeeyNIUfARoffHgELTh9CNUxj2yw92EX8GsG/Kvrb9RZTqEEU38Pw/R9eHe0J3em8Gs93LT8cwDqFpj6aa+V5sDK7bQdc0ClmXQtbl1KFRmm2P5UqTW4tVlitN1mttaq0urY5PEMZEUYxUyYzYMg3SjkU+41DKpxgp55gYKjAxXCDtWPsuAa+LHFn7BSxjmDBaQSovab82RtHFo+noMQydgUKagUKa00fG6HgBa7U2c8s1ltYbVOodas0u9ZaHH4QJcexdb0NPrpFtGuTSNrmMQymfEL2xwTzDpSyZlH3XYO8HEWuNNuuNDn6Q1DYZhsbMSIn8Y2wZvhO6rjE5UuS3fvHVLf8upaTjh1xdWL8rjXlxbnXL5VOOycRggbRjYui3Z3FhEBH3Zqu2ayE0QRxJAj/E6NW1KHV7IDg8Xub4TNLa6HcDlFQITaDpGrWVOkols2w7lZBDJRUyloR+SKgUhmWglMLvBCCS71ozdOprTXRDx7QNTMsgCmNkLHuy/wrTMlAKQj9EN/Tej7a5rbX5Kq1ah9EDg2QNbU/PQdq1SLsWk8NJ+rndDajU2yyuN1iutKjU2zRaHo22R6dHZqIoUecVve/KNHQsQ8c0dWzTwHFMihmXQi5FMetSzKUo5VIUsm7f6cso+AC//T+g5CKacRghnEdKZPIZl1/66jOPZNuaJiiUs9iOQW29zZVPZzFMncpKnTCIUCg6LY9WvUMcxQxNlBACCuUslmOy+W4XoOka6ZxDbbXB1U/nsFyT4YkSgRfSqndoN7rkBzIMjZfwuyFOykJ7iPeiUl2i4F381j8B4t7P3e93zXwG043wTRcvjmmGyyx3LzCSOoUuTFrhGuv+Dcb0Z9GFhVQxCoWpORg92xdNaASyTSi7hMrDEA66MDE0B0vPoKEj0JDEtKM1/LhFyhhAElPxbxDEbSwthSFsNKHjb2xLepiaSyg7dOM6ftxiqXsOQ7Nx9e0zFXH4GX7rHwIS3TiFmfq1z4nMw0PDsV9Hyhrt9r+kUv2v0LRCLyqT2jHFpIkchcJ/g6Yd2p9D2QVS7a2uI5OyyaRsDk6U75g1qB0nEOKOB/vRBkslEh9bH8fWJ9k4QKm6SBWgi0dPDl3bZHI4iTapPq8PcBe53WmCv9Zo83tvneP337rA/God09Ap5VL8n/7Gt3jtVP+aO48TQRRzdWGd//qf/H5fLdpHJsr8/V//Miemhijc0dXUrLZpVlrEsWTi8AimbdBpdlmZXSc/kEU3knqY0I+QUpIrZciVkpqr6nKdwAswbRMnbfPe9z8jCmNOvXqYyWOjKJLiTb8TUFmuURrKUxjKEUcxi9dX0HSNbClDKusQeCFOSmD00mJe26Pb8nr7VRSHckipWJ1bJ51LkRvI4GYclFIsz65z69IifscnP5gllXMfSmghieQlkwMFfT+TcLcM2OZvP2t5ogeApmuUhvNMHR1l6fuf8f/5P/+7zS6igZEcAsHgWIEb5xe4cWGRf/4Pfp9sIYXX8SmPFtD12xfRcgyOPTfNZ29f5d0/O8fwRIm//r/9BUpDOYqDOZrVNr//T39MOu+i6xrlkQJO6mGirjpC5NH0UZRqomQL8Lk3xCOJaYZLtOKAdrRKqLqoXj2hpbkMOkcYdI7g6gVa0Sq2niFjDuHoSXGqQMfSUvT8orH1dJKaEoKceVs2wxA2jp5HE8nQGqmAejDPsHuCojWF1dP3su7R+erGNerBPO1oHV+2kGpv1jZ/0bFvxb7tzr+j2/0DwugcYCQ1MsIC9M2w41ZQ4pGP9vfskD2FMTfNtzb/c9cvTxRedJ3l5j9jOPtbuOaBzc/XW/8BQ8tTSt8vYLTfuDPNIB7F9en5kUkpNw3/ko6pp11sRG0e826Q6u7Qut8JWLi+wuL1FWQsKY8XUUpx/bM51peSUP7lD24QxzFO2iY/kGXu8hIHnplk5tQEq3PrrMyuoxsapeEChmngdXxQoBk6CEFjrcmtCwvcujjP+OER8uUscSRp17uEfsStiwtIKfnqr73K4vUVbNciW8yQLaW59OENaqsN3LTD3OUlBseLZEsZvLbP6nyFw89NMzhe4tbFRVCKtfkKXsfn+EsPP1m5637b/M9dv3yOPWLjmp548QBjM4N0234ififAsgyKQzks2+TVbz3LkdNTaLqOYWrIOIn4ZQqpTXNYJ2Xz9V99mVe+9QxxFGM5FqXhPKZlcOiZSf7z/8uvoesauqEjBPheyPiBQcQDplyFlsFyv4NhPYciAhWh5DpxeBa/8y9AJRMJpRSduE4gk+nfPVu5y2QxWV6y4l0CBEVrgm5coxbM0Y4qxCpkPHWml3a6va4ft2iES1T8Gz2vKUnOHKPsHKHq32Ldv07aKFG0pnrbWiNWEeOpM3hxAy9u9ARiFX7coh4uUvFvIoSGVDFl+yCaeLIdm08r9q392vffIYpvYugz2KnX0LRCz425j64lUdifw+gDaq9M5ilGLNt0gvNItdEm2Xugojmk/vha2j/H3dA0jXza5fVnDrBebxOEiZ9PEEn8MKTeTkwzt0PgBcxdXkQ3dIpDeYpDeXRDY32xSrPaZubkOGtzFerrTWQsmTkxzuzlRbyOTxRE2K6F1/HxOwECwfBUGcs20U2dfDmLpglCPyTo6QIVBnOkMg6BH7I6V6GyXGfxxioohWZoxFFMGEREYQQK6mtNgm7AyFSZm+djlm6uUVtrYtoGzUqLbtOjsd7i5vl5jr90kHw5i9o6q/Y5niIUylkK5e3r9wbHiwyO79xhapg6ozPlLf+WK6bJFfc35S2EiTAm0Ta95RQyXiEhGNZmlFgTOnlzjIxwycsxBqwZHD2LJgwy5iCOymFqSara0lKMus+ga1YvtSTQhEnRniZlDODqBXRhUrAm7oqcaELH1fOMpp7F0ZJiYlNzKFoTCJKWa0tPo2s2RXuKlFHE1YvowiRlDAACpSQZY5CsOYytZxlLncbV89h65vPw4Q7Yv64lVUMIF9t+jVz2f9eLyDzdfe8//RB3uIknVg9sihB+ftM/KRiaRjmf5le+eCqp3fBDOn5A14+otbpcml1ltXZt2/WlVHRbHkOTAxQGs7jppJMkjiSarpEppMkWMzRrHdyMw8ypCZZn17FdK2lzHS5gWibrC1VQ4KTtzW4U4w7NGDfjMH5omLGDw6RzLuuLNRZvrNKotGjV2liOSRxJEIIwiGg3uuQGsigpyRTTTJ8YZ+nmGuuLVbotjyg0cDI2lmOilCIMQpy0TbaYIfCiz2/Jz/EYIAAdxB21O4AmDIbcowiRpCTv1GSx9bslMGw9w3TmFaRKIi6a0HD1PBOp5+5aztHvJn2m5lK0pyjaU/d87pAyinckPwUZ426y5+hZCuZEr/36tmVPyX460+dPG/ava8k4jsDqEZi7b6LP8aigUMTEqoFUXYSwiWUDqXz4vF3viUGIRGzuxPRwEv/rpY4Uinrb4wcfXeWNT7YnMnbK4vBzM3z640ucfesKI9NlXvn5MwxOFLl1cZE3fvsd6mtNlErqU+4MNQd+SH29ReAFeJ0gKcqVChkrVufX+fiH53nxG3cUjd4p+OWHVFbqyDgpMA79iLX5CrOXFuk2PUI/JD+QIe4V0m6sO3pwiNCPmL28iO2Y2K7FwGiBmZMTfPrjiyzdXMN2LU69tn+ddJ/jczwYdpNovXPJ/Z6I777n/d/nXwzsY9fSyyjVIQw/o9H4B0nXkpZDYO0oziaEhW29+li1ZH5WYOqDlNzvUO38CdXOHyMwiVQDWx8nZZ580of3FxYbxGIrzRg/jHfVkjFtczMdFPghTsrGSdkMTw+SKaTpNr2ETAhI5VJomsaxFw+iGxpOyiYqxqTSNgefmUQ3DUzb4NkvHqPb9nBSNqZtUBjK4WYcZCyxe3pG+cEcL3/rWaSUSVpKE5RGCnz5l19KJOnzKdysy+kvHkc3tE1yohv6ZopLNzQKQ3mclM2xFw4wdnAYv+NjWAYDI/m+jAc/x+d4VNhLfcn+1qLsTqA+r315cOzTW0URRbeI4quE4SXC8AKaXkYTGcDqzdy261rKYBrHPicyDwBDK5B3vwRdSRgnRQimKJGxn8MxH08X2OfYf+i6Rjrnkso6m5EcIQSmnSKdczfbqu988ZXH7q5dSGUdhBCbra3D0+UkMiOT9JRhGrj3tK+7aZvxw8MotSEO3fPeOnm3D9rw9O2w+PBU8rtSivJ4MXld9wo/i8N5CkP5u7b1KNGOOtTCOq2ozYH0FJZ2fzdMN/ZY9deo+BUMzWTAKjLsDP9MKfV+js/xFw37WOz7Zk8QjyTdES8Ts7zrmppWeKxeSz9L0LUUjnYQxzyAlJ2k5VrL9rw7Pg9R/rRjQ2H1zrmcEAKh7z7o6ltEfYSW+M3sts8HGdOT9cQWn+19Ww+KetjgQuMSc90FRpyhbYhMl2ut63xSP4ut2ZzIHWPIGWS3poSHRVLDFqBkFaUSZWKBgdAKIJy7VGCVbKNUC6U8hHARIg0iBUhQXZTq3mHKm2wpWd9CCBeEjRD9v9oTsbUQlJe8i1V497bRQGxs24EtvIFAoWQbKSuASmQ3tEyyzr37Um2UaifnAcmyIoPQUvcsK3vH1EyuBTpCH+x1wz5+4qmUBILe9feTDikkoHrXyODu76AfZfs7tx/1zrcLhMm/UYiNmh9hJ/fC5r2qca9mzg5b7+1j4zv2e9uX9NfKKxAi3ftOt9fvSq5RDKqDUv4d95ICNtSOzWQbwrnjPn2473OfiIxBPvd/QMq9O9YKoaPrj8f5+mcXAk2k0IQLn+dYP8fn2BZ5M8fLpRcwNINKUH1s+1WqiQzP47X+ITKeAwVCH8TJ/D106zmEuB1Ri4J3Cbq/TRx8gGF/AdP5Sxj2l1GyQRT8mCh4BxleRMkaihAhbIRWQjeOYtivo5vPI/TBPRxdhIzniIJ3iYMPiaNrIOvJtrFAK2KYRzGsV9GtFxDaEPcPPIrI/xHd5v8TlI/pfAvT/UsY1iv37S30f0jo/QFx8B4Ahv1lTOe7mM5X71kyIAo/JOj8T8TB+witSKrw/0A3T+zh3PYRqkscXSLyf0wcnkPKRZRsAiFCZND0UTTjGIb1Crr17F3faV+bl1Xi4APC4CfI6DIyXgX8RCfHmMGwnsewv4mmD5GQGYO91UIq4vAzouA94uBTZDzX091J7BJ2ho7pfgfT+Q6G9fwOywWoeJXQ/wFx+CkyuolU66A27tMymj6DYb+Cbp5BN/anmHnfin0NY7rHxh5gbfF0mAH+NOK2+N7nofHP8WjRitqcrZ+nHjbxpY8mBGVrgEOZA5TtASIZcaV9nWVvhVbUBhQpPcXB9AzT6aQ99mrrOoveMvWwgULhag6TqQmOZpNU6GJ3mfnuAsv+KkopcmaGUWeEQ5kDXGheoh428GKfalBn2CkTyBAv9jmZO0akYiIVUQmq/HjtbUBhagaD9iBHM4dJGS660HF1F1d3McTdYoXVoMayt0In7jKTnqJkFfHjgMutK4CgZBUZc0f2fN2kXCfy3ybs/jZx+AlKBejGcUznu2jG9BZRiw4yXkHGc8ThFXTjKrFWJOj+DnF4DhXP9SI7HskApINwkNEt4ugShvkJpvtLaPrEfVGOe46MOJol8t8iCn6CjK4j5QpK1pKIw+a2bVQ8SxyeR/N/gul8Bd18Hk2/s/Om9/5RITJeJI6uoccLW+81ukYcXkLGswDE4Xl087n7F1QRMrrVO641dK3Ak4nE+MjoejI4B+8RRzdQch2l2tDzTEKYyHgeEV0lDj9ED57DsL+EYb1IEqXZVkkNkETB+4TeD4mDt5HxAlLWQHVIiIqdfBZdIwo+xnK+g5JVhEihNsyCdzz+GBnPE3rfIw7e713P1R6J6UVQ2E6AzwTh9oydTXaKXMbRLeLgPSL/B8Sb91IDlNfbh4EQ80jtOnF0Ht04gWG9iul8876o5F6xL0Qm+ZLsPY2lSoVI2UDKCoYxCTwZuflHgVhK6i2P60sV1uptDF3j2QMjFLMplFLUWh7za3UanaRo0zINBvNpBosZipnkxSNE4tRda3ksrNVpdnwiKTF0jULGZaSUZSCXxngAee+uH9Joe1SaHZodHy+IiOKYWKrERsDQcW2TfMZhIJcm7Vj7YnYolaLd9VlvdKg0u4k/ThQTx8l5OZZBxrUpZl3K+TS2mYhyaTvUWPWLKJZ0/YD1Rod6y6PtBYm+SyzRhEDXNWxTJ+vaFLIpChmHjPvTT7CjOKbR8Td9sDpeiB9GxFL2UgoCXUu8tGzTxLVNsimbXMoms2GSuCkSpghkcNfPbGeBrJllwC4Rqoiz9Qs0oya2ZuHoNoYwidXtWePl1jUWu0vYuoWt2ehCJ1YxUilCGXCzM8tsdw5dGKAUs50F2lGXYWeQm505GmGTWMU9wtFJnIplRMpwKZoFQhXRjjsEMjH4qwY1FrsrDNplTM3E1LZ/5UklaUYtzjUukjdzFK0CgQw417hI2R4gb+6tjk+pGFST2H+b0Pt9Qv8NAHTzGUzn57Dcv4zQijt60im5RhR+hIxXCLu/C4DQ8mjGcYQwUIQo2UTJVWQ81/tZAGFiur+CJia3TTMnJOaHhN3fJQo/BhUjtDy6cQB6iuyKCGQHKReJgvcR0SWUXMV0YwzrhV50AJIHNYtmTPUGsAoqXrv3bEh0XmZRsspGREHJZZRcBSXpFVj1lk4GYKWaCJFCM2Z6qa3HR2aU7CDjWQLv94i6f0wcXQFhoOkjaGI8SbmhoZTfO+clougKcXQTJZsIDHTr+d653n/cSkUJSfL+hND7Q2R0DbARehlNO5CQlY2UoqwTeX9EkrqJkyaaPiTRlFwiCt4i6PxbZHwLgYnQR9HNMwgtTaJf00XJCjK6iZLrJOTMQTefQTeOI7Ri7/u+XyNIqRAlq8m95P0Bkf8WkEQc9d53JtCT5VQDFa/2iNl1ZLyEEE4S6dMHtrxG/eCJtRBI2SIMz+EHb5NO/w0Mfe8znacVYRRzdWGNf/q993nz7A3SjsX/7W9/m+cOj+MHEe9fmud7717g0twqXT+kmE3x6skpvnL6EC8encA2daJYsVpr88GlOb737kWuLqzT9QPSjs3JmWG+8cJhXj0xzUBug/jsfAOonjWDH0TMrda5cGuFT64ucHl+jbVam5aXEJqUY1HIuIwN5Dg+NcSLxyY4ODpAMetiGYml/V6DPxv7bnsBV+fXeO/iHB9fXWR+rU612cUPQtKuxUAuzcxIkTOHxnj1xBQjpSxKqd5g+mDfhVKKMIqptz1mV2q8f2mO8zeXmVutU2l2aHUDDF0jZZuUcikOjg7w7MFRTs0Mc2hsANsy0bX76z+edkipCOOYWqvLhVsrfHptkctzayysNag0O3T9gFgqNCGwLYNC2mEgn2aklOPQWImjk0McGC0xkEthGjqaEKQMlxeKZxKdC2AtqPCPr/2PVIMaKJAqZrG7xERqjBeKZyhbAxiajn7HTGvFW8XWbF4beJlBewBTMzGEgVQx9bDBzc4soPjuyDcQwBurb7LoLbPsrxLKkLyZI29kGbCKeLFHzsyRNTJc79zE1iykUpjC4Evl1yhZRc42LvAfFv6QNX+dnJHF1DLbXDEoWgXG5Sg/WP0xrahNLGMCGbDsrTCTnmLUGd5mza0GKAmqTRxewO/8G6LgzURgUJ/Ecn8N0/0OmjbQx/e4gvLfAHSUCnrmlC+jm8cRWhYlm8TRFSL/J8ThRyhZQ0Y38dv/A7p5Gk0f5t5JYkJgIyL/jV4K6z3AQNPH0c0zGM6X0bSxnqBcGxnNEwZ/RhyeRcUrhN7v9bakJbPpnu6JENkkwhR8jJJVpFzl9kgrNmtxZLwAqp0cv+oi5TpKriW1Q8K6fTV7kQQlmwiRRjMOPNbovVIKKZcI/TcIWv8/lKojRA7NmMZwvo5uHEKIhIhKuUYcfJSkbcJzqHiW0Pse0O01XhTuizgkXmVtQu8PCL0/7ZEYHU0fxbBfx7C+gKZPJGQ1XiQOPyP0v0/k/XlCMHepbdkwmoyDTwm7v4eMLgIamnkC0/3LmM53esRER6kGcfARQe9eVbKB0ApY7q9jpX6zV5e09T6UahMF7ycpwPBDwEIzRjGsL6JbZ9C0YYSwk31s3qtnE/IT/ASlqrja/zHpct5mP7vhiREZpeoE4ad0ur9Lyv3L8DNEZO6FVIq1eofLc2tcX6zwL7//AbV2Fy9IvGqCsMmfvH+Z9XqbdtfnK2cOsrje4E8/vMLv/PAzam0PP0yW9cOY9y/OsVxtMbtS5z/7zsvour7rQB/GMWu1Nn/wzgXevzjHrZUaXT/EDyLCOEZKtRkx8fyQ9Xqby3OrvPHJNU7NDPPl0wd5/dTMrk7JW+47ilmttfndN8/y7oVZFtbqdPwwMfCUic5JYvoXslxt8tn1JX746TW+8fwRjk4OknIsdO3Ban9iqXj3wixvfHKNj68uUGt16foRYRQRxYndQRRLgiim1Q1YrrT46MoCI6Uszxwc4Ve++Azj5Tz2T1nbcLPr8/b5m3z/vctcmV+j1fXxw4igFwGTSm12J0VS4gURa40O1xYrvH9pFte2mBws8HMvHeHVk9OU8ylqQZ2frL9LJ06KNL3YpxbWiVRiLGhrNt8a/ho3O7f4weqPCWTAM/mTHErPULaTQfuL5Ve50b7Fj9fexos9juWOcDhzkAGzSCWoUQkqLHkrNMOk3q4VtcgaGQIZJiRFM3ANl27soRkaju6gCY1YJlEdW7MomHkMYSAQOJpNySrQiTt40ifL9kRGIHB1l4PpGTpxl5udWWIlGXaGyJu5uwjZvWveS2aUrBGHH+G3/jvi8AKgo5kzOJm/g2G+iBD5/r5I5aFUgNCGcTK/hWG/jqZP9wouk2JP3TyNaX8Vr/X/JfJ/jJLzKNkgDs+hGVPoxr0djB4yvEzk/4g4PAeYaMYR7NRfw7C/itAKd2iBSZR5BsP+AqH3BwSd30bGN4j8HyO0AXTzGJo+DhgILYOmT4GwkLKCilfvO5eEmDQQWhahj6BkExnPI2UVKRfQ9EluD0sRMl5AqSaaXkbXZ4DHGSUNeoP7v0KpFkIbxHS+gZX+W2haqRdpSCItGlFCMsMv4Xf+BXHwXhJNCz4h8t/EsF5B6PcQYdVFRfOE3p8ho5uAhaYP42T/Prr1IkIrJTVKKJR5At16Ed3+EkHrnxAF75PUtuwWkomIo6vE4UeAQjOmMZ1vYrm/jNDybESKhMihWy9h9gqwQ+/3UfEacXQJGV3doS4pQsbz+O3/vhetyqKbJ3Eyfzchnlq2d40SsVbdfAHT/jZ+558T+T9ARjeIw7NEwdsIPanzehA8QSLT7d28673q6Z9dKKVYqbVYrjY5f3OZxUqTtGNhmwbdnkN00Ozy6bUlbNPg+PQQb352kx9+cp3FSpOUbeJYJlEU44URza7P9cV1sq7NZzeWOTxeJruDa2/HC7g4u8qff3SF9y7OMbdao9W9nVu1TYO0Y2DoGn4Y4QcRbS+g7QVUW13aXZ96y6Pe9njl+CRj5Xzf7apeEHJrucbvvXWOd87PMrtSpRsk37cg0VpxXRNdS/bd9UKaHT9JpUWSWys1BvPpPRl9JtccVmstPrg8x59/dJWzN5ZYrrSIZVLHZRo6KcfCMnRiqZJ9+0napdHxqLe71FpdfD/i6y8c5tmDo6SdhzG3ezyIZULK/uT9S7zx8TU+u7FErdXdlGo3dG3zvGEj5RYSxjFhFNP1w96W2uiawNCTaEw7arPoLbPkLTOTnqJoFakGNbSq2Jz5aUJjIjWK/f9v701jLM3u877fOedd7l731r51Ve/L9Owrh+RwKJKitVCyrVgwDFsIHAFB/CEBjCABkg/5kA8B8iGBkRgxYCS24ERKbAuWRduSKEuUuAzJGZKzz/TeXV1L177d/b7ve87Jh/PWreruquqqZnM21TOYme67vMvdzvP+/8/z/FVAX9hHLa5zuXoVhewSmeHMEIEMqARl6kmD6eYs2mq+0PcSWZUhIzP0Bb1cLJ1HCIHFklNZBsI+phrTyPQfIYT7k5AIRDc1VVtN23QwqV4vsQkt3U4rPw92bGVVhsdK55hrLXAluk5WZTiRn6Q36N2jKifcD/WObRu9QtL5AXHr35HE7yNQeMFz+NlvuAVN9h/CVWQRsoznP44X/gLKO4m4p6okKGFlD374Zay+QxLN4RaYadfeuYfIWNMg7nzXLTy2iZAVguw38MIvpK2Aey4abHoM4atYU6PT+B2s3UDHH5J0fkSQ/QYIz1Vk1IRz2Jg21lYxZtM5koSHtS10ch1rGwhZQnknsabq9D5mA6OnkWoE8NJ2Rc21oGwbuhWZj06CYJJb6Pj9lGTEeMEL+JlfRHnnuNe55f5UBgKCbIt2Mu1aTWaJpPMa0juZVse2Yc0aSfwWRs8DLYQawwu/muqPJu/6jAjAiiJCljHB2xizjEmuPuAMrBOEm2WnewKkOoH0TiPvE4N7jsz459HxpfS2GKuXMHphTyJjzRI6eheTXANbQ/lPEWR/HRU8hRA997U1BWBlH0Hml9y2k5tgmyTxO0j/3EdHZJyNcIt4eAih0h+ywyXJGtt29qy/AtDacH12xS2Q9TbPnz9GfylPlCTMr1a5NrtMlGhWNxu8ff0Ob3w4zWvv32JxrcaFySHGB3rwlGS12uTa7DKbjTbtKOHO6iY/vjzDYKWwK5Gx6VX3zfk1vvvODf7oR5fZbLQwxpILfQbKBXpLOSqFLIVcSOApmjv0M8sbdTbrbRbX62w22mw0WvhKEvoeA+W9r2zdvt3/F9ZqvH5pmj9+/TKbjTZJqofpK+UYKBfoL+UpFTJ4Unb3vV5rMr9W4/L0EovrNZ46NUqz/WBR205s1lt8MLXAN3/wAR/cWqDeigh9xVClRF8pR6WYo5QPHUHUhkY7YqPeYrXaZHm9Tr3VYXppnbVqEyEF2cDniZMjbA3H+6SiHSVMLazx7Tev89b1OdpRgpSC4UqB/nKBciFLTz5D6CsEblJ3rRXRTIlrvdWh2uzQiRJKuQwnhnvJhj4d26ClWzR1M616lDBWk1FOQOsSjA3LnVUSk1DyCvjC493N96kl9S7RWIvWaekWRa9AIAOu1K6xGW0ihaIc9DCcGaKQ5OkPexFCYq0l62Wo+OUd+pa9X39jDR3dZrY1x1q8zkJ7ESkEPV6JUIY0kibLnRWW2susReu0dZvp5iz9QS9ZlSWQAZP5CWZacyy2l+jxS5wvnaXH33sGkWuHeFibpGX2N1y7oPMdEMpVTDK/jJ/5VWdhPaSoUaoRVPjiriRmGx5e8CRJ5zvdW6xecWLLHXDjTGqufaCXnKVXDeOFryK9iV3DS93n3UN5Z7DhF4la38SaFYyeIYl+iJ/5RYR1FmqlxhEig8VgTR2rFxFeiCMnDbf42iZCjiPVCaysI1IHlkmmwH8OxNaF7hJu6KNNSdI4PGTr4WGg48vo5Bqu8uGjgmecI2wfEipkGT/8IlHzdzHJtfTz8CZ+9hv3PdaYFdeKsU0ApBzCz3wdoYZ23YcQHkKUUcHTqOTaAYgMTpNim2y5kqQaRMryPsffi1TbLU83UXxzz8ebZA4dv5UKhwXSO4EffgUhivtEgEjX9vRO4ChIgomvYpKZB57PXngIItPBmHUQEilKqa/dKe238hEOAmPWsbb+4Ad+BhBrww8/uE1PPsMLF47xX/7GK1QKWaI44a3rc/zP/+9fuOGCiWZls8E/+6M3qLcjLp4Y5rd+8VmePj1GNgz4YGqB3/uzN/nRh7fZbLSpNTu8c+MOrz69e/idBSKt+fab1/jWj6+wVnNfGF9JJgbLfP3Fc7zyxElG+0t3tYy25gH98euXeePyNItrNdpRwrs35inlMoS+zy8+7+Lm91PjWwvv3VzgP/zwQ9ZrrW5VpZAN+eITJ/j68+e4MDlEPk2WNcayXmtxaXqR3//uO3xwa5G5lSpzK9U99rHLXtN9XJ5Z4ttvXef1D6cBkFLQXy7w1WfP8OWnT3NqtPcuMW+caNZqTV57/xZ/9MPLfDC1kC7yHf7yretI4PzkIP4B2ngfJ9aqTb71xhWmFtYciRGOhH35mdN89dkzPDY5RLBj3pK1jmKsV5tML21weXqRt6/fYWGtxrHBMqfH+/GUxDc4F4+J+MHqG+RUjkD6FL0CWeVcN23T4U8Xv81KZ82paARM5iYYyWy3jb+38gNuNW4jkVgs49lRJnLHUELS45c4XTzBe5sf8q9n/xCJwAIn8pP89bFfwVU/nEZL7PxHCGSaY7PlSPru8g+o6wYZGXKqcLIr9r3dnOE/Ln6b5c4q7bRSs9Be4pX+lzlZmCTv5cmqDFmVwZc+UkgKKr9rJk0XIsAt1E10fJmo8S9Iop+430h1nCD/9/DDV1Nh5eEh5Ihz9ey7iEtnue4SHesEnNx7AaBTXc1Vp/mQgyjvHFINPLDaIWQeqUZR/ml01MKaVUx8xbW/sAiZSXNeioDvyIieRahh58xKXx9XkSm7hcw2SWQeazad0JTE0VRTd/ZgGzuyJXt2qSL8fKH1TYyexlVfikg5gJA9D17nRA4hSo7Q2SYmuQWmgbX2rt/LLc2IpQMIJ6gNnwGye24aQHon0hbcQbCVE7MFxf6ZSYqdER7Wxrt8hnZs3SymFRztBNmyL7X+2/1fJxGmlbo81m5i9HzqBHPHetiLxUMTmSh+m1rtHyNEQD73d8hm/xqQsFH9n4jjDw+8HWvqaLN62N1/apFozYXJQV596hSVQhZPSaT0OTZY5tWnTvKdd24yv1pFa8NavcWp0T5eOH+MiyeGu/qM4UqRLz99mg+mFthstGlFMbcX1+lEu39g6q0OP3h/ivdvLbBWS8OngC8+eZKvPXeGp0+P0pPPdtsMWyhkA85PDDJYKTDSV+K7797g0u0lAN6/OU9/Kc+FiQGG+kr3PXcnbtxZ6Qprt0jM5FCFLzxxnG987rGUQO0onwoo5UMePzFMuZDl3//wQ/7sp1dZT4/9IDDWslFv8calaX58ebp7+7Nnxvnac2d4+eIk5UL2Pq2PUpJKIcsrT5ykJ5/l229e40/euAKkxG5uhdc/nOap06P05D+5DrtmO+LD24vU0wpWKZ/hy0+f4tWnTnF6rH9X95lIH3d6rJ/R/hIvXpik3YkJfYVKE3k94TGaHeZvH/sNgLStI4hNQm9QQeLaMr828kvEJuluOCNDit52FeGrg6/S1tuV2FAF5NX2Aj+WHaXs9/Bc+enubVmVIZA+z1eeRiLxpMex7BgWixIKgWA0M0yoQow1nCueBsCgkUgyKkvByyOFYiw7zK+P/gqxiTHWIIQgkAE9fomM2n5f27pDRoWczB/Hl/vrwoQIsbTQ8Vt06v8HOrmEkHmUf5Ew99uo4InUbfNwELKUtlwepBMLUz2CgwsiuzsSw9oG1iyCjXFtq0K6KB5Q+yZCpJpEyxugN1Or+DxKlkDkAR+pxlMS0sQkMyj/iXTfzZRAtbruKJu2jayZcy2c1OHmqjezQJxmj+wltP75weolrF4FNNZWadf/NzrN3z3Qc101pkV6OZmSFWdB7m7fNDHJgnsvRN6FI/LgLDAhKqm+5UEQCNGfEkunpzJ6NhVh7w6rFzF6oft3Kcv7itKt2cToOdxr1CRq/XuS+N0DHBtYPb+jmBGnlv+YA38Wd+DwGhkboc1aaqfaWmAMcXyJOH4/LSkdxCLXwdrmQ6uUP22wwPhgmQsTgwS+W0wkgp58hsdPjPDWtTnmV93j4kQzOVTh3PgAPfltdl7MhZw91k8udK9ZnBjWa03akbPT7hTEGmOpNdp8/71bTC+tEycaT0mODZZ58fwxnjs7vmd7yFOKUl5Rymf40lMnqbc6zC5vUm912Gi0uTa3zE+uzvKVZ8/sS2QuTy9x884qrZRohb7HmfF+vvLMGY6P9Dp79Q4IIfA9RU8+Qy4MeOH8MRbXa3z3nb0HLN6LONF8MLXI1dllVjabCAGD5QLPnBnj5YvHGesv7cr2pRDOBl8u8PTpUdZrTd66NsdqtUmsDfOrVV57/xYnR3s/0UQmSjRLG3Wi2JGJ0Pc4PtzLSF9x1/bjzrlQvqco5kKGKmCMues6zk0AzjKR2zu8UqEYzY7se3xDmcF978+pLDmV3VXT2RvsHTBW9Lc/y5WgvOfjsirLWHb3K95qXGOps8x8a5HVaI3eoJfx3Oi+lm0ArEZH75LwBkn0Nth2aqsuIv1z6Z8fPr5AiEy6yO03sy6d+nzX7+4urhbbdq2CVEMkyCBk5S6Nz77HgtcVobpaXoy161jbQVBIrcnjCFEC66zL2Ci1J69j9Txbuh+hRhE2Rogi1rYxZgFsHWt7sLaePjdBqgHkR2oGca+bSxRuprclmOQ6cP0htmfAJtxLZNxrtwkY9x4fsO0oZO6+7KG9H5tHqnFni0+m0Ml1dPQm2juJ9E4h8EEI1xY1qyTRj9DRW+mzM0h1DLlnaJ1JbdtV3GumsWYeHc0f6NjuhhuAjE2cxuqQODSRkWqQbObLgEWp0bvuU3II37+A8sZ4EJHReokkuYbewf4+y1DS6UIGK/eMjPc9jg2VyYZ3E7qxgR5G++9m3YGv6C3mCH0PKUXqYkpcLoi+m8h04oSljQbv31roVjRC3+O5s+M8dnzogRqXLZwd72fu9ChvX5/j6syyW9TXavzwg9u8eH6CYjZE3jNHx2lzLFdmlpld2e6v9haznBkf4ImTI90r/d0ghMs1OT8xyMJajdfeu4U2Dxb7WutcXT++PM3s8ibWWpQUnBkf4OJxpzU6CPpKeU6M9HF6rJ9aa45EGzbqLd68Nsuvf+EiI33moV1UP29YnCZrS6NkrKUdxSTa3Ffa3g/yE3p+P090TIflzgo3Gjfp8XsYzQzvS562YMw6JvqRc+lsZeZYN47A6NsImUeIMg+ff+Id8OLwALBJqk1MPyBCHW7bQrpFNF1wLSYN5nPnLVBI7xhClpwbSc+61oSpYfSyy4SRA06LIXsA68LWhHJX92YZJfuwW60lYqTqR8iP2tVqcGF3Me61cdkuW1KKw8JVRe6Fphuoh39g/Y8QPhxALO6+6wHSP48XvESkl50YvPMDhMjhhS+nVTQBto3Wt4jbf4qO3wNCpH8G5V/cp42VjrZgq8IaIGQPQvYe6DzuhZT7X+Tsh0MTGd87h1/8h7s+3fcfI5//LcLwpQeOI4+id2i2/oBmGvL0WYYQrpqSz4T3lfa3Whq+d/fr1Vt0gtR7IaUgG/oEStFOS/hRrIm1YWe3ZK2WCoPrTmALjsi8cH6C0b4DWj9xX4ax/h6eOzvO1MI6sY5YrzX5IHXDDJTzBPdcsVrr3DBTC2ssrW/roE6N9XN8uLIvidmJoUqRU6N9ZAKfVhRjDkBmOlHCu9fnWVirAY5APnFymLH+g58zQDmf4eyxAT68vUizHdOOEuZXa2zUW0Rxch/x/KTAV5JKMUu12SFKNI1WhzcuTfP06TGODZZRn2SBz8eMvqCP3t4Kz1eeAQTyQPPKbJrPYYEAoQbTTJQaOv6ATv2fkin8A2T4Ig9NRITg5zt65HCOwPsrPTvPy0OqYwjRg7XX0XourcisYPUsgKsQyP7uc4UaRsp+thJ0pRpLKzKzYD++1tL2OfpIb4Ig+7dQwdMPtSXlnWb35fawrz2413trjtODn+/5T4M16PgDdHILnXyIrl+Fxv/pqoVI13K09bTlKBFqiDD/n+KFLwIHJFhqED/8yq7C5oNAqmF4yJygQxMZV/ravfwlRBYpSwi2Bmbtt538gctjn3YIBLmMf5fIcgtSiLu0CO7xkM8E5MK7S2xbz3X6mu3HuzyUu3vhG/UWtxc3SLTuPqeUzzA+0LOvVfu+YxeC3lKO02P93RRha13FZ3ppncFK4b7qTpQkLK3Xabbjrt0ZYKSvxGC5cOCqgJKCfCZgfLCHmcUNmp39RXbNTszCWpVaq9M9byEEI30legqHawcFvke5kLunXWdYqzZptONPLJEpZEOeOTPOeq1Fsx3RiROuz63w/337LW7Or/Lc2XEmBiv43idbtPxxQKbtmb3zYvaCQvlP4GdeQQUvELf+hKTzA4y+jY7fJmr9B0DghS899LE9svdKBAiRY4t8uLTVBgcePmh1uuBtVWBkWqVIf6uEh1QTCFkC23E6E9tJiYmrvruKTX/3GKQaQagBbHITo29jzMVUy7MCJI7IfOQVGZkuqj5g0xbX8EPPeXKDMe9dExWILNgGEDmb+QFgbdzVOB1s5yHKv0Cm9N/Tqf8zl3FjN8EK52wT0g3klH0oNYHyL6KCF1D+kwjZt8/vtUxbU6HTt1jtbPUP/Ro9mDfshUeUIyMJg2dQchh1gLRKcOWxvyr6GARkA/++qgu4Hygl5V0fFpWOCdjt8eCqMjsfb4zhHh5Drdlx4uG0ipEJPPpLOYq5+6tCD0IxGzLSV7rreYk23FmtUm917icysWZxvUYnujsfqL8nT7lwcPIq0tTZkd4Si2v1BxOZdsT8apV2lHRbK9oY3rw6x/JG4z5Nzn6oNtrcnF+jvUNIbS3UWxHt6JObe9STz/CFx49zY27FhQx2YqrNDm9dm2Ot1mRmcYMTo32M9ZcY6S0yUC6QCfz72oNHODiE7MELnsXPfAPpncEtfoK4XXVZIdH3XXtJ9afVio/vd89Npe53OgQrwLYweinVcDwYlgirF9N2kkgdRf07HE8KqQZTMapICUkVYxadBoa0IrMj6l6qEaQcwNgr6GQKpRdSHU/HiYtlr9PxfGRwAYdCltP05M1Uy2LTC/DDazh230uIlBWMbmNN0+Xm2BjXStz7++iqJ809779vP0JhEVjjpnYLWULKk3j+k2mLUKQEt5gOvjyJ9M6mup391goBMo+QvVi96AiZ7aQFiv3mSz16PCIio8hmv4EQBaQc4CAlVGfVGsJTx/msD40UQOApvD10B3cFK6WCV6nkgT8Idsd/t9DsxKxWG91KTSbw6evJ73kM+yETeJQLWafNEQJjLcZYVjYaNNv3k4s40alI9u5soWIu7FqtDwpfKXpLuT1J3U60OjGL6/VuKw0c4fq333//UPvcCxZSvcnhMpM+ShRyIc+dHefDqQWa7Yib82s0OxGbjTbv3pjng1sLVIo5Lh4f5unTo1w8PsxgJU8hmyEb+l2yd1StOTiclfg0yj8PgB+8BLaDMYuuMpPcJOl8Byn7XNldDhwiEO8RH6vIdbNEtNnE2poLzrNtrDX7XhFbq7GmgU6m0uyTACl6kLK/+xsuhARRSgXOBSydNHNmNq3IiHSY5fYFr1SjCDmAtZGblqxn0wA3gZSVlFB89NV7KUfccZk1N4dIL2LNxiEni++3gxxSjWDMCtgm1m647csy+zl3XD7Q2oF3Y22M0beJGv8CHb+P9E4QZP9mOnogy27ZQQeFEBWkmkCn+iejl7B6OU0xfjSE7yB4ZNOvA/9J3JJ9sKt9pQbJ5X6NTPgKnre3E+KzgnurKHtBAJ6U/KwXyEmiaXfiLr3xlCSf8REPuWEpBZnApf9GiU5nJ7lU4nuhraXVubutBJDxvUNVRcC1l3KhfyBxbaINzU7czSL4eUAbeyCtzscFgXuv/+YrTzI+UOZbP77CDz+YohO790kby1q1yY8+vM2b12bJhwHnJgZ44fwxnj0zztljDy+4O8IWJF7wPIgAoxcwyQ10fJWO+V0XPhe8hFAf1+ssECKP9C9iTBVrVtDxFXdFLftB7DMt2zaw+k5qLa4j1AjSPwsy5N6LVyH7nV5IT2PMGiaZdQF3eAg1dpcgVKqRNCMmxugZZ7u2bUAh1MQBrcaPHi6Nd6KrgdLx++j4PFL9wiPZvhA9SO8MIr6KpYk1GyTxWy79WZT3fJ7Wt1PL88FgzTI6fp8kfgNs5Nqg2b/u2lo/o4BcqiGUdwEdvYNLk54h7nyfIPurD+U+elg8wunXh7vSFsJHUkZ6Ljzpsw1xYCLzqKCNoZPo7qKupKv0PMwRbAWOBZ7qtiCcQyi5j6wAXTfVvWMFPE8delq3lM4WfZCRCNoY2lF8136FcC6k/WziB4XvKQrZ4KEmjn9U2PqMlfIhT58Zo1LM8dy5Y7x38w5XppeZXd50n43U7dZoRXQSJ2T+6dU5To328dKFCU6O9FEu/tXQsD1yCAHkUN45MvnfptP8PXT0NkbfIWr+HgB++KWPZ4EWAiGL+OHXMMkMOp1UHbX+LYGQThi6y3BDSEiit4naf9TNR1HqOF74CoLMfSU8qfqRahiTTLmKj7kDNkF440hZ4q7ffFFwlm5R6rqVQKd6m2POyv0xQPmPo/xLJNFrYFsk8ZuITiVtvxx/YICgNVUX/ieKIDL3VeGkGsALXiDu/AXodYyeJ259E6XGsbu0sLaSo3X0kx1jBB4M17ZaT3NajAsy1DdR4ixW5BAHLD7sBqkm8MIXidt/klbepojb/8EFJwZPOEfafsdmO66CIwu4IMFP2dBId2XgfbyH8BnHx9od2KVo8bDHc9C5TgD3FmM8Jfn848cPJTLeC1tW7sOIpT8uKCkZLBfoLeY4NdbHxGCZU6OL3LizyuJajaWNOhs1F6q4XmuxXmsxtbjG5eklljcavHj+GI+nbq9HTcAT0ybSGzSSeYr+JBnv4eyablsttO0QyFK3LWKtJjJ1WskiUniUgpOP6tAPDCEUyDJe+GWMXnT21vh9kugnCDmEEAW88As4PcRHTIxFFi94jiR+E6MXsGaRpPNdhMhhTR3pjaUCXgWpGFjr2yTtvyDp/AhIEGoUFTyJ8p/Z9cp722mkXfy8mUcIP50Ynb/rnIXwXStKDWGT6xg95W7HQ6ljhyR8BmcF35oOnd5mNVjzwPbZTkg1ghc8hY5fJIleT63LP0SIPMp/2omURTE9f0f2sC4fzc2OWgA0XvgLSDXIvWudkD1I/zGUdxpt6uk06NeJ22dQwTpSbb0PzuK+NcQxid7EmFUcGTxAmr4Iu2QK20Yn14ha/walToPI3qODkTgRcgYpexBqNE003r1SJ2QZ5T+GF75CEr2G1Qsk8dvI1jcx+k46VqOyw1puUhdbM52z5UZdeOGXnLPr00dkjvDzhKekm6cjnEVPG0sU6/sW+oPAWjcZO0p0t60ihCD0vV1bPlvhcvcSkEQbtD788Edt7IH0+VJunfP2bZ5U/NrLj/HkqdFPdCXl5wVPSXqLOb74xAlevDDBRt0F/f3kyiyXby9xZ7VKJ0qIkoQo1syvVvnD197n9uIa1VabX3v5IqGvHmmuTGxqrEdXmK3/GadKf+tnIjJtvUorWaQ38wQqrQobq2km88w3vocnsx8LkQEQwkOoXvzsrwAJWs+A2SDpfBsQaetiGGzwkYqShAgQ3rgbMmk2iNvfwuhpota/IonfxQ+/gFSjQJiG2s0Sd76DTm6AradE6GW84AuoPWQBUm5lv2h0cgmbhgQq72za0rjnmGQPypt0FZwkTeQW25k098JaR1icg2qLtDgyYfTyLpk+axh9B2E76QW0E/QiPARhuqDf0x4TPsp/kiD3tzH6Ttr2ukanfstls3iPubA4WQI0mC0CM++mRus7qdPpCZAV7k13FiKLUlvvQw0dv4fVi3Qa/xwv/gAVvJC+Dxqrl0niyySdvwThO3JnfVdpeQCk7Ed6p5ByMG11XiFKruzxaOekkrIX6Z3By7yCF3wOJU4D/n0kUAgPKUcI878Ftk5samCqRK1/iYzeQPkXUN559xoJ6YieqWH0PCa5hdFTWLNCVvY57RQPV307IjKfUfjKTXfe+m7Giabejg49RXoLxlhaUdIV0krhLOW7OaBc1o133+LXid2CmT9EG9IYS7Md3Wcv3w1usrN/F4GyWBqpDdlTf0VccnvAT0nN5y+e4JnT46xWG9y8s8YPP5ji3Zt3uLNS7RLGy9NLCATHBipcmBw8lNvso0Q1usFS6yeUw3OoT6gLUqoRvPDLhLZF1Px/sKaKjt6gU/8nhIX/AqUm+Djqp17wgqsmiICk9S13XPHb6TBCPz0mg0ugbaSZLn344S8Q5P5Wd/TAbtiuyNhUuGsRYgzln0vt3/c8XpSRahKEh7W19La+vVtLtoXRc0TN30UnV9PE4rYbBWAjlzibbsck00TN3yNufdNd8YsAQRYhQ6R3Bj/4PF7ml/Y4j1684CUyxf+aqPkv0dEbzkoeXyVOptNqjMKRKIOrnsQ7wvRgO/Nltx1k8bO/noqtm5jksiOXne+RRD+lG3xnk3R7hiD7G+4963wXHdX2fA92vFgIWcQLv0jc/nOs2S95V6fktYMxq+jkAxL/x4SF38bzzsNuwX4ig/LOE+T/PkINELe+6ZxqehZjVkii19nWzqavk42xbL1OW6/Rw18sHRGZzyhy2YD+cgE5vQwY2lHCykYDrQ+YF7ED7SjpBsFtESEpBf09eXKZ+8vKvqfoK+Xw76mA1JptGu1o16C/vRBrzUa9RXyA484EHv09hbsqL9bC8kaDWrNDPvPJXOg+Kggh8JSimHOjCHoKGfpKeUb6ijx+YpifXp3le+/eJNHu8zKzvMFfvn2DoUrh0ESmEc+z0PwBOW8QbSM6ep3EthnNvYISIVhDYpostn7EZnQNYyM8mac/8zSB6qEeTxObOgV/koI/hrWGtc57aBvhyyIZ1cd65zLzje+x3rnCreq/RYkMxeA4faFbYLXt0IwWuFn9A7RpIUVAwR9nMPsCK+13aOs1tGnR1qvk/VG07ZCYJv2ZZ8n7o/gPOeTx/tc9QHmTkPkaRs+RRD/C6EWSzmtINQGZr6H8s49kX4c6LllA+Y8RigClTqLj9zD6RtpuWgOMq1iIHMo7g/ROobyLeMGzSP/svkMwhSymupd8qqkxqWPq7K5ERsoy0pt0U8QxuJTYStqW2E2LolPidclVMkjYHgNw78Va5AYSsgrdMDkfhEJZjfL2rtgJ4YGs4AUvIESGJHgGHX+I1TPdpOJu/osInJBaDSDlMEIdQwWPu6rKHs5cgXTkMPM1hKyQRG9gkmvuPbCbYBIQOaQcRHon8YJn8cIvYMwSJr6KFpf3PHZrDTb9vMWd72OSq+lsreeRaih9H3b+RrukXmPrWL2MTq5h9RKaN4mbBWThH6BkgfuE3UKCyKL8CwgRINVJdPw+Rt/G6gU3msK2cUTPpVQL2dze8KYAAB/QSURBVOPOSY0iveOo4MmHHqoKnxEiMz5U5vnHJhgfqtBqRigl8H1nYb4XZycHGKjsFhf92UIpl2GsvwdPSTqxIyOrtQbVZpuBuNCd93QQ1Jod5laqxDscSp6SjPaVKGbv/4KGvmKoUuwOu9zCymaT9VqL8YHygfZr7XYmzdbsoP2QzwSM9pcIfa+beWms5fbiOucnBhnu/ey/74dB6HsMVgoMVgpMDvdSLmSZW9lkemmDViem3urwkysz/NKL5w697Y5eY775PSrhBTyRQxORmCbGRkgRYNBEpkpHbyAQxKZBM1kgVGV6xGkiXWW5/SYCRd4fxWJYab+DxKcSnsfIhNjUiUyN2NTo6A2UyJA1re4ylpgWHb1BR2+gTZOWXqGtV6iEF9iIrtBOVjA2oR7P0NarWIwjVCKPLwv7EhnpHcfLvApm3bl3HuC8FLLkrlqzv4aU/akORIGex8ZvuhaJrIAaRqohvOB5pCxh9YK73o/eBNkLctgJI/eA8s/iZ76OszmPIWyETW6AOrGrNkTKXoT/HFIdR8fn0cklTDKTLtDGtTFEAelNoLwLKP98mhvzoMDTAKkm8LO/gjUNrK2h1EmkGt+dmIgAJfvw/NNYQoTsc8e/55wqlRKxxxHy4b/XUk3uO/7AjVtRCNmPF34J6Z1Dex9i9A2sXsCY9R1EJp3oLHuR6hjKO4n0TqSEThJFCVEnIQgUXhqxYSy0WglKncQL+pHe8ZQEpBZ0G4MoINUIyj/vCJUsI3QRG34OhI+UAyjv+D3HbcC2SaIfELW+iY7eRsgyXvjFdOzAGEIUuLta5AZcGrPpBNftP8UkV7F6mbj9F/jZv54+b3eRs5RlhP8M0juLjh/DJNfT81h2ZNZuE2P3Go0g1XGkf9Z9Dn+GwaqHIjLuTd3++71tXXefm+ey9bitx7jbLNZs37F13/3jze8f5b29X8vdjNDypWdP8aVnT6G1Yeb2KpmMT7mSJ5P17zvereMwW1f46XF8lI6ijwLlQpbJoUq3OqGNod7sMLO0wWC5QF/PwdivtZbVaoPrc8vdtpIQbu7T+ECZUu7+D1/geQz3lciFAUqKbijfndUqSxv1A49qt9bS7MTMLG3cF663G3KZgLF+l1zseZI4cYnHV2eXeer0COcnB11n/DP2Xj8K9JVyXDwxzFefO8MffO99Wh03lmFmaZ1WJ8bah5NxSHwGs89RyTzWva2VLGOtITY1hrIvMpJ/hY5e5+2V/4VWskI5OEtveJHp+rccwbAaazX1eJZyeI5K5nEkHhOFr6NNi9g0OVv+LULlRKHauHJ1YltkvQHOlf8uAo8b1d9nM7pOM1lE24iM10eo+sj5wySmSagqhKrCevsy5fAMsPcAzCD7ywTZXz7UayFkDj/zNfzM1wDn2KD959jou9jOnyP8ZyH763jBc3jBc+4x7b/ERt/B1v8x+M8gMr8Kcu/qTZD7TYLcb7ofzOQqNvoBtP4dFP4Bu07ixAmThRpEqkF8Ho21GMALnsQL/ld3HsktsHWEKu9+DMQo4ZPL/g0IP4/wzuy7bSHzKHmebM//cOjj2vr96a5R99xnrUUgQECSGHSi8QMPKZWrIKhR4Gu7bFPsuiZaY0FaGrU2K0tVKv0FisUsgRRobVia3ySbC+iplMhkPo8XfH7XdXZr7bRWINUpwvxpwvzf3+MsY6xZJWr+Pkn0Zlr1+WsEub+L8k89+DUyTYQoEjX/Fdr8BGs2sPoO1mwi1N6EQwjpgvXClyF8+YH7eVQ4FJFpt2La7Yg40ihPks+H+IFHHGmSRFOvtzHaMjTSw+pyDWst+UKGXD5ECKjV2qwu1UBAqSdLJuOTJIbNjSalnizFUhZrLAvzG0ghKJQyFEtZWs2IVisi6iRYYwmzrp0RdRKstZR6cmSyAVFHE3cSMhkfi6W62SKJNNoYfF9R7nWLd3WzxdpKHQT0lHMUChmC8DNRnOqiUsxyeqyP/p48rSgmijXtWPPG5RlG+kqHIjJzK5u8eXWuWxUp57OcPzZIbym7q0ZGCKfRmRyqMLWw1p17dOPOClML62hjDzRvabXaYHpxnc1G+0AtMYFrLz1xcoTNRju1GluuTC9xbXaFJ0+O0Fd6NO2CzyIygcdwb/GRWNXBlc0LwRjhLmJeJXzy3ii+Krp4dCR+WrY2NsFXIeXwLBbDRucKUgRk1SBZNYg84M9WRvXiyQwCJ+5UIkSJDNp2wBqkDPBlnkQ3EFLiSWdFNSQcTF7+s8KH8GWE8LF7iS+DZ9z9so+H09F8Qkh7/AFWzyD20tXIAQhedJqJn6HCchAkiaFZ7xDHCcZY/EBRrrjfhVq1Tb3WIpsNUJ5kdnqVW9eXOHN+hJHxCmHo02p0sEAYukTsdiumXm/h+1663gXEsSaJNe12THWjSf9giUajw/JSlSD0EQjC2EcIQdSJyeYCVHrR2ah3iDoJOnHmjJ7eHK1mxPpqAwSUyznyD1izrKmSxO+k7qYYKXvxM1+/K1F5X3SF1luOMZtWVaJ9n/Zx4VCr99zMKmurdTIZn/XVBhMn+hkY6mFzo8HM1CpCQL6QYXOjQRJrWq0IKSXnL45R3Wyxud6gWm2xtLDJ+GQf2YzPwvwmPeUsmayP0ZZWq0Oz3mF5aROlFC9+/jTXryywtLCJMYZsNsAYS5xodOL+3j9YpH+wRC4fsjjv5gtpbbh9c5lcPqTZ6CCE4NkXT7C2Umdjo0F1wx3HxacnKBxyDs+nAYGn6O/J8+zZcZqdmPnVKlGc8Na1WU6N9jFYKTB0gBbb1dll3r0xz8zyBkkquB3qLfDShQkKu0y+hq2Kh+XcxADX51a6RGZ10w2yfP/WPI9NDhHsEY635ZK6NrfCT6/O3pXUux+2nFQvXZjk9uI6cytVrLVUmx3evDrHQE+eX3z+HNnQP5SDqTtdXEl8pXY9508CEq1JEoOx1jnKDunS2hqMGSWOsG7N5/pZ3F5KZJG7iXCFxJNZ5I44dilU2hK0KOHTl3mSWnSblfbb+DJPMZik4I93H2+tcNOXSQCDJb2STiFFgNoxv0Ug0wF5Jq3rqnT/EoGX5mlsCVwPMm29BXoZkg/B1NIj14jg8wg17iouySXQC2meiAdyBLzTCDWQagsqWNmPEPO4mTv3vkw96f1lsPcLO218GfQtMHUgAVkGNYnw0nk3NsLqO4j2n2Bt5BxDahz8xxAicO6e5CqYRaxNnN1XjYLnNDBWL0H0A5CDQARmwzmQgs+l59iG+C0wa2mLRYAaAu8cQlZceyS+jI1eA30H20rbON5ZhO+qdDaZguQWmBW3H/8CqKwTzSa3wG6CKCLS5GSbTIFeAJkHdQJsE5Ip0DO4/JkCqJH0HO+vQlU3mlz+YI58PqReb+N5imdfPMnS4iab603qtTaNeovjpwbZXG+ycGeDY5N9GG2pbjS5cXWBODH09uYJQ5+NjSZKCTqdhDD0OH9xnFq1xeL8BuurdUqVHOXePEZb2q2YzY0Gqys1CsUsI6NlFuY3MdZSLGVQSrrtpxXo5aUa4xO9KCXZ3GyyNL/Jk88eJ198QIaNbWP0fKpNsSDCvdt6u2/BtbW6M7jSrLiPKZH6QTjUUS3c2WBlucbkiX7mZtfI5UNy+ZCN9Qaz06tUevNksgHXLs8zPtlHs9Fhc73JyFiF1eUa66t16rU2d2bWyOYCMhmf2akVys9M4PuKONGsrzao19tM31ohjjXPv3yKuZlVpm+tEAQeg8M9rK3WaTY6eJ5icLiEtRalJPlCho2NJkHooRPD1Ut3OHthlI21Bq1WxIXHx1hZrrG2WqNWbTE3s8aZ8yO7amk+7RBCUMiGfPGJE8ytbLJea9KOEqYW1vnx5WlK+ZBnz4xTLmQJfHWXjTrRmmY7ZrXa5Dvv3OTNq7PUmm5UezEXcmq0n+fPHyMb7h9keGFiiEu3l/hgaoF25ALYrs4s8+03r5MJfEb7SuQz21cizmptaHUi5tdq/PjyDG9fP3iCJTih8RMnhnnv5hA376yxuO4qg5enlxACego5JgbL9JVy5DIBwY7hiRbQ2hAnmnac0O7EbtTDZgMhBSdSHYmUj6Zi8ajRjhKW1+tMLa7T35OnJ58hnw3IhUE3zPDetppNbfXVRpvrcyu8fX2OestddWUCjxPDveQy/s/gDt7viXu7OQQePcEZGvEstfg2vsxzrPB1ct7wzgehZIgUHhvRNUJZJlRlAllO7xYP2P9BjnEvWDDr2Oh7kMwAMQLpfvz9J1z4mFmCzmtO/4LBIkDcSjNmiodYVPaBWUtJwIYjKmiEtwBdItN2BCGZ2p5urKYQW4uansJ2vseWSNaiEPqWI0RizD23/UfgXXSPtw1HmvyLbIlD0bNdsoaNIH4Xkc04y61tgp5zxMMsQ3LTHZfspysTMFVscs0RQukGSJJm0GBWsfE7rjLgp1qt5Ao2fh8RPOOITzKFjV5H2EYqFLYIXXFkRvZxb85Nvdbm+pV5zpwfZX21QZK4i+6pG0uuip8YFubW6OsvdeMmcvkQ31esr9WZurlMEmvWV3P4vqJeb3Pi1CC1aovVTsLoeC/VzSaL8xusrdYZGC6hlAIBcayZnV7DWsvImGtfbaw3yOZ84ighCH2mp1bwfUUuH3L75jJSQJjxqaZr1vmL4weIQ3Bus21CbrC2ibD6gR9318ptopPrWJMKpEWYCq8PbtT4KHFIjQwUi1nOPjZGox7heZJG3VU7xid6GRnrJQg93nz9Js997hRSCKanVlhfaxBmfBAwP7dOpa/gSFAu4PT5YU6fG3YkZM0RovW1ButrDfxULJrNBpQreYqlLCfPDqFuLLGx3iRfCDl1ZohGvUOSuPaRaxP53eM9cXqIWrXFndk12u2IYinLylKVpflN+gdLFIoZlPpkXmH/rMhnAl6+OMmVmSXurGwyteAyB77//pTTq6zX+dJTJxnt6yEbbn8xGq2Iq7Mr/PHrl3j90nS3ogJw7tggL5x3ya8Pwumxfh6bHOKnV2aYWljvCm836i3aUcLXXzjLhYmhu+YvNdoRN++s8m+++y5vXptjca3Wne90EEghKBezvPz4cTbqLf7N994DoN5ygxOvzq7wlWdO89KFCc5PDDJUKeJ5EkGqI2p1WNqoM7u8yc07q1yZXuLK7DKjfSX+q994hXwmOPTQzY8KtWaHH1+d4R/96+9x7tgAj58c5uLxYc6MDzBUKTg7/j3PMdayWm3y+oe3+cu3b/DDD2537yvlMrx4YeJjs157IosnC6h0wfdlsftnB0HOGyaj+vlg9Z/gyyKj+Vc5Xvy1j+DoLOhpaPwOovjfQfA8iAICC8h00b4G+joi+3dc28SsQv0fudvlINwj0HwoBC9C8AJuErHBNv6pW+S7C1gMsgfy/5kTU7b/BNv+YwjmAAXxNYRZheI/BDkE8ZvQ+B3wzoHY0YYVyu3Lfyr9DG2JH3sg85+AcN8hzBJ2878BfQf8F0Edh9wxsFV33oV/uLXB7W37TyJkARtVHOnZPjkInoboDWxyDZFWB6yeBbMI/kvpdq86wlX8bxEiC51vO/KV3AI/BLFbuqzg1NlhKr15lhY3aTU7LC5s0qh3yOdDypU8QejhRx6lnhzDoxWyuQApJUHgkc2FrK3UaNQ75PIh4xN99PYXuxf7AOXePL39BU6dHcHzJBtrddqtmA/fneHZl04yOl7B9xXFYoYg2CZb1liGhsuMT/ZR3WwxNFphdbnK8kKVweEe8sXwgWuWEIGbXI3nEnZsjST6KX6mhOABZMR2MPoOSfs/uuwgApcro0Y/tpTlB+FQREYIWF2u8fr3rjEztcLFp8YplrK02xFCCoSEbC7gxJkhblxZoFZtYYzl1Nlh2q2IuJPQanYoV/KpwHZ7BpEQgjjWrK/WiaLk/nk5W/lF3P3v1veh045ZWthk5vYKC/Mb5PJh95h3inmtNURRQqsVUe4tuDaGsZ9ZMiOF4NWnTmEt/OH332dpww1VnFve5A9f+4Dvv3eLSiFLIRcSeIpWOi15rdpkebPOZt0p8kNfMTncy9efP8sL58bvu7K/H0749sTJEb7x+cf4v//0p1QbHbQxNNoR33vvJpenl+jryVHOZ1FK0I4Sqs0OG7Umi+t14kRzaryflx+b5D/+5OpdhGrPvabHdXq0j688e5p6O+KNS7fZqDudTb3V4bX3bvH+rXnymZDQV3jK5UAk2qCNcXqiKKHZiWi03b9DleKBVBMb9RZr1SZrtSZRrInihE6S/j9OqDY6XJlZuus567UWf/bTq1yeXqSUyxD4HqGvCHxF4HtkfI9CNmS4t0gp/6CSsiXRmpvzqyxt1Hnj0gz5TEAu9MllAvJpFcpi6cSaRiui2myzXnPHvIVyIcv5yUG+8uzph9IVFf1JHuv9z8l6Q6n2ZRuB6qEvfIKCN0rWGwLAV0VO9/xtPJlPU3rd++hs04q+zJP48u50ZoGgJzjL2XKFxLSQeISqghQeBf8YE8VfdlWSFEO5l+jLPEmoesl4/UjhoURIMTgJ2G6bqehPkPEeQNRNy/0rciArzsq6w8ljbR3MhmvTyCJCSKwI3N+tE2IKjh/6dd2xAyCB6A1IrjjrsVAQvZVafbfMDAU3oFH4COFhZdG5n8xa2pKLwRtPHTcSKwqgxly1yez4vqljsNUO6x6DBrMOnT9LW1sWbAv0Yuq22Wr1CVeNgl3dTkKQNgXvsfVutTPUKFZrbPT21o4R6rg7T73mclGSD6H+vwOeI4ypa8davWsBQrBzXXCPmDwxwMLcOpsbTTxfkckFRFFCs9HmnZ9McfbiKEI4R2yukGF1uYqUgokT/Xz47iybG02CwOPcY6Osr9Vpt6J0fdv+XcoXQl792mNUqy2uX5ln4ng/01MrLC1W0Vpz7rExJ+q9a82yRJ2Ediui0pfHmgevWUL24PnPEKlB0PMYPU+n+TsYPevs82oydb95uGpNKw0NnHaW9ugtdHwZbBupJglyv4lUAx9lbuOhcOiGlx8oevsdWx0aKVMsZTDWUChmKPXkyGYDTp8bZmO9QaGYwQ8UPeUsi/MbCCm48MQxOp0YpSS5fIbeviJ+agXOZgMmTw5grGVopIznK6QQHDveT6WvQBj69PYXQAjarYgg8OjtL1AsuSvGTNbnzPkRhBAEocfoWIVCIYPnq27r6c7MOr6vOH9xjE47cULiRodC6ZMZ+PWzYOuLMDFY5guPH8cYw48+nGZqYY3NRpvm0gYzSxuEvucGQnqSKHJtlS2rtRDQ35PnzHg/X3ziBM+fG2egsrf9c3vf7v/DvUU+d2GSlc0mb1yaZnZ5g3aUsLReZ3m9ju8rsqGPkoIo1nTihDgxFHMhF48P8/y5Yzx5cpgfX545EJHZQimf4dzEIN8wllIu5N2bd5hZ2qTViVnaqLO0Ue8+VgqRXrXsTlUCT5FLJ0M/iMDNLG3w9vU7XJ5eJE4McapbiRNNrDWdNJNnJ2rNDj+9MsOVmSVC38f3nBbH9ySep8gGHkOVIl959sy+REZJlxMjpaTeitIWUa17joGv0u07V2GcuNlUcaK7JE0IGO0r8fTpMb745AmODZQfSiPjqwIVdX734xQByuu9K9FXiaCbwBvrGhudK9TjGWrxNIHsoRycw9slETZUPV230l37F/n77NNZb3tQY6D21oeFarcr+HsgZEpc7o3DJ/2zci0N26GbMIt1fxdbC8jDw6Jduya57qoR3gVc7kuYHs8WEixxWqGx6bHEbAe5yfQYt7QQGminxy+3T0vkXQLuzmOwDdC3IbnsiI4cdOTNNWm522FqdtzGjtv3gRDuOLyTCNuG6HWcKzYP/pn0GANcJkzGETARAMfc372xXdt3pZ4sF5861r3YDUIn0p040U+pJ0t1s4XnKSqVPNmsz4XHx11ryVMEJY/T50YIMx7FUgadGEaP9bK53iBfcFKL7nqUhnNuaeqKPVlOnB6k3JtnZdGFT2ZzAWcvOHlDuZLH8ySnzw9T6S2QzQWcOjvE6lKNIPQ4d3GU9taa1Ywo7KuTySDVKH74FbAROr6Eia8QW5PmyQy56pVwF3HWdsDUMGZxe3AnAumdxQ9fwc981em0Pini8XtwqG9TEHgMDffw5LPH04XKsc1M9m4x3/hEH6PjFWD7TWw2OoQZnxNnhrh+eYFsNqDSm6d/cLtUVezJ8vjTE10r29ZzT5292+c/MLT37I2d2+tulywD6e2tZkSuEDI+0c/1K/MkiSY6gLX304zA9zgzPsBguUApn+H1S7e5eWeNRjuiHTnSUmt2MNa5iZSU5DMBmdCnlAs5d2yQLzx+nFefOpXqJQ7+Yc6GPsdHevnNLz9JJvD4yZUZ5lerNNtuAU20odpoI9OwtmwYMFAOODHcy9eeO8NLFyYJfEV+l7bIg9BbzPG5i5P09eQYrBT48eUZ5lY2aXeSNGXYOdpsOgJh60fHUxLfU12C11vKcWZ8gFI+fKCAdm55kzcuTfODD6YOfJydOGFmeXPP+33POcAuHh/m/MTeU5MD36O/lOf8sQE2Gm1anTglhk78HiWOKG7xta3zzQQ+ga/IhD7FbMAzZ8b58tOneObMGJ6SH7ldPbEt6vEsy+238ESWcniGvD/6kR7DAyFCEEUQZSdiFQUnpMWC7HetEjnoNCLmDjYpgqm6CoY3iVD9WJu46oFZSof6tRB6wQ3ySzM+rFl3QlyzDraBMItY3ev2ZdawZsMRDP/J1PHztmtrbcHGYDZBz2JRriVjjTtG4bvgu+QS6DQ7Rs+Babjqiyxh9U7Sfa+3uOU0LKaG8EfAPw96KSVq6q7nCRE4YXZyA/BB9nQdMVavOY2NWQNTRehFrBxKRxN4CO+Eu6/5mjuE8MvbYmbZB2ocazZdi22LuAgFcgixy0DiSl+B5/rcxVihmGFw2B1HseTWCWu3q/1lkWdopIzRBiElUgp6Kq41c+K0qyZaaxmf6GXnmtXbV4C+uy/4ypV81x3V179NpO9dzy4+NdH984XHx/nun31IoZhhZLyX65fnieMHr1lCSCy5dDyGI7FGL2L1HeLkFo7sbhFswfaMJR8hMi4wT/bjZb6CH34F5V/Yd38fNw5FZEbGKhhjD1ReuvfH78z5Ee7MrnHpnRkKpSyDwz2uunKA5z5KnH98jLnpNa58MEexlGVwpNy1ZX+WoaSgUszxN774OC9dmODyzDJvXZ3l6swKSxt1aq0OUZyQywSUCxlG+kqcPzbIs2fHOTXaR28p99C23MBTjPX38Ftff56XLx7nx5eneefGHe4sV1lPE4NzmYD+njynRvt4+swYL54/xlClQOg762w+E+B5ameg0IEgheD02AATgxW+9uxZ3rlxh8vTS9ycX+XOSpVas0MrirHWpoQppK8nx0hvicmhCmePDXB6rI/h3hLZ4MEVmY8TW5qW02P9vH9rgSuzy9xeWGNpo85atUm9HRGlYyakdA6vfCZgsFLg2GCZc8cGeeb0GKP9JZfF8zGJ4DOqn5H8KwzlPucaDp9Ip4QANQG5v4eN/hLa/x63eGtE7rcdsfBOg3cO2/kLaP0B4IN3CuGddXZjW4XW72Pjd537CY3Vi4jMqxB83hGC9h9ho9dd9YUYq2cRwcuQ+YprU+Fh4x8j9BSIkhMY7wx4k0XQGhr/l3MQiTzCfzytXuRc+yW5Bc1/7oTBouzcQd6ZVFuyuM9LUHIuLFPHtv8Q0fkOVoROW3NXO1G4ik18CVv9H0H2IjK/BJm/BhjofAsb/SgVBDexZgGhZyD8qhP9irITH9MGK9LtpzZiUQb/MVcJqv9jui01Oeiyc2Q/h42+3+0r/iBDyM/zd+HCE+PMzaxy7dIdCqUsQ6NlypWDiG4FUk0Q5P4uXvh5ks530PF76GQKo5dSl5zFhdRlEaIHoUZQ3imU/7gL31PD7JbE/EmDsHvV03dBqxlhre2W5A6DKHIlsWajQxB6FAoZJwD+iOF6nh3azQg/9CgUM4QPcN8cFtoYNuttphbXWN1sIoRbZCaHKgzdky5rrNNmvH9zntWq0yh4SvLY5NB9jwXnKHrv5gIb9VbXlvz4iWH6evIHJhrtKKaW6mCqzQ7tKE71IRZPSQLPtXtK+Qy9xSyFbPhIBK7GWuott9/1WotmJyKK3SBKT0nCwGlBeotZ+kp5gu7QS3j7+hyrm02X3+ArLkwO0X/ALBzYduds1FpsNtvUmx1anZhYa7S2WCxSynTYpkc29MlnAkq5kGIuPFBbCWB+tcr8arX7Xj4KbM21OjXWz2B5/7aesRatDRv1FtVmh0ar4xxjSdK1Zm/14FV6vpnAI5cJKOUy9BZzZMLdh4Ee4W5YG6XVjrlt6zHWkRXZ6zQoet5VYWwnbUeVQQ0hRNHNmkmmnL3YdtxzRR7koHPu4GH1dKr52LLRZl01RQ0DEpvMuNaS8OhOQxY5hHcebN25Tsy6OzYbudaLrIAad5oZU3VVGrPh2k4iTC3cIwiRcfcnV0EdQ8gyd0Xt23RoY3KNrfh5NzG7lVadhtmq4li96IiKrbttqFGEcoGDNpnacY46Pcc+hBxxFRYhXUUmvu72640j1HaFzpqaq0KZle0LHZEB/6yrVn3U08UfMaJOuma1nZyiUMweKvfM2nR+Ulr5c0nLbbZbkCJNTw5cu0kWkKKMUH3AdnzBJxmHIjJHOMIRjnCEIxzhCJ8kfPKp1hGOcIQjHOEIRzjCHjgiMkc4whGOcIQjHOFTiyMic4QjHOEIRzjCET61OCIyRzjCEY5whCMc4VOLIyJzhCMc4QhHOMIRPrU4IjJHOMIRjnCEIxzhU4sjInOEIxzhCEc4whE+tTgiMkc4whGOcIQjHOFTiyMic4QjHOEIRzjCET61OCIyRzjCEY5whCMc4VOLIyJzhCMc4QhHOMIRPrX4/wEgwtDeJSD0GgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n"]}]},{"cell_type":"code","source":["anomalous_titles_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aEcw0xenkzf","executionInfo":{"status":"ok","timestamp":1708915436014,"user_tz":300,"elapsed":205,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}},"outputId":"e34518c6-87b0-4434-fc7b-5673ed22f25c"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data science': ['The evolving of Data Science and the Saudi Arabia case. How much have we changed in 13 years?',\n","  'A Comparative Study of Portfolio Optimization Methods for the Indian Stock Market',\n","  'AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness',\n","  'In-class Data Analysis Replications: Teaching Students while Testing Science',\n","  'ALPCAH: Sample-wise Heteroscedastic PCA with Tail Singular Value Regularization',\n","  'Bayesian Renormalization',\n","  'Understanding cirrus clouds using explainable machine learning',\n","  'Model-Driven Quantum Federated Learning (QFL)',\n","  'Infinite Action Contextual Bandits with Reusable Data Exhaust',\n","  'Dialectograms: Machine Learning Differences between Discursive Communities',\n","  'Design and analysis of tweet-based election models for the 2021 Mexican legislative election',\n","  'AMD-DBSCAN: An Adaptive Multi-density DBSCAN for datasets of extremely variable density',\n","  'CBLab: Supporting the Training of Large-scale Traffic Control Policies with Scalable Traffic Simulation',\n","  'Factorized Fusion Shrinkage for Dynamic Relational Data',\n","  'Machine Unlearning Method Based On Projection Residual',\n","  'Information Theoretic Measures of Causal Influences during Transient Neural Events',\n","  'Data Science Approach to predict the winning Fantasy Cricket Team Dream 11 Fantasy Sports',\n","  'Towards Learning in Grey Spatiotemporal Systems: A Prophet to Non-consecutive Spatiotemporal Dynamics',\n","  'Data Science and Machine Learning in Education',\n","  'Stock Performance Evaluation for Portfolio Design from Different Sectors of the Indian Stock Market',\n","  'A Deep Learning Framework to Reconstruct Face under Mask',\n","  'Towards Revenue Maximization with Popular and Profitable Products',\n","  'Geometry of the Minimum Volume Confidence Sets',\n","  'A Review of Topological Data Analysis for Cybersecurity',\n","  'Tensor Moments of Gaussian Mixture Models: Theory and Applications'],\n"," 'artificial intelligence': ['Describing Images $\\\\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes',\n","  \"Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario\",\n","  'BrainSLAM: SLAM on Neural Population Activity Data',\n","  'Genetic-based Constraint Programming for Resource Constrained Job Scheduling',\n","  'Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability',\n","  'Deterministic Computing Power Networking: Architecture, Technologies and Prospects',\n","  'Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking',\n","  'Arrows of Time for Large Language Models',\n","  'Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment',\n","  'Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control',\n","  'AutoIE: An Automated Framework for Information Extraction from Scientific Literature',\n","  'Textual Entailment for Effective Triple Validation in Object Prediction',\n","  'A technical note for the 91-clauses SAT resolution with Indirect QAOA based approach',\n","  'EEG for fatigue monitoring'],\n"," 'reinforcement learning': ['A Hybrid Strategy for Chat Transcript Summarization',\n","  'Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning',\n","  'Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management',\n","  'A Centralized Reinforcement Learning Framework for Adaptive Clustering with Low Control Overhead in IoT Networks',\n","  'Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning',\n","  'Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading',\n","  'Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things',\n","  'Learning safety critics via a non-contractive binary bellman operator',\n","  'Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization',\n","  'Emergent Dominance Hierarchies in Reinforcement Learning Agents',\n","  'Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback',\n","  'EgoGen: An Egocentric Synthetic Data Generator',\n","  'Sum Throughput Maximization in Multi-BD Symbiotic Radio NOMA Network Assisted by Active-STAR-RIS',\n","  'Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models',\n","  'Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey',\n","  'Reinforcement Learning for Optimizing RAG for Domain Chatbots',\n","  'Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel',\n","  'Efficient Two-Phase Offline Deep Reinforcement Learning from Preference Feedback',\n","  'Adaptive Control Strategy for Quadruped Robots in Actuator Degradation Scenarios',\n","  'Hybrid quantum cycle generative adversarial network for small molecule generation',\n","  'Astrocyte Regulated Neuromorphic Central Pattern Generator Control of Legged Robotic Locomotion',\n","  'Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach',\n","  'RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications',\n","  'Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics'],\n"," 'deep learning': ['Multi-class Regret Detection in Hindi Devanagari Script',\n","  'Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models',\n","  'GPU Cluster Scheduling for Network-Sensitive Deep Learning',\n","  'Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep Learning Technology',\n","  'Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs',\n","  'Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward Accurate Reconstruction of the Surgical Zone',\n","  'Optimising network interactions through device agnostic models',\n","  'Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study',\n","  'A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models',\n","  'PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler',\n","  'Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?',\n","  'Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems',\n","  'Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts',\n","  'Real Time Human Detection by Unmanned Aerial Vehicles',\n","  'State Derivative Normalization for Continuous-Time Deep Neural Networks',\n","  'Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces',\n","  'Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning',\n","  'Train-Free Segmentation in MRI with Cubical Persistent Homology']}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["dfg"],"metadata":{"id":"U2nxF48PnvrF","executionInfo":{"status":"aborted","timestamp":1708913616085,"user_tz":300,"elapsed":7,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5ZRn8rDPp61n","executionInfo":{"status":"aborted","timestamp":1708912647089,"user_tz":300,"elapsed":42158,"user":{"displayName":"Karan 29","userId":"02986125489029553801"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMy0oGi1nD8oA0ZwOHdgotq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc02c7b472ac46b29b348f3020012c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e97872a76de4271b727f8a78593f4d5","IPY_MODEL_30d33f19e39442eb90d63a24e71f40de","IPY_MODEL_c5ee27338a3d47429b36acad1c385608"],"layout":"IPY_MODEL_1d181e0bbd1046dc910538407a02ea60"}},"5e97872a76de4271b727f8a78593f4d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de57e6276bbb44c5a4efbdb8a4e0fb08","placeholder":"​","style":"IPY_MODEL_f6d5170173864c43bcbc2d88d2bc36e5","value":"tokenizer_config.json: 100%"}},"30d33f19e39442eb90d63a24e71f40de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f05ffaa8a85a441f8985b6bfe1148add","max":87,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e69c198f6fa4a8594325e4b1b8e9173","value":87}},"c5ee27338a3d47429b36acad1c385608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_499cee988c7b4968a79a5c11e92e9658","placeholder":"​","style":"IPY_MODEL_38c99b2073b24006be124a7c7d34b3c6","value":" 87.0/87.0 [00:00&lt;00:00, 1.48kB/s]"}},"1d181e0bbd1046dc910538407a02ea60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de57e6276bbb44c5a4efbdb8a4e0fb08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d5170173864c43bcbc2d88d2bc36e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f05ffaa8a85a441f8985b6bfe1148add":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e69c198f6fa4a8594325e4b1b8e9173":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"499cee988c7b4968a79a5c11e92e9658":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38c99b2073b24006be124a7c7d34b3c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de073607f554462383fa5fa60b3f6d2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69792dd6b1be475cacab0ae3a20d66bc","IPY_MODEL_a485937a0a9f4b03813862b99e5b16ea","IPY_MODEL_9badc085c0bc46cebf812d3073555cf4"],"layout":"IPY_MODEL_9da84f6ffa2047efaeb4fa1bceaa3a14"}},"69792dd6b1be475cacab0ae3a20d66bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f3fe455d7aa4642a41eae4341149fc7","placeholder":"​","style":"IPY_MODEL_c32901aaf43840ec8908815edc673cef","value":"spiece.model: 100%"}},"a485937a0a9f4b03813862b99e5b16ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ca4977bbfc4432da4f2b225494bfe2f","max":1912529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f47afba8f55b4698bb137591163a2d1d","value":1912529}},"9badc085c0bc46cebf812d3073555cf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28002d4d00a34707a82163bbf2b8e43f","placeholder":"​","style":"IPY_MODEL_2ff46154a9164c98b1eb44e730645c65","value":" 1.91M/1.91M [00:00&lt;00:00, 10.6MB/s]"}},"9da84f6ffa2047efaeb4fa1bceaa3a14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f3fe455d7aa4642a41eae4341149fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c32901aaf43840ec8908815edc673cef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ca4977bbfc4432da4f2b225494bfe2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f47afba8f55b4698bb137591163a2d1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28002d4d00a34707a82163bbf2b8e43f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ff46154a9164c98b1eb44e730645c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b506c9ed77b5419ca9ddccc7afc2a3d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26d8546325964e1fb99817beb02b96df","IPY_MODEL_8d85a403f5b445e58287e79641d55464","IPY_MODEL_b7c7df7a3a5348b2abb5172d26cca15e"],"layout":"IPY_MODEL_719ee74f798e4d28b395086a967a2e8d"}},"26d8546325964e1fb99817beb02b96df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42979e0ebd464373be6ec9eda5500a03","placeholder":"​","style":"IPY_MODEL_041116e0d6684bfe87f1e7e42edc0876","value":"special_tokens_map.json: 100%"}},"8d85a403f5b445e58287e79641d55464":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e43f437e561b47bcaced0bd81751eb91","max":65,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e1be574536c4f6c9e2711ec465c1369","value":65}},"b7c7df7a3a5348b2abb5172d26cca15e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddb218caab8d48a386baf1a663ea53e6","placeholder":"​","style":"IPY_MODEL_dc6e6df88fd440f2a53bba68ec3ec11c","value":" 65.0/65.0 [00:00&lt;00:00, 962B/s]"}},"719ee74f798e4d28b395086a967a2e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42979e0ebd464373be6ec9eda5500a03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"041116e0d6684bfe87f1e7e42edc0876":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e43f437e561b47bcaced0bd81751eb91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e1be574536c4f6c9e2711ec465c1369":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddb218caab8d48a386baf1a663ea53e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc6e6df88fd440f2a53bba68ec3ec11c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c882052b7594b8583d39058e123e195":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26a3d6b97b854286b06091e315034081","IPY_MODEL_42331376d221485c9e4020e91ab9b1df","IPY_MODEL_dd165c4b5ef84d6f87665f909843fcfa"],"layout":"IPY_MODEL_10d65a12162c47ee91d048c96858de36"}},"26a3d6b97b854286b06091e315034081":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f43f90ed4e8c4e50b6b71e6df5befd9d","placeholder":"​","style":"IPY_MODEL_67c0a8dd3ce04084bdebcd885b7645d4","value":"tokenizer.json: 100%"}},"42331376d221485c9e4020e91ab9b1df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0301251ebf424dd4a4f0577d52fd4e17","max":3520083,"min":0,"orientation":"horizontal","style":"IPY_MODEL_004e84bce9fc47a0aaad7c415c7896c7","value":3520083}},"dd165c4b5ef84d6f87665f909843fcfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45d8bfd8d0f94e31bdb712b17f86b475","placeholder":"​","style":"IPY_MODEL_76bf02c48d5540a297772a6522d91bd7","value":" 3.52M/3.52M [00:00&lt;00:00, 30.8MB/s]"}},"10d65a12162c47ee91d048c96858de36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f43f90ed4e8c4e50b6b71e6df5befd9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67c0a8dd3ce04084bdebcd885b7645d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0301251ebf424dd4a4f0577d52fd4e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"004e84bce9fc47a0aaad7c415c7896c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45d8bfd8d0f94e31bdb712b17f86b475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76bf02c48d5540a297772a6522d91bd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a27b1a6cab1b44bc821f8b62bd4810c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc4fdd17d1394c5e9c141684041c4b29","IPY_MODEL_afcc82dfcd0848b093607ae49431ce36","IPY_MODEL_17042715442141ada04a2cb2012e8b38"],"layout":"IPY_MODEL_1093e0baec3449abb72994ba68c0f4c4"}},"fc4fdd17d1394c5e9c141684041c4b29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d551e5b3f59453c8cae72dda9fd7f34","placeholder":"​","style":"IPY_MODEL_f3e1c487d5554107be0c1a072a3941fa","value":"config.json: 100%"}},"afcc82dfcd0848b093607ae49431ce36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f980d001c25942f8922b05f928ca1329","max":1392,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a5bec70840e4c0db3dbef4ca6aa54e7","value":1392}},"17042715442141ada04a2cb2012e8b38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a990731e9c1f4aa4aef3b9abb0d4426f","placeholder":"​","style":"IPY_MODEL_ee55848a11554d8db0e7c03780caf147","value":" 1.39k/1.39k [00:00&lt;00:00, 14.2kB/s]"}},"1093e0baec3449abb72994ba68c0f4c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d551e5b3f59453c8cae72dda9fd7f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3e1c487d5554107be0c1a072a3941fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f980d001c25942f8922b05f928ca1329":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5bec70840e4c0db3dbef4ca6aa54e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a990731e9c1f4aa4aef3b9abb0d4426f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee55848a11554d8db0e7c03780caf147":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4b47e660ab248f5a8a70b17d242e893":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26690985ef894271a817cca6d9c46fc7","IPY_MODEL_461aba4120654f80ae1d35f2c741d32c","IPY_MODEL_1035bab296a94aea8171c188d2cae19b"],"layout":"IPY_MODEL_01c2b2a536f249c89af2bf9d9d198b5a"}},"26690985ef894271a817cca6d9c46fc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdea11be3d2342b2b6c22c4784a8edb6","placeholder":"​","style":"IPY_MODEL_8a2932375394426592dee1419b2752c0","value":"pytorch_model.bin: 100%"}},"461aba4120654f80ae1d35f2c741d32c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff3c82e4b85a4fc093a0e34e1c72ccf4","max":2275329241,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aeb4106473e24565a3fc802e3f169437","value":2275329241}},"1035bab296a94aea8171c188d2cae19b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44c16520df8d4ead9b6bac890f8d62c0","placeholder":"​","style":"IPY_MODEL_2b195b8ed7454fd9aa38da874b8771de","value":" 2.28G/2.28G [00:29&lt;00:00, 88.9MB/s]"}},"01c2b2a536f249c89af2bf9d9d198b5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdea11be3d2342b2b6c22c4784a8edb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a2932375394426592dee1419b2752c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff3c82e4b85a4fc093a0e34e1c72ccf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aeb4106473e24565a3fc802e3f169437":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44c16520df8d4ead9b6bac890f8d62c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b195b8ed7454fd9aa38da874b8771de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"475df9b03def4f1e8ffdf6bba7bb2d9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a4f303582e14233a09f8c71fff955cb","IPY_MODEL_f22ec414cf2a4ea5a3e50b0467768197","IPY_MODEL_689f4178d9234ac9a4d15807b5fe7c9c"],"layout":"IPY_MODEL_90542c38639f481f9de828db0618de23"}},"5a4f303582e14233a09f8c71fff955cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d8605f751aa4ab29f7521612a10323f","placeholder":"​","style":"IPY_MODEL_c047419fe8ca4bc5ab31117ade4974f0","value":"generation_config.json: 100%"}},"f22ec414cf2a4ea5a3e50b0467768197":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dce5022d1313470787d14b6d95745b48","max":259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7bffdb273afa462eb407e6d21507f2a4","value":259}},"689f4178d9234ac9a4d15807b5fe7c9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_146859c0fcbb4b5195edf8457014040f","placeholder":"​","style":"IPY_MODEL_3570267360e54aa0b1a5a517505e3999","value":" 259/259 [00:00&lt;00:00, 13.7kB/s]"}},"90542c38639f481f9de828db0618de23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d8605f751aa4ab29f7521612a10323f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c047419fe8ca4bc5ab31117ade4974f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dce5022d1313470787d14b6d95745b48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bffdb273afa462eb407e6d21507f2a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"146859c0fcbb4b5195edf8457014040f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3570267360e54aa0b1a5a517505e3999":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}